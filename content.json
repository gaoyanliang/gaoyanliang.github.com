{"meta":{"title":"yanliang‘s blog","subtitle":null,"description":null,"author":"yanliang","url":"https://gyl-coder.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-10-01T01:18:53.711Z","updated":"2019-10-01T01:18:53.711Z","comments":true,"path":"404.html","permalink":"https://gyl-coder.github.io/404.html","excerpt":"","text":"404 Not Found **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2019-10-01T01:32:00.322Z","updated":"2019-10-01T01:32:00.322Z","comments":true,"path":"projects/index.html","permalink":"https://gyl-coder.github.io/projects/index.html","excerpt":"","text":""},{"title":"留言板","date":"2019-10-04T11:42:10.000Z","updated":"2019-10-04T11:42:10.000Z","comments":true,"path":"comment/index.html","permalink":"https://gyl-coder.github.io/comment/index.html","excerpt":"","text":"欢迎前来灌水。。。"},{"title":"","date":"2019-10-01T01:16:39.406Z","updated":"2019-10-01T01:16:39.406Z","comments":true,"path":"mylist/index.html","permalink":"https://gyl-coder.github.io/mylist/index.html","excerpt":"","text":""},{"title":"关于","date":"2019-10-04T11:40:51.045Z","updated":"2019-10-04T11:40:51.045Z","comments":true,"path":"about/index.html","permalink":"https://gyl-coder.github.io/about/index.html","excerpt":"","text":"我是 yanliang喜欢自己胡乱捣鼓东西"},{"title":"所有分类","date":"2019-10-01T01:15:50.699Z","updated":"2019-10-01T01:15:50.699Z","comments":true,"path":"categories/index.html","permalink":"https://gyl-coder.github.io/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2019-10-09T14:50:37.310Z","updated":"2019-10-09T14:50:37.310Z","comments":true,"path":"friends/index.html","permalink":"https://gyl-coder.github.io/friends/index.html","excerpt":"","text":"欢迎和我交换友链~各位大佬想交换友链的话可以在下方留言，必须要提供名称、头像和链接哦~请先将本站添加到你滴友链中喔，谢谢~ 友链提交模板： 名称：【请填入名称，9字以内】 头像：【请输入地址，支持png、jpg、gif等常见格式】 地址：【请填入地址，不支持url.cn等短链形式，必须以https://协议标准开头】 描述：【请填入描述，14字以内】 名称：yanliang 头像：https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg 地址：https://gyl-coder.top 描述：yanliang java backgroundColor: ‘#eae3c9’ textColor: ‘#624739’"},{"title":"所有标签","date":"2019-10-01T01:16:11.969Z","updated":"2019-10-01T01:16:11.969Z","comments":true,"path":"tags/index.html","permalink":"https://gyl-coder.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka Rebalance机制分析","slug":"kafka-rebalance","date":"2019-10-10T14:44:25.158Z","updated":"2019-10-10T14:52:51.211Z","comments":true,"path":"2019/10/10/kafka-rebalance/","link":"","permalink":"https://gyl-coder.github.io/2019/10/10/kafka-rebalance/","excerpt":"","text":"什么是 RebalanceRebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 consumer 如何达成一致，来分配订阅 Topic 的每个分区。 例如：某 Group 下有 20 个 consumer 实例，它订阅了一个具有 100 个 partition 的 Topic 。正常情况下，kafka 会为每个 Consumer 平均的分配 5 个分区。这个分配的过程就是 Rebalance。 触发 Rebalance 的时机Rebalance 的触发条件有3个。 组成员个数发生变化。例如有新的 consumer 实例加入该消费组或者离开组。 订阅的 Topic 个数发生变化。 订阅 Topic 的分区数发生变化。 Rebalance 发生时，Group 下所有 consumer 实例都会协调在一起共同参与，kafka 能够保证尽量达到最公平的分配。但是 Rebalance 过程对 consumer group 会造成比较严重的影响。在 Rebalance 的过程中 consumer group 下的所有消费者实例都会停止工作，等待 Rebalance 过程完成。 Rebalance 过程分析Rebalance 过程分为两步：Join 和 Sync。 Join 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求加入消费组。一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。 Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。 Rebalance 场景分析新成员加入组 组成员“崩溃”组成员崩溃和组成员主动离开是两个不同的场景。因为在崩溃时成员并不会主动地告知coordinator此事，coordinator有可能需要一个完整的session.timeout周期(心跳周期)才能检测到这种崩溃，这必然会造成consumer的滞后。可以说离开组是主动地发起rebalance；而崩溃则是被动地发起rebalance。 组成员主动离开组 提交位移 如何避免不必要的rebalance要避免 Rebalance，还是要从 Rebalance 发生的时机入手。我们在前面说过，Rebalance 发生的时机有三个： 组成员数量发生变化 订阅主题数量发生变化 订阅主题的分区数发生变化 后两个我们大可以人为的避免，发生rebalance最常见的原因是消费组成员的变化。 消费者成员正常的添加和停掉导致rebalance，这种情况无法避免，但是时在某些情况下，Consumer 实例会被 Coordinator 错误地认为 “已停止” 从而被“踢出”Group。从而导致rebalance。 当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向 Coordinator 发送心跳请求，表明它还存活着。如果某个 Consumer 实例不能及时地发送这些心跳请求，Coordinator 就会认为该 Consumer 已经 “死” 了，从而将其从 Group 中移除，然后开启新一轮 Rebalance。这个时间可以通过Consumer 端的参数 session.timeout.ms进行配置。默认值是 10 秒。 除了这个参数，Consumer 还提供了一个控制发送心跳请求频率的参数，就是 heartbeat.interval.ms。这个值设置得越小，Consumer 实例发送心跳请求的频率就越高。频繁地发送心跳请求会额外消耗带宽资源，但好处是能够更加快速地知晓当前是否开启 Rebalance，因为，目前 Coordinator 通知各个 Consumer 实例开启 Rebalance 的方法，就是将 REBALANCE_NEEDED 标志封装进心跳请求的响应体中。 除了以上两个参数，Consumer 端还有一个参数，用于控制 Consumer 实际消费能力对 Rebalance 的影响，即 max.poll.interval.ms 参数。它限定了 Consumer 端应用程序两次调用 poll 方法的最大时间间隔。它的默认值是 5 分钟，表示你的 Consumer 程序如果在 5 分钟之内无法消费完 poll 方法返回的消息，那么 Consumer 会主动发起 “离开组” 的请求，Coordinator 也会开启新一轮 Rebalance。 通过上面的分析，我们可以看一下那些rebalance是可以避免的： 第一类非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被 “踢出”Group 而引发的。这种情况下我们可以设置 session.timeout.ms 和 heartbeat.interval.ms 的值，来尽量避免rebalance的出现。（以下的配置是在网上找到的最佳实践，暂时还没测试过） 设置 session.timeout.ms = 6s。 设置 heartbeat.interval.ms = 2s。 要保证 Consumer 实例在被判定为 “dead” 之前，能够发送至少 3 轮的心跳请求，即 session.timeout.ms &gt;= 3 * heartbeat.interval.ms。 将 session.timeout.ms 设置成 6s 主要是为了让 Coordinator 能够更快地定位已经挂掉的 Consumer，早日把它们踢出 Group。 第二类非必要 Rebalance 是 Consumer 消费时间过长导致的。此时，max.poll.interval.ms 参数值的设置显得尤为关键。如果要避免非预期的 Rebalance，你最好将该参数值设置得大一点，比你的下游最大处理时间稍长一点。 总之，要为业务处理逻辑留下充足的时间。这样，Consumer 就不会因为处理这些消息的时间太长而引发 Rebalance 。 相关概念coordinatorGroup Coordinator是一个服务，每个Broker在启动的时候都会启动一个该服务。Group Coordinator的作用是用来存储Group的相关Meta信息，并将对应Partition的Offset信息记录到Kafka内置Topic(__consumer_offsets)中。Kafka在0.9之前是基于Zookeeper来存储Partition的Offset信息(consumers/{group}/offsets/{topic}/{partition})，因为ZK并不适用于频繁的写操作，所以在0.9之后通过内置Topic的方式来记录对应Partition的Offset。 每个Group都会选择一个Coordinator来完成自己组内各Partition的Offset信息，选择的规则如下： 1，计算Group对应在__consumer_offsets上的Partition 2，根据对应的Partition寻找该Partition的leader所对应的Broker，该Broker上的Group Coordinator即就是该Group的Coordinator Partition计算规则： 1partition-Id(__consumer_offsets) = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount) 其中groupMetadataTopicPartitionCount对应offsets.topic.num.partitions参数值，默认值是50个分区。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg","url":"https://gyl-coder.github.io"}},{"title":"Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理","slug":"kafka-partition-alocation","date":"2019-10-09T14:06:38.599Z","updated":"2019-10-10T14:35:37.104Z","comments":true,"path":"2019/10/09/kafka-partition-alocation/","link":"","permalink":"https://gyl-coder.github.io/2019/10/09/kafka-partition-alocation/","excerpt":"在 kafka 中，分区分配是一个很重要的概念，它会影响Kafka整体的性能均衡。kafka 中一共有三处地方涉及此概念，分别是：生产者发送消息、消费者消费消息和创建主题。虽然这三处的对应操作都可以被称之为“分区分配”，但是其实质上所包含的内容却并不相同。","text":"在 kafka 中，分区分配是一个很重要的概念，它会影响Kafka整体的性能均衡。kafka 中一共有三处地方涉及此概念，分别是：生产者发送消息、消费者消费消息和创建主题。虽然这三处的对应操作都可以被称之为“分区分配”，但是其实质上所包含的内容却并不相同。 生产者的分区分配用户在使用 kafka 客户端发送消息时，调用 send 方法发送消息之后，消息就自然而然的发送到了 broker 中。 其实这一过程需要经过拦截器、序列化器、分区器等一系列作用之后才能被真正发往 broker。消息在发往 broker 之前需要确认它需要发送到的分区，如果 ProducerRecord 中指定了 partition 字段，那就不需要分区器的作用，因为 partition 就代表的是所要发往的分区号。如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。 Kafka中提供的默认分区器是DefaultPartitioner，它实现了Partitioner接口（用户可以实现这个接口来自定义分区器），其中的partition方法就是用来实现具体的分区分配逻辑： 12public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); 默认情况下，如果消息的key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往 topic 的各个可用分区。 注意：如果key不为null，那么计算得到的分区号会是所有分区中的任意一个；如果key为null并且有可用分区，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。 消费者的分区分配在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费。消费者的分区分配是指为消费组中的消费者分配所订阅主题中的分区。 如图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。 对于消费者的分区分配而言，Kafka自身提供了三种策略，分别为 RangeAssignor、RoundRobinAssignor 以及 StickyAssignor ，其中 RangeAssignor 为默认的分区分配策略。 RangeAssignorRangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。 假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。 为了更加通俗的讲解RangeAssignor策略，我们不妨再举一些示例。假设消费组内有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有4个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为： 12消费者C0：t0p0、t0p1、t1p0、t1p1消费者C1：t0p2、t0p3、t1p2、t1p3 这样分配的很均匀，那么此种分配策略能够一直保持这种良好的特性呢？我们再来看下另外一种情况。假设上面例子中2个主题都只有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 12消费者C0：t0p0、t0p1、t1p0、t1p1消费者C1：t0p2、t1p2 可以明显的看到这样的分配并不均匀，如果将类似的情形扩大，有可能会出现部分消费者过载的情况。对此我们再来看下另一种RoundRobinAssignor策略的分配效果如何。 RoundRobinAssignorRoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。RoundRobinAssignor策略对应的partition.assignment.strategy参数值为：org.apache.kafka.clients.consumer.RoundRobinAssignor。 如果同一个消费组内所有的消费者的订阅信息都是相同的，那么RoundRobinAssignor策略的分区分配会是均匀的。举例，假设消费组中有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 12消费者C0：t0p0、t0p2、t1p1消费者C1：t0p1、t1p0、t1p2 如果同一个消费组内的消费者所订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个topic，那么在分配分区的时候此消费者将分配不到这个topic的任何分区。 举例，假设消费组内有3个消费者C0、C1和C2，它们共订阅了3个主题：t0、t1、t2，这3个主题分别有1、2、3个分区，即整个消费组订阅了t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。具体而言，消费者C0订阅的是主题t0，消费者C1订阅的是主题t0和t1，消费者C2订阅的是主题t0、t1和t2，那么最终的分配结果为： 123消费者C0：t0p0消费者C1：t1p0消费者C2：t1p1、t2p0、t2p1、t2p2 可以看到RoundRobinAssignor策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区t1p1分配给消费者C1。 StickyAssignor我们再来看一下StickyAssignor策略，“sticky”这个单词可以翻译为“粘性的”，Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的： 分区的分配要尽可能的均匀； 分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。我们举例来看一下StickyAssignor策略的实际效果。 假设消费组内有3个消费者：C0、C1和C2，它们都订阅了4个主题：t0、t1、t2、t3，并且每个主题有2个分区，也就是说整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下： 123消费者C0：t0p0、t1p1、t3p0消费者C1：t0p1、t2p0、t3p1消费者C2：t1p0、t2p1 这样初看上去似乎与采用RoundRobinAssignor策略所分配的结果相同，但事实是否真的如此呢？再假设此时消费者C1脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor策略，那么此时的分配结果如下： 12消费者C0：t0p0、t1p0、t2p0、t3p0消费者C2：t0p1、t1p1、t2p1、t3p1 如分配结果所示，RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配。而如果此时使用的是StickyAssignor策略，那么分配结果为： 12消费者C0：t0p0、t1p1、t3p0、t2p0消费者C2：t1p0、t2p1、t0p1、t3p1 可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果，并将原来消费者C1的“负担”分配给了剩余的两个消费者C0和C2，最终C0和C2的分配还保持了均衡。 如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor策略如同其名称中的“sticky”一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。 到目前为止所分析的都是消费者的订阅信息都是相同的情况，我们来看一下订阅信息不同的情况下的处理。 举例，同样消费组内有3个消费者：C0、C1和C2，集群中有3个主题：t0、t1和t2，这3个主题分别有1、2、3个分区，也就是说集群中有t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。消费者C0订阅了主题t0，消费者C1订阅了主题t0和t1，消费者C2订阅了主题t0、t1和t2。 如果此时采用RoundRobinAssignor策略，那么最终的分配结果如下所示（和讲述RoundRobinAssignor策略时的一样，这样不妨赘述一下）： 1234【分配结果集1】消费者C0：t0p0消费者C1：t1p0消费者C2：t1p1、t2p0、t2p1、t2p2 如果此时采用的是StickyAssignor策略，那么最终的分配结果为： 1234【分配结果集2】消费者C0：t0p0消费者C1：t1p0、t1p1消费者C2：t2p0、t2p1、t2p2 可以看到这是一个最优解（消费者C0没有订阅主题t1和t2，所以不能分配主题t1和t2中的任何分区给它，对于消费者C1也可同理推断）。假如此时消费者C0脱离了消费组，那么RoundRobinAssignor策略的分配结果为： 12消费者C1：t0p0、t1p1消费者C2：t1p0、t2p0、t2p1、t2p2 可以看到RoundRobinAssignor策略保留了消费者C1和C2中原有的3个分区的分配：t2p0、t2p1和t2p2（针对结果集1）。而如果采用的是StickyAssignor策略，那么分配结果为： 12消费者C1：t1p0、t1p1、t0p0消费者C2：t2p0、t2p1、t2p2 可以看到StickyAssignor策略保留了消费者C1和C2中原有的5个分区的分配：t1p0、t1p1、t2p0、t2p1、t2p2。 从结果上看StickyAssignor策略比另外两者分配策略而言显得更加的优异，这个策略的代码实现也是异常复杂。 自定义分区分配策略kafka 处理支持默认提供的三种分区分配算法，还支持用户自定义分区分配算法，自定义的分配策略必须要实现org.apache.kafka.clients.consumer.internals.PartitionAssignor接口。PartitionAssignor接口的定义如下： 123456789101112131415Subscription subscription(Set&lt;String&gt; topics);String name();Map&lt;String, Assignment&gt; assign(Cluster metadata, Map&lt;String, Subscription&gt; subscriptions);void onAssignment(Assignment assignment);class Subscription &#123; private final List&lt;String&gt; topics; private final ByteBuffer userData;（省略若干方法……）&#125;class Assignment &#123; private final List&lt;TopicPartition&gt; partitions; private final ByteBuffer userData;（省略若干方法……）&#125; PartitionAssignor接口中定义了两个内部类：Subscription和Assignment。 Subscription类用来表示消费者的订阅信息，类中有两个属性：topics和userData，分别表示消费者所订阅topic列表和用户自定义信息。PartitionAssignor接口通过subscription()方法来设置消费者自身相关的Subscription信息，注意到此方法中只有一个参数topics，与Subscription类中的topics的相互呼应，但是并没有有关userData的参数体现。为了增强用户对分配结果的控制，可以在subscription()方法内部添加一些影响分配的用户自定义信息赋予userData，比如：权重、ip地址、host或者机架（rack）等等。 举例，在subscription()这个方法中提供机架信息，标识此消费者所部署的机架位置，在分区分配时可以根据分区的leader副本所在的机架位置来实施具体的分配，这样可以让消费者与所需拉取消息的broker节点处于同一机架。参考下图，消费者consumer1和broker1都部署在机架rack1上，消费者consumer2和broker2都部署在机架rack2上。如果分区的分配不是机架感知的，那么有可能与图（上部分）中的分配结果一样，consumer1消费broker2中的分区，而consumer2消费broker1中的分区；如果分区的分配是机架感知的，那么就会出现图（下部分）的分配结果，consumer1消费broker1中的分区，而consumer2消费broker2中的分区，这样相比于前一种情形而言，既可以减少消费延迟又可以减少跨机架带宽的占用。 再来说一下Assignment类，它是用来表示分配结果信息的，类中也有两个属性：partitions和userData，分别表示所分配到的分区集合和用户自定义的数据。可以通过PartitionAssignor接口中的onAssignment()方法是在每个消费者收到消费组leader分配结果时的回调函数，例如在StickyAssignor策略中就是通过这个方法保存当前的分配方案，以备在下次消费组再平衡（rebalance）时可以提供分配参考依据。 接口中的name()方法用来提供分配策略的名称，对于Kafka提供的3种分配策略而言，RangeAssignor对应的protocol_name为“range”，RoundRobinAssignor对应的protocol_name为“roundrobin”，StickyAssignor对应的protocol_name为“sticky”，所以自定义的分配策略中要注意命名的时候不要与已存在的分配策略发生冲突。这个命名用来标识分配策略的名称，在后面所描述的加入消费组以及选举消费组leader的时候会有涉及。 真正的分区分配方案的实现是在assign()方法中，方法中的参数metadata表示集群的元数据信息，而subscriptions表示消费组内各个消费者成员的订阅信息，最终方法返回各个消费者的分配信息。 Kafka中还提供了一个抽象类org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor，它可以简化PartitionAssignor接口的实现，对assign()方法进行了实现，其中会将Subscription中的userData信息去掉后，在进行分配。Kafka提供的3种分配策略都是继承自这个抽象类。如果开发人员在自定义分区分配策略时需要使用userData信息来控制分区分配的结果，那么就不能直接继承AbstractPartitionAssignor这个抽象类，而需要直接实现PartitionAssignor接口。 下面代码参考Kafka中的RangeAssignor策略来自定义一个随机的分配策略，这里笔者称之为RandomAssignor，具体代码实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package org.apache.kafka.clients.consumer;import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor;import org.apache.kafka.common.TopicPartition;import java.util.*;public class RandomAssignor extends AbstractPartitionAssignor &#123; @Override public String name() &#123; return \"random\"; &#125; @Override public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign( Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions); Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) &#123; assignment.put(memberId, new ArrayList&lt;&gt;()); &#125; // 针对每一个topic进行分区分配 for (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123; String topic = topicEntry.getKey(); List&lt;String&gt; consumersForTopic = topicEntry.getValue(); int consumerSize = consumersForTopic.size(); Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic == null) &#123; continue; &#125; // 当前topic下的所有分区 List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic); // 将每个分区随机分配给一个消费者 for (TopicPartition partition : partitions) &#123; int rand = new Random().nextInt(consumerSize); String randomConsumer = consumersForTopic.get(rand); assignment.get(randomConsumer).add(partition); &#125; &#125; return assignment; &#125; // 获取每个topic所对应的消费者列表，即：[topic, List[consumer]] private Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) &#123; Map&lt;String, List&lt;String&gt;&gt; res = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) &#123; String consumerId = subscriptionEntry.getKey(); for (String topic : subscriptionEntry.getValue().topics()) put(res, topic, consumerId); &#125; return res; &#125;&#125; 在使用时，消费者客户端需要添加相应的Properties参数，示例如下： 12properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RandomAssignor.class.getName()); 分配的实施我们了解了Kafka中消费者的分区分配策略之后是否会有这样的疑问：如果消费者客户端中配置了两个分配策略，那么以哪个为准？如果有多个消费者，彼此所配置的分配策略并不完全相同，那么以哪个为准？多个消费者之间的分区分配是需要协同的，那么这个协同的过程又是怎样？ 在kafka中有一个组协调器（GroupCoordinator）负责来协调消费组内各个消费者的分区分配，对于每一个消费组而言，在kafka服务端都会有其对应的一个组协调器。具体的协调分区分配的过程如下：1.首先各个消费者向GroupCoordinator提案各自的分配策略。如下图所示，各个消费者提案的分配策略和订阅信息都包含在JoinGroupRequest请求中。 2.GroupCoordinator收集各个消费者的提案，然后执行以下两个步骤：一、选举消费组的leader；二、选举消费组的分区分配策略。 选举消费组的分区分配策略比较好理解，为什么这里还要选举消费组的leader，因为最终的分区分配策略的实施需要有一个成员来执行，而这个leader消费者正好扮演了这一个角色。在Kafka中把具体的分区分配策略的具体执行权交给了消费者客户端，这样可以提供更高的灵活性。比如需要变更分配策略，那么只需修改消费者客户端就醒来，而不必要修改并重启Kafka服务端。 怎么选举消费组的leader? 这个分两种情况分析：如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader；如果某一时刻leader消费者由于某些原因退出了消费组，那么就会重新选举一个新的leader，这个重新选举leader的过程又更为“随意”了，相关代码如下： 123//scala code.private val members = new mutable.HashMap[String, MemberMetadata]var leaderId = members.keys.head 解释一下这2行代码：在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的名称，而value是消费者相关的元数据信息。leaderId表示leader消费者的名称，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机挑选无异。总体上来说，消费组的leader选举过程是很随意的。 怎么选举消费组的分配策略？投票决定。每个消费者都可以设置自己的分区分配策略，对于消费组而言需要从各个消费者所呈报上来的各个分配策略中选举一个彼此都“信服”的策略来进行整体上的分区分配。这个分区分配的选举并非由leader消费者来决定，而是根据消费组内的各个消费者投票来决定。这里所说的“根据组内的各个消费者投票来决定”不是指GroupCoordinator还要与各个消费者进行进一步交互来实施，而是根据各个消费者所呈报的分配策略来实施。最终所选举的分配策略基本上可以看做是被各个消费者所支持的最多的策略，具体的选举过程如下： 收集各个消费者所支持的所有分配策略，组成候选集candidates。每个消费者从候选集candidates中找出第一个自身所支持的策略，为这个策略投上一票。计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。如果某个消费者并不支持所选举出的分配策略，那么就会报错。3.GroupCoordinator发送回执给各个消费者，并交由leader消费者执行具体的分区分配。 如上图所示，JoinGroupResponse回执中包含有GroupCoordinator中投票选举出的分配策略的信息。并且，只有leader消费者的回执中包含各个消费者的订阅信息，因为只需要leader消费者根据订阅信息来执行具体的分配，其余的消费并不需要。 4.leader消费者在整理出具体的分区分配方案后通过SyncGroupRequest请求提交给GroupCoordinator，然后GroupCoordinator为每个消费者挑选出各自的分配结果并通过SyncGroupResponse回执以告知它们。 broker端的分区分配生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区，而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。分区分配是否均衡会影响到Kafka整体的负载均衡，具体还会牵涉到优先副本等概念。 在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。如果集群中所有的broker节点都没有配置broker.rack参数，或者使用disable-rack-aware参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg","url":"https://gyl-coder.github.io"}},{"title":"kafka 如何保证数据的可靠性和一致性","slug":"kafka-reliability","date":"2019-10-09T13:06:31.058Z","updated":"2019-10-10T14:36:46.571Z","comments":true,"path":"2019/10/09/kafka-reliability/","link":"","permalink":"https://gyl-coder.github.io/2019/10/09/kafka-reliability/","excerpt":"数据可靠性Kafka 作为一个商业级消息中间件，消息可靠性的重要性可想而知。本文从 Producter 往 Broker 发送消息、Topic 分区副本以及 Leader 选举几个角度介绍数据的可靠性。","text":"数据可靠性Kafka 作为一个商业级消息中间件，消息可靠性的重要性可想而知。本文从 Producter 往 Broker 发送消息、Topic 分区副本以及 Leader 选举几个角度介绍数据的可靠性。 Producer 往 Broker 发送消息如果我们要往 Kafka 对应的主题发送消息，我们需要通过 Producer 完成。前面我们讲过 Kafka 主题对应了多个分区，每个分区下面又对应了多个副本；为了让用户设置数据可靠性， Kafka 在 Producer 里面提供了消息确认机制。也就是说我们可以通过配置来决定有几个副本收到这条消息才算消息发送成功。可以在定义 Producer 时通过 acks 参数指定（在 0.8.2.X 版本之前是通过 request.required.acks 参数设置的，详见 KAFKA-3043）。这个参数支持以下三种值： acks=0：生产者不会等待任何来自服务器的响应。 如果当中出现问题，导致服务器没有收到消息，那么生产者无从得知，会造成消息丢失 由于生产者不需要等待服务器的响应所以可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量 acks=1（默认值）：只要集群的Leader节点收到消息，生产者就会收到一个来自服务器的成功响应 如果消息无法到达Leader节点（例如Leader节点崩溃，新的Leader节点还没有被选举出来）生产者就会收到一个错误响应，为了避免数据丢失，生产者会重发消息 如果一个没有收到消息的节点成为新Leader，消息还是会丢失 此时的吞吐量主要取决于使用的是同步发送还是异步发送，吞吐量还受到发送中消息数量的限制，例如生产者在收到服务器响应之前可以发送多少个消息 acks=-1：只有当所有参与复制的节点全部都收到消息时，生产者才会收到一个来自服务器的成功响应 这种模式是最安全的，可以保证不止一个服务器收到消息，就算有服务器发生崩溃，整个集群依然可以运行 延时比acks=1更高，因为要等待不止一个服务器节点接收消息 根据实际的应用场景，我们设置不同的 acks，以此保证数据的可靠性。 另外，Producer 发送消息还可以选择同步（默认，通过 producer.type=sync 配置） 或者异步（producer.type=async）模式。如果设置成异步，虽然会极大的提高消息发送的性能，但是这样会增加丢失数据的风险。如果需要确保消息的可靠性，必须将 producer.type 设置为 sync。 Topic 分区副本在 Kafka 0.8.0 之前，Kafka 是没有副本的概念的，那时候人们只会用 Kafka 存储一些不重要的数据，因为没有副本，数据很可能会丢失。但是随着业务的发展，支持副本的功能越来越强烈，所以为了保证数据的可靠性，Kafka 从 0.8.0 版本开始引入了分区副本（详情请参见 KAFKA-50）。也就是说每个分区可以人为的配置几个副本（比如创建主题的时候指定 replication-factor，也可以在 Broker 级别进行配置 default.replication.factor），一般会设置为3。 Kafka 可以保证单个分区里的事件是有序的，分区可以在线（可用），也可以离线（不可用）。在众多的分区副本里面有一个副本是 Leader，其余的副本是 follower，所有的读写操作都是经过 Leader 进行的，同时 follower 会定期地去 leader 上复制数据。当 Leader 挂掉之后，其中一个 follower 会重新成为新的 Leader。通过分区副本，引入了数据冗余，同时也提供了 Kafka 的数据可靠性。 Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。 Leader 选举在介绍 Leader 选举之前，让我们先来了解一下 ISR（in-sync replicas）列表。每个分区的 leader 会维护一个 ISR 列表，ISR 列表里面就是 follower 副本的 Borker 编号，只有“跟得上” Leader 的 follower 副本才能加入到 ISR 里面，这个是通过 replica.lag.time.max.ms 参数配置的。只有 ISR 里的成员才有被选为 leader 的可能。 所以当 Leader 挂掉了，而且 unclean.leader.election.enable=false 的情况下，Kafka 会从 ISR 列表中选择第一个 follower 作为新的 Leader，因为这个分区拥有最新的已经 committed 的消息。通过这个可以保证已经 committed 的消息的数据可靠性。 综上所述，为了保证数据的可靠性，我们最少需要配置一下几个参数： producer 级别：acks=all（或者 request.required.acks=-1），同时发生模式为同步 producer.type=sync topic 级别：设置 replication.factor&gt;=3，并且 min.insync.replicas&gt;=2； broker 级别：关闭不完全的 Leader 选举，即 unclean.leader.election.enable=false； 数据一致性这里介绍的数据一致性主要是说不论是老的 Leader 还是新选举的 Leader，Consumer 都能读到一样的数据。那么 Kafka 是如何实现的呢？ 假设分区的副本为3，其中副本0是 Leader，副本1和副本2是 follower，并且在 ISR 列表里面。虽然副本0已经写入了 Message3，但是 Consumer 只能读取到 Message1。因为所有的 ISR 都同步了 Message1，只有 High Water Mark 以上的消息才支持 Consumer 读取，而 High Water Mark 取决于 ISR 列表里面偏移量最小的分区，对应于上图的副本2，这个很类似于木桶原理。 这样做的原因是还没有被足够多副本复制的消息被认为是“不安全”的，如果 Leader 发生崩溃，另一个副本成为新 Leader，那么这些消息很可能丢失了。如果我们允许消费者读取这些消息，可能就会破坏一致性。试想，一个消费者从当前 Leader（副本0） 读取并处理了 Message4，这个时候 Leader 挂掉了，选举了副本1为新的 Leader，这时候另一个消费者再去从新的 Leader 读取消息，发现这个消息其实并不存在，这就导致了数据不一致性问题。 当然，引入了 High Water Mark 机制，会导致 Broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕）。延迟时间可以通过参数 replica.lag.time.max.ms 参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。 输入中。。。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/tags/kafka/"},{"name":"数据可靠性","slug":"数据可靠性","permalink":"https://gyl-coder.github.io/tags/数据可靠性/"},{"name":"数据一致性","slug":"数据一致性","permalink":"https://gyl-coder.github.io/tags/数据一致性/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg","url":"https://gyl-coder.github.io"}},{"title":"kafka 问题总结","slug":"kafka-todo-list","date":"2019-10-09T12:59:38.693Z","updated":"2019-10-10T14:53:31.063Z","comments":true,"path":"2019/10/09/kafka-todo-list/","link":"","permalink":"https://gyl-coder.github.io/2019/10/09/kafka-todo-list/","excerpt":"kafka如何保证数据可靠性和数据一致性 Kafka Rebalance机制分析 Kafka的用途有哪些？使用场景如何？ Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么？ Kafka中的HW、LEO、LSO等分别代表什么？ Kafka中是怎么体现消息顺序性的？","text":"kafka如何保证数据可靠性和数据一致性 Kafka Rebalance机制分析 Kafka的用途有哪些？使用场景如何？ Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么？ Kafka中的HW、LEO、LSO等分别代表什么？ Kafka中是怎么体现消息顺序性的？ Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？ kafka 主题管理 Kafka生产者客户端的整体结构是什么样子的？ Kafka生产者客户端中使用了几个线程来处理？分别是什么？ Kafka的旧版Scala的消费者客户端的设计有什么缺陷？ “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果正确，那 有没有什么hack的手段？ 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1? 有哪些情形会造成重复消费？ 那些情景下会造成消息漏消费？ KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？ 简述消费者与消费组之间的关系 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？ topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？ topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？ 创建topic时如何选择合适的分区数？ Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？ 优先副本是什么？它有什么特殊的作用？ Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理 简述Kafka的日志目录结构 Kafka中有那些索引文件？ 如果我指定了一个offset，Kafka怎么查找到对应的消息？ 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？ 聊一聊你对Kafka的Log Retention的理解 聊一聊你对Kafka的Log Compaction的理解 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层） 聊一聊Kafka的延时操作的原理 聊一聊Kafka控制器的作用 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器） Kafka中的幂等是怎么实现的 Kafka中的事务是怎么实现的（这题我去面试6加被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸 Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？ 失效副本是指什么？有那些应对措施？ 多副本下，各个副本中的HW和LEO的演变过程 为什么Kafka不支持读写分离？ Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch） Kafka中怎么实现死信队列和重试队列？ Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？） Kafka中怎么做消息审计？ Kafka中怎么做消息轨迹？ Kafka中有那些配置参数比较有意思？聊一聊你的看法 Kafka中有那些命名比较有意思？聊一聊你的看法 Kafka有哪些指标需要着重关注？ 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同) Kafka的那些设计让它有如此高的性能？ Kafka有什么优缺点？ 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？ 为什么选择Kafka? 在使用Kafka的过程中遇到过什么困难？怎么解决的？ 怎么样才能确保Kafka极大程度上的可靠性？ 聊一聊你对Kafka生态的理解","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg","url":"https://gyl-coder.github.io"}},{"title":"Kafka中的HW、LEO、LSO等分别代表什么？","slug":"Kafka中的HW、LEO、LSO等分别代表什么？","date":"2019-10-04T15:49:03.584Z","updated":"2019-10-09T13:01:27.710Z","comments":true,"path":"2019/10/04/Kafka中的HW、LEO、LSO等分别代表什么？/","link":"","permalink":"https://gyl-coder.github.io/2019/10/04/Kafka中的HW、LEO、LSO等分别代表什么？/","excerpt":"HW 、 LEO 等概念和上一篇文章所说的 ISR有着紧密的关系，如果不了解 ISR 可以先看下ISR相关的介绍。 HW （High Watermark）俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。 下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最有一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。","text":"HW 、 LEO 等概念和上一篇文章所说的 ISR有着紧密的关系，如果不了解 ISR 可以先看下ISR相关的介绍。 HW （High Watermark）俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。 下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最有一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。 LEO （Log End Offset），标识当前日志文件中下一条待写入的消息的offset。上图中offset为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的offset值加1.分区 ISR 集合中的每个副本都会维护自身的 LEO ，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息。 下面具体分析一下 ISR 集合和 HW、LEO的关系。 假设某分区的 ISR 集合中有 3 个副本，即一个 leader 副本和 2 个 follower 副本，此时分区的 LEO 和 HW 都分别为 3 。消息3和消息4从生产者出发之后先被存入leader副本。 在消息被写入leader副本之后，follower副本会发送拉取请求来拉取消息3和消息4进行消息同步。 在同步过程中不同的副本同步的效率不尽相同，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO 为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset0至3之间的消息。 当所有副本都成功写入消息3和消息4之后，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。 由此可见kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的follower副本都复制完，这条消息才会被确认已成功提交，这种复制方式极大的影响了性能。而在异步复制的方式下，follower副本异步的从leader副本中复制数据，数据只要被leader副本写入就会被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，然后leader副本宕机，则会造成数据丢失。kafka使用这种ISR的方式有效的权衡了数据可靠性和性能之间的关系。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg","url":"https://gyl-coder.github.io"}},{"title":"kafka中的ISR、AR代表什么？ISR的伸缩性又指什么？","slug":"kafka-isr-ar","date":"2019-10-04T15:47:20.937Z","updated":"2019-10-10T14:41:49.421Z","comments":true,"path":"2019/10/04/kafka-isr-ar/","link":"","permalink":"https://gyl-coder.github.io/2019/10/04/kafka-isr-ar/","excerpt":"相信大家已经对 kafka 的基本概念已经有一定的了解了，下面直接来分析一下 ISR 和 AR 的概念。","text":"相信大家已经对 kafka 的基本概念已经有一定的了解了，下面直接来分析一下 ISR 和 AR 的概念。 ISR and AR简单来说，分区中的所有副本统称为 AR (Assigned Replicas)。所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成 ISR (In Sync Replicas)。 ISR 集合是 AR 集合的一个子集。消息会先发送到leader副本，然后follower副本才能从leader中拉取消息进行同步。同步期间，follow副本相对于leader副本而言会有一定程度的滞后。前面所说的 ”一定程度同步“ 是指可忍受的滞后范围，这个范围可以通过参数进行配置。于leader副本同步滞后过多的副本（不包括leader副本）将组成 OSR （Out-of-Sync Replied）由此可见，AR = ISR + OSR。正常情况下，所有的follower副本都应该与leader 副本保持 一定程度的同步，即AR=ISR，OSR集合为空。 ISR 的伸缩性leader副本负责维护和跟踪 ISR 集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从 ISR 集合中剔除。如果 OSR 集合中所有follower副本“追上”了leader副本，那么leader副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当leader副本发生故障时，只有在 ISR 集合中的follower副本才有资格被选举为新的leader，而在 OSR 集合中的副本则没有任何机会（不过这个可以通过配置来改变）。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg","url":"https://gyl-coder.github.io"}},{"title":"kafka 主题管理","slug":"kafka 主题管理","date":"2019-10-03T06:47:01.321Z","updated":"2019-10-09T12:43:45.699Z","comments":true,"path":"2019/10/03/kafka 主题管理/","link":"","permalink":"https://gyl-coder.github.io/2019/10/03/kafka 主题管理/","excerpt":"对于 kafka 主题（topic）的管理（增删改查），使用最多的便是kafka自带的脚本。","text":"对于 kafka 主题（topic）的管理（增删改查），使用最多的便是kafka自带的脚本。 创建主题kafka提供了自带的 kafka-topics 脚本，用来帮助用户创建主题（topic）。 1bin/kafka-topics.sh --bootstrap-server broker_host:port --create --topic my_topic_name --partitions 1 --replication-factor 1 create 表明我们要创建主题，而 partitions 和 replication factor 分别设置了主题的分区数以及每个分区下的副本数。 这里为什么用的 --bootstrap-server 参数，而不是 --zookeeper ?--zookeeper 参数是之前版本的用法，从kafka 2.2 版本开始，社区推荐使用 --bootstrap-server 参数替换 --zoookeeper ，并且显式地将后者标记为 “已过期”，因此，如果你已经在使用 2.2 版本了，那么创建主题请指定 --bootstrap-server 参数。 推荐使用 --bootstrap-server 而非 --zookeeper 的原因主要有两个。 使用 –zookeeper 会绕过 Kafka 的安全体系。这就是说，即使你为 Kafka 集群设置了安全认证，限制了主题的创建，如果你使用 –zookeeper 的命令，依然能成功创建任意主题，不受认证体系的约束。这显然是 Kafka 集群的运维人员不希望看到的。 使用 –bootstrap-server 与集群进行交互，越来越成为使用 Kafka 的标准姿势。换句话说，以后会有越来越少的命令和 API 需要与 ZooKeeper 进行连接。这样，我们只需要一套连接信息，就能与 Kafka 进行全方位的交互，不用像以前一样，必须同时维护 ZooKeeper 和 Broker 的连接信息。 查询主题创建好主题之后，Kafka 允许我们使用相同的脚本查询主题。你可以使用下面的命令，查询所有主题的列表。 1bin/kafka-topics.sh --bootstrap-server broker_host:port --list 如果要查询单个主题的详细数据，你可以使用下面的命令。 1bin/kafka-topics.sh --bootstrap-server broker_host:port --describe --topic &lt;topic_name&gt; 如果 describe 命令不指定具体的主题名称，那么 Kafka 默认会返回所有 “可见” 主题的详细数据给你。 这里的 “可见”，是指发起这个命令的用户能够看到的 Kafka 主题。这和前面说到主题创建时，使用 –zookeeper 和 –bootstrap-server 的区别是一样的。如果指定了 –bootstrap-server，那么这条命令就会受到安全认证体系的约束，即对命令发起者进行权限验证，然后返回它能看到的主题。否则，如果指定 –zookeeper 参数，那么默认会返回集群中所有的主题详细数据。基于这些原因，我建议你最好统一使用 –bootstrap-server 连接参数。 修改主题修改主题分区其实就是增加分区，目前 Kafka 不允许减少某个主题的分区数。你可以使用 kafka-topics 脚本，结合 –alter 参数来增加某个主题的分区数，命令如下： 1bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic &lt;topic_name&gt; --partitions &lt; 新分区数 &gt; 这里要注意的是，你指定的分区数一定要比原有分区数大，否则 Kafka 会抛出 InvalidPartitionsException 异常。 修改主题级别参数在主题创建之后，我们可以使用 kafka-configs 脚本修改对应的参数。 假设我们要设置主题级别参数 max.message.bytes，那么命令如下： 1bin/kafka-configs.sh --zookeeper zookeeper_host:port --entity-type topics --entity-name &lt;topic_name&gt; --alter --add-config max.message.bytes=10485760 也许你会觉得奇怪，为什么这个脚本就要指定 –zookeeper，而不是 –bootstrap-server 呢？其实，这个脚本也能指定 –bootstrap-server 参数，只是它是用来设置动态参数的。在专栏后面，我会详细介绍什么是动态参数，以及动态参数都有哪些。现在，你只需要了解设置常规的主题级别参数，还是使用 –zookeeper。 变更副本数使用自带的 kafka-reassign-partitions 脚本，帮助我们增加主题的副本数。 假设kafka的内部主题 __consumer_offsets 只有 1 个副本，现在我们想要增加至 3 个副本。下面是操作： 创建一个 json 文件，显式提供 50 个分区对应的副本数。注意，replicas 中的 3 台 Broker 排列顺序不同，目的是将 Leader 副本均匀地分散在 Broker 上。该文件具体格式如下 12345678&#123;\"version\":1, \"partitions\":[ &#123;\"topic\":\"__consumer_offsets\",\"partition\":0,\"replicas\":[0,1,2]&#125;, &#123;\"topic\":\"__consumer_offsets\",\"partition\":1,\"replicas\":[0,2,1]&#125;, &#123;\"topic\":\"__consumer_offsets\",\"partition\":2,\"replicas\":[1,0,2]&#125;, &#123;\"topic\":\"__consumer_offsets\",\"partition\":3,\"replicas\":[1,2,0]&#125;, ... &#123;\"topic\":\"__consumer_offsets\",\"partition\":49,\"replicas\":[0,1,2]&#125;]&#125; 执行 kafka-reassign-patitions 脚本，命令如下： 1bin/kafka-reassign-partitions.sh --zookeeper zookeeper_host:port --reassignment-json-file reassign.json --execute 除了修改内部主题，我们可能还想查看这些内部主题的消息内容。特别是对于 __consumer_offsets 而言，由于它保存了消费者组的位移数据，有时候直接查看该主题消息是很方便的事情。下面的命令可以帮助我们直接查看消费者组提交的位移数据。 1bin/kafka-console-consumer.sh --bootstrap-server kafka_host:port --topic __consumer_offsets --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --from-beginning 除了查看位移提交数据，我们还可以直接读取该主题消息，查看消费者组的状态信息。 1bin/kafka-console-consumer.sh --bootstrap-server kafka_host:port --topic __consumer_offsets --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$GroupMetadataMessageFormatter&quot; --from-beginning 对于内部主题 __transaction_state 而言，方法是相同的。你只需要指定 kafka.coordinator.transaction.TransactionLog$TransactionLogMessageFormatter 即可。 修改主题限速这里主要是指设置 Leader 副本和 Follower 副本使用的带宽。有时候，我们想要让某个主题的副本在执行副本同步机制时，不要消耗过多的带宽。Kafka 提供了这样的功能。我来举个例子。假设我有个主题，名为 test，我想让该主题各个分区的 Leader 副本和 Follower 副本在处理副本同步时，不得占用超过 100MBps 的带宽。注意是大写 B，即每秒不超过 100MB。那么，我们应该怎么设置呢？ 要达到这个目的，我们必须先设置 Broker 端参数 leader.replication.throttled.rate 和 follower.replication.throttled.rate，命令如下： 1bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config &apos;leader.replication.throttled.rate=104857600,follower.replication.throttled.rate=104857600&apos; --entity-type brokers --entity-name 0 这条命令结尾处的 –entity-name 就是 Broker ID。倘若该主题的副本分别在 0、1、2、3 多个 Broker 上，那么你还要依次为 Broker 1、2、3 执行这条命令。 设置好这个参数之后，我们还需要为该主题设置要限速的副本。在这个例子中，我们想要为所有副本都设置限速，因此统一使用通配符 * 来表示，命令如下： 1bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config &apos;leader.replication.throttled.replicas=*,follower.replication.throttled.replicas=*&apos; --entity-type topics --entity-name test 主题分区迁移同样是使用 kafka-reassign-partitions 脚本，对主题各个分区的副本进行 “手术” 般的调整，比如把某些分区批量迁移到其他 Broker 上。 删除主题1bin/kafka-topics.sh --bootstrap-server broker_host:port --delete --topic &lt;topic_name&gt; 删除主题的命令并不复杂，关键是删除操作是异步的，执行完这条命令不代表主题立即就被删除了。它仅仅是被标记成 “已删除” 状态而已。Kafka 会在后台默默地开启主题删除操作。因此，通常情况下，你都需要耐心地等待一段时间。 主题删除失败当运行完上面的删除命令后，很多人发现已删除主题的分区数据依然 “躺在” 硬盘上，没有被清除。这时该怎么办呢？ 实际上，造成主题删除失败的原因有很多，最常见的原因有两个： 副本所在的 Broker 宕机了 待删除主题的部分分区依然在执行迁移过程。 如果是因为前者，通常你重启对应的 Broker 之后，删除操作就能自动恢复；如果是因为后者，那就麻烦了，很可能两个操作会相互干扰。 不管什么原因，一旦你碰到主题无法删除的问题，可以采用这样的方法： 手动删除 ZooKeeper 节点 /admin/delete_topics 下以待删除主题为名的 znode。 手动删除该主题在磁盘上的分区目录。 在 ZooKeeper 中执行 rmr /controller，触发 Controller 重选举，刷新 Controller 缓存。 在执行最后一步时，你一定要谨慎，因为它可能造成大面积的分区 Leader 重选举。事实上，仅仅执行前两步也是可以的，只是 Controller 缓存中没有清空待删除主题罢了，也不影响使用。 常见问题__consumer_offsets 占用太多的磁盘一旦你发现这个主题消耗了过多的磁盘空间，那么，你一定要显式地用 jstack 命令查看一下 kafka-log-cleaner-thread 前缀的线程状态。通常情况下，这都是因为该线程挂掉了，无法及时清理此内部主题。倘若真是这个原因导致的，那我们就只能重启相应的 Broker 了。另外，请你注意保留出错日志，因为这通常都是 Bug 导致的，最好提交到社区看一下。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.github.io/tags/kafka/"},{"name":"topic","slug":"topic","permalink":"https://gyl-coder.github.io/tags/topic/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/gyl-coder.github.com@v1.0.0/img/touxiang.jpg","url":"https://gyl-coder.github.io"}},{"title":"Hello World","slug":"hello-world","date":"2019-09-30T16:06:08.620Z","updated":"2019-10-03T07:19:55.274Z","comments":true,"path":"2019/10/01/hello-world/","link":"","permalink":"https://gyl-coder.github.io/2019/10/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}