[{"title":"理解 Go 之 ==","path":"//blog/go/go-equal/","content":"golang 中对==的处理有一些细节的地方需要特别注意。虽然平时可能不太会遇到，但是碰到了就是大坑。本文将对 golang 中==操作做一个系统的介绍。希望能对大家有所帮助。 类型golang 中的数据类型可以分为以下 4 大类： 基本类型：整型（int/uint/int8/uint8/int16/uint16/int32/uint32/int64/uint64/byte/rune等）、浮点数（float32/float64）、复数类型（complex64/complex128）、字符串（string）。 复合类型（又叫聚合类型）：数组和结构体类型。 引用类型：切片（slice）、map、channel、指针。 接口类型：如error。 == 操作最重要的一个前提是：两个操作数类型必须相同！类型必须相同！类型必须相同！ 如果类型不同，那么编译时就会报错。 注意： golang 的类型系统非常严格，没有C/C++中的隐式类型转换。虽然写起来稍微有些麻烦，但是能避免今后非常多的麻烦！！！ golang 中可以通过type定义新类型。新定义的类型与底层类型不同，不能直接比较。 为了更容易看出类型，示例代码中的变量定义都显式指定了类型。 看下面的代码： 12345678910package mainimport &quot;fmt&quot;func main() &#123; var a int8 var b int16 // 编译错误：invalid operation a == b (mismatched types int8 and int16) fmt.Println(a == b)&#125; 没有隐式类型转换。 1234567891011package mainimport &quot;fmt&quot;func main() &#123; type int8 myint8 var a int8 var b myint8 // 编译错误：invalid operation a == b (mismatched types int8 and myint8) fmt.Println(a == b)&#125; 虽然 myint8 的底层类型是 int8，但是他们是不同的类型。 下面依次通过这 4 种类型来说明==是如何做比较的。 基本类型这是最简单的一种类型。比较操作也很简单，直接比较值是否相等。没啥好说的，直接看例子。 12345var a uint32 = 10var b uint32 = 20var c uint32 = 10fmt.Println(a == b) // falsefmt.Println(a == c) // true 有一点需要注意，浮点数的比较问题： 1234var a float64 = 0.1var b float64 = 0.2var c float64 = 0.3fmt.Println(a + b == c) // false 因为计算机中，有些浮点数不能精确表示，浮点运算结果会有误差。如果我们分别输出a+b和c的值，会发现它们确实是不同的： 12345fmt.Println(a + b)fmt.Println(c)// 0.30000000000000004// 0.3 这个问题不是 golang 独有的，只要浮点数遵循 IEEE 754 标准的编程语言都有这个问题。需要特别注意，**尽量不要做浮点数比较，确实需要比较时，计算两个浮点数的差的绝对值，如果小于一定的值就认为它们相等，比如1e-9**。 复合类型复合类型也叫做聚合类型。golang 中的复合类型只有两种：数组和结构体 。它们是逐元素 &#x2F; 字段比较的。 注意：数组的长度视为类型的一部分，长度不同的两个数组是不同的类型，不能直接比较。 对于数组来说，依次比较各个元素的值。根据元素类型的不同，再依据是基本类型、复合类型、引用类型或接口类型，按照特定类型的规则进行比较。所有元素全都相等，数组才是相等的。 对于结构体来说，依次比较各个字段的值。根据字段类型的不同，再依据是 4 中类型中的哪一种，按照特定类型的规则进行比较。所有字段全都相等，结构体才是相等的。 例如： 123456789101112131415a := [4]int&#123;1, 2, 3, 4&#125;b := [4]int&#123;1, 2, 3, 4&#125;c := [4]int&#123;1, 3, 4, 5&#125;fmt.Println(a == b) // truefmt.Println(a == c) // falsetype A struct &#123; a int b string&#125;aa := A &#123; a : 1, b : &quot;test1&quot; &#125;bb := A &#123; a : 1, b : &quot;test1&quot; &#125;cc := A &#123; a : 1, b : &quot;test2&quot; &#125;fmt.Println(aa == bb) // truefmt.Println(aa == cc) // false 引用类型引用类型是间接指向它所引用的数据的，保存的是数据的地址。引用类型的比较实际判断的是两个变量是不是指向同一份数据，它不会去比较实际指向的数据。 例如： 12345678910type A struct &#123; a int b string&#125;aa := &amp;A &#123; a : 1, b : &quot;test1&quot; &#125;bb := &amp;A &#123; a : 1, b : &quot;test1&quot; &#125;cc := aafmt.Println(aa == bb) // falsefmt.Println(aa == cc) // true 因为aa和bb指向的两个不同的结构体，虽然它们指向的值是相等的（见上面复合类型的比较），但是它们不等。 aa和cc指向相同的结构体，所以它们相等。 再看看channel的比较： 123456ch1 := make(chan int, 1)ch2 := make(chan int, 1)ch3 := ch1fmt.Println(ch1 == ch2) // falsefmt.Println(ch1 == ch3) // true ch1和ch2虽然类型相同，但是指向不同的channel，所以它们不等。 ch1和ch3指向相同的channel，所以它们相等。 关于引用类型，有两个比较特殊的规定： 切片之间不允许比较。切片只能与nil值比较。 map之间不允许比较。map只能与nil值比较。 为什么要做这样的规定？我们先来说切片。因为切片是引用类型，它可以间接的指向自己。例如： 1234567a := []interface&#123;&#125;&#123; 1, 2.0 &#125;a[1] = afmt.Println(a)// !!!// runtime: goroutine stack exceeds 1000000000-byte limit// fatal error: stack overflow 上面代码将a赋值给a[1]导致递归引用，fmt.Println(a)语句直接爆栈。 切片如果直接比较引用地址，是不合适的。首先，切片与数组是比较相近的类型，比较方式的差异会造成使用者的混淆。另外，长度和容量是切片类型的一部分，不同长度和容量的切片如何比较？ 切片如果像数组那样比较里面的元素，又会出现上来提到的循环引用的问题。虽然可以在语言层面解决这个问题，但是 golang 团队认为不值得为此耗费精力。 基于上面两点原因，golang 直接规定切片类型不可比较。使用==比较切片直接编译报错。 例如： 12345var a []intvar b []int// invalid operation: a == b (slice can only be compared to nil)fmt.Println(a == b) 错误信息很明确。 因为map的值类型可能为不可比较类型（见下面，切片是不可比较类型），所以map类型也不可比较。 接口类型接口类型是 golang 中比较重要的一种类型。接口类型的值，我们称为接口值。一个接口值是由两个部分组成的，具体类型（即该接口存储的值的类型）和该类型的一个值。引用《go 程序设计语言》的名称，分别称为动态类型和动态值。接口值的比较涉及这两部分的比较，只有当动态类型完全相同且动态值相等（动态值使用==比较），两个接口值才是相等的。 例如： 1234567var a interface&#123;&#125; = 1var b interface&#123;&#125; = 2var c interface&#123;&#125; = 1var d interface&#123;&#125; = 1.0fmt.Println(a == b) // falsefmt.Println(a == c) // truefmt.Println(a == d) // false a和b动态类型相同（都是int），动态值也相同（都是1，基本类型比较），故两者相等。 a和c动态类型相同，动态值不等（分别为1和2，基本类型比较），故两者不等。 a和d动态类型不同，a为int，d为float64，故两者不等。 123456789101112131415type A struct &#123; a int b string&#125;var aa interface&#123;&#125; = A &#123; a: 1, b: &quot;test&quot; &#125;var bb interface&#123;&#125; = A &#123; a: 1, b: &quot;test&quot; &#125;var cc interface&#123;&#125; = A &#123; a: 2, b: &quot;test&quot; &#125;fmt.Println(aa == bb) // truefmt.Println(aa == cc) // falsevar dd interface&#123;&#125; = &amp;A &#123; a: 1, b: &quot;test&quot; &#125;var ee interface&#123;&#125; = &amp;A &#123; a: 1, b: &quot;test&quot; &#125;fmt.Println(dd == ee) // false aa和bb动态类型相同（都是A），动态值也相同（结构体A，见上面复合类型的比较规则），故两者相等。 aa和cc动态类型相同，动态值不同，故两者不等。 dd和ee动态类型相同（都是*A），动态值使用指针（引用）类型的比较，由于不是指向同一个地址，故不等。 注意： 如果接口的动态值不可比较，强行比较会panic！！！ 1234var a interface&#123;&#125; = []int&#123;1, 2, 3, 4&#125;var b interface&#123;&#125; = []int&#123;1, 2, 3, 4&#125;// panic: runtime error: comparing uncomparable type []intfmt.Println(a == b) a和b的动态值是切片类型，而切片类型不可比较，所以a == b会panic。 接口值的比较不要求接口类型（注意不是动态类型）完全相同，只要一个接口可以转化为另一个就可以比较。例如： 123456789var f *os.Filevar r io.Reader = fvar rc io.ReadCloser = ffmt.Println(r == rc) // truevar w io.Writer = f// invalid operation: r == w (mismatched types io.Reader and io.Writer)fmt.Println(r == w) r的类型为io.Reader接口，rc的类型为io.ReadCloser接口。查看源码，io.ReadCloser的定义如下： 1234type ReadCloser interface &#123; Reader Closer&#125; io.ReadCloser可转化为io.Reader，故两者可比较。 而io.Writer不可转化为io.Reader，编译报错。 使用type定义的类型使用type可以基于现有类型定义新的类型。新类型会根据它们的底层类型来比较。例如： 12345678910111213type myint intvar a myint = 10var b myint = 20var c myint = 10fmt.Println(a == b) // falsefmt.Println(a == c) // truetype arr4 [4]intvar aa arr4 = [4]int&#123;1, 2, 3, 4&#125;var bb arr4 = [4]int&#123;1, 2, 3, 4&#125;var cc arr4 = [4]int&#123;1, 2, 3, 5&#125;fmt.Println(aa == bb)fmt.Println(aa == cc) myint根据底层类型int来比较。 arr4根据底层类型[4]int来比较。 不可比较性前面说过，golang 中的切片类型是不可比较的。所有含有切片的类型都是不可比较的。例如： 数组元素是切片类型。 结构体有切片类型的字段。 指针指向的是切片类型。 不可比较性会传递，如果一个结构体由于含有切片字段不可比较，那么将它作为元素的数组不可比较，将它作为字段类型的结构体不可比较。 谈谈 map 由于map的key是使用==来判等的，所以所有不可比较的类型都不能作为map的key。例如： 123456789// invalid map key type []intm1 := make(map[[]int]int)type A struct &#123; a []int b string&#125;// invalid map key type Am2 := make(map[A]int) 由于切片类型不可比较，不能作为map的key，编译时m1 := make(map[[]int]int)报错。 由于结构体A含有切片字段，不可比较，不能作为map的key，编译报错。 总结本文详尽介绍了 golang 中==操作的细节，希望能对大家有所帮助。","tags":["Go"],"categories":["Go"]},{"title":"用 Ebitengine 制作一款简易射击游戏","path":"//blog/go/shooting-games/","content":"在上一篇 Ebitengine - 一款Go语言编写的2D游戏引擎 中，已经了解了 Ebitengine 的一些基本用法。下面我们将使用 Ebitengie + Go 写一款简易的射击游戏。 限制飞船的活动范围上一篇文章还留了个尾巴，体验过的同学应该发现了：飞船可以移动出屏幕！！！现在我们就来限制一下飞船的移动范围。我们规定飞船可以左右超过半个身位，如下图所示： 很容易计算得出，左边位置的 x 坐标为： 右边位置的坐标为： 修改 input.go 的代码如下： 123456789101112131415161718func (g *Game) Update() error &#123; // -------省略------- if ebiten.IsKeyPressed(ebiten.KeyLeft) &#123; g.ship.x -= g.cfg.ShipSpeedFactor if g.ship.x &lt; -float64(g.ship.width)/2 &#123; g.ship.x = -float64(g.ship.width) / 2 &#125; &#125; else if ebiten.IsKeyPressed(ebiten.KeyRight) &#123; g.ship.x += g.cfg.ShipSpeedFactor if g.ship.x &gt; float64(g.cfg.ScreenWidth)-float64(g.ship.width)/2 &#123; g.ship.x = float64(g.cfg.ScreenWidth) - float64(g.ship.width)/2 &#125; &#125; // -------省略------- &#125; 飞船可以发射子弹这里进行简化，我们不另外准备子弹的图片，直接画一个矩形就 ok。为了可以灵活控制，我们将子弹的宽、高、颜色以及速率都用配置文件来控制： 1234567891011&#123; &quot;bulletWidth&quot;: 3, &quot;bulletHeight&quot;: 15, &quot;bulletSpeedFactor&quot;: 2, &quot;bulletColor&quot;: &#123; &quot;r&quot;: 80, &quot;g&quot;: 80, &quot;b&quot;: 80, &quot;a&quot;: 255 &#125;&#125; 新增一个文件 bullet.go，定义子弹的结构类型和 New 方法： 1234567891011121314151617181920212223type Bullet struct &#123; image *ebiten.Image width int height int x float64 y float64 speedFactor float64&#125;func NewBullet(cfg *Config, ship *Ship) *Bullet &#123; rect := image.Rect(0, 0, cfg.BulletWidth, cfg.BulletHeight) img := ebiten.NewImageWithOptions(rect, nil) img.Fill(cfg.BulletColor) return &amp;Bullet&#123; image: img, width: cfg.BulletWidth, height: cfg.BulletHeight, x: ship.x + float64(ship.width-cfg.BulletWidth)/2, y: float64(cfg.ScreenHeight - ship.height - cfg.BulletHeight), speedFactor: cfg.BulletSpeedFactor, &#125;&#125; 首先根据配置的宽高创建一个 rect 对象，然后调用ebiten.NewImageWithOptions创建一个*ebiten.Image对象。 子弹都是从飞船头部发出的，所以它的横坐标等于飞船中心的横坐标，左上角的纵坐标 &#x3D; 屏幕高度 - 飞船高 - 子弹高。 随便增加子弹的绘制方法： 12345func (bullet *Bullet) Draw(screen *ebiten.Image) &#123; op := &amp;ebiten.DrawImageOptions&#123;&#125; op.GeoM.Translate(bullet.x, bullet.y) screen.DrawImage(bullet.image, op)&#125; 我们在 Game 对象中增加一个 map 来管理子弹： 1234567891011type Game struct &#123; // -------省略------- bullets map[*Bullet]struct&#123;&#125;&#125;func NewGame() *Game &#123; return &amp;Game&#123; // -------省略------- bullets: make(map[*Bullet]struct&#123;&#125;), &#125;&#125; 然后在Draw方法中，我们需要将子弹也绘制出来： 1234567func (g *Game) Draw(screen *ebiten.Image) &#123; screen.Fill(g.cfg.BgColor) g.ship.Draw(screen) for bullet := range g.bullets &#123; bullet.Draw(screen) &#125;&#125; 子弹位置如何更新呢？在Game.Update中更新，与飞船类似，只是飞船只能水平移动，而子弹只能垂直移动。 123456func (g *Game) Update() error &#123; for bullet := range g.bullets &#123; bullet.y -= bullet.speedFactor &#125; // -------省略-------&#125; 子弹的更新、绘制逻辑都完成了，可是我们还没有子弹！现在我们就来实现按空格发射子弹的功能。我们需要在 Update 方法中判断空格键是否按下： 1234567891011121314func (g *Game) Update() error &#123; if ebiten.IsKeyPressed(ebiten.KeyLeft) &#123; // -------省略------- &#125; else if ebiten.IsKeyPressed(ebiten.KeyRight) &#123; // -------省略------- &#125; else if ebiten.IsKeyPressed(ebiten.KeySpace) &#123; if len(g.bullets) &lt; g.cfg.MaxBulletNum &amp;&amp; time.Now().Sub(g.ship.lastBulletTime).Milliseconds() &gt; g.cfg.BulletInterval &#123; bullet := NewBullet(g.cfg, g.ship) g.addBullet(bullet) g.ship.lastBulletTime = time.Now() &#125; &#125;&#125; 给 Game 对象增加一个addBullet方法： 123func (g *Game) addBullet(bullet *Bullet) &#123; g.bullets[bullet] = struct&#123;&#125;&#123;&#125;&#125; 这里要注意，飞船移动和发射子弹是两个独立的事件，不能将 KeyLeft/KeyRight/KeySpace 在同一个 if-else 中处理 限制子弹数量为了防止子弹太多，通过配置 g.cfg.MaxBulletNum 来限制子弹的数量（屏幕中仅允许出现固定数量的子弹） 123&#123; &quot;maxBulletNum&quot;: 10&#125; 123type Config struct &#123; MaxBulletNum int `json:&quot;maxBulletNum&quot;`&#125; 然后我们在 Update 方法中判断，如果目前存在的子弹数小于MaxBulletNum才能创建新的子弹。 同时由于 Update() 的调用间隔太短了，导致我们一次 space 按键会发射多个子弹。可以控制两个子弹之间的时间间隔。同样用配置文件来控制（单位毫秒） 距离上次发射子弹的时间大于 BulletInterval 毫秒，才能再次发射，发射成功之后更新时间： 123&#123; &quot;bulletInterval&quot;: 50&#125; 123type Config struct &#123; BulletInterval int64 `json:&quot;bulletInterval&quot;`&#125; 当子弹消失（飞出屏幕之外）后，需要把离开屏幕的子弹删除。否则无法发射新的子弹。 在 Bullet 中添加判断是否处于屏幕外的方法： 123func (bullet *Bullet) outOfScreen() bool &#123; return bullet.y &lt; -float64(bullet.height)&#125; 在 Update 函数中移除子弹： 12345678func (g *Game) Update() error &#123; for bullet := range g.bullets &#123; if bullet.outOfScreen() &#123; delete(g.bullets, bullet) &#125; &#125; return nil&#125; 新增怪物怪物图片如下： 同飞船一样，编写 Monster 类，添加绘制自己的方法： 12345678910111213141516171819202122232425262728293031type Monster struct &#123; image *ebiten.Image width int height int x float64 y float64 speedFactor float64&#125;func NewMonster(cfg *Config) *Monster &#123; img, _, err := ebitenutil.NewImageFromFile(&quot;../images/monster.png&quot;) if err != nil &#123; log.Fatal(err) &#125; width, height := img.Size() return &amp;Monster&#123; image: img, width: width, height: height, x: 0, y: 0, speedFactor: cfg.MonsterSpeedFactor, &#125;&#125;func (monster *Monster) Draw(screen *ebiten.Image) &#123; op := &amp;ebiten.DrawImageOptions&#123;&#125; op.GeoM.Translate(monster.x, monster.y) screen.DrawImage(monster.image, op)&#125; 游戏开始时需要创建一组怪物，计算一行可以容纳多少个怪物，考虑到左右各留一定的空间，两个怪物之间留一点空间。 123456789101112131415161718192021222324252627type Game struct &#123; // Game结构中的map用来存储怪物对象 monsters map[*Monster]struct&#123;&#125;&#125;func NewGame() *Game &#123; g := &amp;Game&#123; // 创建map monsters: make(map[*Monster]struct&#123;&#125;), &#125; // 调用 CreateMonsters 创建一组怪物 g.CreateMonsters() return g&#125;func (g *Game) CreateMonsters() &#123; monster := NewMonster(g.cfg) availableSpaceX := g.cfg.ScreenWidth - 2*monster.width numMonsters := availableSpaceX / (2 * monster.width) for i := 0; i &lt; numMonsters; i++ &#123; monster = NewMonster(g.cfg) monster.x = float64(monster.width + 2*monster.width*i) g.addMonster(monster) &#125;&#125; 左右各留一个怪物宽度的空间： 1availableSpaceX := g.cfg.ScreenWidth - 2*monster.width 然后，两个怪物之间留一个怪物宽度的空间。所以一行可以创建的怪物的数量为： 1numMonsters := availableSpaceX / (2 * monster.width) 创建一组怪物，依次排列。 同样地，我们需要在Game.Draw方法中添加绘制怪物的代码： 123456func (g *Game) Draw(screen *ebiten.Image) &#123; // -------省略------- for monster := range g.monsters &#123; monster.Draw(screen) &#125;&#125; 再创建两行： 1234567891011func (g *Game) CreateMonsters() &#123; // -------省略------- for row := 0; row &lt; 2; row++ &#123; for i := 0; i &lt; numMonsters; i++ &#123; monster = NewMonster(g.cfg) monster.x = float64(monster.width + 2*monster.width*i) monster.y = float64(monster.height*row) * 1.5 g.addMonster(monster) &#125; &#125;&#125; 让怪物都动起来，同样地还是在Game.Update方法中更新位置： 1234567func (g *Game) Update() error &#123; // -------省略------- for monster := range g.monsters &#123; monster.y += monster.speedFactor &#125; // -------省略-------&#125; 新增判断游戏胜负逻辑当前子弹碰到怪物直接穿过去了，我们希望能击杀怪物。这需要检查子弹和怪物之间的碰撞。新增检查子弹与怪物是否碰撞的检查函数。这里采用最直观的碰撞检测方法，即子弹的 4 个顶点只要有一个位于怪物矩形中，就认为它们碰撞。 123456789101112131415161718192021222324252627282930// CheckCollision 检查子弹和怪物之间是否有碰撞func CheckCollision(bullet *Bullet, monster *Monster) bool &#123; monsterTop, monsterLeft := monster.y, monster.x monsterBottom, monsterRight := monster.y+float64(monster.height), monster.x+float64(monster.width) // 左上角 x, y := bullet.x, bullet.y if y &gt; monsterTop &amp;&amp; y &lt; monsterBottom &amp;&amp; x &gt; monsterLeft &amp;&amp; x &lt; monsterRight &#123; return true &#125; // 右上角 x, y = bullet.x+float64(bullet.width), bullet.y if y &gt; monsterTop &amp;&amp; y &lt; monsterBottom &amp;&amp; x &gt; monsterLeft &amp;&amp; x &lt; monsterRight &#123; return true &#125; // 左下角 x, y = bullet.x, bullet.y+float64(bullet.height) if y &gt; monsterTop &amp;&amp; y &lt; monsterBottom &amp;&amp; x &gt; monsterLeft &amp;&amp; x &lt; monsterRight &#123; return true &#125; // 右下角 x, y = bullet.x+float64(bullet.width), bullet.y+float64(bullet.height) if y &gt; monsterTop &amp;&amp; y &lt; monsterBottom &amp;&amp; x &gt; monsterLeft &amp;&amp; x &lt; monsterRight &#123; return true &#125; return false&#125; 接着我们在Game.Update方法中调用这个方法，并且将碰撞的子弹和怪物删除。 12345678910111213141516171819func (g *Game) CheckCollision() &#123; for monster := range g.monsters &#123; for bullet := range g.bullets &#123; if CheckCollision(bullet, monster) &#123; delete(g.monsters, monster) delete(g.bullets, bullet) &#125; &#125; &#125;&#125;func (g *Game) Update() error &#123; // -------省略------- g.CheckCollision() // -------省略------- return nil&#125; 注意将碰撞检测放在位置更新之后。 新增游戏界面现在一旦运行程序，怪物们就开始运动了。我们想要增加一个按下空格键才开始的功能，并且游戏结束之后，我们也希望能显示一个 Game Over 的界面。首先，我们定义几个常量，表示游戏当前所处的状态： 123456type Mode intconst ( ModeTitle Mode = iota ModeGame ModeOver) Game 结构中需要增加 mode 字段表示当前游戏所处的状态： 1234type Game struct &#123; mode Mode // ...&#125; 为了显示开始界面，涉及到文字的显示，文字显示和字体处理起来都比较麻烦。ebitengine 内置了一些字体，我们可以据此创建几个字体对象： 12345678910111213141516171819202122232425262728293031323334353637var ( titleArcadeFont font.Face arcadeFont font.Face smallArcadeFont font.Face)func (g *Game) CreateFonts() &#123; tt, err := opentype.Parse(fonts.PressStart2P_ttf) if err != nil &#123; log.Fatal(err) &#125; const dpi = 72 titleArcadeFont, err = opentype.NewFace(tt, &amp;opentype.FaceOptions&#123; Size: float64(g.cfg.TitleFontSize), DPI: dpi, Hinting: font.HintingFull, &#125;) if err != nil &#123; log.Fatal(err) &#125; arcadeFont, err = opentype.NewFace(tt, &amp;opentype.FaceOptions&#123; Size: float64(g.cfg.FontSize), DPI: dpi, Hinting: font.HintingFull, &#125;) if err != nil &#123; log.Fatal(err) &#125; smallArcadeFont, err = opentype.NewFace(tt, &amp;opentype.FaceOptions&#123; Size: float64(g.cfg.SmallFontSize), DPI: dpi, Hinting: font.HintingFull, &#125;) if err != nil &#123; log.Fatal(err) &#125;&#125; fonts.PressStart2P_ttf 就是 ebitengine 提供的字体。创建字体的方法一般在需要的时候微调即可。将创建怪物和字体封装在 Game 的 init 方法中： 12345678910func (g *Game) init() &#123; g.CreateMonsters() g.CreateFonts()&#125;func NewGame() *Game &#123; // ... g.init() return g&#125; 启动时游戏处于 ModeTitle 状态，处于 ModeTitle 和 ModeOver 时只需要在屏幕上显示一些文字即可。只有在 ModeGame 状态才需要显示飞船和怪物： 123456789101112131415161718192021222324252627282930func (g *Game) Draw(screen *ebiten.Image) &#123; screen.Fill(g.cfg.BgColor) var titleTexts []string var texts []string switch g.mode &#123; case ModeTitle: titleTexts = []string&#123;&quot;ALIEN INVASION&quot;&#125; texts = []string&#123;&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;PRESS SPACE KEY&quot;, &quot;&quot;, &quot;OR LEFT MOUSE&quot;&#125; case ModeGame: g.ship.Draw(screen) for bullet := range g.bullets &#123; bullet.Draw(screen) &#125; for monster := range g.monsters &#123; monster.Draw(screen) &#125; case ModeOver: texts = []string&#123;&quot;&quot;, &quot;GAME OVER!&quot;&#125; &#125; for i, l := range titleTexts &#123; x := (g.cfg.ScreenWidth - len(l)*g.cfg.TitleFontSize) / 2 text.Draw(screen, l, titleArcadeFont, x, (i+4)*g.cfg.TitleFontSize, color.White) &#125; for i, l := range texts &#123; x := (g.cfg.ScreenWidth - len(l)*g.cfg.FontSize) / 2 text.Draw(screen, l, arcadeFont, x, (i+4)*g.cfg.FontSize, color.White) &#125;&#125; 在Game.Update方法中，我们判断在 ModeTitle 状态下按下空格，鼠标左键游戏开始，切换为 ModeGame 状态。游戏结束时切换为 GameOver 状态，在 GameOver 状态后按下空格或鼠标左键即重新开始游戏。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081func (g *Game) Update() error &#123; switch g.mode &#123; case ModeTitle: // 左键 或 鼠标左键 开始游戏 if ebiten.IsKeyPressed(ebiten.KeySpace) || ebiten.IsMouseButtonPressed(ebiten.MouseButtonLeft) &#123; g.mode = ModeGame &#125; case ModeGame: for bullet := range g.bullets &#123; bullet.y -= bullet.speedFactor &#125; for monster := range g.monsters &#123; monster.y += monster.speedFactor &#125; if ebiten.IsKeyPressed(ebiten.KeyLeft) &#123; g.ship.x -= g.cfg.ShipSpeedFactor if g.ship.x &lt; -float64(g.ship.width)/2 &#123; g.ship.x = -float64(g.ship.width) / 2 &#125; &#125; else if ebiten.IsKeyPressed(ebiten.KeyRight) &#123; g.ship.x += g.cfg.ShipSpeedFactor if g.ship.x &gt; float64(g.cfg.ScreenWidth)-float64(g.ship.width)/2 &#123; g.ship.x = float64(g.cfg.ScreenWidth) - float64(g.ship.width)/2 &#125; &#125; if ebiten.IsKeyPressed(ebiten.KeySpace) &#123; if len(g.bullets) &lt; g.cfg.MaxBulletNum &amp;&amp; time.Now().Sub(g.ship.lastBulletTime).Milliseconds() &gt; g.cfg.BulletInterval &#123; bullet := NewBullet(g.cfg, g.ship) g.addBullet(bullet) g.ship.lastBulletTime = time.Now() &#125; &#125; g.CheckCollision() for bullet := range g.bullets &#123; if bullet.outOfScreen() &#123; delete(g.bullets, bullet) &#125; &#125; for monster := range g.monsters &#123; if monster.outOfScreen(g.cfg) &#123; g.failCount++ delete(g.monsters, monster) continue &#125; if CheckCollision(g.ship, monster) &#123; log.Print(&quot;---- 飞船碰撞怪物 ----&quot;) g.failCount++ delete(g.monsters, monster) continue &#125; &#125; if g.failCount &gt;= g.cfg.FailCount &#123; g.overMsg = &quot;Game Over!&quot; &#125; else if len(g.monsters) == 0 &#123; g.overMsg = &quot;You Win!&quot; &#125; if len(g.overMsg) &gt; 0 &#123; g.mode = ModeOver g.monsters = make(map[*Monster]struct&#123;&#125;) g.bullets = make(map[*Bullet]struct&#123;&#125;) &#125; case ModeOver: if ebiten.IsKeyPressed(ebiten.KeySpace) || ebiten.IsMouseButtonPressed(ebiten.MouseButtonLeft) &#123; g.init() g.mode = ModeTitle &#125; &#125; return nil&#125; 新增判断游戏胜负逻辑我们规定如果击杀一定怪物则游戏胜利，有 failCount 个怪物移出屏幕外或者碰撞到飞船则游戏失败。 首先增加一个字段failCount用于记录移出屏幕外的怪物数量和与飞船碰撞的怪物数量之和： 1234type Game struct &#123; // -------省略------- failCount int // 被怪物碰撞和移出屏幕的怪物数量之和&#125; 然后我们在Game.Update方法中检测怪物是否移出屏幕，以及是否与飞船碰撞： 12345678910111213for monster := range g.monsters &#123; if monster.outOfScreen(g.cfg) &#123; g.failCount++ delete(g.monsters, monster) continue &#125; if CheckCollision(monster, g.ship) &#123; g.failCount++ delete(g.monsters, monster) continue &#125;&#125; 这里有一个问题，还记得吗？我们前面编写的CheckCollision函数接受的参数类型是*Monster和*Bullet，这里我们需要重复编写接受参数为*Ship和*Monster的函数吗？不用！ 我们将表示游戏中的实体对象抽象成一个GameObject结构： 12345678910111213141516171819202122type GameObject struct &#123; width int height int x float64 y float64&#125;func (gameObj *GameObject) Width() int &#123; return gameObj.width&#125;func (gameObj *GameObject) Height() int &#123; return gameObj.height&#125;func (gameObj *GameObject) X() float64 &#123; return gameObj.x&#125;func (gameObj *GameObject) Y() float64 &#123; return gameObj.y&#125; 然后定义一个接口Entity： 123456type Entity interface &#123; Width() int Height() int X() float64 Y() float64&#125; 最后让我们游戏中的实体内嵌这个GameObject对象，即可自动实现Entity接口： 12345type Monster struct &#123; GameObject image *ebiten.Image speedFactor float64&#125; 这样CheckCollision即可改为接受两个Entity接口类型的参数： 1234567891011121314151617181920212223242526272829303132// CheckCollision 检查两个物体之间是否有碰撞func CheckCollision(entity1, entity2 Entity) bool &#123; // ps: 这里判断时需要注意两个实体的大小，小的在前，大的在后 // ps：判断逻辑是以大实体框定范围，判断小实体是否在这个范围内。（子弹可以在怪物体内，但是怪物不一定在子弹体内） top, left := entity1.Y(), entity1.X() bottom, right := entity1.Y()+float64(entity1.Height()), entity1.X()+float64(entity1.Width()) // 左上角 x, y := entity2.X(), entity2.Y() if y &gt; top &amp;&amp; y &lt; bottom &amp;&amp; x &gt; left &amp;&amp; x &lt; right &#123; return true &#125; // 右上角 x, y = entity2.X()+float64(entity2.Width()), entity2.Y() if y &gt; top &amp;&amp; y &lt; bottom &amp;&amp; x &gt; left &amp;&amp; x &lt; right &#123; return true &#125; // 左下角 x, y = entity2.X(), entity2.Y()+float64(entity2.Height()) if y &gt; top &amp;&amp; y &lt; bottom &amp;&amp; x &gt; left &amp;&amp; x &lt; right &#123; return true &#125; // 右下角 x, y = entity2.X()+float64(entity2.Width()), entity2.Y()+float64(entity2.Height()) if y &gt; top &amp;&amp; y &lt; bottom &amp;&amp; x &gt; left &amp;&amp; x &lt; right &#123; return true &#125; return false&#125; 如果游戏失败则切换为 ModeOver 模式，屏幕上显示 “Game Over!”。如果游戏胜利，则显示 “You Win!”： 1234567891011if g.failCount &gt;= g.cfg.FailCount &#123; g.overMsg = &quot;Game Over!&quot;&#125; else if len(g.monsters) == 0 &#123; g.overMsg = &quot;You Win!&quot;&#125;if len(g.overMsg) &gt; 0 &#123; g.mode = ModeOver g.monsters = make(map[*Monster]struct&#123;&#125;) g.bullets = make(map[*Bullet]struct&#123;&#125;)&#125; 注意，为了下次游戏能顺序进行，这里需要清除所有的子弹和怪物。运行： 新增记分板在游戏界面中显示分数（击杀怪物数量 ） 在 Game 中新增分数 1234type Game struct &#123; // ... score int // 击杀怪物数量&#125; 击杀怪物时，分数增加 123456789101112func (g *Game) CheckCollision() &#123; for monster := range g.monsters &#123; for bullet := range g.bullets &#123; if CheckCollision(monster, bullet) &#123; log.Print(&quot;---- 子弹击中怪物 ----&quot;) g.score++ delete(g.monsters, monster) delete(g.bullets, bullet) &#125; &#125; &#125;&#125; 在屏幕中，将分数显示出来。 1234567891011121314151617181920func (g *Game) Draw(screen *ebiten.Image) &#123; // ... switch g.mode &#123; case ModeTitle: // ... case ModeGame: // ... scoreStr := strings.Builder&#123;&#125; scoreStr.WriteString(&quot;scores: &quot;) scoreStr.WriteString(strconv.Itoa(g.score)) // print the score ebitenutil.DebugPrintAt(screen, scoreStr.String(), 0, g.cfg.ScreenHeight-20) case ModeOver: // ... &#125; // ...&#125; 总结至此一个简答的游戏就做出来了。可以看出使用 ebitengine 做一个游戏还是很简单的，非常推荐尝试呢！上手之后，建议看看官方仓库 examples 目录中的示例，会非常有帮助。 大家可以亲自上手尝试一下，如果想要本地调试，可参考如下项目 如果想要将项目打包并在网页中运行，可参考 大俊 的一起用Go来做一个游戏(下)","tags":["Go","Ebitengine","游戏"],"categories":["Go"]},{"title":"Ebitengine - 一款Go语言编写的2D游戏引擎","path":"//blog/go/ebitengine/","content":"简介最近使用 Go 写项目的过程中，接触到 labuladong 大佬曾经实现的一款用消息队列做的联机游戏 Play with Apache Pulsar 项目中使用到了 Ebitengine 为了更好的了解项目，先简单了解下 Ebitengine 官网上展示了几款 Ebitengine 实现的小游戏，在线即可体验（利用 WASM 技术） 更多 Ebiten 游戏：30+ More Examples 当然只要安装了 Go，我们也键入下面的命令本地运行这个游戏(或者本地使用 GoLand 启动)： 1$ go run -tags=example github.com/hajimehoshi/ebiten/v2/examples/2048@latest 这些瞬间让我产生了极大的兴趣。简单浏览一下文档，整体感觉下来，虽然与成熟的游戏引擎（如 Cocos2dx，DirectX，Unity3d 等）相比，ebiten 功能还不算丰富。但是麻雀虽小，五脏俱全。ebiten 的 API 设计比较简单，使用也很方便，即使对于新手也可以在 1-2 个小时内掌握，并开发出一款简单的游戏。更妙的是，Go 语言让 ebitengine 实现了跨平台！ 下面体验一下 ebitengine 的基础功能。 安装ebitengine 要求 Go 版本 &gt;&#x3D; 1.15。使用 go module 下载这个包： 1$ go get -u github.com/hajimehoshi/ebiten/v2 生成游戏窗口游戏开发第一步是将游戏窗口显示出来，并且能在窗口上显示一些文字。先看代码： 12345678910111213141516171819202122232425262728293031package mainimport ( &quot;log&quot; &quot;github.com/hajimehoshi/ebiten/v2&quot; &quot;github.com/hajimehoshi/ebiten/v2/ebitenutil&quot;)// 仅用来展示窗口，暂不定义游戏数据type Game struct&#123;&#125;func (g *Game) Update() error &#123; return nil&#125;func (g *Game) Draw(screen *ebiten.Image) &#123; ebitenutil.DebugPrint(screen, &quot;Hello, World&quot;)&#125;func (g *Game) Layout(outsideWidth, outsideHeight int) (screenWidth, screenHeight int) &#123; return 100, 100&#125;func main() &#123; ebiten.SetWindowSize(200, 200) ebiten.SetWindowTitle(&quot;生成游戏窗口&quot;) if err := ebiten.RunGame(&amp;Game&#123;&#125;); err != nil &#123; log.Fatal(err) &#125;&#125; 使用命令go run运行该程序： 我们会看到一个窗口，标题为生成游戏窗口，并且显示了文字 Hello，World： 现在我们来分析使用 ebiten 开发的游戏程序的结构。 首先，ebiten 引擎运行时要求传入一个游戏对象，该对象的必须实现ebiten.Game这个接口： 12345678910// Game defines necessary functions for a game.type Game interface &#123; // 在 Update 函数里填写数据更新的逻辑 Update() error // 在 Draw 函数里填写图像渲染的逻辑 Draw(screen *Image) Layout(outsideWidth, outsideHeight int) (screenWidth, screenHeight int)&#125; 我们知道显示器能够显示动态影像的原理其实就是快速的刷新一帧一帧的图像，肉眼看起来就好像是动态影像了。 在每一帧图像刷新之前，这个游戏框架会先调用 Update 方法更新游戏数据，再调用 Draw 方法渲染出每一帧图像，这样就能够制作出简单的 2D 小游戏了。 ebiten.Game接口定义了 ebiten 游戏需要的 3 个方法：Update,Draw和Layout。 Update：每个 tick 都会被调用。tick 是引擎更新的一个时间单位，默认为 1&#x2F;60s。tick 的倒数我们一般称为帧，即游戏的更新频率。默认 ebiten 游戏是 60 帧，即每秒更新 60 次。该方法主要用来更新游戏的逻辑状态，例如子弹位置更新。上面的例子中，游戏对象没有任何状态，故Update方法为空。注意到Update方法的返回值为error类型，当Update方法返回一个非空的error值时，游戏停止。在上面的例子中，我们一直返回 nil，故只有关闭窗口时游戏才停止。 Draw：每帧（frame）调用。帧是渲染使用的一个时间单位，依赖显示器的刷新率。如果显示器的刷新率为 60Hz，Draw将会每秒被调用 60 次。Draw接受一个类型为*ebiten.Image的 screen 对象。ebiten 引擎每帧会渲染这个 screen。在上面的例子中，我们调用ebitenutil.DebugPrint函数在 screen 上渲染一条调试信息。由于调用Draw方法前，screen 会被重置，故DebugPrint每次都需要调用。 Layout：该方法接收游戏窗口的尺寸作为参数，返回游戏的逻辑屏幕大小。我们实际上计算坐标是对应这个逻辑屏幕的，Draw将逻辑屏幕渲染到实际窗口上。这个时候可能会出现伸缩。在上面的例子中游戏窗口大小为 (200, 200)，Layout返回的逻辑大小为 (100, 100)，所以显示会放大 1 倍。 在 main 函数中， 1ebiten.SetWindowSize(200, 200) 设置游戏窗口的大小。 而 1ebiten.SetWindowTitle(&quot;生成游戏窗口&quot;) 设置窗口标题，标题显示在窗口的左上角。 一切准备就绪，创建一个 Game 对象，调用ebiten.RunGame()运行。是不是很简单？ 响应键盘输入没有交互的游戏不是真的游戏！下面我们来监听键盘的输入，当前只处理 ⬆️ ⬇️ ⬅️ ➡️ 四类。 ebiten 提供函数 IsKeyPressed 来判断某个键是否按下，同时内置了 100 多个键的常量定义，见源码 keys.go 文件。ebiten.KeyLeft表示左方向键，ebiten.KeyRight表示右方向键，ebiten.KeySpace表示空格。 ebiten 提供函数 inpututil.IsKeyJustPressed 来监听刚按下的键。 因此我们可以在 Update 函数中添加如下逻辑 1234567891011121314func (g *Game) Update() error &#123; if inpututil.IsKeyJustPressed(ebiten.KeyArrowLeft) || inpututil.IsKeyJustPressed(ebiten.KeyA) &#123; fmt.Println(&quot;左键 ⬅️&quot;) &#125; else if inpututil.IsKeyJustPressed(ebiten.KeyArrowRight) || inpututil.IsKeyJustPressed(ebiten.KeyD) &#123; fmt.Println(&quot;右键 ➡️️&quot;) &#125; else if inpututil.IsKeyJustPressed(ebiten.KeyArrowDown) || inpututil.IsKeyJustPressed(ebiten.KeyS) &#123; fmt.Println(&quot;下键 ⬇️️&quot;) &#125; else if inpututil.IsKeyJustPressed(ebiten.KeyArrowUp) || inpututil.IsKeyJustPressed(ebiten.KeyW) &#123; fmt.Println(&quot;上键 ⬆️️&quot;) &#125; else &#123; // ... 暂不处理 &#125; return nil&#125; 使用go run命令运行： 窗口与前一个例子相同，然而我们可以在窗口上按 ⬆️ ⬇️ ⬅️ ➡️ and W S A D，观察控制台输出： 设置背景黑色背景看起来有些无趣，我们现在就来换一个背景。 1234func (g *Game) Draw(screen *ebiten.Image) &#123; screen.Fill(color.RGBA&#123;R: 200, G: 200, B: 200, A: 255&#125;) ebitenutil.DebugPrint(screen, g.input.msg)&#125; ebiten.Image定义了一个名为Fill的方法，可以传入一个颜色对象color.RGBA，将背景填充为特定颜色。Draw函数的参数为*ebiten.Image类型，它表示的是屏幕对象，ebitengine 引擎最终会将 screen 显示出来，故填充它的背景即可修改窗口的背景。代码中我们将背景颜色修改为灰色 (R:200,G:200,B:200)。 注意：由于每帧都会调用Draw方法刷新屏幕内容，所以每次调用都需要填充背景。 运行结果如下： 在屏幕中显示图片接下来我们尝试在屏幕中显示一张飞船的图片： ebitengine 引擎提供了ebitenutil.NewImageFromFile函数，传入图片路径即可加载该图片，so easy。为了很好的管理游戏中的各个实体，我们给每个实体都定义一个结构。先定义飞船结构： 123456789101112131415161718192021type Ship struct &#123; image *ebiten.Image width int height int&#125;func NewShip() *Ship &#123; img, _, err := ebitenutil.NewImageFromFile(&quot;../images/ship.png&quot;) if err != nil &#123; log.Fatal(err) &#125; width, height := img.Size() ship := &amp;Ship&#123; image: img, width: width, height: height, &#125; return ship&#125; 我提供了两种图片格式，一种是 png，一种是 bmp，用哪种都可以。注意，需要将对应的图片解码包导入。Go 标准库提供了三种格式的解码包，image/png，image/jpeg，image/gif。也就是说标准库中没有 bmp 格式的解码包，所幸 golang.org&#x2F;x 仓库没有让我们失望，golang.org&#x2F;x&#x2F;image&#x2F;bmp 提供了解析 bmp 格式图片的功能。我们这里不需要显式的使用对应的图片库，故使用import _这种方式，让init函数产生副作用。 然后在游戏对象中添加飞船类型的字段： 12345func NewGame() *Game &#123; return &amp;Game &#123; ship: NewShip(), &#125;&#125; 为了在屏幕上显示飞船图片，我们需要调用*ebiten.Image的DrawImage方法，该方法的第二个参数可以用于指定坐标相对于原点的偏移： 123456func (g *Game) Draw(screen *ebiten.Image) &#123; screen.Fill(g.cfg.BgColor) op := &amp;ebiten.DrawImageOptions&#123;&#125; op.GeoM.Translate(float64(g.cfg.ScreenWidth-g.ship.width)/2, float64(g.cfg.ScreenHeight-g.ship.height)) screen.DrawImage(g.ship.image, op)&#125; 我们给Ship类型增加一个绘制自身的方法，传入屏幕对象 screen 和配置，让代码更好维护： 12345func (ship *Ship) Draw(screen *ebiten.Image) &#123; op := &amp;ebiten.DrawImageOptions&#123;&#125; op.GeoM.Translate(float64(screenWidth-ship.width)/2, float64(screenHeight-ship.height)) screen.DrawImage(ship.image, op)&#125; 这样游戏对象中的Draw方法就可以简化为： 1234func (g *Game) Draw(screen *ebiten.Image) &#123; screen.Fill(g.cfg.BgColor) g.ship.Draw(screen)&#125; 运行： 移动图片现在我们来实现使用左右方向键来控制飞船的移动。首先给飞船的类型增加 x&#x2F;y 坐标字段： 12345type Ship struct &#123; // 与前面的代码一样 x float64 // x坐标 y float64 // y坐标&#125; 我们前面已经计算出飞船位于屏幕中心时的坐标，在创建飞船时将该坐标赋给 xy： 123456789func NewShip(screenWidth, screenHeight int) *Ship &#123; ship := &amp;Ship&#123; // ... x: float64(screenWidth-width) / 2, y: float64(screenHeight - height) / 2, &#125; return ship&#125; 然后我们在 Update 方法中根据按下的是左方向键还是右方向键来更新飞船的坐标： 123456789101112131415161718func (g *Game) Update() error &#123; if inpututil.IsKeyJustPressed(ebiten.KeyArrowLeft) || inpututil.IsKeyJustPressed(ebiten.KeyA) &#123; fmt.Println(&quot;左键 ⬅️&quot;) g.ship.x -= 10 // 移动范围太小不容易出效果 &#125; else if inpututil.IsKeyJustPressed(ebiten.KeyArrowRight) || inpututil.IsKeyJustPressed(ebiten.KeyD) &#123; fmt.Println(&quot;右键 ➡️️&quot;) g.ship.x += 10 &#125; else if inpututil.IsKeyJustPressed(ebiten.KeyArrowDown) || inpututil.IsKeyJustPressed(ebiten.KeyS) &#123; fmt.Println(&quot;下键 ⬇️️&quot;) g.ship.y -= 10 &#125; else if inpututil.IsKeyJustPressed(ebiten.KeyArrowUp) || inpututil.IsKeyJustPressed(ebiten.KeyW) &#123; fmt.Println(&quot;上键 ⬆️️&quot;) g.ship.y += 10 &#125; else &#123; // ... 暂不处理 &#125; return nil&#125; 好了，现在可以运行程序了go run .，效果如下： 总结以上是 2D 游戏开发库 ebiten 的基本使用。 对于游戏引擎来说，只介绍它的 API 用法似乎有点纸上谈兵。恰好我想起之前看到一个《射击游戏》的小游戏，刚好可以拿来练手。","tags":["Go","Ebitengine"],"categories":["Go"]},{"title":"聊聊 Kubernetes","path":"//blog/cloud_native/kubernetes/","content":"1. 什么是 kubernetesKubernetes，它是一个全新的基于容器技术的分布式架构方案，近些年在容器领域使用非常广泛，作为容器化部署实施的典型方案。 Kubernetes 是用于自动部署，扩展和管理容器化应用程序的开源系统，它将组成应用程序的容器组合成逻辑单元，以便于管理和服务发现。 Kubernetes，构建在 Docker 技术之上，为跨主机的容器化应用提供资源调度、服务发现、高可用管理和弹性伸缩等一整套功能，它提供了完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。它的目标不仅仅是一个编排系统，更是提供一个规范，可以让你来描述集群的架构，定义服务的最终状态，Kubernetes可以帮你将系统自动达到和维持在这个状态。 2. 特性 服务发现与负载均衡： 无需修改您的应用程序即可使用陌生的服务发现机制。Kubernetes 为容器提供了自己的 IP 地址和一个 DNS 名称，并且可以在它们之间实现负载均衡。 自我修复： 重新启动失败的容器，在节点死亡时替换并重新调度容器，杀死不响应用户定义的健康检查的容器，并且在它们准备好服务之前不会将它们公布给客户端。 自动化上线和回滚： Kubernetes 会分步骤地将针对应用或其配置的更改上线，同时监视应用程序运行状况以确保你不会同时终止所有实例。如果出现问题，Kubernetes 会为你回滚所作更改。你应该充分利用不断成长的部署方案生态系统。 自动装箱： 根据资源需求和其他约束自动放置容器，同时避免影响可用性。将关键性工作负载和尽力而为性质的服务工作负载进行混合放置，以提高资源利用率并节省更多资源。 IPv4&#x2F;IPv6 双协议栈： 为 Pod 和 Service 分配 IPv4 和 IPv6 地址。 水平扩缩： 使用一个简单的命令、一个 UI 或基于 CPU 使用情况自动对应用程序进行扩缩。 Service 拓扑： 基于集群拓扑的服务流量路由。可以让一个服务基于集群的Node拓扑进行流量路由。例如，一个服务可以指定流量是被优先路由到一个和客户端在同一个Node或者在同一可用区域的端点。 端点切片： Kubernetes 集群中网络端点的可扩展跟踪。 存储编排： 自动挂载所选存储系统，包括本地存储、诸如 GCP 或 AWS 之类公有云提供商所提供的存储或者诸如 NFS、iSCSI、Gluster、Ceph、Cinder 或 Flocker 这类网络存储系统。 Secret 和配置管理： 部署和更新 Secrets 和应用程序的配置而不必重新构建容器镜像，且不必将软件堆栈配置中的秘密信息暴露出来。 批量执行： 除了服务之外，Kubernetes 还可以管理你的批处理和 CI 工作负载，在期望时替换掉失效的容器。 3. 亮点3.1 一切以服务（Service）为中心Kubernetes以“一切以服务（Service）为中心，一切围绕服务运转”作为指导思想的创新型产品。 它在功能和架构设计上始终遵循着这一指导思想，构建在Kubernetes上的系统不仅可以独立运行在物理机、虚拟机集群或企业私有云上，也可以被托管在公有云上。 3.2 开放的开发平台Kubernetes是一个开放的开发平台。与 J2EE 不同，它不局限于任何一种语言，没有限定任何编程接口，所以不论是用 Java、Go、C++还是 Python 编写的程序，都可以被映射为Kubernetes的 Service，并通过标准的 TCP 通讯协议进行交互。此外，Kubernetes平台对现有的编程语言、编程框架、中间件没有任何侵入性，做到了零侵入，因此现有的系统也很容易改造升级并迁移到Kubernetes平台之上。 3.3 自动化Kubernetes的另一个亮点是自动化。在Kubernetes的解决方案中，一个可以自我扩展、自我诊断，并且容易升级，在收到服务扩容的请求后，Kubernetes会触发调度流程，最终在选定的目标节点上启动相应数据的服务实例副本，这些服务实例副本在启动成功后会自动加入负载均衡器中并生效，整个过程无须额外的人工操作。 另外，Kubernetes会定时巡查每个服务的所有实例的可用性，确保服务实例的数量始终保持为预期的数量，当它发现某个实例不可用时，会自动重启该实例或者其他节点上重新调度、运行一个新实例，这样一个复杂的过程无须人工干预即可全部自动完成。 3.4 分布式系统支撑平台Kubernetes是一个完备的分布式系统支撑平台。具备完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建的智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。 同时，Kubernetes提供了完善的管理工具，这些涵盖了包括开发、部署测试、运维监控在内的各个环节。因此，Kubernetes是一个全新的基于容器技术的分布式架构解决方案，并且是一个一站式的完备的分布式系统开发和支持平台。 随着容器化部署环境限制、语言差异、容器数量的庞大、负载均衡、故障检测、故障修复等问题，倘若将过多的精力、时间放在这些地方，其工作量将会多大，将会让很多企业、产品对容器望而止步。 在容器化的时代，Kubernetes足以免去上述面临的问题，让容器化使用变得的更加容易、轻松，只需花费更多的时间去完成业务功能的开发。 4. 为什么使用 kubernetes使用Kubernetes的理由很多，最重要的理由是，IT 行业从来都是由新技术驱动的。 4.1 一个平台搞定所有使用 Kubernetes部署任何应用都是小菜一碟。只要应用可以打包成镜像，能够容器部署，Kubernetes就一定能启动它。 不管什么语言、什么框架写的应用（如：Java, Python, Node.js），Kubernetes都可以在任何环境中安全的启动它，如：物理服务器、虚拟机、云环境。 4.2 云环境无缝迁移如果你有换云环境的需求，例如从 GCP 到 AWS，使用Kubernetes的话，你就不用有任何担心。 Kubernetes完全兼容各种云服务提供商，例如 Google Cloud、Amazon、Microsoft Azure，还可以工作在 CloudStack, OpenStack, OVirt, Photon, VSphere等。 4.3 高效的利用资源看下图，左边是 4 个虚拟机，黄色和蓝色部分是运行的应用，白色部分是未使用的内存和处理器资源。 右边，同样的应用打包运行在容器中。 Kubernetes如果发现有节点工作不饱和，便会重新分配pod，帮助我们节省开销，高效的利用内存、处理器等资源。 如果一个节点宕机了，Kubernetes会自动重新创建之前运行在此节点上的pod，在其他节点上运行。 4.4 开箱即用的自动缩放能力网络、负载均衡、复制等特性，对于Kubernetes都是开箱即用的。 pod 是无状态运行的，任何时候有 pod 宕了，立马会有其他 pod 接替它的工作，用户完全感觉不到。 如果用户量突然暴增，现有的 pod 规模不足了，那么会自动创建出一批新的 pod，以适应当前的需求。 反之亦然，当负载降下来的时候，Kubernetes也会自动缩减 pod 的数量。 4.5 使 CI&#x2F;CD 更简单你不必精通于Chef 和 Ansible这类工具，只需要对 CI 服务写个简单的脚本然后运行它，就会使用你的代码创建一个新的 pod，并部署到 Kubernetes集群里面。 应用打包在容器中使其可以安全的运行在任何地方，例如你的 PC、一个云服务器，使得测试极其简单。 4.6 可靠性Kubernetes如此流行的一个重要原因是：应用会一直顺利运行，不会被 pod 或节点的故障所中断。 如果出现故障，Kubernetes会创建必要数量的应用镜像，并分配到健康的 pod 或节点中，直到系统恢复。 而且用户不会感到任何不适。 一个容器化的基础设施是有自愈能力的，可以提供应用程序的不间断操作，即使一部分基础设施出现故障。 kubernetes 组件当你部署完 Kubernetes，便拥有了一个完整的集群。 一组工作机器，称为 节点， 会运行容器化应用程序。每个集群至少有一个工作节点。 工作节点会托管 Pod ，而 Pod 就是作为应用负载的组件。 控制平面管理集群中的工作节点和 Pod。 在生产环境中，控制平面通常跨多台计算机运行， 一个集群通常运行多个节点，提供容错性和高可用性。 更详细的介绍参考：Kubernetes 组件","tags":["云原生","kubernetes"],"categories":["云原生","kubernetes"]},{"title":"何谓云原生？","path":"//blog/cloud_native/cloud_native/","content":"什么是云原生不同的企业对于云原生有不同的解释，当前在业界具有广泛影响力的云原生计算基金会（Cloud Native Computing Foundation,CNCF）认为，云原生是一类技术的统称，通过云原生技术，我们可以构建出更易于弹性扩展的应用程序，这些应用可以被运行在不同的环境当中，比如说私有云、公有云、混合云、还有多云的场景。 通过云原生技术构建出来的应用程序，称之为云原生应用，底层基础架构的耦合比较轻，因此易于迁移，它可以充分地利用云所提供的能力，因此云原生应用的开发、部署、管理相对于传统的应用程序更加高效和便捷。 云原生涉及到许多技术领域，每一个技术领域都有相应的工具、框架与平台，来帮助落地具体的应用。 云原生主要包含了当前业界的一些热门的技术，比如容器、微服务、DevOps，服务网格、Serverless、API管理、不可变基础架构等。 CNCF维护了一个 云原生技术全景图，在其中收集了和云原生技术相关的工具、平台和项目，全景图的内容十分丰富，可谓种类繁多、琳琅满目。通过这个云原生全景图可以快速地了解到每一个技术领域当中流行的工具。 云原生的作用对于应用开发团队而言，原来云原生技术可以提升应用开发的效率，提升应用交付的质量。比如通过容器，技术开发团队可以更容易地获取开发所需要的环境与资源，开发出来的应用可以被运维团队更容易地部署和管理。通过DevOps的最佳实践，应用交付的速度和质量可以被有效的提升。 对于业务方来说，云原生的好处是所提交的需求，可以更快地被响应和实现。因为云原生技术可以有效地缩短应用交付的周期，让需求更快地变成代码，代码更快地变成线上的应用，最终为用户服务，实现价值。云原生应用可以更好地弹性扩展，满足不同业务的需求。例如容器应用提供的应用自愈能力，可以帮助减少应用的停机时间提升用户的体验。 云原生技术可以提升应用开发的交付效率，缩短应用上线所需要的时间，开发和业务团队人员可以有更多的时间和精力进行业务创新，有效地提升团队的创新能力，从而提升企业在市场的竞争能力。 如何使用云原生当一个企业拥抱云原生技术，具体要在什么方面来落实？ CNCF有一个建议的技术路线图（CNCF trail map）。这个图上列出了10个方面，比如说通过应用容器化，使得应用更易于迁移的交付，通过持续集成的区域部署提升云原生软件的质量，通过容器编排简化应用的部署。","tags":["云原生"],"categories":["云原生"]},{"title":"Go 系列(二)：context 源码分析","path":"//blog/go/context/","content":"本文主要简单介绍了 Go 语言 (golang) 中的context包。给出了 context 的基本用法和使用建议，并从源码层面对其底层结构和具体实现原理进行分析。 以下分析基于 Go 1.17.1 1. 概述1.1 什么是 context上下文 context.Context在 Go 语言中用来设置截止日期、同步信号，传递请求相关值的结构体。上下文与 Goroutine 有比较密切的关系，是 Go 语言中独特的设计，在其他编程语言中我们很少见到类似的概念。 context 用来解决 goroutine 之间退出通知、数据传递的功能。 注：这里的数据传递主要指全局数据，如 链路追踪里的 traceId 之类的数据，并不是普通的参数传递 (也非常不推荐用来传递参数)。 1.2 设计原理因为context.Context主要作用就是进行超时控制，然后外部程序监听到超时后就可以停止执行任务，取消 Goroutine。 网上有很多用 Context 来取消 Goroutine 的字眼，初学者 (比如笔者) 可能误会，以为 Context 可以直接取消 Goroutine。 实际，Context 只是完成了一个信号的传递，具体的取消逻辑需要由程序自己监听这个信号，然后手动处理。 Go 语言中的 Context 通过构建一颗 Context 树，从而将没有层级的 Goroutine 关联起来。如下图所示： 所有 Context 都依赖于 BackgroundCtx 或者 TODOCtx，其实这二者都是一个 emptyCtx，只是语义上不一样。 在超时或者手动取消的时候信号都会从最顶层的 Goroutine 一层一层传递到最下层。这样该 Context 关联的所有 Goroutine 都能收到信号，然后进入自定义的退出逻辑。 比如这里手动取消了 ctxB1，然后 ctxB1 的两个子 ctx(C1 和 C2) 也会收到取消信号，这样 3 个 Goroutine 都能收到取消信号进行退出了。 1.3 使用场景最常见的就是 后台 HTTP&#x2F;RPC Server。 在 Go 的 server 里，通常每来一个请求都会启动若干个 goroutine 同时工作：有些去数据库拿数据，有些调用下游接口获取相关数据, 具体如下图： 而客户端一般不会无限制的等待，都会被请求设定超时时间，比如 100ms。 比如这里 GoroutineA 消耗 80ms，GoroutineB3 消耗 30ms，已经超时了，那么后续的 GoroutineCDEF 都没必要执行了，客户端已经超时返回了，服务端就算计算出结果也没有任何意义了。 所以这里就可以使用 Context 来在多个 Goroutine 之间进行超时信号传递。 同时引入超时控制后有两个好处： 1）客户端可以快速返回，提升用户体验 2）服务端可以减少无效的计算 2. 使用案例2.1 WithCancel返回一个可以手动取消的 Context，手动调用 cancel() 方法以取消该 context。 123456789101112131415161718192021222324252627282930313233343536// 启动一个 worker goroutine 一直产生随机数，知道找到满足条件的数时，手动调用 cancel 取消 ctx，让 worker goroutine 退出func main() &#123; rand.Seed(time.Now().UnixNano()) ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*100) // defer cancel() // 一般推荐 defer 中调用cancel() ret := make(chan int) go RandWithCancel(ctx, ret) for r := range ret &#123; // 当找到满足条件的数时就退出 if r ==20 &#123; fmt.Println(&quot;find r:&quot;, r) break &#125; &#125; cancel() // 这里测试就手动调用cancel() 取消context time.Sleep(time.Second) // sleep 等待 worker goroutine 退出&#125;func RandWithCancel(ctx context.Context, ret chan int) &#123; defer close(ret) timer := time.NewTimer(time.Millisecond) for &#123; select &#123; case &lt;-ctx.Done(): fmt.Println(&quot;ctx cancel&quot;) timer.Stop() return case &lt;-timer.C: r := rand.Intn(100) ret &lt;- r timer.Reset(time.Millisecond) &#125; &#125;&#125; 2.2 WithDeadline &amp; WithTimeout可以自定义超时时间，时间到了自动取消 context。 其实 WithTimeout 就是对 WithDeadline 的一个封装： 123func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) &#123; return WithDeadline(parent, time.Now().Add(timeout))&#125; 1234567891011121314151617181920212223242526272829303132333435// 启动一个 worker goroutine 一直产生随机数，直到 ctx 超时后退出func main() &#123; rand.Seed(time.Now().UnixNano()) // ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*100) ctx, cancel := context.WithDeadline(context.Background(), time.Now().Add(time.Millisecond*100)) // defer cancel() // 一般推荐 defer 中调用cancel() ret := make(chan int) go RandWithTimeout(ctx, ret) for r := range ret &#123; // 当找到满足条件的数时就退出 if r == 20 &#123; fmt.Println(&quot;find r:&quot;, r) break &#125; &#125; cancel() // 这里测试就手动调用cancel() 取消context time.Sleep(time.Second) // sleep 等待 worker goroutine 退出&#125;func RandWithTimeout(ctx context.Context, ret chan int) &#123; defer close(ret) timer := time.NewTimer(time.Millisecond) for &#123; select &#123; case &lt;-ctx.Done(): fmt.Println(&quot;ctx cancel&quot;) timer.Stop() return case &lt;-timer.C: r := rand.Intn(100) ret &lt;- r timer.Reset(time.Millisecond) &#125; &#125;&#125; 在这个案例中，因为限制了超时时间，所以并不是每次都能找到满足条件的 r 值。 2.3 WithValue可以传递数据的 context，携带关键信息，为全链路提供线索，比如接入 elk 等系统，需要来一个 trace_id，那 WithValue 就非常适合做这个事。 1234567891011121314151617181920212223242526272829303132333435// 通过 ctx 进行超时控制的同时，在 ctx 中存放 traceId 进行链路追踪。func main() &#123; withTimeout, cancel := context.WithTimeout(context.Background(), time.Millisecond*1) defer cancel() ctx := context.WithValue(withTimeout, &quot;traceId&quot;, &quot;id12345&quot;) r := f1(ctx) fmt.Println(&quot;r:&quot;, r)&#125;func f1(ctx context.Context) int &#123; fmt.Println(&quot;f1 traceId:&quot;, fromCtx(ctx)) var ret = make(chan int, 1) go f2(ctx, ret) r1 := rand.Intn(10) fmt.Println(&quot;r1:&quot;, r1) select &#123; case &lt;-ctx.Done(): return r1 case r2 := &lt;-ret: return r1 + r2 &#125;&#125;func f2(ctx context.Context, ret chan int) &#123; fmt.Println(&quot;f2 traceId:&quot;, fromCtx(ctx)) // sleep 模拟耗时逻辑 time.Sleep(time.Millisecond * 10) r2 := rand.Intn(10) fmt.Println(&quot;r2:&quot;, r2) ret &lt;- r2&#125;func fromCtx(ctx context.Context) string &#123; return ctx.Value(&quot;traceId&quot;).(string)&#125; 为了进行超时控制，本就需要在多个 goroutine 之前传递 ctx，所以把 traceId 这种信息存放到 ctx 中是非常方便的。 3. 源码分析Context 在 Go 1.7 版本引入标准库中，主要内容可以概括为： 1 个接口 Context 4 种实现 emptyCtx cancelCtx timerCtx valueCtx 6 个方法 Background TODO WithCancel WithDeadline WithTimeout WithValue 整体类图如下： 3.1 1 个接口12345678910111213type Context interface &#123; // 当 context 被取消或者到了 deadline，返回一个被关闭的 channel Done() &lt;-chan struct&#123;&#125; // 在 channel Done 关闭后，返回 context 取消原因 Err() error // 返回 context 是否会被取消以及自动取消时间（即 deadline） Deadline() (deadline time.Time, ok bool) // 获取 key 对应的 value Value(key interface&#123;&#125;) interface&#123;&#125;&#125; Context 是一个接口，定义了 4 个方法，它们都是幂等的。也就是说连续多次调用同一个方法，得到的结果都是相同的。 Done()： 返回一个 只读channel，可以表示 context 被取消的信号：当这个 channel 被关闭时，说明 context 被取消了。读一个关闭的 channel 会读出相应类型的零值。并且源码里没有地方会向这个 channel 里面塞入值。换句话说，这是一个 receive-only 的 channel。因此在子协程里读这个 channel，除非被关闭，否则读不出来任何东西。也正是利用了这一点，子协程从 channel 里读出了值（零值）后，就可以做一些收尾工作，尽快退出。 Err()： 返回一个错误，表示 channel 被关闭的原因。例如是被取消，还是超时。 Deadline()： 返回 context 的截止时间，通过此时间，函数就可以决定是否进行接下来的操作，如果时间太短，就可以不往下做了，否则浪费系统资源。当然，也可以用这个 deadline 来设置一个 I&#x2F;O 操作的超时时间。 Value(key)：返回 key 对应的 value，是协程安全的 同时包中也定义了提供 cancel 功能需要实现的接口。这个主要是后文会提到的 “取消信号、超时信号” 需要去实现。 123456// A canceler is a context type that can be canceled directly. The// implementations are *cancelCtx and *timerCtx.type canceler interface &#123; cancel(removeFromParent bool, err error) Done() &lt;-chan struct&#123;&#125;&#125; 实现了上面定义的两个方法的 Context，就表明该 Context 是可取消的。源码中有两个类型实现了 canceler 接口：*cancelCtx 和 *timerCtx。注意是加了 * 号的，是这两个结构体的指针实现了 canceler 接口。 Context 接口设计成这个样子的原因： “取消”操作应该是建议性，而非强制性 caller 不应该去关心、干涉 callee 的情况，决定如何以及何时 return 是 callee 的责任。caller 只需发送“取消”信息，callee 根据收到的信息来做进一步的决策，因此接口并没有定义 cancel 方法。 “取消”操作应该可传递 “取消”某个函数时，和它相关联的其他函数也应该“取消”。因此，Done() 方法返回一个只读的 channel，所有相关函数监听此 channel。一旦 channel 关闭，通过 channel 的“广播机制”，所有监听者都能收到。 3.2 4 种实现为了更方便的创建 Context，包里定义了 Background 来作为所有 Context 的根，它是一个 emptyCtx 的实例。 3.2.1 emptyCtx这也是最简单的一个 ctx 1234567891011121314151617type emptyCtx intfunc (*emptyCtx) Deadline() (deadline time.Time, ok bool) &#123; return&#125;func (*emptyCtx) Done() &lt;-chan struct&#123;&#125; &#123; return nil&#125;func (*emptyCtx) Err() error &#123; return nil&#125;func (*emptyCtx) Value(key interface&#123;&#125;) interface&#123;&#125; &#123; return nil&#125; 空方法实现了 context.Context 接口，它没有任何功能。 Background 和 TODO 这两个方法都会返回预先初始化好的私有变量 background 和 todo，它们会在同一个 Go 程序中被复用： 1234567891011var ( background = new(emptyCtx) todo = new(emptyCtx) )func Background() Context &#123; return background&#125;func TODO() Context &#123; return todo&#125; 从源代码来看，context.Background 和 context.TODO 和也只是互为别名，没有太大的差别，只是在使用和语义上稍有不同： context.Background 是上下文的默认值，所有其他的上下文都应该从它衍生出来； context.TODO 应该仅在不确定应该使用哪种上下文时使用； 在多数情况下，如果当前函数没有上下文作为入参，我们都会使用 context.Background 作为起始的上下文向下传递。 3.2.2 cancelCtx这是一个带 cancel 功能的 context。 123456789type cancelCtx struct &#123; // 直接嵌入了一个 Context，那么可以把 cancelCtx 看做是一个 Context Context mu sync.Mutex // protects following fields done atomic.Value // of chan struct&#123;&#125;, created lazily, closed by first cancel call children map[canceler]struct&#123;&#125; // set to nil by the first cancel call err error // set to non-nil by the first cancel call&#125; 同时 cancelCtx 还实现了 canceler 接口，提供了 cancel 方法，可以手动取消： 1234type canceler interface &#123; cancel(removeFromParent bool, err error) Done() &lt;-chan struct&#123;&#125;&#125; 实现了上面定义的两个方法的 Context，就表明该 Context 是可取消的。 创建 cancelCtx 的方法如下： 1234567891011func WithCancel(parent Context) (ctx Context, cancel CancelFunc) &#123; if parent == nil &#123; panic(&quot;cannot create context from nil parent&quot;) &#125; c := newCancelCtx(parent) propagateCancel(parent, &amp;c) return &amp;c, func() &#123; c.cancel(true, Canceled) &#125;&#125;func newCancelCtx(parent Context) cancelCtx &#123; return cancelCtx&#123;Context: parent&#125;&#125; 这是一个暴露给用户的方法，传入一个父 Context（这通常是一个 background，作为根节点），返回新建的 context，并通过闭包的形式，返回了一个 cancel 方法。 newCancelCtx将传入的上下文包装成私有结构体context.cancelCtx。 propagateCancel则会构建父子上下文之间的关联，形成树结构，当父上下文被取消时，子上下文也会被取消： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func propagateCancel(parent Context, child canceler) &#123; // 1.如果 parent ctx 是不可取消的 ctx，则直接返回 不进行关联 done := parent.Done() if done == nil &#123; return // parent is never canceled &#125; // 2.接着判断一下 父ctx 是否已经被取消 select &#123; case &lt;-done: // 2.1 如果 父ctx 已经被取消了，那就没必要关联了 // 然后这里也要顺便把子ctx给取消了，因为父ctx取消了 子ctx就应该被取消 // 这里是因为还没有关联上，所以需要手动触发取消 // parent is already canceled child.cancel(false, parent.Err()) return default: &#125; // 3. 从父 ctx 中提取出 cancelCtx 并将子ctx加入到父ctx 的 children 里面 if p, ok := parentCancelCtx(parent); ok &#123; p.mu.Lock() // double check 一下，确认父 ctx 是否被取消 if p.err != nil &#123; // 取消了就直接把当前这个子ctx给取消了 // parent has already been canceled child.cancel(false, p.err) &#125; else &#123; // 否则就添加到 children 里面 if p.children == nil &#123; p.children = make(map[canceler]struct&#123;&#125;) &#125; p.children[child] = struct&#123;&#125;&#123;&#125; &#125; p.mu.Unlock() &#125; else &#123; // 如果没有找到可取消的父 context。新启动一个协程监控父节点或子节点取消信号 atomic.AddInt32(&amp;goroutines, +1) go func() &#123; select &#123; case &lt;-parent.Done(): child.cancel(false, parent.Err()) case &lt;-child.Done(): &#125; &#125;() &#125;&#125; 上述函数总共与父上下文相关的三种不同的情况： 1）当 parent.Done() == nil，也就是 parent 不会触发取消事件时，当前函数会直接返回； 2）当 child 的继承链包含可以取消的上下文时，会判断 parent 是否已经触发了取消信号； 如果已经被取消，child 会立刻被取消； 如果没有被取消，child 会被加入 parent 的 children 列表中，等待 parent 释放取消信号； 3）当父上下文是开发者自定义的类型、实现了 context.Context 接口并在 Done() 方法中返回了非空的管道时； 运行一个新的 Goroutine 同时监听 parent.Done() 和 child.Done() 两个 Channel； 在 parent.Done() 关闭时调用 child.cancel 取消子上下文； propagateCancel 的作用是在 parent 和 child 之间同步取消和结束的信号，保证在 parent 被取消时，child 也会收到对应的信号，不会出现状态不一致的情况。 1234567891011121314151617181920func parentCancelCtx(parent Context) (*cancelCtx, bool) &#123; done := parent.Done() // 如果 done 为 nil 说明这个ctx是不可取消的 // 如果 done == closedchan 说明这个ctx不是标准的 cancelCtx，可能是自定义的 if done == closedchan || done == nil &#123; return nil, false &#125; // 然后调用 value 方法从ctx中提取出 cancelCtx p, ok := parent.Value(&amp;cancelCtxKey).(*cancelCtx) if !ok &#123; return nil, false &#125; // 最后再判断一下cancelCtx 里存的 done 和 父ctx里的done是否一致 // 如果不一致说明parent不是一个 cancelCtx pdone, _ := p.done.Load().(chan struct&#123;&#125;) if pdone != done &#123; return nil, false &#125; return p, true&#125; cancelCtx 的 done 方法肯定会返回一个 chan struct&#123;&#125; 123456789101112131415func (c *cancelCtx) Done() &lt;-chan struct&#123;&#125; &#123; d := c.done.Load() if d != nil &#123; return d.(chan struct&#123;&#125;) &#125; c.mu.Lock() defer c.mu.Unlock() d = c.done.Load() if d == nil &#123; d = make(chan struct&#123;&#125;) c.done.Store(d) &#125; return d.(chan struct&#123;&#125;)&#125;var closedchan = make(chan struct&#123;&#125;) c.done 是“懒汉式”创建，只有调用了 Done() 方法的时候才会被创建。再次说明，函数返回的是一个只读的 channel，而且没有地方向这个 channel 里面写数据。所以，直接调用读这个 channel，协程会被 block 住。一般通过搭配 select 来使用。一旦关闭，就会立即读出零值。 123456func (c *cancelCtx) Value(key interface&#123;&#125;) interface&#123;&#125; &#123; if key == &amp;cancelCtxKey &#123; return c &#125; return c.Context.Value(key)&#125; 所以这里parent.Value(&amp;cancelCtxKey)返回值就是 parent 内部的 cancelCtx。 parentCancelCtx 其实就是判断 parent context 里面有没有一个 cancelCtx，有就返回，让子 context 可以 “挂靠” 到 parent context 上，如果不是就返回 false，不进行挂靠，自己新开一个 goroutine 来监听。 最后再看一下比较重要的 cancel 方法。 1234567891011121314151617181920212223242526272829303132333435func (c *cancelCtx) cancel(removeFromParent bool, err error) &#123; // 参数校验 调用 cancel 必须传一个 err 进来，说明 cancel 的原因 if err == nil &#123; panic(&quot;context: internal error: missing cancel error&quot;) &#125; c.mu.Lock() if c.err != nil &#123; // 如果 err 不为空，说明已经取消过了，直接返回 c.mu.Unlock() return // already canceled &#125; c.err = err // 然后更新 done 的值 d, _ := c.done.Load().(chan struct&#123;&#125;) if d == nil &#123; // 如果为空就直接赋值为一个已经关闭的chan c.done.Store(closedchan) &#125; else &#123; // 如果有值就把对应chan直接关闭 close(d) &#125; // 接下来就是循环调用 取消掉 子context for child := range c.children &#123; // NOTE: acquiring the child&#x27;s lock while holding parent&#x27;s lock. child.cancel(false, err) &#125; c.children = nil c.mu.Unlock() // 最后更加参数来确定是否需要将该context从父content.children 中移除 if removeFromParent &#123; // 大部分情况下该参数都为 true 因为取消子context后肯定要和父context解开关联 // 只有当前子context还没添加到父context时，父context就被取消了，这种情况下会传false进来 removeChild(c.Context, c) &#125;&#125; 总体来看，cancel() 方法的功能就是关闭 channel：c.done；递归地取消它的所有子节点；从父节点从删除自己。达到的效果是通过关闭 channel，将取消信号传递给了它的所有子节点。goroutine 接收到取消信号的方式就是 select 语句中的读 c.done 被选中。 3.2.3 timerCtxtimerCtx 内部不仅通过嵌入 cancelCtx 的方式承了相关的变量和方法，还通过持有的定时器 timer 和截止时间 deadline 实现了定时取消的功能： 1234567891011121314151617181920212223type timerCtx struct &#123; cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time&#125;func (c *timerCtx) Deadline() (deadline time.Time, ok bool) &#123; return c.deadline, true&#125;func (c *timerCtx) cancel(removeFromParent bool, err error) &#123; c.cancelCtx.cancel(false, err) if removeFromParent &#123; removeChild(c.cancelCtx.Context, c) &#125; c.mu.Lock() if c.timer != nil &#123; c.timer.Stop() c.timer = nil &#125; c.mu.Unlock()&#125; timerCtx.cancel 不仅调用了 cancelCtx.cancel 方法，还会停止持有的定时器减少不必要的资源浪费。 实际上对外提供了 WithTimeout 方法只是 WithDeadline 的封装： 1234567891011121314151617181920212223242526272829303132333435func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) &#123; return WithDeadline(parent, time.Now().Add(timeout))&#125;func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) &#123; if parent == nil &#123; panic(&quot;cannot create context from nil parent&quot;) &#125; if cur, ok := parent.Deadline(); ok &amp;&amp; cur.Before(d) &#123; // The current deadline is already sooner than the new one. // 如果父节点 context 的 deadline 早于指定时间。直接构建一个可取消的 context。 // 原因是一旦父节点超时，自动调用 cancel 函数，子节点也会随之取消。 // 所以不用单独处理子节点的计时器时间到了之后，自动调用 cancel 函数 // 毕竟如果父节点1分钟后过期，那不可能在这个父节点下创建一个两分钟后过期的子节点 return WithCancel(parent) &#125; c := &amp;timerCtx&#123; cancelCtx: newCancelCtx(parent), deadline: d, &#125; propagateCancel(parent, c) dur := time.Until(d) if dur &lt;= 0 &#123; c.cancel(true, DeadlineExceeded) // deadline has already passed return c, func() &#123; c.cancel(false, Canceled) &#125; &#125; c.mu.Lock() defer c.mu.Unlock() if c.err == nil &#123; c.timer = time.AfterFunc(dur, func() &#123; c.cancel(true, DeadlineExceeded) &#125;) &#125; return c, func() &#123; c.cancel(true, Canceled) &#125;&#125; 和 WithCancel 大致逻辑是相同的，除了多了一个 timer 来定时取消： 123c.timer = time.AfterFunc(dur, func() &#123; c.cancel(true, DeadlineExceeded) &#125;) 里面的一个 deadLine 判断也比较有意思： 123if cur, ok := parent.Deadline(); ok &amp;&amp; cur.Before(deadline)&#123; return WithCancel(parent) &#125; 如果父节点 context 的 deadline 早于本次创建子节点的 deadline ，那就没必要给子节点创建一个 timerCtx 了，因为根据 deadline 来看，父节点肯定会早与这个子节点取消，而父节点取消后，子节点也会跟着被取消，所以没必要给子节点创建 timer，直接创建一个 cancelCtx 将子节点挂到父节点上就行了，效果是一样的，还剩下一个 timer。 1234type valueCtx struct &#123; Context key, val interface&#123;&#125;&#125; 3.2.4 valueCtxvalueCtx 则是多了 key、val 两个字段来存数据。 123456789101112func WithValue(parent Context, key, val interface&#123;&#125;) Context &#123; if parent == nil &#123; panic(&quot;cannot create context from nil parent&quot;) &#125; if key == nil &#123; panic(&quot;nil key&quot;) &#125; if !reflectlite.TypeOf(key).Comparable() &#123; panic(&quot;key is not comparable&quot;) &#125; return &amp;valueCtx&#123;parent, key, val&#125;&#125; 直接基于 parent 构建了一个 valueCtx，比较简单。注意点是这个方法对 key 的要求是**可比较的 (comparable)**，因为之后需要通过 key 取出 context 中的值，可比较是必须的。 取值的过程，实际上是一个递归查找的过程： 123456func (c *valueCtx) Value(key interface&#123;&#125;) interface&#123;&#125; &#123; if c.key == key &#123; return c.val &#125; return c.Context.Value(key)&#125; 如果 key 和当前 ctx 中存的 value 一致就直接返回，没有就去 parent 中找。最终找到根节点（一般是 emptyCtx），直接返回一个 nil。所以用 Value 方法的时候要判断结果是否为 nil。 因为这里要比较两个 key 是否一致，所以创建的时候必须要求 key 是 comparable。 类似于一个链表，其实效率是很低的，不建议用来传参数。 4. 使用建议在官方博客里，对于使用 context 提出了几点建议： Do not store Contexts inside a struct type; instead, pass a Context explicitly to each function that needs it. The Context should be the first parameter, typically named ctx. Do not pass a nil Context, even if a function permits it. Pass context.TODO if you are unsure about which Context to use. Use context Values only for request-scoped data that transits processes and APIs, not for passing optional parameters to functions. The same Context may be passed to functions running in different goroutines; Contexts are safe for simultaneous use by multiple goroutines. 翻译过来就是： 不要将 Context 塞到结构体里。直接将 Context 类型作为函数的第一参数，而且一般都命名为 ctx。 不要向函数传入一个 nil 的 context，如果你实在不知道传什么，标准库给你准备好了一个 context：todo。 不要把本应该作为函数参数的类型塞到 context 中，context 存储的应该是一些共同的数据。例如：登陆的 session、cookie 等。 同一个 context 可能会被传递到多个 goroutine，别担心，context 是并发安全的。","tags":["Go","context"],"categories":["Go"]},{"title":"Go 系列(一)：chan 源码分析","path":"//blog/go/channel/","content":"本文主要介绍了 Go 语言 (golang) 中的 channel，并从源码层面分析其具体实现，包括创建 channel，发送数据，接收数据以及相关调度等。 以下分析基于 Go 1.17.5 1. 概述官方对 chan 的描述如下： A channel provides a mechanism for concurrently executing functions to communicate by sending and receivingvalues of a specified element type. The value of an uninitialized channel is nil. chan 提供了一种并发通信机制，用于生产和消费某一指定类型数据，未初始化的 chan 的值是 nil。 Chan 是 Go 里面的一种数据结构，具有以下特性： goroutine-safe，多个 goroutine 可以同时访问一个 channel 而不会出现并发问题 hchan mutex，通过加锁来避免数据竞争。 可以用于在 goroutine 之间存储和传递值 copying into and out of hchan buffer 其语义是先入先出（FIFO） 可以导致 goroutine 的 block 和 unblock 通过 sudog queues 来记录阻塞的 goroutine。 通过 runtime scheduler(gopark, goready) 来实现阻塞与唤醒。 Channel 字面意义是“通道”，类似于 Linux 中的管道。声明 channel 的语法如下： 123chan T // 声明一个双向通道chan&lt;- T // 声明一个只能用于发送的通道&lt;-chan T // 声明一个只能用于接收的通道 单向通道的声明，用 &lt;- 来表示，它指明通道的方向。 因为 channel 是一个引用类型，所以在它被初始化之前，它的值是 nil，channel 使用 make 函数进行初始化。可以向它传递一个 int 值，代表 channel 缓冲区的大小（容量），构造出来的是一个缓冲型的 channel；不传或传 0 的，构造的就是一个非缓冲型的 channel。 12345// 无缓冲ch1 := make(chan int)// 缓冲区为 3ch2 := make(chan int, 3) chan（即 hchan 结构体） 默认会被分配在堆上，make 返回的只是一个指向该对象的指针。 无缓冲 channel 无缓冲的 channel（unbuffered channel），其缓冲区大小则默认为 0。在功能上其接受者会阻塞等待并阻塞应用程序，直至收到通信和接收到数据。 （引用 William Kennedy 的图） 缓冲 channel 有缓存的 channel（buffered channel），其缓存区大小是根据所设置的值来调整。在功能上，若缓冲区未满则不会阻塞，会源源不断的进行传输。当缓冲区满了后，发送者就会阻塞并等待。而当缓冲区为空时，接受者就会阻塞并等待，直至有新的数据： （引用 William Kennedy 的图） 2. 数据结构本质上 channel 在设计上就是环形队列。其包含发送方队列、接收方队列，加上互斥锁 mutex 等结构。 channel 是一个有锁的环形队列： 123456789101112131415161718192021222324252627282930313233343536// src/runtime/chan.gotype hchan struct &#123; closed uint32 // channel是否关闭的标志 elemtype *_type // channel中的元素类型 // channel分为无缓冲和有缓冲两种。 // 对于有缓冲的channel存储数据，使用了 ring buffer（环形缓冲区) 来缓存写入的数据，本质是循环数组 // 为啥是循环数组？普通数组不行吗，普通数组容量固定更适合指定的空间，弹出元素时，普通数组需要全部都前移 // 当下标超过数组容量后会回到第一个位置，所以需要有两个字段记录当前读和写的下标位置 buf unsafe.Pointer // 指向底层循环数组的指针（环形缓冲区） qcount uint // 循环数组中的元素数量 dataqsiz uint // 循环数组的长度 elemsize uint16 // 元素的大小 sendx uint // 下一次写下标的位置 recvx uint // 下一次读下标的位置 // 尝试读取channel或向channel写入数据而被阻塞的goroutine recvq waitq // 读等待队列 sendq waitq // 写等待队列 lock mutex //互斥锁，保证读写channel时不存在并发竞争问题&#125;type waitq struct &#123; first *sudog last *sudog&#125;type sudog struct &#123; g *g // 指向当前的 goroutine。 next *sudog // 指向下一个 g。 prev *sudog // 指向上一个 g。 elem unsafe.Pointer // 数据元素，可能会指向堆栈。 c *hchan ...&#125; 3. 实现原理3.1 创建 chan在源码中通道的创建由 makechan 方法实现： 123456789101112131415161718// 通用创建方法func makechan(t *chantype, size int) *hchan// 类型为 int64 的进行特殊处理func makechan64(t *chantype, size int64) *hchan//go:linkname reflect_makechan reflect.makechanfunc reflect_makechan(t *chantype, size int) *hchan &#123; return makechan(t, size)&#125;func makechan64(t *chantype, size int64) *hchan &#123; if int64(int(size)) != size &#123; panic(plainError(&quot;makechan: size out of range&quot;)) &#125; return makechan(t, int(size))&#125; 内部都是调用的 makechan 方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243func makechan(t *chantype, size int) *hchan &#123; elem := t.elem // 编译器检查 typesize 和 align if elem.size &gt;= 1&lt;&lt;16 &#123; throw(&quot;makechan: invalid channel element type&quot;) &#125; if hchanSize%maxAlign != 0 || elem.align &gt; maxAlign &#123; throw(&quot;makechan: bad alignment&quot;) &#125; // 计算存放数据元素的内存大小以及是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem &gt; maxAlloc-hchanSize || size &lt; 0 &#123; panic(plainError(&quot;makechan: size out of range&quot;)) &#125; var c *hchan switch &#123; case mem == 0: // chan的size为0，或者每个元素占用的大小为0（比如struct&#123;&#125;大小就是0，不占空间） // 这种情况就不需要单独为buf分配空间 c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: // 如果队列中不存在指针，那么每个元素都需要被存储并占用空间，占用大小为前面乘法算出来的mem // 同时还要加上hchan本身占用的空间大小，加起来就是整个hchan占用的空间大小 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) // 把buf指针指向空的hchan占用空间大小的末尾 c.buf = add(unsafe.Pointer(c), hchanSize) default: // 如果chan中的元素是指针类型的数据，为buf单独开辟mem大小的空间，用来保存所有的数据 c = new(hchan) c.buf = mallocgc(mem, elem, true) &#125; // 元素大小、类型以及缓冲区大小赋值 c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) // 初始化锁 lockInit(&amp;c.lock, lockRankHchan) return c&#125; 具体流程如下： 1）首先是编译器检查，包括通道元素类型的 size 以及通道和元素的对齐，然后计算存放数据元素的内存大小以及是否溢出 2）然后根据不同条件进行内存分配 总体的原则是：总内存大小 &#x3D; hchan 需要的内存大小 + 元素需要的内存大小 队列为空或元素大小为 0：只需要开辟的内存空间为 hchan 本身的大小 元素不是指针类型：需要开辟的内存空间 &#x3D; hchan 本身大小 + 每个元素的大小 * 申请的队列长度 元素是指针类型：这种情况下 buf 需要单独开辟空间，buf 占用内存大小为每个元素的大小 * 申请的队列长度3）最后则对 chan 的其他字段赋值 3.2 发送数据发送数据到 channel 时，直观的理解是将数据放到 chan 的环形队列中，不过 go 做了一些优化： 先判断是否有等待接收数据的 groutine，如果有，直接将数据发给 Groutine，唤醒 groutine，就不放入队列中了。 这样省去了两次内存拷贝和加锁的开销 当然还有另外一种情况就是：队列如果满了，那就只能放到队列中等待，直到有数据被取走才能发送。 chan 的发送逻辑涉及到 5 个方法： 12345func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool) &#123;&#125;func chansend1(c *hchan, elem unsafe.Pointer) &#123;…&#125;func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123;…&#125;func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123;…&#125;func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) &#123;…&#125; chansend1 方法是 go 编译代码中c &lt;- x这种写法的入口点，即当我们编写代码 c &lt;- x其实就是调用此方法。 这四个方法的调用关系：chansend1 -&gt; chansend -&gt; send -&gt; sendDirect 具体发送逻辑在chansend这个方法里，然后真正使用的方法其实是对该方法的一层包装。 1234567func chansend1(c *hchan, elem unsafe.Pointer) &#123; chansend(c, elem, true, getcallerpc())&#125;func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool) &#123; return chansend(c, elem, false, getcallerpc())&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123; // 判断 channel 是否为 nil if c == nil &#123; if !block &#123;// 如果非阻塞，直接返回 false return false &#125; // 当向 nil channel 发送数据时，会调用 gopark // 而 gopark 会将当前的 goroutine 休眠，并用过第一个参数的 unlockf 来回调唤醒 // 但此处传递的参数为 nil，因此向 channel 发送数据的 goroutine 和接收数据的 goroutine 都会阻塞，进而死锁 gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(&quot;unreachable&quot;) &#125; if raceenabled &#123; racereadpc(c.raceaddr(), callerpc, funcPC(chansend)) &#125; // 对于不阻塞的 send，快速检测失败场景 // 如果 channel 未关闭且 channel 没有多余的缓冲空间。这可能是： // 1. channel 是非缓冲型的，且等待接收队列里没有 goroutine // 2. channel 是缓冲型的，但循环数组已经装满了元素 // 主要用于 select 语句中，涉及到指令重排队+可观测性 if !block &amp;&amp; c.closed == 0 &amp;&amp; full(c) &#123; return false &#125; var t0 int64 if blockprofilerate &gt; 0 &#123; t0 = cputicks() &#125; // 加锁,避免竞争 lock(&amp;c.lock) // 检查 channel 是否已关闭，不允许向关闭的 channel 发送数据 if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError(&quot;send on closed channel&quot;)) // 直接panic &#125; // 从 recvq 队首取出一个接收者，如果存在接收者，就绕过环形队列（buf）直接把 ep 拷贝给 sg，并释放锁 // 这就是前面提到的，官方做的一个优化，如果有goroutine在等待就直接把数据给该goroutine，没必要在写到buf，然后接收者又从buf中拷贝出来 if sg := c.recvq.dequeue(); sg != nil &#123; send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true &#125; // 到这里说明当前没有等待状态的接收者 // 如果环形队列还未满 if c.qcount &lt; c.dataqsiz &#123; // 拿到 sendx 索引的位置 qp := chanbuf(c, c.sendx) if raceenabled &#123; racenotify(c, c.sendx, nil) &#125; // 直接把数据从 qp 拷贝到 qp，就是把数据拷贝到环形队列中 typedmemmove(c.elemtype, qp, ep) // 维护 snedx 的值，因为是环形队列，所以到最大值时就重置为0 c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; //qcount即当前chan中的元素个数 c.qcount++ unlock(&amp;c.lock) return true &#125; // 到这里说明环形队列已经满了 // 如果还是要非阻塞的方式发送，就只能返回错误了 if !block &#123; unlock(&amp;c.lock) return false &#125; // 到这里说明缓存队列满了，然后调用法指定是阻塞方式进行发送 // channel 满了，发送方会被阻塞。接下来会构造一个 sudog gp := getg() // 获取当前 goroutine mysg := acquireSudog()// 从对象池获取 sudog mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; // 把发送的数据(ep)、当前g(gp)、已经当前这个chan(c)都存到sudog中 mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c // 保存当前 sudog，下面要用到做校验 gp.waiting = mysg gp.param = nil // 把这个sudog存入sendq队列 c.sendq.enqueue(mysg) atomic.Store8(&amp;gp.parkingOnChan, 1) // 调用gopark，挂起当前的 g，将当前的 g 移出调度器的队列 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) // 等到有接收者从chan中取值的时候，这个发送的g又会被重新调度，然后从这里开始继续执行 KeepAlive(ep) // 检验是否为当前的 sudog if mysg != gp.waiting &#123; throw(&quot;G waiting list is corrupted&quot;) &#125; gp.waiting = nil gp.activeStackChans = false // 这里sudog中的success表示的是当前这个通道上是否进行过通信 // 为 true 则说明是真正的唤醒，chan上有活动（有数据写进来，或者有数据被读取出去） // 为 false 则说明是假的唤醒，即当前唤醒是否关闭chan导致的 // 这里主要根据这个值判断chan是否被关闭了 closed := !mysg.success gp.param = nil if mysg.releasetime &gt; 0 &#123; blockevent(mysg.releasetime-t0, 2) &#125; mysg.c = nil // 将 sudog 放回对象池 releaseSudog(mysg) if closed &#123; // 如果chan被关闭了也是直接panic if c.closed == 0 &#123; throw(&quot;chansend: spurious wakeup&quot;) &#125; panic(plainError(&quot;send on closed channel&quot;)) &#125; return true&#125; 核心逻辑 如果 recvq 不为空，从 recvq 中取出一个等待接收数据的 Groutine，直接将数据发送给该 Groutine 如果 recvq 为空，才将数据放入 buf 中 如果 buf 已满，则将要发送的数据和当前的 Groutine 打包成 Sudog 对象放入 sendq，并将 groutine 置为等待状态 等 goroutine 再次被调度时程序继续执行 然后追踪一下 send 方法： 1234567891011121314151617func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; // 忽略 race 检查.. if sg.elem != nil &#123; // 直接拷贝到接受者内存，使用写屏障 sendDirect(c.elemtype, sg, ep) sg.elem = nil &#125; gp := sg.g // 取出sudog中记录的g，这里的g就是被阻塞接收者 unlockf() gp.param = unsafe.Pointer(sg) // 更新接收者g的param字段，在recv方法中会用到 sg.success = true if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 最后把被阻塞的接收者g唤醒 goready(gp, skip+1)&#125; 继续看 sendDirect 方法： 1234567891011func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) &#123; // src 在当前 goroutine 的栈上，dst 是另一个 goroutine 的栈 // 直接进行内存&quot;搬迁&quot; // 如果目标地址的栈发生了栈收缩，当我们读出了 sg.elem 后 // 就不能修改真正的 dst 位置的值了 // 因此需要在读和写之前加上一个屏障 dst := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) // 拷贝内存 memmove(dst, src, t.size)&#125; 这里涉及到一个 goroutine 直接写另一个 goroutine 栈的操作，一般而言，不同 goroutine 的栈是各自独有的。而这也违反了 GC 的一些假设。为了不出问题，写的过程中增加了写屏障，保证正确地完成写操作。这样做的好处是减少了一次内存 copy：不用先拷贝到 channel 的 buf，直接由发送者到接收者，没有中间商赚差价，效率得以提高，完美。 3.3 接收数据从 channel 读取数据的流程和发送的类似，基本是发送操作的逆操作。 这里同样存在和 send 一样的优化：从 channel 读取数据时，不是直接去环形队列中去数据，而是先判断是否有等待发送数据的 groutine。如果有，直接将 groutine 出队列，取出数据返回，并唤醒 groutine。如果没有等待发送数据的 groutine，再从环形队列中取数据。 chan 的接收涉及到 7 个方法： 1234567func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool) &#123;&#125;func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool) &#123;&#125;func chanrecv1(c *hchan, elem unsafe.Pointer) &#123;…&#125;，func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) &#123;…&#125;func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123;…&#125;func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123;…&#125;func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) &#123;…&#125; 按照发送时的套路可知，只有 chanrecv 是具体逻辑，上面几个都是包装方法： 1234567891011121314151617//go:linkname reflect_chanrecv reflect.chanrecvfunc reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool) &#123; return chanrecv(c, elem, !nb)&#125;func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool) &#123; return chanrecv(c, elem, false)&#125;//go:nosplitfunc chanrecv1(c *hchan, elem unsafe.Pointer) &#123; chanrecv(c, elem, true)&#125;//go:nosplitfunc chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) &#123; _, received = chanrecv(c, elem, true) return&#125; 接收操作有两种写法，一种带 “ok”，反应 channel 是否关闭； 一种不带 “ok”，这种写法，当接收到相应类型的零值时无法知道是真实的发送者发送过来的值，还是 channel 被关闭后，返回给接收者的默认类型的零值。 两种写法，都有各自的应用场景。 经过编译器的处理后，这两种写法最后对应源码里的就是不带ok的chanrecv1和带ok的chanrecv2这两个函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141// chanrecv 函数接收 channel c 的元素并将其写入 ep 所指向的内存地址。// 如果 ep 是 nil，说明忽略了接收值。比如 &lt;-ch 这样，没有接收取到的值// 如果 block == false，即非阻塞型接收，在没有数据可接收的情况下，返回 (false, false)// 否则，如果 c 处于关闭状态，将 ep 指向的地址清零，返回 (true, false)// 否则，用返回值填充 ep 指向的内存地址。返回 (true, true)// 如果 ep 非空，则应该指向堆或者函数调用者的栈func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123; // 如果是一个 nil 的 channel if c == nil &#123; // 如果不阻塞，直接返回 (false, false) if !block &#123; return &#125; // 否则，接收一个 nil 的 channel，调用gopark将goroutine 挂起 gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(&quot;unreachable&quot;) // 被挂起之后不会执行到这一句 &#125; // 这块主要用在 select 语句中，先大概了解下，比较难懂。。。 // 快速路径: 在不需要锁的情况下检查失败的非阻塞操作 // 注意到 channel 不能由已关闭转换为未关闭，则失败的条件是： // 1. channel 是非缓冲型的，recvq 队列为空 // 2. channel 是缓冲型的，buf 为空 if !block &amp;&amp; empty(c) &#123; // 此处的 c.closed 必须在条件判断之后进行验证， // 因为指令重排后，如果先判断 c.closed，得出 channel 未关闭，无法判断失败条件中channel 是已关闭还是未关闭（从而需要 atomic 操作） if atomic.Load(&amp;c.closed) == 0 &#123; return &#125; // 再次检查 channel 是否为空 if empty(c) &#123; // 接收者不为 nil 时返回该类型的零值 if ep != nil &#123; // typedmemclr 逻辑是根据类型清理相应地址的内存 typedmemclr(c.elemtype, ep) &#125; // 返回（true,fasle） // 返回值1--true：表示被 select case 选中， // 返回值2--fasle 表示是否正常收到数据 return true, false &#125; &#125; var t0 int64 if blockprofilerate &gt; 0 &#123; t0 = cputicks() &#125; // 加锁，保证并发安全 lock(&amp;c.lock) // channel 已关闭，并且循环数组 buf 里没有元素 // 这里可以处理非缓冲型关闭 和 缓冲型关闭但 buf 无元素的情况 // 也就是说即使是关闭状态，但在缓冲型的 channel， // buf 里有元素的情况下还能接收到元素 if c.closed != 0 &amp;&amp; c.qcount == 0 &#123; unlock(&amp;c.lock) if ep != nil &#123; typedmemclr(c.elemtype, ep) &#125; return true, false &#125; // 等待发送队列里有 goroutine 存在，说明 buf 是满的 // 这有可能是： // 1. 非缓冲型的 channel // 2. 缓冲型的 channel，但 buf 满了 // 针对 1，直接进行内存拷贝（从 sender goroutine -&gt; receiver goroutine） // 针对 2，接收到循环数组头部的元素，并将发送者的元素放到循环数组尾部 if sg := c.sendq.dequeue(); sg != nil &#123; recv(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true, true &#125; // chan的buf 里有元素，可以正常接收 if c.qcount &gt; 0 &#123; // 直接从循环数组里找到要接收的元素 qp := chanbuf(c, c.recvx) // ep != nil表示代码里，没有忽略要接收的值 // 即接收的代码不是 &quot;&lt;- ch&quot;，而是 &quot;val &lt;- ch&quot;这种，ep 指向 val if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; // 清理掉循环数组里相应位置的值 typedmemclr(c.elemtype, qp) // 维护接收游标 c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; // buf 数组里的元素个数减 1 c.qcount-- // 处理完成，解锁返回 unlock(&amp;c.lock) return true, true &#125; // 到这里说明chan的buf里没有数据了，如果是非阻塞接收就直接返回了 if !block &#123; unlock(&amp;c.lock) return false, false &#125; // 接下来就是要被阻塞的情况了 // 和发送类似的，构造一个 sudog gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; // 这里需要注意一下，ep就是我们用来接收值得对象 // 这里把ep直接存到sudog.elem字段上 mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg // 这个waiting同样是用来唤醒后做校验的 mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil // 加入到chan的recvq队列里 c.recvq.enqueue(mysg) atomic.Store8(&amp;gp.parkingOnChan, 1) // 将当前 goroutine 挂起 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) // 唤醒后，继续往下执行 // 同样是进行数据校验 if mysg != gp.waiting &#123; throw(&quot;G waiting list is corrupted&quot;) &#125; gp.waiting = nil gp.activeStackChans = false if mysg.releasetime &gt; 0 &#123; blockevent(mysg.releasetime-t0, 2) &#125; // 又是mysg.success，如果chan活动过就是true，否则是false success := mysg.success gp.param = nil mysg.c = nil releaseSudog(mysg)// 将 sudog 放回对象池 // 到这里如果goroutine被正常唤醒肯定是可以取到数据的 // 因为recvq的数据是由发送的时候直接copy过来了 return true, success&#125; 继续追踪一下 recv 方法 123456789101112131415161718192021222324252627282930313233343536373839func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; // 非缓冲型的 channel if c.dataqsiz == 0 &#123; // 并且需要接收值 if ep != nil &#123; // 直接进行内存拷贝 recvDirect(c.elemtype, sg, ep) &#125; &#125; else &#123; // 需要注意：进入recv方法说明sendq队列里是有值的 // 那么对缓冲型的 channel来说，sendq有值就意味着buf满了 // 也就是 recvx和sendx重合了都 // 这里要做的就是先从buf中读一个数据出来，然后再把发送者发送的数据写入buf qp := chanbuf(c, c.recvx) // 将接收游标处的数据拷贝给接收者 if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; // 从发送者把数据写入 recvx typedmemmove(c.elemtype, qp, sg.elem) // 然后修改 recvx和sendx 的位置 c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz &#125; sg.elem = nil gp := sg.g // 解锁 unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 最后唤醒发送的 goroutine goready(gp, skip+1)&#125; 再看一下 recvDirect： 1234567func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) &#123; // 如果是非缓冲型的，就直接从发送者的栈拷贝到接收者的栈。 // 和sendDirect一样的需要加内存屏障 src := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125; 看了接收部分代码后，整个流程就更新清晰了。 根据前面的发送逻辑可以知道，不管是接收还是发送只要被阻塞了，加入到了 sendq 或者 recvq 之后，那么后续的发送或者接收都是由对方进行处理了。 比如接收被阻塞了，当前 g 构成成一个 sudog 然后加入到 recvq，接着调用了 gopark 就已经阻塞了，啥也干不了了。 只能等到有发送者来的时候直接从 recvq 里把这个 sudog 取出来，并且直接把要他发送的值拷贝到这个 sudog.elem 字段上，也就是调用 chan 接收方法是传进来的哪个值. 最后发送方再调用 goready 把这个 g 给唤醒，这样再把剩下的逻辑走完，这个被阻塞了一会的接收者就可以拿着数据返回了。 核心逻辑： 1）如果有等待发送数据的 groutine，从 sendq 中取出一个等待发送数据的 Groutine，取出数据 2）如果没有等待的 groutine，且环形队列中有数据，从队列中取出数据 3）如果没有等待的 groutine，且环形队列中也没有数据，则阻塞该 Groutine，并将 groutine 打包为 sudogo 加入到 recevq 等待队列中 3.4 关闭 chanclose 就比较简单了，相关方法就两个： 12345//go:linkname reflect_chanclosefunc reflect_chanclose(c *hchan) &#123; closechan(c)&#125;func closechan(c *hchan)&#123;&#125; 其中一个还是包装方法，真正逻辑就在 clsoechan 里。 每个逻辑都有一个 reflect_xxx 的方法，根据名字猜测是反射的时候用的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071func closechan(c *hchan) &#123; // 关闭一个nil的chan直接panic if c == nil &#123; panic(plainError(&quot;close of nil channel&quot;)) &#125; // 同样是先加锁 lock(&amp;c.lock) // 判断一下是否被关闭过了，关闭一个已经关闭的chan也是直接panic if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError(&quot;close of closed channel&quot;)) &#125; // 修改closed标记为，表示chan已经被关闭了 c.closed = 1 // gList 是通过 g.schedlink 链接 G 的列表，一个 G 只能是一次在一个 gQueue 或 gList 上 // gList 模拟的是栈操作（FILO） // gQueue 模拟的是队列操作（FIFO） var glist gList // 释放所有的接收者 for &#123; sg := c.recvq.dequeue() // sg == nil，表示接收队列已为空，跳出循环 if sg == nil &#123; break &#125; // 如果 elem 不为空说明未忽略接收值，赋值为该类型的零值 if sg.elem != nil &#123; typedmemclr(c.elemtype, sg.elem) sg.elem = nil &#125; if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled &#123; raceacquireg(gp, c.raceaddr()) &#125; glist.push(gp) &#125; // release all writers (they will panic) // 释放所有的发送者，抛出异常 for &#123; sg := c.sendq.dequeue() if sg == nil &#123; break &#125; sg.elem = nil if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled &#123; raceacquireg(gp, c.raceaddr()) &#125; glist.push(gp) &#125; unlock(&amp;c.lock) // 循环读取 glist 里面的数据，挨个唤醒 for !glist.empty() &#123; gp := glist.pop() gp.schedlink = 0 goready(gp, 3) &#125;&#125; 核心流程： 设置关闭状态 唤醒所有等待读取 chanel 的协程 所有等待写入 channel 的协程，抛出异常 4. 进阶4.1 操作 chan总结一下操作 channel 的结果： 操作 nil channel closed channel not nil, not closed channel close panic panic 正常关闭 读 &lt;- ch 阻塞 读到对应类型的零值 阻塞或正常读取数据。缓冲型 channel 为空或非缓冲型 channel 没有等待发送者时会阻塞 写 ch &lt;- 阻塞 panic 阻塞或正常写入数据。非缓冲型 channel 没有等待接收者或缓冲型 channel buf 满时会被阻塞 总结一下，发生 panic 的情况有三种：向一个关闭的 channel 进行写操作；关闭一个 nil 的 channel；重复关闭一个 channel。 读、写一个 nil channel 都会被阻塞。 4.2 发送和接收元素的本质Channel 发送和接收元素的本质是什么？参考资料【深入 channel 底层】里是这样回答的： Remember all transfer of value on the go channels happens with the copy of value. 就是说 channel 的发送和接收操作本质上都是 “值的拷贝”，无论是从 sender goroutine 的栈到 chan buf，还是从 chan buf 到 receiver goroutine，或者是直接从 sender goroutine 到 receiver goroutine。 这里再引用文中的一个例子，我会加上更加详细地解释。 12345678910111213141516171819202122232425262728293031323334type user struct &#123; name string age int8&#125;var u = user&#123;name: &quot;Ankur&quot;, age: 25&#125;var g = &amp;ufunc modifyUser(pu *user) &#123; fmt.Println(&quot;modifyUser Received Vaule&quot;, pu) pu.name = &quot;Anand&quot;&#125;func printUser(u &lt;-chan *user) &#123; time.Sleep(2 * time.Second) fmt.Println(&quot;printUser goRoutine called&quot;, &lt;-u)&#125;func main() &#123; c := make(chan *user, 5) c &lt;- g fmt.Println(g) // modify g g = &amp;user&#123;name: &quot;Ankur Anand&quot;, age: 100&#125; go printUser(c) go modifyUser(g) time.Sleep(5 * time.Second) fmt.Println(g)&#125;&amp;&#123;Ankur 25&#125;modifyUser Received Value &amp;&#123;Ankur Anand 100&#125;printUser goRoutine called &amp;&#123;Ankur 25&#125;&amp;&#123;Anand 100&#125; 一开始构造一个结构体 u，地址是 0x56420，图中地址上方就是它的内容。接着把 &amp;u 赋值给指针 g，g 的地址是 0x565bb0，它的内容就是一个地址，指向 u。 main 程序里，先把 g 发送到 c，根据 copy value 的本质，进入到 chan buf 里的就是 0x56420，它是指针 g 的值（不是它指向的内容），所以打印从 channel 接收到的元素时，它就是 &amp;{Ankur 25}。因此，这里并不是将指针 g “发送” 到了 channel 里，只是拷贝它的值而已。 4.3 资源泄漏Channel 可能会引发 goroutine 泄漏。 泄漏的原因是 goroutine 操作 channel 后，处于发送或接收阻塞状态，而 channel 处于满或空的状态，一直得不到改变。同时，垃圾回收器也不会回收此类资源，进而导致 gouroutine 会一直处于等待队列中，不见天日。","tags":["Go","chan","channel"],"categories":["Go"]},{"title":"Docker 系列(三)：Docker 核心原理","path":"//blog/docker/docker_core/","content":"本文主要介绍了 Docker容器的核心实现原理，包括 Namespace、Cgroups、rootfs 等。 容器与进程 进程就是程序运行起来后的计算机执行环境的总和。 即：计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。 对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。 隔离与限制 NamespaceNamespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。但对于宿主机来说，这些被 “隔离” 来的进程和其他进程没有什么不同。 在 Linux 下可以根据隔离的属性不同分为不同的 Namespace ： Mount Namespace（CLONE_NEWNS &#x2F; Linux 2.4.19） UTS Namespace（CLONE NEWUTS &#x2F; Linux 2.6.19） IPC Namespace（CLONE NEWIPC &#x2F; Linux 2.6.19） PID Namespace（CLONE NEWPID &#x2F; Linux 2.6.24） Network Namespace（CLONE NEWNET &#x2F; 始于Linux 2.6.24完成于 Linux 2.6.29） User Namespace（CLONE NEWUSER &#x2F; 始于 Linux 2.6.23 完成于 Linux 3.8） Time Namespace UTS Namespace Namespace 存在的问题 最大的问题就是隔离得不彻底。 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。 尽管可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。 而相比之下，拥有硬件虚拟化技术和独立 Guest OS 的虚拟机就要方便得多了。最极端的例子是，Microsoft 的云计算平台 Azure，实际上就是运行在 Windows 服务器集群上的，但这并不妨碍你在它上面创建各种 Linux 虚拟机出来。 其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就 是：时间。 这就意味着，如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。相比于在虚拟机里面可以随便折腾的自由度，在容器里部署应用的时候，“什么能做，什么不能做”，就是用户必须考虑的一个问题。 下面通过一个案例，来看下 Namespace 的效果： 1234567891011# 使用 python3.6.8 的官方镜像，建立了一个运行 django 的环境# 进入该容器后，使用 ps 命令，查看运行的进程root@8729260f784a:/src# ps -A PID TTY TIME CMD 1 ? 00:01:22 gunicorn 22 ? 00:01:20 gunicorn 23 ? 00:01:24 gunicorn 25 ? 00:01:30 gunicorn 27 ? 00:01:16 gunicorn 41 pts/0 00:00:00 bash 55 pts/0 00:00:00 ps 可以看到，容器内 PID &#x3D;1 的进程，是 gunicorn 启动的 django 应用。熟悉 Linux 的同学都知道，PID &#x3D;1 的进程是系统启动时的第一个进程，也称 init 进程。其他的进程，都是由它管理产生的。而此时，PID&#x3D;1 确实是 django 进程。 接着，退出容器，在宿主机执行 ps 命令 12345678# 环境为 Centos7[root@localhost ~]# ps -ef | grep gunicornroot 9623 8409 0 21:29 pts/0 00:00:00 grep --color=auto gunicornroot 30828 30804 0 May28 ? 00:01:22 /usr/local/bin/python /usr/local/bin/gunicorn -c gunicorn_config.py ctg.wsgiroot 31171 30828 0 May28 ? 00:01:20 /usr/local/bin/python /usr/local/bin/gunicorn -c gunicorn_config.py ctg.wsgiroot 31172 30828 0 May28 ? 00:01:24 /usr/local/bin/python /usr/local/bin/gunicorn -c gunicorn_config.py ctg.wsgiroot 31174 30828 0 May28 ? 00:01:30 /usr/local/bin/python /usr/local/bin/gunicorn -c gunicorn_config.py ctg.wsgiroot 31176 30828 0 May28 ? 00:01:16 /usr/local/bin/python /usr/local/bin/gunicorn -c gunicorn_config.py ctg.wsgi 如果以宿主机的视角，发现 django 进程 PID 变成了 30828. 这也就不难证明，在容器中，确实做了一些处理。把明明是 30828 的进程，变成了容器内的第一号进程，同时在容器还看不到宿主机的其他进程。这也说明容器内的环境确实是被隔离了。 这种处理，其实就是 Linux 的 Namespace 机制。比如，上述将 PID 变成 1 的方法就是通过PID Namespace。在 Linux 中创建线程的方法是 clone, 在其中指定 CLONE_NEWPID 参数，这样新创建的进程，就会看到一个全新的进程空间。而此时这个新的进程，也就变成了 PID&#x3D;1 的进程。 1int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 在 Linux 类似于 PID Namespace 的参数还有很多，比如： Namespace Flag Page Isolates Cgroup CLONE_NEWCGROUP cgroup_namespaces Cgroup root directory IPC CLONE_NEWIPC ipc_namespaces System V IPC,POSIX message queues 隔离进程间通信 Network CLONE_NEWNET network_namespaces Network devices,stacks, ports, etc. 隔离网络资源 Mount CLONE_NEWNS mount_namespaces Mount points 隔离文件系统挂载点 PID CLONE_NEWPID pid_namespaces Process IDs 隔离进程的 ID Time CLONE_NEWTIME time_namespaces Boot and monotonic clocks User CLONE_NEWUSER user_namespaces User and group IDs 隔离用户和用户组的 ID UTS CLONE_NEWUTS uts_namespaces Hostname and NIS domain name 隔离主机名和域名信息 Cgroups通过上面的 Linux Namespace 已经可以创建 “容器” 了，但是会存在 资源抢占 的问题。 还是以 PID Namespace 为例，来给你解释这个问题。 虽然容器内的第1号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器)占用的。当然，这个 100号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。 而 Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 Linux Cgroups 的全称是 Linux Control Group 主要的作用就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。在本文中，只重点探讨它与容器关系最紧密的“限制”能力，并通过一组实践来带你认识一下 Cgroups。 每一个 CGroup 都是一组被相同的标准和参数限制的进程，不同的 CGroup 之间是有层级关系的，也就是说它们之间可以从父类继承一些用于限制资源使用的标准和参数。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 &#x2F;sys&#x2F;fs&#x2F;cgroup 路径下。在 Ubuntu 16.04 机器里，我可以用 mount 指令把它们展示出来，这条命令是： 12345678910111213#查看 cgroups 相关文件$ mount -t cgroupcgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) 它的输出结果，是一系列文件系统目录。如果你在自己的机器上没有看到这些目录，那就需要自己去挂载 Cgroups，具体做法可以自行 Google。 在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，就可以看到该类资源具体可以被限制的方法。 比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是： 1234ls /sys/fs/cgroup/cpu# 目录下大概有这么一些内容assist cgroup.event_control cgroup.sane_behavior cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat notify_on_release system.slicecgroup.clone_children cgroup.procs cpuacct.usage cpu.cfs_period_us cpu.rt_period_us cpu.shares release_agent tasks 下面以限制 cpu 为例，展示 Cgroups 是如何进行资源限制的： 首先在 cpu 子系统下面创建一个目录 container，比如，我们现在进入 &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu 目录下： 123[root@iz2ze0ephck4d0aztho5r5z cpu]# mkdir container[root@iz2ze0ephck4d0aztho5r5z cpu]# ls container/cgroup.clone_children cgroup.event_control cgroup.procs cpuacct.stat cpuacct.usage cpuacct.usage_percpu cpu.cfs_period_us cpu.cfs_quota_us cpu.rt_period_us cpu.rt_runtime_us cpu.shares cpu.stat notify_on_release tasks container 这个目录就称为一个“控制组”。操作系统会在你新创建的 container 目录下，自动生成 该子系统对应的资源限制文件。 现在，我们在后台执行这样一条脚本: 12$ while : ; do : ; done &amp;[1] 27218 上面的脚本执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 27218。 查看一下CPU占用 1234$ topPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 27218 root 20 0 115680 672 152 R 99.9 0.0 2:07.07 bash 果然这个PID&#x3D;27218的进程占用了差不多100%的 CPU 资源。 接下来我们就通过 Cgroups 对其进行限制，就用前面创建的 container 这个“控制组”。 我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）： 1234$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us -1$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 100000 文件功能说明(设定CPU使用周期使用时间上限): cpu.cfs_period_us：设定周期时间，必须与cfs_quota_us配合使用。 cpu.cfs_quota_us ：设定周期内最多可使用的时间。这里的配置指task对单个cpu的使用上限，若cfs_quota_us是cfs_period_us的两倍，就表示在两个核上完全使用。数值范围为1000 - 1000,000（微秒）。其他功能说明可参考：https://blog.csdn.net/zhonglinzhang/article/details/64905759 接下来，我们可以通过修改这些文件的内容来设置限制。比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）： 1$ echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 这样意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。 接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了： 1$ echo 27218 &gt; /sys/fs/cgroup/cpu/container/tasks 使用 top 指令查看一下 1234$ topPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 27218 root 20 0 115680 672 152 R 20 0.0 2:07.07 bash 可以看到，计算机的 CPU 使用率立刻降到了 20% (%Cpu0 : 20.3 us) 除 CPU 子系统外，Cgroups 的每一个子系统都有其独有的资源限制能力，比如： blkio，为块设备设定I&#x2F;O 限制，一般用于磁盘等设备； cpuset，为进程分配单独的 CPU 核和对应的内存节点； memory，为进程设定内存使用的限制。 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。 而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。 而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令： 1$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash 在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认： 1234$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000 5d5c9f67d 其实就是我们运行的一个 Docker 容器，启动这个容器时，Docker 会为这个容器创建一个与容器标识符相同的 CGroup，在当前的主机上 CGroup 就会有以下的层级关系： 12cpu --- docker --- 5d5c9f67d | --- ... Cgroups 存在的问题 跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 &#x2F;proc 文件系统的问题。 Linux 下的 &#x2F;proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。 但是，如果在容器里执行 top 指令，会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。 造成这个问题的原因就是，&#x2F;proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：&#x2F;proc 文件系统不了解 Cgroups 限制的存在。 在生产环境中，这个问题必须进行修正，否则应用程序在容器里读取到的 CPU 核数、可用内存等信息都是宿主机上的数据，这会给应用的运行带来非常大的困惑和风险。这也是在企业中，容器化应用碰到的一个常见问题，也是容器相较于虚拟机另一个不尽如人意的地方。 解决方案: 使用 lxcfs top 命令是从 &#x2F;prof&#x2F;stats 目录下获取数据，所以从道理上来讲，容器不挂载宿主机的 &#x2F;prof&#x2F;stats 目录就可以了。 lxcfs 就是来实现这个功能的，做法是把宿主机的 &#x2F;var&#x2F;lib&#x2F;lxcfs&#x2F;proc&#x2F;memoinfo 文件挂载到Docker容器的 &#x2F;proc&#x2F;meminfo 位置后。容器中进程读取相应文件内容时，LXCFS 的 FUSE 实现会从容器对应的Cgroup中读取正确的内存限制。从而使得应用获得正确的资源约束设定。kubernetes环境下，也能用，以 ds 方式运行 lxcfs ，自动给容器注入争取的 proc 信息。 容器镜像 上面介绍了，Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不 见的墙。这么一折腾，进程就真的被“装”在了一个与世隔绝的房间里，而这些房间就是 PaaS 项目赖以生存的应用“沙盒”。 可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？换句话说，容器里的进程看到的文件系统又是什么样子的呢？ 以 Dorcker 容器为例： Docker 镜像是一个只读的 Docker 容器模板，含有启动 Docker 容器所需的文件系统结构及其内容，因此是启动一个 Docker 容器的基础。Docker 镜像的文件内容以及一些运行 Docker 容器的配置文件组成了 Docker 容器的静态文件系统运行环境一rootfs。 可以这么理解，Docker镜像是Docker容器的静态视角，Docker容器是Docker像的运行状态。 rootfsrootfs 是Docker容器在启动时内部进程可见的文件系统，即Docker容器的根目录。rootfs 通常包含一个操作系统运行所需的文件系统，例如可能包含典型的类Unix操作系统中的目录系统，如&#x2F;dev, &#x2F;proc, &#x2F;bin, &#x2F;etc, &#x2F;lib, &#x2F;usr, &#x2F;tmp及运行Docke容器所需的配置文件、工具等。 在Docker架构中，当Docker daemon 为Docker容器挂载rootfs时，沿用了Linux内核启动时的方法，即将rootfs设为只读模式。在挂载完毕之后，利用联合挂载(union mount )技术在已有的只读rootfs上再挂载一个读写层。这样，可读写层处于Docker容器文件系统的最顶层，其下可能联合挂载多个只读层，只有在Docker容器运行过程中文件系统发生变化时，才会把变化的文件内容写到可读写层，并隐藏只读层中的老版本文件。 镜像的主要特点分层 Docker镜像是采用分层的方式构建的，每个镜像都由一系列的“镜像层”组成。分层结构是Docker镜像如此轻量的重要原因，当需要修改容器镜像内的某个文件时，只对处于最上方的读写层进行变动，不覆写下层已有文件系统的内容，已有文件在只读层中的原始版本仍然存在，但会被读写层中的新版文件所隐藏。当使用docker commit提交这个修改过的容器文件系统为一个新的镜像时，保存的内容仅为最上层读写文件系统中被更新过的文件。分层达到了在不同镜像之间共享镜像层的效果。 写时复制 Docker镜像使用了写时复制(copy-on-write)策略，在多个容器之间共享镜像，每个容器在启动的时候并不需要单独复制一份镜像文件，而是将所有镜像层以只读的方式挂载到一个挂载点，再在上面覆盖一个可读写的容器层。在未更改文件内容时，所有容器共享同一份数据，只有在Docker容器运行过程中文件系统发生变化时，才会把变化的文件内容写到可读写层，并隐藏只读层中的老版本文件。写时复制配合分层机制减少了镜像对磁盘空间的占用和容器启动时间。 内容寻址 内容寻址存储( content-addressable storage)的机制，根据文件内容来索引镜像和镜像层。docker对镜像层的内容计算校验和，生成一个内容哈希值，并以此哈希值代替之前的UUID作为镜像层的唯一标志。该机制主要提高了镜像的安全性，并在pull, push, load和save操作后检测数据的完整性。另外，基于内容哈希来索引镜像层，在一定程度上减少了ID的冲突并且增强了镜像层的共享。对于来自不同构建的镜像层，只要拥有相同的内容哈希，也能被不同的镜像共享。 联合挂载 通俗地讲，联合挂载技术可以在一个挂载点同时挂载多个文件系统，将挂载点的原目录与被挂载内容进行整合，使得最终可见的文件系统将会包含整合之后的各层的文件和目录。实现这种联合挂载技术的文件系统通常被称为联合文件系统(union filesystem )。 如下图所示，以运行Ubuntu:14.04镜像后容器中的aufs文件系统为例。由于初始挂载时读写层为空，所以从用户的角度看，该容器的文件系统与底层的rootfs没有差别;然而从内核的角度来看，则是显式区分开来的两个层次。当需要修改镜像内的某个文件时，只对处于最上方的读写层进行了变动，不覆写下层已有文件系统的内容，已有文件在只读层中的原始版本仍然存在，但会被读写层中的新版文件所隐藏，当docker commit这个修改过的容器文件系统为一个新的镜像时，保存的内容仅为最上层读写文件系统中被更新过的文件。 解答灵魂三问 (1). docker 镜像的本质是什么？ 答：是一个分层的文件系统。 (2). docker中一个centos镜像大约200M左右，为什么一个centos系统的iso安装文件要好几个G？ 答：centos的iso文件包括bootfs和rootfs，而docker的centos镜像复用操作系统的bootfs。 (3). docker中一个tomcat镜像大约500M左右，为什么一个tomcat安装包不足100M呢？ 答：docker中的镜像是分层的，tomcat虽然只有70多M，但是它需要依赖父镜像和基础镜像，所有整个对外暴露的tomcat镜像大约500M左右。","tags":["container","容器","Docker"],"categories":["Docker"]},{"title":"Docker 系列(二)：Dockerfile & Docker Compose","path":"//blog/docker/docker_compose/","content":"什么是 Dockerfile？ Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 下面通过一个具体的案例来展示如何使用 Dockerfile 来构建镜像。 1、下面以定制一个 nginx 镜像（构建好的镜像内会有一个 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html 文件） 在一个空目录下，新建一个名为 Dockerfile 文件，并在文件内添加以下内容： 12FROM nginxRUN echo &#x27;这是一个本地构建的nginx镜像&#x27; &gt; /usr/share/nginx/html/index.html 2、开始构建镜像 在 Dockerfile 文件的存放目录下，执行构建动作。 以下示例，通过目录下的 Dockerfile 构建一个 nginx:v3（镜像名称:镜像标签）。 注：最后的 . 代表本次执行的上下文路径，下一节会介绍。 1$ docker build -t nginx:v3 . Dockerfile 中所用到的命令可参考： Dockerfile | 菜鸟教程https://www.runoob.com/docker/docker-dockerfile.html Dockerfilehttps://docs.docker.com/engine/reference/builder/ Docker Compose Docker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用。 其代码目前在 https://github.com/docker/compose上开源。 Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。 它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 Compose 使用的三个步骤： 使用 Dockerfile 定义应用程序的环境。 使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行。 最后，执行 docker-compose up 命令来启动并运行整个应用程序。 docker-compose.yml 的配置案例如下（配置参数参考下文）： 12345678910111213141516# yaml 配置实例version: &#x27;3&#x27;services: web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code - logvolume01:/var/log links: - redis redis: image: redisvolumes: logvolume01: &#123;&#125; Docker Compose 中所用到的命令可参考： Docker Compose | 菜鸟教程https://www.runoob.com/docker/docker-compose.html Docker Composehttps://docs.docker.com/compose/","tags":["container","容器","Docker"],"categories":["Docker"]},{"title":"为什么说容器是单进程模型？","path":"//blog/docker/docker_single_process/","content":"僵尸进程/孤儿进程 在继续讲解之前，我介绍几个概念： 父进程：父进程拥有一系列的子进程。 子进程：父进程所管理的进程称为子进程。当一个子进程结束时，正常的情况下，父进程会对子进程进行善后处理（获取终止子进程的有关信息、释放它仍占用的资源）。 僵尸进程：当一个子进程终止时，其父进程没有对其进行善后处理，此时我们称这个子进程为僵尸进程。僵尸进程会一直等待父进程来处理自己，如果父进程没有对其进行处理，会造成资源的浪费。 孤儿进程：一个进程都有一个父进程来管理（系统进程除外），如果父进程在子进程结束之前终止，那么我们就称这个子进程为孤儿进程（失去了父亲）。 容器为单进程的原因 我们说 Docker 容器是”单进程模型”，并不代表容器只能运行一个进程，而是指容器没有回收孤儿进程的能力。 通常容器会起一个&#x2F;bin&#x2F;sh 进程（称为容器的 1 号进程），如果我们继续在容器中创建进程，那么新的进程都是这个 1 号进程的子进程。Docker 判断一个容器是否启动正常，是判断 Docker 容器的 1 号进程的状态，如果 1 号进程启动正常，那么就认为容器运行正常，否则，容器运行失败。 因此，如果在容器内部启动了过多的进程，那么当容器的 1 号进程结束之后，由于 1 号进程不具有管理多进程，多线程的能力，所以在容器内部创建的其他进程都会处于没有人接管的状态，此时这些进程都会变为孤儿进程。","tags":["container","容器"],"categories":["Docker"]},{"title":"Docker 系列(一)：什么是容器？","path":"//blog/docker/container/","content":"容器，到底是怎么一回事？ 容器其实是一种沙盒技术。顾名思义，沙盒就是能够像一个集装箱一样，把你的应用 “装” 起来的技术。这样，应用于应用之间，就因为有了边界而不至于互相干扰；而被装进集装箱的应用，也可以被方便的搬来搬去。 容器技术的核心功能，就是通过约束和修改进程（应用）的动态表现，从而为其创造一个 “边界” 动态表现：应用程序启动之后的设涉及到的数据和状态的总和。 在 Linux 中，实现容器的边界，主要有两种技术 Cgroups 和 Namespace. Cgroups 用于对运行的容器进行资源的限制； Namespace 则会将容器隔离起来，实现边界。 所以，容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，为它加上了各种各样的 Namespace 参数。 这时，容器进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，而对于宿主机以及其他不相关的程序，它就完全看不到了。 这样看来，容器只是一种被限制的了特殊进程而已。 虚拟机 vs 容器虚拟机：指的是在一个宿主机上搭建出来的一个完全隔离的环境，这个环境的特点就是，从底层的硬件开始逐级的进行虚拟，虚拟机中的 cpu 内存 硬盘等均进行虚拟（甚至包括网卡 显卡 声卡等也是虚拟的）在这一套虚拟的硬件基础上建立一个虚拟的操作系统，然后在这个虚拟的操作系统里运行应用程序。整套环境和宿主系统完全没有关系的。（可以在 windows 上跑一个 linux） Docker（容器技术）：不会虚拟硬件层，应用软件和系统之间仅隔着一个用于任务调度的 docker engine，docker engine 就是利用 linux 内核技术中的 namespace 和 cgroup（control group） 来隔离进程和资源。进而为宿主和容器，以及容器与容器创建相对独立的环境。这里的环境指的是文件系统，cpu，网络，内存等一系列的资源。这些资源并非独立于宿主之外的一套硬件系统，而是和宿主共用的。 应用程序和宿主共用一套内核空间 虚拟机：虚拟硬件，独立环境； 容器：资源隔离，半独立环境。 虚拟机和容器，两者没有绝对的好与坏，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 什么是 Docker说实话关于Docker是什么并太好说，下面我通过四点向你说明Docker到底是个什么东西。 Docker 是世界领先的软件容器平台。 Docker 是 Google 公司推出的，用 Go 语言进行开发实现，基于 Linux 内核的Cgroup，Namespace，以及 UnionFS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。Docke最初实现是基于 LXC. Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。 Docker 使用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 Docker 是容器技术的一种实现，并不是唯一的。只不过 docker 做的太成功了。所以一提容器技术就会想到 docker 。 docker 成功的地方就在于它将应用程序进行了打包生成镜像，并且提供了一个集中的发行平台。 docker 的侧重点：如果快速部署一个应用 Docker 容器特点 轻量：在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。 标准：Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。 安全：Docker 赋予应用的隔离性不仅限于彼此隔离，还独立于底层的基础设施。Docker 默认提供最强的隔离，因此应用出现问题，也只是单个容器的问题，而不会波及到整台机器。 Docker 存在的问题 资源隔离不彻底（使用 namesapce 和 cgroup 来实现资源隔离，但依然存在一个无法隔离的东西，例如系统时间，如果一个容器是以一个比较高的权限在运行时，可能会将该容器中的病毒传染给宿主系统） 跨平台受限（和技术实现方式有关，因为资源隔离用的是 linux 内核的技术） 容器间资源抢夺 为什么要用 Docker ? 一致的运行环境：Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题。 更快速的启动时间：可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。 隔离性：避免公用的服务器，资源会容易受到其他用户的影响。 弹性伸缩，快速扩展： 善于处理集中爆发的服务器使用压力。 迁移方便：可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。 持续交付和部署：使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。 Docker 概念Docker 包括三个基本概念: 镜像（Image）：Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:16.04 就包含了完整的一套 Ubuntu16.04 最小系统的 root 文件系统。 容器（Container）：镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 仓库（Repository）：仓库可看成一个代码控制中心，用来保存镜像。 Docker 使用客户端-服务器 (C&#x2F;S) 架构模式，使用远程API来管理和创建Docker容器。 Docker 容器通过 Docker 镜像来创建。 镜像(Image):一个特殊的文件系统操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。　比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 容器(Container):镜像运行时的实体镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 仓库(Repository):集中存放镜像文件的地方镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。","tags":["container","容器","Docker"],"categories":["Docker"]},{"title":"实现微服务高可用的种种手段","path":"//blog/design/highly-available/","content":"1. 什么是高可用在定义什么是高可用之前，我们可以先定义下什么是不可用：一个网站的内容最终呈现在用户面前需要经过若干个环节，而其中只要任何一个环节出现了故障，都可能导致网站页面不可访问，这个也就是网站不可用的情况。 参考维基百科，看看维基怎么定义高可用： 系统无中断地执行其功能的能力，代表系统的可用性程度，是进行系统设计时的准则之一。 这个难点或是重点在于 “无中断”，要做到 7x24 小时无中断无异常的服务提供。 2. 为什么需要高可用一套对外提供服务的系统是需要硬件、软件相结合，但是我们的硬件总是会出故障，软件会有 Bug，硬件会慢慢老化，网络总是不稳定，软件会越来越复杂和庞大。 除了硬件软件在本质上无法做到 “无中断”，外部环境也可能导致服务的中断，例如断电，地震，火灾，光纤被挖掘机挖断，这些影响的程度可能更大。 3. 高可用的评价纬度在业界有一套比较出名的评定网站可用性的指标，常用 N 个 9 来量化可用性，可以直接映射到网站正常运行时间的百分比上： 之前就职的一家互联网公司也是按照这个指标去界定可用性，不过在执行的过程中也碰到了一些问题。 例如，有一些服务的升级或数据迁移明明可以在深夜停机或停服务进行，然而考虑到以后的报告要显示出我们的系统达到了多少个 9 的高可用，而放弃停服务这种简单的解决方案，例如停机 2 个小时，就永远也达不到 4 个 9。 然而在一些高并发的场合，例如在秒杀或拼团，虽然服务停止了几分钟，但是这个对整个公司业务的影响可能是非常重大的，分分钟丢失的订单可能是一个庞大的数量。 所以 N 个 9 来量化可用性其实也得考虑业务的情况。 4. 微服务高可用设计手段高可用是一个比较复杂的命题，基本上在所有的处理中都会涉及到高可用，所有在设计高可用方案也涉及到了方方面面，如服务冗余、负载均衡、服务限流等。 这中间将会出现的细节是多种多样的，所以我们需要对这样一个微服务高可用方案进行一个顶层的设计，围绕服务高可用，先检查下我们手里有多少张牌。 4.1 服务冗余4.1.1 冗余策略每一个访问可能都会有多个服务组合而成，每个机器每个服务都可能出现问题，所以第一个考虑到的就是每个服务必须不止一份可以是多份。 所谓多份一致的服务就是服务的冗余，这里说的服务泛指了机器的服务，容器的服务，还有微服务本身的服务。 在机器服务层面需要考虑，各个机器间的冗余是否有在物理空间进行隔离冗余。 例如是否所有机器分别部署在不同机房，如果在同一个机房是否做到了部署在不同的机柜，如果是 Docker 容器是否部署在分别不同的物理机上面。 采取的策略其实也还是根据服务的业务而定，所以需要对服务进行分级评分，从而采取不同的策略。 不同的策略安全程度不同，伴随着的成本也是不同，安全等级更高的服务可能还不止考虑不同机房，还需要把各个机房所处的区域考虑进行。 例如，两个机房不要处在同一个地震带上等等。 4.1.2 无状态化服务的冗余会要求我们可以随时对服务进行扩容或者缩容，有可能我们会从 2 台机器变成 3 台机器。 想要对服务进行随时随地的扩缩容，就要求我们的服务是一个无状态化，所谓无状态化就是每个服务的服务内容和数据都是一致的。 例如，从我们的微服务架构来看，我们总共分水平划分了好几个层，正因为我们每个层都做到了无状态，所以在这个水平架构的扩张是非常的简单。 假设，我们需要对网关进行扩容，我们只需要增加服务就可以，而不需要去考虑网关是否存储了一个额外的数据。 网关不保存任何的 Session 数据，不提供会造成一致性的服务，将不一致的数据进行几种存储，借助更加擅长数据同步的中间件来完成。 这个是目前主流的方案，服务本身尽可能提供逻辑的服务，将数据的一致性保证集中式处理，这样就可以把 “状态” 抽取出来，让网关保持一个“无状态”。 这里仅仅是举了网关的例子，在微服务基本所有的服务，都应该按照这种思路去做。 如果服务中有状态，就应该把状态抽取出来，让更加擅长处理数据的组件来处理，而不是在微服务中去兼容有数据的状态。 4.2 数据存储高可用之前上面说的服务冗余，可以简单的理解为计算的高可用，计算高可用只需要做到无状态既可简单的扩容缩容，但是对于需要存储数据的系统来说，数据本身就是有状态。 跟存储与计算相比，有一个本质的差别：将数据从一台机器搬到另一台机器，需要经过线路进行传输。 网络是不稳定的，特别是跨机房的网络，Ping 的延时可能是几十几百毫秒，虽然毫秒对于人来说几乎没有什么感觉，但是对于高可用系统来说，就是本质上的不同，这意味着整个系统在某个时间点上，数据肯定是不一致的。 按照 “数据 + 逻辑 &#x3D; 业务” 的公式来看，数据不一致，逻辑一致，最后的业务表现也会不一致。 举个例子： 无论是正常情况下的传输延时，还是异常情况下的传输中断，都会导致系统的数据在某个时间点出现不一致。 而数据的不一致又会导致业务出现问题，但是如果数据不做冗余，系统的高可用无法保证。 所以，存储高可用的难点不在于怎么备份数据，而在于如何减少或者规避数据不一致对业务造成的影响。 分布式领域中有一个著名的 CAP 定理，从理论上论证了存储高可用的复杂度，也就是说，存储高可用不可能同时满足 “一致性，可用性，分区容错性”。 最多只能满足 2 个，其中分区容错在分布式中是必须的，就意味着，我们在做架构设计时必须结合业务对一致性和可用性进行取舍。 存储高可用方案的本质是将数据复制到多个存储设备中，通过数据冗余的方式来实现高可用，其复杂度主要呈现在数据复制的延迟或中断导致数据的不一致性。 我们在设计存储架构时必须考虑到以下几个方面： 数据怎么进行复制 架构中每个节点的职责是什么 数据复制出现延迟怎么处理 当架构中节点出现错误怎么保证高可用 4.2.1 数据主从复制主从复制是最常见的也是最简单的存储高可用方案，例如 MySQL，Redis 等等。 其架构的优点就是简单，主机复制写和读，而从机只负责读操作，在读并发高时候可用扩张从库的数量减低压力，主机出现故障，读操作也可以保证读业务的顺利进行。 缺点就是客户端必须感知主从关系的存在，将不同的操作发送给不同的机器进行处理。 而且主从复制中，从机器负责读操作，可能因为主从复制时延大，出现数据不一致性的问题。 4.2.2 数据主从切换刚说了主从复制存在两个问题： 主机故障写操作无法进行。 需要人工将其中一台从机器升级为主机。 为了解决这个两个问题，我们可以设计一套主从自动切换的方案，其中涉及到对主机的状态检测，切换的决策，数据丢失和冲突的问题。 主机状态检测： 需要多个检查点来检测主机的机器是否正常，进程是否存在，是否出现超时，是否写操作不可执行，是否读操作不可执行，将其进行汇总，交给切换决策。 切换决策： 确定切换的时间决策，什么情况下从机就应该升级为主机，是进程不存在，是写操作不可行，连续检测多少失败次就进行切换。 应该选择哪一个从节点升级为主节点，一般来说或应该选同步步骤最大的从节点来进行升级。切换是自动切换还是半自动切换，通过报警方式，让人工做一次确认。 数据丢失和数据冲突： 数据写到主机，还没有复制到从机，主机就挂了，这个时候怎么处理，这个也得考虑业务的方式，是要确保 CP 或 AP。 还要考虑一个数据冲突的问题，这个问题在 MySQL 中大部分是由自增主键引起。 就算不考虑自增主键会引起数据冲突的问题，其实自增主键还要引起很多的问题，这里不细说，避免使用自增主键。 4.2.3 数据分片上述的数据冗余可以通过数据的复制来进行解决，但是数据的扩张需要通过数据的分片来进行解决（如果在关系型数据库是分表）。 何为数据分片（Segment、Fragment、Shard、 Partition），就是按照一定的规则，将数据集划分成相互独立、正交的数据子集，然后将数据子集分布到不同的节点上。 HDFS ， MongoDB 的 Sharding 模式也基本是基于这种分片的模式去实现。 我们在设计分片主要考虑到的点是： 做数据分片，如何将数据映射到节点。 数据分片的特征值，即按照数据中的哪一个属性（字段）来分片。 数据分片的元数据的管理，如何保证元数据服务器的高性能、高可用，如果是一组服务器，如何保证强一致性。 4.3 柔性化 &#x2F; 异步化4.3.1 异步化在每一次调用，时间越长存在超时的风险就越大，逻辑越复杂执行的步骤越多，存在失败的风险也就越大。 如果在业务允许的情况下，用户调用只给用户必须要的结果，而不是需要同步的结果可以放在另外的地方异步去操作，这就减少了超时的风险也把复杂业务进行拆分减低复杂度。 当然异步化的好处是非常多，例如削峰解耦等等，这里只是从可用的角度出发。 异步化大致有这三种的实现方式： 服务端接收到请求后，创建新的线程处理业务逻辑，服务端先回应答给客户端。 服务端接收到请求后，服务端先回应答给客户端，再继续处理业务逻辑。 服务端接收到请求后，服务端把信息保存在消息队列或者数据库，回应答给客户端，服务端业务处理进程再从消息队列或者数据库上读取信息处理业务逻辑。 4.3.2 柔性化什么是柔性化？想象一个场景，我们的系统会给每个下单的用户增加他们下单金额对应的积分，当一个用户下单完毕后，我们给他增加积分的服务出现了问题。 这个时候，我们是要取消掉这个订单还是先让订单通过，积分的问题通过重新或者报警来处理呢？ 所谓的柔性化，就是在我们业务中允许的情况下，做不到给予用户百分百可用的，通过降级的手段给到用户尽可能多的服务，而不是非得每次都交出去要么 100 分或 0 分的答卷。 怎么去做柔性化，更多其实是对业务的理解和判断，柔性化更多是一种思维，需要对业务场景有深入的了解。 在电商订单的场景中，下单，扣库存，支付是一定要执行的步骤，如果失败则订单失败。 但是加积分，发货，售后是可以柔性处理，就算出错也可以通过日志报警让人工去检查，没必要为加积分损失整个下单的可用性。 4.4 兜底 &#x2F; 容错兜底可能是我们经常谈论的一种降级的方案，方案是用来实施，但是这里兜底可能更多是一种思想，更多的是一种预案，每个操作都可以犯错，我们也可以接受犯错。 但是每个犯错我们都必须有一个兜底的预案，这个兜底的预案其实就是我们的容错或者说最大程度避免更大伤害的措施，实际上也是一个不断降级的过程。 举个例子： 例如我们首页请求的用户个性化推荐商品的接口，发现推荐系统出错，我们不应该去扩大（直接把异常抛给用户）或保持调用接口的错误，而是应该兼容调用接口的错误，做到更加柔性化。 这时候可以选择获取之前没有失败接口的缓存数据，如果没有则可以获取通用商品不用个性化推荐，如果也没有可以读取一些静态文字进行展示。 由于我们架构进行了分层，分层 App，网关，业务逻辑层，数据访问层等等，在组织结构也进行了划分，与之对应的是前端组，后端业务逻辑组，甚至有中台组等等。 既然有代码和人员架构的层级划分，那么每一层都必须有这样的思想：包容下一层的错误，为上一层提供尽可能无错的服务。 举个例子： 商品的美元售价假设要用商品人民币售价 &#x2F; 汇率，这个时候错误发生在低层的数据层，上一层如果直接进行除，肯定就抛出 java.lang.ArithmeticException: &#x2F; by zero。 本着我们对任何一层调用服务都不可信的原则，应该对其进行容错处理，不能让异常扩散，更要保证我们这一层对上一次尽可能的作出最大努力确定的服务。 4.5 负载均衡相信负载均衡这个话题基本已经深入每个做微服务开发或设计者的人心，负载均衡的实现有硬件和软件。 硬件有 F5，A10 等机器；软件有 LVS，Nginx，HAProxy 等等，负载均衡的算法有 Random，RoundRobin，ConsistentHash 等等。 4.5.1 Nginx 负载均衡故障转移 转移流程：Nginx 根据给定好的负载均衡算法进行调度，当请求到 Tomcat1，Nginx 发现 Tomcat1 出现连接错误（节点失效），Nginx 会根据一定的机制将 Tomcat1 从调用的负载列表中清除。 在下一次请求，Nginx 不会分配请求到有问题的 Tomcat1 上面，会将请求转移到其他的 Tomcat 之上。 节点失效： Nginx 默认判断节点失效是以 connect refuse 和 timeout 为标准，在对某个节点进行 fails 累加，当 fails 大于 max_fails 时，该节点失效。 节点恢复： 当某个节点失败的次数大于 max_fails 时，但不超过 fail_timeout，Nginx 将不再对该节点进行探测，直到超过失效时间或者所有的节点都失效，Nginx 会对节点进行重新探测。 4.5.2 zookeeper 负载均衡故障转移 在使用 zookeeper 作为注册中心时，故障的发现是由 ZK 去进行发现，业务逻辑层通过 Watch 的心跳机制将自己注册到 ZK 上，网关对 ZK 进行订阅就可以知道有多少可以调用的列表。 当业务逻辑层在重启或者被关闭时就会跟 ZK 断了心跳，ZK 会更新可调用列表。 使用 ZK 作为负载均衡的协调器，最大的问题是 ZK 对于服务是否可用是基于 Pingpong 的方式。 只要服务心跳存在，ZK 就认为服务是处在可用状态，但是服务如果处在假死的状态，ZK 是无从得知的。这个时候，业务逻辑服务是否真正可用只能够由网关知道。 幂等设计： 为何会牵出幂等设计的问题，主要是因为负载均衡的 Failover 策略，就是对失败的服务会进行重试。 一般来说，如果是读操作的服务，重复执行也不会出问题，但想象一下，如果是一个创建订单减库存的操作，第一次调用也 Tomcat1 超时，再重新调用了 Tomcat2。 这个时候我们都不能确认超时调用的 Tomcat1 是否真的被调用，有可能根本就调用不成功，有可能已经调用成功但是因为某些原因返回超时而已。 所以，很大程度这个接口会被调用 2 次。如果我们没有保证幂等性，就有可能一个订单导致了减少 2 次的库存。 所谓的幂等性，就是得保证在同一个业务中，一个接口被调用了多次，其导致的结果都是一样的。 4.6 服务限流降级熔断先来讲讲微服务中限流 &#x2F; 熔断的目的是什么，微服务后，系统分布式部署，系统之间通过 RPC 框架通信，整个系统发生故障的概率随着系统规模的增长而增长，一个小的故障经过链路的传递放大，有可能会造成更大的故障。 限流跟高可用的关系是什么？假定我们的系统最多只能承受 500 个人的并发访问，但某个时候突然增加到 1000 个人进来，一下子就把整个系统给压垮了。 本来还有 500 个人能享受到我们系统的服务，突然间变成了所有人都无法得到服务。 与其让 1000 人都无法得到服务，不如就让 500 个人得到服务，拒绝掉另外 500 个人。限流是对访问的隔离，是保证了部门系统承受范围内用户的可用性。 熔断跟高可用的关系是什么？上面说了微服务是一个错综复杂的调用链关系，假设模块 A 调用模块 B，模块 B 又调用了模块 C，模块 C 调用了模块 D。 这个时候，模块 D 出了问题出现严重的时延，这个时候，整个调用链就会被模块 D 给拖垮。 A 等 B，B 等 C，C 等 D，而且 A B C D 的资源被锁死得不到释放，如果流量大的话还容易引起雪崩。 熔断，主动丢弃模块 D 的调用，并在功能上作出一些降级才能保证到我们系统的健壮性。熔断是对模块的隔离，是保证了最大功能的可用性。 4.7 服务治理4.7.1 服务模块划分服务模块与服务模块之间有着千丝万缕的关系，但服务模块在业务中各有权重。 例如订单模块可能是一家电商公司的重中之重，如果出问题将会直接影响整个公司的营收。 而一个后台的查询服务模块可能也重要，但它的重要等级绝对是没有像订单这么重要。 所以，在做服务治理时，必须明确各个服务模块的重要等级，这样才能更好的做好监控，分配好资源。 这个在各个公司有各个公司的一个标准，例如在电商公司，确定服务的级别可能会更加倾向对用户请求数和营收相关的作为指标。 可能真正的划分要比这个更为复杂，必须根据具体业务去定，这个可以从平时服务模块的访问量和流量去预估。 往往更重要的模块也会提供更多的资源，所以不仅要对技术架构了如指掌，还要对公司各种业务形态了然于心才可以。 服务分级不仅仅在故障界定起到重要主要，而且决定了服务监控的力度，服务监控在高可用中起到了一个保障的作用。 它不仅可以保留服务崩溃的现场以等待日后复盘，更重要的是它可以起到一个先知，先行判断的角色，很多时候可以预先判断危险，防范于未然。 4.7.2 服务监控服务监控是微服务治理的一个重要环节，监控系统的完善程度直接影响到我们微服务质量的好坏。 我们的微服务在线上运行的时候有没有一套完善的监控体系能去了解到它的健康情况，对整个系统的可靠性和稳定性是非常重要，可靠性和稳定性是高可用的一个前提保证。 服务的监控更多是对于风险的预判，在出现不可用之间就提前的发现问题，如果系统获取监控报警系统能自我修复则可以将错误消灭在无形，如果系统发现报警无法自我修复则可以通知人员提早进行接入。 一个比较完善的微服务监控体系需要涉及到哪些层次？如下图，大致可以划分为五个层次的监控： 基础设施监控： 例如网络，交换机，路由器等低层设备，这些设备的可靠性稳定性就直接影响到上层服务应用的稳定性。所以需要对网络的流量，丢包情况，错包情况，连接数等等这些基础设施的核心指标进行监控。 系统层监控： 涵盖了物理机，虚拟机，操作系统这些都是属于系统级别监控的方面，对几个核心指标监控，如 CPU 使用率，内存占用率，磁盘 IO 和网络带宽情况。 应用层监控： 例如对 URL 访问的性能，访问的调用数，访问的延迟，还有对服务提供性能进行监控，服务的错误率。对 SQL 也需要进行监控，查看是否有慢 SQL，对于 Cache 来说，需要监控缓存的命中率和性能，每个服务的响应时间和 QPS 等等。 业务监控： 比方说一个电商网站，需要关注它的用户登录情况，注册情况，下单情况，支付情况。这些直接影响到实际触发的业务交易情况，这个监控可以提供给运营和公司高管他们需要关注的数据，直接可能对公司战略产生影响。 端用户监控： 用户通过浏览器，客户端打开连到到我们的服务，那么在用户端用户的体验是怎么样，用户端的性能是怎么样，有没有产生错误，这些信息也是需要进行监控并记录下来。如果没有监控，有可能用户因为某些原因出错或者性能问题造成体验非常的差，而我们并没有感知。 这里面包括了，监控用户端的使用性能，返回码，在哪些城市地区他们的使用情况是怎么样，还有运营商的情况，包括电信，联通用户的连接情况。 我们需要进一步去知道是否有哪些渠道哪些用户接入的时候存在着问题，包括我们还需要知道客户端使用的操作系统浏览器的版本。 5. 总结出了那么多张牌，出牌只是术，真正的道还是得静下心来看看整个服务高可用的本质是什么。 随着微服务架构的相互调用越来越复杂，环节只会越来越多，只有建立清晰的架构和层次才能理清楚每个环节高可用的保障，保持简单。 5.1 从手段看高可用主要使用的技术手段是服务和数据的冗余备份和失效转移，一组服务或一组数据都能在多节点上，之间相互备份。 当一台机器宕机或出现问题的时候，可以从当前的服务切换到其他可用的服务，不影响系统的可用性，也不会导致数据丢失。 5.2 从架构看高可用保持简单的架构，目前多数网站采用的是比较经典的分层架构，应用层，服务层，数据层。 应用层是处理一些业务逻辑，服务层提供一些数据和业务紧密相关服务，数据层负责对数据进行读写。 简单的架构可以使应用层，服务层可以保持无状态化进行水平扩展，这个属于计算高可用。 相比计算高可用，在数据层思考的高可用则属于数据高可用，数据高可用相比计算高可用需要考虑到数据的一致性问题会更加的复杂。 这个时候 CAP 理论在里面会发挥关键的作用，究竟是选择 AP 或 CP，这个得根据业务去选择模型。 5.3 从硬件看高可用首先得确认硬件总是可能坏的，网络总是不稳定的。解决它的方法也是一个服务器不够就来多几个，一个机柜不够就来几个，一个机房不够就来几个。 5.4 从软件看高可用软件的开发不严谨，发布不规范也是导致各种不可用出现，通过控制软件开发过程质量监控，通过测试，预发布，灰度发布等手段也是减少不可用的措施。 5.5 从治理看高可用一个系统在线上跑的好好的，但我们也不能确保它在下一秒会不会出现不可用状态。 将服务规范化，事前做好服务分割，做好服务监控，预判不可用的出现，在不可用出现之前发现问题，解决问题。","tags":["高可用","微服务"],"categories":["高可用","微服务"]},{"title":"GIT 在线练习平台","path":"//blog/tools/git-online/","content":"这是个叫做 Learning Git Branching 的项目，是我一定要推荐的： 正如对话框中的自我介绍，这确实也是我至今发现的最好的 Git 动画教程，没有之一。 想当年我用 Git 就会 add .，clone，push，pull，commit 几个命令，其他的命令完全不会，Git 就是一个下载器，Github 就是个资源网站加免费图床，命令能不能达成目的都是靠运气。什么版本控制，我根本搞不懂，也懒得去看那一堆乱七八糟的文档。 这个网站的教程不是给你举那种修改文件的细节例子，而是将每次 commit 都抽象成树的节点，用动画闯关的形式，让你自由使用 Git 命令完成目标： 所有 Git 分支都被可视化了，你只要在左侧的命令行输入 Git 命令，分支会进行相应的变化，只要达成任务目标，你就过关啦！网站还会记录你的命令数，试试能不能以最少的命令数过关！ 我一开始以为这个教程只包含本地 Git 仓库的版本管理，后来我惊奇地发现它还有远程仓库的操作教程！ 真的跟玩游戏一样，难度设计合理，流畅度很好，我一玩都停不下来了，几小时就打通了，哈哈哈！ 总之，这个教程很适合初学和进阶，如果你觉得自己对 Git 的掌握还不太好，用 Git 命令还是靠碰运气，就可以玩玩这个教程，相信能够让你更熟练地使用 Git。 它是一个开源项目，Github 项目地址： https://github.com/pcottle/learnGitBranching 教程网站地址： https://learngitbranching.js.org","tags":["开发工具","git"],"categories":["开发工具"]},{"title":"正则表达式 在线练习平台","path":"//blog/tools/regex-online/","content":"正则表达式是个非常强有力的工具，可以说计算机中的一切数据都是字符，借助正则表达式这种模式匹配工具，操作计算机可以说是如虎添翼。 我这里要推荐两个网站，一个是练习平台，一个是测试正则表达式的平台。 先说练习平台，叫做 RegexOne： 前面有基本教程，后面有一些常见的正则表达式题目，比如判断邮箱、URL、电话号，或者抽取日志的关键信息等等。 只要写出符合要求的正则表达式，就可以进入下一个问题，关键是每道题还有标准答案，可以点击下面的 solution 按钮查看： RegexOne 网址： https://regexone.com/ 再说测试工具，是个叫做 RegExr 的 Github 项目，这是它的网站： 可以看见，输入文本和正则模式串后，网站会给正则表达式添加好看且容易辨认的样式，自动在文本中搜索模式串，高亮显示匹配的字符串，并且还会显示每个分组捕获的字符串。 这个网站可以配合前面的正则练习平台使用，在这里尝试各种表达式，成功匹配之后粘贴过去。 RegExr 网址： https://regexr.com/","tags":["开发工具","regex","正则表达式"],"categories":["开发工具"]},{"title":"SQL 在线练习平台","path":"//blog/tools/sql-online/","content":"这是一个叫做 SQLZOO 的网站，左侧是所有的练习内容： SQLZOO 是一款很好用的 SQL 练习平台，英文不难理解，可以直接看英文版，但是也可以切换繁体中文，比较友好。 这里都是比较常用的 SQL 命令，给你一个需求，你写 SQL 语句实现正确的查询结果。最重要的是，这里不仅对每个命令的用法有详细解释，每个专题后面还有选择题（quiz），而且有判题系统，甚至有的比较难的题目还有视频讲解： 至于难度，循序渐进，即便对新手也很友好，靠后的问题确实比较有技巧性，相信这是热爱思维挑战的人喜欢的！LeetCode 也有 SQL 相关的题目，不过难度一般比较大，我觉得 SQLZOO 刷完基础 SQL 命令再去 LeetCode 刷比较合适。 网站地址： https://sqlzoo.net/","tags":["开发工具","sql"],"categories":["开发工具"]},{"title":"Redis 持久化： AOF & RDB","path":"//blog/redis/redis_persistence/","content":"Redis 的一个普遍使用场景是把它当作缓存使用，因为它把后端数据库中的数据存储在内存中，然后直接从内存中读取数据，响应速度会非常快。但是，这里也有一个绝对不能忽略的问题：一旦服务器宕机，内存中的数据将全部丢失。 很容易想到的一个解决方案是，从后端数据库恢复这些数据，但这种方式存在两个问题：一是，需要频繁访问数据库，会给数据库带来巨大的压力；二是，这些数据是从慢速数据库中读取出来的，性能肯定比不上从 Redis 中读取，导致使用这些数据的应用程序响应变慢。所以，对 Redis 来说，实现数据的持久化，避免从后端数据库中进行恢复，是至关重要的。目前，Redis 的持久化主要有两大机制，即 AOF（Append Only File）日志和 RDB 快照。 AOF说到日志，我们比较熟悉的是数据库的写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。不过，AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志，如下图所示： AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。 我们以 Redis 收到“set testkey testvalue”命令后记录的日志为例，看看 AOF 日志的内容。 其中，“*3”表示当前命令有三个部分，每部分都是由“$+数字”开头，后面紧跟着具体的命令、键或值。这里，“数字”表示这部分中的命令、键或值一共有多少字节。例如，“$3 set”表示这部分有 3 个字节，也就是“set”命令。 好处 避免额外的检查命令开销 不会阻塞当前的写操作 风险 数据丢失：如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。 阻塞风险：AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。 仔细分析的话，这两个风险都是和 AOF 写回磁盘的时机相关的。这也就意味着，如果我们能够控制一个写命令执行完后 AOF 日志写回磁盘的时机，这两个风险就解除了。 三种写回策略AOF 机制给我们提供了三个选择，也就是 AOF 配置项 appendfsync 的三个可选值。 Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘； Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘； No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。 “同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能； 虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了； “每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。 想要获得高性能，就选择 No 策略；如果想要得到高可靠性保证，就选择 Always 策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择 Everysec 策略。 AOF 重写AOF 是以文件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越大。这也就意味着，我们一定要小心 AOF 文件过大带来的性能问题。 这里的“性能问题”，主要在于以下三个方面： 一是，文件系统本身对文件大小有限制，无法保存过大的文件； 二是，如果文件太大，之后再往里面追加命令记录的话，效率也会变低； 三是，如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。 所以，我们就要采取一定的控制手段，这个时候，AOF 重写机制就登场了。 简单来说，AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。 比如说，当读取了键值对“testkey”: “testvalue”之后，重写机制会记录 set testkey testvalue 这条命令。这样，当需要恢复时，可以重新执行该命令，实现“testkey”: “testvalue”的写入。 虽然 AOF 重写后，日志文件会缩小，但是，要把整个数据库的最新数据的操作日志都写回磁盘，仍然是一个非常耗时的过程。 和 AOF 日志由主线程写回不同，重写过程是由后台子进程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。 我把重写的过程总结为“一个拷贝，两处日志”。 “一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。 “两处日志”又是什么呢？ 因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。 总结来说，每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。 RDBAOF 是每次执行只需要记录操作命令，需要持久化的数据量不大。一般而言，只要你采用的不是 always 的持久化策略，就不会对性能造成太大影响。但是，也正因为记录的是操作命令，而不是实际的数据，所以，用 AOF 方法进行故障恢复的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。 RDB 采用的是内存快照的方式。所谓内存快照，就是指内存中的数据在某一个时刻的状态记录。对 Redis 来说，就是把某一时刻的状态以文件的形式写到磁盘上，也就是快照。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就得到了保证。 和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。 全量快照Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中。 这样做的好处是，一次性记录了所有数据，一个都不少。当你给一个人拍照时，只用协调一个人就够了，但是，拍 100 人的大合影，却需要协调 100 个人的位置、状态，等等，这当然会更费时费力。同样，给内存的全量数据做快照，把它们全部写入磁盘也会花费很多时间。而且，全量数据越多，RDB 文件就越大，往磁盘上写数据的时间开销就越大。 Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。 save：在主线程中执行，会导致阻塞； bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。 除此之外，Redis 借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。 此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本（键值对 C’）。然后，主线程在这个数据副本上进行修改。同时，bgsave 子进程可以继续把原来的数据（键值对 C）写入 RDB 文件。 这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。 增量快照对于快照来说，所谓“连拍”就是指连续地做快照。这样一来，快照的间隔时间变得很短，即使某一时刻发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。 但是，如果频繁地执行全量快照，也会带来两方面的开销。 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。 此时，我们可以做增量快照，所谓增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。 增量快照的问题：如果我们对每一个键值对的修改，都做个记录，那么，如果有 1 万个被修改的键值对，我们就需要有 1 万条额外的记录。而且，有的时候，键值对非常小，比如只有 32 字节，而记录它被修改的元数据信息，可能就需要 8 字节，这样的画，为了“记住”修改，引入的额外空间开销比较大。这对于内存资源宝贵的 Redis 来说，有些得不偿失。 混合快照Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。 这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。 总结最后，关于 AOF 和 RDB 的选择问题，有三点建议： 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择； 如果允许分钟级别的数据丢失，可以只使用 RDB； 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。","tags":["可靠性","redis","持久化"],"categories":["Redis"]},{"title":"Mysql 性能优化","path":"//blog/mysql/performance_optimization/","content":"数据库优化维度有四个： 硬件升级 系统配置 表结构设计 SQL语句及索引 优化选择： 优化成本：硬件升级 &gt; 系统配置 &gt; 表结构设计 &gt; SQL语句及索引。 优化效果：硬件升级 &lt; 系统配置 &lt; 表结构设计 &lt; SQL语句及索引。 系统配置优化从内存中读取数据MySQL 会在内存中保存一定的数据，通过LRU算法将不常访问的数据保存在硬盘文件中。尽可能的扩大内存中的数据量，将数据保存在内存中，从内存中读取数据，可以提升MySQL性能。 扩大innodb_buffer_pool_size，能够全然从内存中读取数据。最大限度降低磁盘操作。 123456789101112mysql&gt; show global status like &#x27;innodb_buffer_pool_pages_%&#x27;;+----------------------------------+-------+| Variable_name | Value |+----------------------------------+-------+| Innodb_buffer_pool_pages_data | 8190 || Innodb_buffer_pool_pages_dirty | 0 || Innodb_buffer_pool_pages_flushed | 12646 || Innodb_buffer_pool_pages_free | 0 | 0 表示已经被用光| Innodb_buffer_pool_pages_misc | 1 || Innodb_buffer_pool_pages_total | 8191 |+----------------------------------+-------+ innodb_buffer_pool_size默认为128M，理论上可以扩大到内存的3&#x2F;4或4&#x2F;5。 预热数据默认情况，仅在某条数据被读取一次之后，才会缓存在 innodb_buffer_pool。所以，数据库刚刚启动，须要进行数据预热，将磁盘上的数据尽可能缓存到内存中。 数据预热能够提高读取速度。 对于InnoDB数据库，进行数据预热的脚本是: loadtomem.sql1234567891011121314151617181920212223242526SELECT DISTINCT CONCAT(&#x27;SELECT &#x27;,ndxcollist,&#x27; FROM &#x27;,db,&#x27;.&#x27;,tb,&#x27; ORDER BY &#x27;,ndxcollist,&#x27;;&#x27;) SelectQueryToLoadCacheFROM ( SELECT engine,table_schema db,table_name tb, index_name,GROUP_CONCAT(column_name ORDER BY seq_in_index) ndxcollist FROM ( SELECT B.engine,A.table_schema,A.table_name, A.index_name,A.column_name,A.seq_in_index FROM information_schema.statistics A INNER JOIN ( SELECT engine,table_schema,table_name FROM information_schema.tables WHERE engine=&#x27;InnoDB&#x27; ) B USING (table_schema,table_name) WHERE B.table_schema NOT IN (&#x27;information_schema&#x27;,&#x27;mysql&#x27;) ORDER BY table_schema,table_name,index_name,seq_in_index ) A GROUP BY table_schema,table_name,index_name ) AAORDER BY db,tb; 在需要数据预热时（比如重启数据库）执行命令： 1mysql -uroot &lt; /root/loadtomem.sql &gt; /dev/null 2&gt;&amp;1 减少磁盘写入次数1. 增大redolog，减少落盘次数 innodb_log_file_size 设置为 0.25 * innodb_buffer_pool_size 2. 关闭通用查询日志、慢查询日志 ，开启binlog 生产中不开通用查询日志，遇到性能问题开慢查询日志 3. 写redolog策略 innodb_flush_log_at_trx_commit设置为0或2 如果不涉及非常高的安全性 (金融系统)，或者基础架构足够安全，或者事务都非常小，都能够用 0或者 2 来减少磁盘操作。 提高磁盘读写性能使用SSD或者内存磁盘 表结构优化1. 设计中间表 设计中间表，一般针对于统计分析功能，或者实时性不高的需求（OLTP、OLAP） 2. 设计冗余字段 为减少关联查询，创建合理的冗余字段（创建冗余字段还需要注意数据一致性问题） 3. 拆表 对于字段太多的大表，考虑拆表（比如一个表有100多个字段） 对于表中经常不被使用的字段或者存储数据比较多的字段，考虑拆表 4. 主键优化 每张表建议都要有一个主键（主键索引），而且主键类型最好是int类型，建议自增主键。 5. 字段的设计 数据库中的表越小，在它上面执行的查询也就会越快。因此，在创建表的时候，为了获得更好的性能，我们可以将表中字段的宽度设得尽可能小。 尽量把字段设置为NOTNULL，这样在将来执行查询的时候，数据库不用去比较NULL值。 对于某些文本字段，例如“省份”或者“性别”，我们可以将它们定义为ENUM类型。因为在MySQL中，ENUM类型被当作数值型数据来处理，而数值型数据被处理起来的速度要比文本类型快得多。 SQL语句及索引优化1. SQL语句中IN包含的值不应过多 MySQL对于IN做了相应的优化，即将IN中的常量全部存储在一个数组里面，而且这个数组是排好序的。但是如果数值较多，产生的消耗也是比较大的。 2. SELECT语句务必指明字段名称 SELECT * 增加很多不必要的消耗（CPU、IO、内存、网络带宽）；减少了使用覆盖索引的可能性；当表结构发生改变时，前端也需要更新。所以要求直接在select后面接上字段名。 12345678910111213141516171819mysql&gt; explain select * from tbiguser ;+----+-------------+----------+------+---------------+------+---------+------+---------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+------+---------------+------+---------+------+---------+-------+| 1 | SIMPLE | tbiguser | ALL | NULL | NULL | NULL | NULL | 9754360 | NULL |+----+-------------+----------+------+---------------+------+---------+------+---------+-------+1 row in set (0.00 sec)mysql&gt; explain select id,nickname from tbiguser ;+----+-------------+----------+-------+---------------+--------------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+-------+---------------+--------------+---------+------+---------+-------------+| 1 | SIMPLE | tbiguser | index | NULL | idx_nickname | 768 | NULL | 9754360 | Using index |+----+-------------+----------+-------+---------------+--------------+---------+------+---------+-------------+1 row in set (0.00 sec) 3. 当只需要一条数据的时候，使用limit 1 limit 是可以停止全表扫描 12345678910111213141516171819mysql&gt; select * from tbiguser limit 1;+----+----------+-----------+------+------+--------+---------+| id | nickname | loginname | age | sex | status | address |+----+----------+-----------+------+------+--------+---------+| 1 | zy1 | zhaoyun1 | 23 | 1 | 1 | beijing |+----+----------+-----------+------+------+--------+---------+1 row in set (0.00 sec)mysql&gt; explain select * from tbiguser limit 1;+----+-------------+----------+------+---------------+------+---------+------+---------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+------+---------------+------+---------+------+---------+-------+| 1 | SIMPLE | tbiguser | ALL | NULL | NULL | NULL | NULL | 9754360 | NULL |+----+-------------+----------+------+---------------+------+---------+------+---------+-------+1 row in set (0.00 sec) 4. 排序字段加索引 1234567891011121314151617mysql&gt; explain select * from tbiguser where loginname = &#x27;zhaoyun9999999&#x27; order by id ;+----+-------------+----------+-------+---------------+---------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+-------+---------------+---------+---------+------+---------+-------------+| 1 | SIMPLE | tbiguser | index | NULL | PRIMARY | 4 | NULL | 9754360 | Using where |+----+-------------+----------+-------+---------------+---------+---------+------+---------+-------------+1 row in set (0.01 sec)mysql&gt; explain select * from tbiguser where loginname = &#x27;zhaoyun9999999&#x27; order by loginname ;+----+-------------+----------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | tbiguser | ALL | NULL | NULL | NULL | NULL | 9754360 | Using where |+----+-------------+----------+------+---------------+------+---------+------+---------+-------------+1 row in set (0.00 sec) 5. 如果限制条件中其他字段没有索引，尽量少用or or两边的字段中，如果有一个不是索引字段，会造成该查询不走索引的情况。 12345678mysql&gt; explain select * from tbiguser where nickname=&#x27;zy1&#x27; or loginname=&#x27;zhaoyun3&#x27;;+----+-------------+----------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | tbiguser | ALL | idx_nickname | NULL | NULL | NULL | 9754360 | Using where |+----+-------------+----------+------+---------------+------+---------+------+---------+-------------+1 row in set (0.00 sec) 6. 尽量用union all代替union union和union all的差异主要是前者需要将结果集合并后再进行唯一性过滤操作，这就会涉及到排序，增加大量的CPU运算，加大资源消耗及延迟。当然，union all的前提条件是两个结果集没有重复数据。 7. 不使用ORDER BY RAND() ORDER BY RAND() 不走索引 12345678910111213141516171819202122232425mysql&gt; select * from tbiguser order by rand() limit 10;+---------+-----------+----------------+------+------+--------+---------+| id | nickname | loginname | age | sex | status | address |+---------+-----------+----------------+------+------+--------+---------+| 416412 | zy416412 | zhaoyun416412 | 23 | 1 | 1 | beijing || 4338012 | zy4338012 | zhaoyun4338012 | 23 | 1 | 1 | beijing || 4275017 | zy4275017 | zhaoyun4275017 | 23 | 1 | 1 | beijing || 8572779 | zy8572779 | zhaoyun8572779 | 23 | 1 | 1 | beijing || 2500546 | zy2500546 | zhaoyun2500546 | 23 | 1 | 1 | beijing |+---------+-----------+----------------+------+------+--------+---------+10 rows in set (10.86 sec)mysql&gt; select * from tbiguser t1 join (select rand()*(select max(id) from tbiguser) nid ) t2 on t1.id&gt;t2.nid limit 10;+---------+-----------+----------------+------+------+--------+---------+-------------------+| id | nickname | loginname | age | sex | status | address | nid |+---------+-----------+----------------+------+------+--------+---------+-------------------+| 6580156 | zy6580156 | zhaoyun6580156 | 23 | 1 | 1 | beijing | 6580155.591089424 || 6580157 | zy6580157 | zhaoyun6580157 | 23 | 1 | 1 | beijing | 6580155.591089424 || 6580158 | zy6580158 | zhaoyun6580158 | 23 | 1 | 1 | beijing | 6580155.591089424 || 6580159 | zy6580159 | zhaoyun6580159 | 23 | 1 | 1 | beijing | 6580155.591089424 || 6580160 | zy6580160 | zhaoyun6580160 | 23 | 1 | 1 | beijing | 6580155.591089424 |+---------+-----------+----------------+------+------+--------+---------+-------------------+10 rows in set (0.01 sec) 8. 区分in和exists、not in和not exists 区分in和exists主要是造成了驱动顺序的改变（这是性能变化的关键），如果是exists，那么以外层表为驱动表，先被访问，如果是IN，那么先执行子查询。所以IN适合于外表大而内表小的情况；EXISTS适合于外表小而内表大的情况。 关于not in和not exists，推荐使用not exists，不仅仅是效率问题，not in可能存在逻辑问题。如何高效的写出一个替代not exists的SQL语句？ 原SQL语句： 1select colname … from A表 where a.id not in (select b.id from B表) 高效的SQL语句： 1select colname … from A表 Left join B表 on where a.id = b.id where b.id is null 9. 使用合理的分页方式以提高分页的效率 分页使用 limit m,n 尽量让m 小。利用主键的定位，可以减小m的值 12345678910111213141516171819mysql&gt; select * from tbiguser limit 9999998, 2;+----------+------------+-----------------+------+------+--------+---------+| id | nickname | loginname | age | sex | status | address |+----------+------------+-----------------+------+------+--------+---------+| 9999999 | zy9999999 | zhaoyun9999999 | 23 | 1 | 1 | beijing || 10000000 | zy10000000 | zhaoyun10000000 | 23 | 1 | 1 | beijing |+----------+------------+-----------------+------+------+--------+---------+2 rows in set (4.72 sec)mysql&gt; select * from tbiguser where id&gt;9999998 limit 2;+----------+------------+-----------------+------+------+--------+---------+| id | nickname | loginname | age | sex | status | address |+----------+------------+-----------------+------+------+--------+---------+| 9999999 | zy9999999 | zhaoyun9999999 | 23 | 1 | 1 | beijing || 10000000 | zy10000000 | zhaoyun10000000 | 23 | 1 | 1 | beijing |+----------+------------+-----------------+------+------+--------+---------+2 rows in set (0.00 sec) 10. 分段查询 一些用户选择页面中，可能一些用户选择的范围过大，造成查询缓慢。主要的原因是扫描行数过多。这个时候可以通过程序，分段进行查询，循环遍历，将结果合并处理进行展示。 11. 不建议使用%前缀模糊查询 例如LIKE“%name”或者LIKE“%name%”，这种查询会导致索引失效而进行全表扫描。但是可以使用LIKE“name%”。 那么如何解决这个问题呢，答案：使用全文索引或ES全文检索 12. 避免在where子句中对字段进行表达式操作 1select user_id,user_project from user_base where age*2=36; 中对字段就行了算术运算，这会造成引擎放弃使用索引，建议改成： 1select user_id,user_project from user_base where age=36/2; 13. 避免隐式类型转换 where子句中出现column字段的类型和传入的参数类型不一致的时候发生的类型转换，建议先确定where中的参数类型。 where age&#x3D;’18’ 14. 对于联合索引来说，要遵守最左前缀法则 举列来说索引含有字段id、name、school，可以直接用id字段，也可以id、name这样的顺序，但是name;school都无法使用这个索引。所以在创建联合索引的时候一定要注意索引字段顺序，常用的查询字段放在最前面。 15. 必要时可以使用force index来强制查询走某个索引 有的时候MySQL优化器采取它认为合适的索引来检索SQL语句，但是可能它所采用的索引并不是我们想要的。这时就可以采用forceindex来强制优化器使用我们制定的索引。 16. 注意范围查询语句 对于联合索引来说，如果存在范围查询，比如between、&gt;、&lt;等条件时，会造成后面的索引字段失效。 17. 使用JOIN优化 LEFT JOIN A表为驱动表，INNER JOIN MySQL会自动找出那个数据少的表作用驱动表，RIGHT JOIN B表为驱动表。 注意： MySQL中没有full join，可以用以下方式来解决： 尽量使用inner join，避免left join： 参与联合查询的表至少为2张表，一般都存在大小之分。如果连接方式是inner join，在没有其他过滤条件的情况下MySQL会自动选择小表作为驱动表，但是left join在驱动表的选择上遵循的是左边驱动右边的原则，即left join左边的表名为驱动表。 合理利用索引： 被驱动表的索引字段作为on的限制字段。 利用小表去驱动大表： 从原理图能够直观的看出如果能够减少驱动表的话，减少嵌套循环中的循环次数，以减少 IO总量及CPU运算的次数。","tags":["性能优化","Mysql"],"categories":["Mysql"]},{"title":"SQL 语句使用了索引, 却还是慢查询？","path":"//blog/mysql/slow-query/","content":"为什么 SQL 语句明明使用了索引，但却还是会记录到慢查询中？ 我有一个大概 13 亿行数据的 MySQL 表 t_people，其中包括字段 ID、AGE、NAME、ADDRESS 等，现在我想查询所有年龄在 10 到 15 岁之间的小朋友，为了提高查询效率，于是我给 AGE 字段建立了索引。 但建完索引之后，我使用 SQL 语句 select * from t_people where age between 10 and 15 开始查询，查询之后发现这条语句居然是个慢查询。 你知道为什么吗？我应该如何优化？ 什么是慢查询为了便于说明，先创建一张表 123456789101112mysql&gt; CREATE TABLE &#x27;t&#x27; ( -&gt; &#x27;id&#x27; int(11) NOT NULL, -&gt; &#x27;a&#x27; int(11) DEFAULT NULL, -&gt; &#x27;b&#x27; int(11) DEFAULT NULL, -&gt; PRIMARY KEY (&#x27;id&#x27;) -&gt; KEY &#x27;a&#x27; (&#x27;a&#x27;) -&gt; ) ENGINE=InnoDB;Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into t values(1,1,1),(2,2,2);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0 mysql判断sql语句是不是慢查询，是根据语句的执行时间来衡量的. mysql会用语句的执行时间和 long_query_time 这个系统参数做比较. 如果语句执行时间大于long_query_time，都会把这个语句记录到慢查询日志里面。long_query_time的默认值是10s，一般生产环境不会设置这么大的值，一般设置1秒。 语句是否用到索引，是指语句在执行的时候有没有用到表的索引。 在上图所示的样例中， 图一: 未用到索引 图二: 使用主键索引 图三: 使用了 ‘a’ 索引。 图二用到了主键索引，并且是等值查询，可以看到explain的执行中只扫描了一行。但是极端情况下（例如数据库上的CPU压力非常高），那么该sql执行的时间也有可能超过long_query_time，会记录到慢查询日期里面 图三虽然用到了普通索引 a, 但是扫描了整个 a 的索引树，如果数据库中的数据非常多（例如 大于100W）效率就会变慢。 是否执行索引只是表示了一个SQL语句执行的过程，而是否记录到慢查询，是由执行时间决定的，而这个执行时间，可能会受外部因素的影响。 也就是说是否使用索引，和是否记录慢查询之间并没有必然的联系。 什么叫做使用了索引InnoDB 是索引组织表。所有的数据都是存储在索引树上面的。如上面建立的表结构中，共建立了两个索引，一个主键索引，一个普通索引 a，在innoDB里数据是放在主键索引里的。 数据索引示意图如下： 如果执行 explanin select * from t where id &gt; 0 如下图：但是从数据上这个sql一定是做了全表扫描，但是优化器认为，这个sql的执行过程中需要根据主键索引定位到第一个满足 id&gt;0 的值。即便这个sql使用到了索引，实际上也可能是全表扫描。 因此innoDB只有一种情况没有使用到索引，就是从主键索引的最左边的叶子节点开始，向右扫描整个索引树. 也就是说没有使用索引并不是一个准确的描述，你可以用全表扫描表示一个查询遍历了整个主键索引树，也可以用全索引扫描说明像 select a from t这样的查询，它扫描了整个普通的索引树，而像 select * from t where id=2 这样的语句，才是我们平时说的使用了索引，它表示的意思是: 我们使用了索引的快速搜索功能，并且有效的减少了扫描行数。 索引的过滤性假设现在维护了一张记录了整个中国人的基本信息表，假设你要查询所有年龄在10到15岁之间的基本信息，通常语句就会是：select * from t_people where age between 10 and 15 一般都会在age这个字段增加一个索引，否则就是一个全表扫描,但是在建了age上的索引后，这个语句还是执行慢，因为满足这个条件的数据有超过1亿行。建立索引表的组织结构图如下： 那么上面的sql语句执行流程是，从索引age上用树搜索，取出第一个age&#x3D;10的记录，得到它的主键ID的值，根据ID值去主键索引树取整行的信息，作为结果集的一部分返回，在索引age上向右扫描，取出下一个ID值，到主键索引上取出整行信息，作为结果集的一部分返回。重复改操作，只到碰到第一个 age &gt; 15 的记录。 其实最终我们关心的是扫描行数，对于一个大表，不止要有索引，索引的过滤性也要足够好，像刚才的例子age这个索引，它的过滤性就不够好，在设计表结构的时候，我们要让索引的过滤性足够好，也就是区分度比较高。 那么过滤性好了，是不是标识查询的扫描行数就一定少呢？在看一个例子，参考下图： 如果有一个索引是姓名、年龄的联合索引，那这个联合索引的过滤性应该不错，如果你的执行语句是：select * from t_people where name =&#39;张三&#39; and age = 8 就可以在联合索引上快速找到第一个姓名是张三，并且年龄是8的小朋友. 这样的数据应该不会很多，因此向右扫描的行数也很少，查询效率就很高，但是在查询的过滤性和索引的过滤性不一定是一样的，如果现在你的需求是查出所有名字第一个字是张，并且年龄是8的所有小朋友，SQL语句通常这样写：select * from t_people where name like &#39;张%&#39; and age = 8 在mysql5.5之前的版本中，这个语句的执行流程是这样的： 从联合索引树上找到第一个姓名字段上第一个姓张的记录，取出主键ID，然后到主键索引上，根据ID取出整行的值，判断年龄是否等于8，如果是就做为结果集的一行返回，如果不是就丢弃。 我们把根据ID到主键索引上查找整行数据的这个动作，叫做回表，在联合索引上向右遍历，并重复做回表和判断的逻辑。直到碰到联合索引树上，第一个姓名第一个字不是张的记录为止。 可以看到这个执行过程里面，最耗时的步骤就是回表。假设全国名字第一个字姓张的人有8000W，那么这个该过程就回表8000W次。在定位第一行记录的时候，只能使用索引和联合索引的最左前缀，称为最左前缀原则。 可以看到这个执行过程它的回表次数特别多，性能不够好，有没有优化的方法呢？ 在Mysql5.6版本引入了index condition pushdown 的优化。优化的执行流程是： 从联合索引树上找到第一个年龄字段是张开头的记录，判断这个索引记录上的年龄值是不是8，如果是就回表，取出整行数据，做为结果集返回的一部分，如果不是就就丢弃，不需要回表，在联合索引树上向右遍历，并判断年龄字段后，根据需要做回表，知道碰到联合索引树上，名字的第一个字不是张的记录为止。 这个过程跟上面的过程的差别，是在遍历联合索引的过程中，将age&#x3D;8这个条件下推到索引遍历的过程中，减少了回表次数。假设全国名字第一个字是张的人里面，有100W个年龄是8的小朋友，那么这个查询过程中，在联合索引里要遍历8000W次，而回表只需要100w次。 可以看到index condition pushdown优化的效果还是很不错的，但是这个优化还是没有绕开最左前缀原则的限制，因此在联合索引里，还是要扫描8000W行，有没有更进一步的优化呢？ 虚拟列可以采用虚拟列的优化方式。 把名字的第一个字，和年龄做一个联合索引，可以使用 mysql5.7 引入的虚拟列来实现，对应的修改表结构的sql语句是这么写的： 1alert table t_people add name_first varchar(2) generated always as (left(name,1)),add index(name_first,age); 虚拟列的值，总是等于 name 字段的前两个字节，虚拟列在插入数据的时候，不能指定值，在更新的时候也不能主动修改，它的值会根据定义自动生成，在那么字段修改的时候，也会自动跟着修改。有了这个新的联合索引，我们再找名字的第一个字是张，并且年龄是8的小朋友的时候，这个SQL语句就可以这么写： 1select * from t_people where name_first=&#x27;张&#x27; and age=8; 这个SQL语句执行的过程，就只需要扫描联合索引的100W行，并回表100W次，这个优化的本质是我么创建了一个更紧凑的索引，来加速了查询的过程。 使用sql优化的过程，往往就是减少扫描行数的过程","tags":["Mysql","索引","慢查询"],"categories":["Mysql"]},{"title":"GitHub不再支持密码验证解决方案：SSH免密与Token登录配置","path":"//blog/tools/github-login-with-token/","content":"问题描述今天提交代码，push到GitHub上，突然出现这个问题。 1234remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.remote: Please see https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information.... 官方的解释：https://github.blog/changelog/2021-08-12-git-password-authentication-is-shutting-down/ As previously announced, starting on August 13, 2021, at 09:00 PST, we will no longer accept account passwords when authenticating Git operations on GitHub.com. Instead, token-based authentication (for example, personal access, OAuth, SSH Key, or GitHub App installation token) will be required for all authenticated Git operations. Please refer to this blog post for instructions on what you need to do to continue using git operations securely. Removal August 13, 2021, at 09:00 PST 大致意思是，密码验证于2021年8月13日不再支持，也就是今天intellij不能再用密码方式去提交代码。请用使用 personal access token 替代。 解决方案GitHub Token打开自己的GitHub主页，点击自己的头像找到Settings并进入，在左边目录栏找到Personal access tokens，点击Generate new token，按照步骤申请即可，过程简单。 Scopes（范围）那里建议勾选 ‘repo’ 即可。 创建Token成功后复制这个Token： 以下操作针对于Window操作系统，首先打开控制面板，将查看方式切换到“小图标”，再打开“凭据管理”。 选择“Window凭据”： 向下滑动找到“github”： 点击编辑，再将刚刚复制的Token粘贴到密码处点击保存： 再次操作就不会出现刚才的报错了，其他操作系统也有相应的修改凭据操作，可以尝试一下。","tags":["开发工具","github"],"categories":["开发工具"]},{"title":"二维码扫码登录原理","path":"//blog/framework/scan-qr-code-to-login/","content":"在日常生活中，二维码出现在很多场景，比如超市支付、系统登录、应用下载等等。了解二维码的原理，可以为技术人员在技术选型时提供新的思路。 二维码最常用的场景之一就是通过手机端应用扫描 PC 或者 WEB 端的二维码，来登录同一个系统。 比如手机微信扫码登录 PC 端微信，手机淘宝扫码登录 PC 端淘宝。那么就让我们来看一下，二维码登录是怎么操作的！ 二维码登录的本质二维码登录本质上也是一种登录认证方式。既然是登录认证，要做的也就两件事情！ 告诉系统我是谁 向系统证明我是谁 那么扫码登录是怎么做到这两件事情的呢？ 手机端应用扫PC端二维码，手机端确认后，账号就在PC端登录成功了！这里，PC端登录的账号肯定与手机端是同一个账号。不可能手机端登录的是账号A，而扫码登录以后，PC端登录的是账号B。 所以，第一件事情，告诉系统我是谁，是比较清楚的！通过扫描二维码，把手机端的账号信息传递到PC端，至于是怎么传的，我们后面再说 第二件事情，向系统证明我是谁。扫码登录过程中，用户并没有去输入密码，也没有输入验证码，或者其他什么码。那是怎么证明的呢？ 有些同学会想到，是不是扫码过程中，把密码传到了PC端呢？ 但这是不可能的。因为那样太不安全的，客户端也根本不会去存储密码。 其实手机端APP它是已经登录过的，就是说手机端是已经通过登录认证。所以说只要扫码确认是这个手机且是这个账号操作的，其实就能间接证明我谁。 认识二维码那么如何做确认呢？我们后面会详细说明，在这之前我们需要先认识一下二维码！ 在认识二维码之前我们先看一下一维码！ 所谓一维码，也就是条形码，超市里的条形码–这个相信大家都非常熟悉，条形码实际上就是一串数字，它上面存储了商品的序列号。 二维码其实与条形码类似，只不过它存储的不一定是数字，还可以是任何的字符串，你可以认为，它就是字符串的另外一种表现形式. 系统认证机制认识了二维码，我们了解一下移动互联网下的系统认证机制。 前面我们说过，为了安全，手机端它是不会存储你的登录密码的。 但是在日常使用过程中，我们应该会注意到，只有在你的应用下载下来后，第一次登录的时候，才需要进行一个账号密码的登录， 之后即使这个应用进程被杀掉，或者手机重启，都是不需要再次输入账号密码的，它可以自动登录。 其实这背后就是一套基于token的认证机制，我们来看一下这套机制是怎么运行的， 账号密码登录时，客户端会将设备信息一起传递给服务端， 如果账号密码校验通过，服务端会把账号与设备进行一个绑定，存在一个数据结构中，这个数据结构中包含了账号ID，设备ID，设备类型等等 12345const token = &#123; acountid:&#x27;账号ID&#x27;, deviceid:&#x27;登录的设备ID&#x27;, deviceType:&#x27;设备类型，如 iso,android,pc......&#x27;,&#125; 然后服务端会生成一个 token，用它来映射数据结构，这个 token 其实就是一串有着特殊意义的字符串，它的意义就在于，通过它可以找到对应的账号与设备信息。 客户端得到这个 token 后，需要进行一个本地保存，每次访问系统 API 都携带上 token 与设备信息。 服务端就可以通过 token 找到与它绑定的账号与设备信息，然后把绑定的设备信息与客户端每次传来的设备信息进行比较，如果相同，那么校验通过，返回 API 接口响应数据， 如果不同，那就是校验不通过拒绝访问 客户端不会也没必要保存你的密码，相反，它是保存了 token。 这个 token 这么重要，万一被别人知道了怎么办? 实际上，知道了也没有影响， 因为设备信息是唯一的，只要你的设备信息别人不知道， 别人拿其他设备来访问，验证也是不通过的。 客户端登录的目的，就是获得属于自己的token。 token 和 设备信息一起发送给服务器。既然可以截获token, 是不是也可以截获设备信息呢？如果都两者都拿到了呢？魔高一尺，道高一丈，没有最安全，只有更安全。系统的安全除了token外，还需要其他的保障，技术上比如SSL&#x2F;TLS通信加密，业务上异常地点登陆验证等。 那么在扫码登录过程中，PC端是怎么获得属于自己的token呢？ 不可能手机端直接把自己的token给PC端用！token只能属于某个客户端私有，其他人或者是其他客户端是用不了的。 下面先梳理一下，扫描二维码登录的一般步骤是什么样的。 扫描二维码登录的一般步骤 扫码前，手机端应用是已登录状态，PC端显示一个二维码，等待扫描 手机端打开应用，扫描PC端的二维码，扫描后，会提示”已扫描，请在手机端点击确认” 用户在手机端点击确认，确认后PC端登录就成功了 可以看到，二维码在中间有三个状态， 待扫描，已扫描待确认，已确认。 基于上面的分析，可以想象 二维码的背后它一定存在一个唯一性的 ID，当二维码生成时，这个 ID 也一起生成，并且绑定了 PC 端的设备信息 手机去扫描这个二维码 二维码切换为 已扫描待确认状态， 此时就会将账号信息与这个 ID 绑定 当手机端确认登录时，它就会生成 PC 端用于登录的 token，并返回给 PC 端 到这里，基本思路就已经清晰了，接下来我们把整个过程再具体化一下 1. 二维码准备按二维码不同状态来看， 首先是等待扫描状态，用户打开PC端，切换到二维码登录界面时。 PC 端向服务端发起请求，告诉服务端，我要生成用户登录的二维码，并且把 PC 端设备信息也传递给服务端 服务端收到请求后，它生成二维码 ID，并将二维码 ID 与 PC 端设备信息进行绑定 然后把二维码 ID 返回给 PC 端 PC 端收到二维码 ID 后，生成二维码(二维码中肯定包含了 ID) 为了及时知道二维码的状态，客户端在展现二维码后，PC 端不断的轮询服务端，比如每隔一秒就轮询一次，请求服务端告诉当前二维码的状态及相关信息 2. 扫描状态切换 用户用手机去扫描PC端的二维码，通过二维码内容取到其中的二维码ID 再调用服务端API将移动端的身份信息与二维码ID一起发送给服务端 服务端接收到后，它可以将身份信息与二维码ID进行绑定，生成临时token。然后返回给手机端 因为PC端一直在轮询二维码状态，所以这时候二维码状态发生了改变，它就可以在界面上把二维码状态更新为已扫描 为什么需要返回给手机端一个临时token呢？ 临时token与token一样，它也是一种身份凭证，不同的地方在于它只能用一次，用过就失效。 在第三步骤中返回临时token，为的就是手机端在下一步操作时，可以用它作为凭证。以此确保扫码，登录两步操作是同一部手机端发出的， 3. 状态确认 手机端在接收到临时 token 后会弹出确认登录界面，用户点击确认时，手机端携带临时 token 用来调用服务端的接口，告诉服务端，我已经确认 服务端收到确认后，根据二维码 ID 绑定的设备信息与账号信息，生成用户 PC 端登录的 token 这时候 PC 端的轮询接口，它就可以得知二维码的状态已经变成了”已确认”。并且从服务端可以获取到用户登录的 token 到这里，登录就成功了， PC 端就可以用 token 去访问服务端的资源了。","tags":["二维码","扫码登录"],"categories":["细节"]},{"title":"kafka的listeners配置错误导致主线程阻塞","path":"//blog/kafka/kafka-listeners-config/","content":"问题背景我们在用kafka的时候，偶尔会遇到这样这样一个问题。 我们写的kafka的客户端程序，在启动的时候，会无缘无故的 卡住（阻塞） 如下图所示： 这时程序会长时间阻塞在这里，无法继续进行后续操作。 问题排查因为日志没有任何报错信息，但是又可以肯定当前项目并没有完全启动成功。感觉像是程序当中有个地方卡到了。通过 VisualVM 工具dump 线程相关的信息，很快发现了问题所在。原来卡在了consumer初始化的地方。 一下是我这边的处理方式，大家可以参考下。如果有更好的方式欢迎大家相互交流。 以下方法是在初始化Consumer的时候进行处理的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public KafkaConsumerImpl init() &#123; if (group == null || group.isEmpty()) &#123; throw new RuntimeException(&quot;phoenix.mq.group is empty&quot;); &#125; Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, namesrvAddr); props.put(ConsumerConfig.GROUP_ID_CONFIG, group); // 是否允许自动提交offset，这里设为false，下面手动提交 props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;false&quot;); // ... consumer = new KafkaConsumer&lt;&gt;(props); // consumer 订阅的topic及partition topicPartition = new TopicPartition(this.topic, this.partitionId); this.partitions = Collections.singletonList(topicPartition); // 元数据初始化和连接测试，3次失败后抛出异常 Callable&lt;Boolean&gt; call = new Callable&lt;Boolean&gt;() &#123; boolean res = false; int tryTimes = 3; @Override public Boolean call() throws Exception &#123; while (tryTimes-- &gt; 0) &#123; try &#123; consumer.assign(partitions); // 默认初始化offset当前最大值 nextBeginOffset = consumer.position(topicPartition); res = true; break; &#125; catch (Exception e) &#123; if (e instanceof InterruptedException) &#123; break; // 如果position在阻塞状态时，调用了 task.cancel 会抛出此异常。直接退出即可 &#125; LOG.error(e.getMessage(), e); LOG.error(&quot; ==&gt; error when trying to fetch metadata for kafka. topic&lt;&#123;&#125;&gt;, partition&lt;&#123;&#125;&gt;&quot;, topic, partitionId); &#125; // sleep try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; return res; &#125; &#125;; FutureTask&lt;Boolean&gt; task = new FutureTask&lt;&gt;(call); new Thread(task).start(); boolean isOk = false; try &#123; isOk = task.get(10000, TimeUnit.MILLISECONDS); &#125; catch (Exception e) &#123; LOG.error(&quot;Get task result timeout&quot;, e); &#125; task.cancel(true); if (isOk) &#123; LOG.info(&quot; ==&gt; init kafka consumer succeed: servers&lt;&#123;&#125;&gt;, topic&lt;&#123;&#125;&gt;, partition&lt;&#123;&#125;&gt;, nextBeginOffset&lt;&#123;&#125;&gt;&quot;, namesrvAddr, topic, partitionId, nextBeginOffset); &#125; else &#123; throw new RuntimeException(String.format( &quot; ==&gt; init kafka consumer failed. please check the conf (listeners or advertised.listeners or ...) and try to ping the host name in the conf value&quot;)); &#125; return this;&#125; 利用 FutureTask 的特性，定义一个定时任务， 在初始化Consumer的时候，尝试去连接kafka，如果配置的kafka的地址有误，或者配置出错在这里可以通过抛出错误体现出来。 最后通过task.get() 方法返回的结果来判断 Consumer 是否成功初始化。","tags":["kafka","线程阻塞"],"categories":["kafka"]},{"title":"[转] kafka 如何体现消息的顺序性","path":"//blog/kafka/kafka-sequence/","content":"顺序消息包括以下两方面： 全局顺序 局部顺序 全局顺序全局顺序，这种要求比较严格。例如Mysql数据库中的binlog日志传输：mysql binlog日志传输要求全局的顺序，不能有任何的乱序。 这种的解决办法通常是最为保守的方式： 全局使用一个生产者 全局使用一个消费者（并严格到一个消费线程） 全局使用一个分区（当然不同的表可以使用不同的分区或者topic实现隔离与扩展） 局部顺序在大部分业务场景下，只需要保证消息局部有序即可，什么是局部有序？ 局部有序是指在某个业务功能场景下保证消息的发送和接收顺序是一致的。如：订单场景，要求订单的创建、付款、发货、收货、完成消息在同一订单下是有序发生的，即消费者在接收消息时需要保证在接收到订单发货前一定收到了订单创建和付款消息。 kafka 中消息顺序性的保障Kafka只能保证单分区内消息顺序有序，无法保证全局有序（同一topic的消息有序） Apache Kafka官方保证了partition内部的数据有效性（追加写、offset读）；为了提高Topic的并发吞吐能力，可以提高Topic的partition数，并通过设置partition的replica来保证数据高可靠，但是在多个Partition时，不能保证Topic级别的数据有序性。 生产者：通过分区的leader副本负责数据顺序写入，来保证消息顺序性 消费者：同一个分区内的消息只能被一个group里的一个消费者消费，保证分区内消费有序 针对这种场景的处理思路是： topic设置一个分区，发送端和消费端开启多线程生产和消费 优缺点： 实现简单 容易遇到瓶颈，服务端压力大 topic设置多个分区，自定义发送端的分区策略，数据发送到同一个分区中，消费端开启多线程消费 优缺点： 扩展多个分区分摊了非同类数据写入同个分区的压力 相同业务的数据在同一个分区依然有热点瓶颈的问题 topic设置多个分区，自定义发送端的分区策略，数据发送不同分区，消费时按发送分区的顺序消费，发送和消费端都启动多线程来提高并发度 自义分区器，使得消息按分区号大小顺序依次发送相同数量大小的数据 发送端和消费端启动多个消费线程进行生产和消费 线程之间按分区号大小顺序消费数据 优缺点： 消费性能极大下降，无法真正并发 消息重试对顺序消息的影响 对于一个有着先后顺序的消息A、B，正常情况下应该是A先发送完成后再发送B，但是在异常情况下，在A发送失败的情况下，B发送成功，而A由于重试机制在B发送完成之后重试发送成功了。这时对于本身顺序为AB的消息顺序变成了BA消息producer在发送消息的时候，对于同一个broker连接是存在多个未确认的消息在同时发送的，也就是存在上面场景说到的情况，虽然A和B消息是顺序的，但是由于存在未知的确认关系，有可能存在A发送失败，B发送成功，A需要重试的时候顺序关系就变成了BA。简之一句就是在发送B时A的发送状态是未知的。针对以上的问题，严格的顺序消费还需要以下参数支持：max.in.flight.requests.per.connection","tags":["kafka","顺序消息"],"categories":["kafka"]},{"title":"Kafka Rebalance机制分析","path":"//blog/kafka/kafka-rebalance/","content":"Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 consumer 如何达成一致，来分配订阅 Topic 的每个分区。 例如：某 Group 下有 20 个 consumer 实例，它订阅了一个具有 100 个 partition 的 Topic 。正常情况下，kafka 会为每个 Consumer 平均的分配 5 个分区。这个分配的过程就是 Rebalance。 触发 Rebalance 的时机Rebalance 的触发条件有3个。 1）组成员数发生变更。比如有新的 Consumer 实例加入组或者离开组，亦或是有 Consumer 实例崩溃被“踢出”组。 2）订阅主题数发生变更。Consumer Group 可以使用正则表达式的方式订阅主题，比如 consumer.subscribe(Pattern.compile(“t.*c”)) 就表明该 Group 订阅所有以字母 t 开头、字母 c 结尾的主题。在 Consumer Group 的运行过程中，你新创建了一个满足这样条件的主题，那么该 Group 就会发生 Rebalance。 3）订阅主题的分区数发生变更。Kafka 当前只能允许增加一个主题的分区数。当分区数增加时，就会触发订阅该主题的所有 Group 开启 Rebalance。 假设目前某个 Consumer Group 下有两个 Consumer，比如 A 和 B，当第三个成员 C 加入时，Kafka 会触发 Rebalance，并根据默认的分配策略重新为 A、B 和 C 分配分区，如下图所示： Rebalance 发生时，Group 下所有 consumer 实例都会协调在一起共同参与，kafka 能够保证尽量达到最公平的分配。但是 Rebalance 过程对 consumer group 会造成比较严重的影响。在 Rebalance 的过程中 consumer group 下的所有消费者实例都会停止工作，等待 Rebalance 过程完成。 Rebalance 过程分析Rebalance 过程分为两步：Join 和 Sync。 Join 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求加入消费组。一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。 Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。 Rebalance 场景分析新成员加入组 组成员“崩溃”组成员崩溃和组成员主动离开是两个不同的场景。因为在崩溃时成员并不会主动地告知coordinator此事，coordinator有可能需要一个完整的session.timeout周期(心跳周期)才能检测到这种崩溃，这必然会造成consumer的滞后。可以说离开组是主动地发起rebalance；而崩溃则是被动地发起rebalance。 组成员主动离开组 提交位移 Rebalance 的弊端在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。这是 Rebalance 为人诟病的一个方面。 其次，目前 Rebalance 的设计是所有 Consumer 实例共同参与，全部重新分配所有分区。 其实更高效的做法是尽量减少分配方案的变动。例如实例 A 之前负责消费分区 1、2、3，那么 Rebalance 之后，如果可能的话，最好还是让实例 A 继续消费分区 1、2、3，而不是被重新分配其他的分区。这样的话，实例 A 连接这些分区所在 Broker 的 TCP 连接就可以继续用，不用重新创建连接其他 Broker 的 Socket 资源。 最后，Rebalance 实在是太慢了。 曾经，有个国外用户的 Group 内有几百个 Consumer 实例，成功 Rebalance 一次要几个小时！这完全是不能忍受的。最悲剧的是，目前社区对此无能为力，至少现在还没有特别好的解决方案。 因此一些大数据框架都使用的 standalone consumer。 如何避免不必要的rebalance要避免 Rebalance，还是要从 Rebalance 发生的时机入手。我们在前面说过，Rebalance 发生的时机有三个： 组成员数量发生变化 订阅主题数量发生变化 订阅主题的分区数发生变化 后两个我们大可以人为的避免，发生rebalance最常见的原因是消费组成员的变化。 消费者成员正常的添加和停掉导致rebalance，这种情况无法避免，但是时在某些情况下，Consumer 实例会被 Coordinator 错误地认为 “已停止” 从而被“踢出”Group。从而导致rebalance。 当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向 Coordinator 发送心跳请求，表明它还存活着。如果某个 Consumer 实例不能及时地发送这些心跳请求，Coordinator 就会认为该 Consumer 已经 “死” 了，从而将其从 Group 中移除，然后开启新一轮 Rebalance。这个时间可以通过Consumer 端的参数 session.timeout.ms进行配置。默认值是 10 秒。 除了这个参数，Consumer 还提供了一个控制发送心跳请求频率的参数，就是 heartbeat.interval.ms。这个值设置得越小，Consumer 实例发送心跳请求的频率就越高。频繁地发送心跳请求会额外消耗带宽资源，但好处是能够更加快速地知晓当前是否开启 Rebalance，因为，目前 Coordinator 通知各个 Consumer 实例开启 Rebalance 的方法，就是将 REBALANCE_NEEDED 标志封装进心跳请求的响应体中。 除了以上两个参数，Consumer 端还有一个参数，用于控制 Consumer 实际消费能力对 Rebalance 的影响，即 max.poll.interval.ms 参数。它限定了 Consumer 端应用程序两次调用 poll 方法的最大时间间隔。它的默认值是 5 分钟，表示你的 Consumer 程序如果在 5 分钟之内无法消费完 poll 方法返回的消息，那么 Consumer 会主动发起 “离开组” 的请求，Coordinator 也会开启新一轮 Rebalance。 通过上面的分析，我们可以看一下那些rebalance是可以避免的： 第一类非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被 “踢出”Group 而引发的。这种情况下我们可以设置 session.timeout.ms 和 heartbeat.interval.ms 的值，来尽量避免rebalance的出现。（以下的配置是在网上找到的最佳实践，暂时还没测试过） 设置 session.timeout.ms &#x3D; 6s。 设置 heartbeat.interval.ms &#x3D; 2s。 要保证 Consumer 实例在被判定为 “dead” 之前，能够发送至少 3 轮的心跳请求，即 session.timeout.ms &gt;&#x3D; 3 * heartbeat.interval.ms。 将 session.timeout.ms 设置成 6s 主要是为了让 Coordinator 能够更快地定位已经挂掉的 Consumer，早日把它们踢出 Group。 第二类非必要 Rebalance 是 Consumer 消费时间过长导致的。此时，max.poll.interval.ms 参数值的设置显得尤为关键。如果要避免非预期的 Rebalance，你最好将该参数值设置得大一点，比你的下游最大处理时间稍长一点。 最后 Consumer 端的因为 GC 设置不合理导致程序频发 Full GC 也可能引发非预期 Rebalance。 总之，要为业务处理逻辑留下充足的时间。这样，Consumer 就不会因为处理这些消息的时间太长而引发 Rebalance 。 小结，调整以下 4 个参数以避免无效 Rebalance： session.timeout.ms heartbeat.interval.ms max.poll.interval.ms GC 参数 相关概念coordinatorGroup Coordinator是一个服务，每个Broker在启动的时候都会启动一个该服务。Group Coordinator的作用是用来存储Group的相关Meta信息，并将对应Partition的Offset信息记录到Kafka内置Topic(__consumer_offsets)中。Kafka在0.9之前是基于Zookeeper来存储Partition的Offset信息(consumers&#x2F;{group}&#x2F;offsets&#x2F;{topic}&#x2F;{partition})，因为ZK并不适用于频繁的写操作，所以在0.9之后通过内置Topic的方式来记录对应Partition的Offset。 每个Group都会选择一个Coordinator来完成自己组内各Partition的Offset信息，选择的规则如下： 1，计算Group对应在__consumer_offsets上的Partition 2，根据对应的Partition寻找该Partition的leader所对应的Broker，该Broker上的Group Coordinator即就是该Group的Coordinator Partition计算规则： 1partition-Id(__consumer_offsets) = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount) 其中groupMetadataTopicPartitionCount对应offsets.topic.num.partitions参数值，默认值是50个分区。","tags":["kafka"],"categories":["kafka"]},{"title":"Kafka有哪几处地方有分区分配的概念?","path":"//blog/kafka/kafka-partition-alocation/","content":"在 kafka 中，分区分配是一个很重要的概念，它会影响Kafka整体的性能均衡。kafka 中一共有三处地方涉及此概念，分别是： 生产者发送消息 消费者消费消息 创建主题。 虽然这三处的对应操作都可以被称之为 分区分配，但是其实质上所包含的内容却并不相同。 生产者的分区分配用户在使用 kafka 客户端发送消息时，调用 send 方法发送消息之后，消息就自然而然的发送到了 broker 中。 其实这一过程需要经过拦截器、序列化器、分区器等一系列作用之后才能被真正发往 broker。消息在发往 broker 之前需要确认它需要发送到的分区，如果 ProducerRecord 中指定了 partition 字段，那就不需要分区器的作用，因为 partition 就代表的是所要发往的分区号。如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。 Kafka中提供的默认分区器是DefaultPartitioner，它实现了Partitioner接口（用户可以实现这个接口来自定义分区器），其中的partition方法就是用来实现具体的分区分配逻辑： 12public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); 默认情况下，如果消息的key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往 topic 的各个可用分区。 注意：如果key不为null，那么计算得到的分区号会是所有分区中的任意一个；如果key为null并且有可用分区，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。 消费者的分区分配在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费。消费者的分区分配是指为消费组中的消费者分配所订阅主题中的分区。 如图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。 对于消费者的分区分配而言，Kafka自身提供了三种策略，分别为 RangeAssignor、 RoundRobinAssignor 以及 StickyAssignor ，其中 RangeAssignor 为默认的分区分配策略。 RangeAssignorRangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。 对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。 假设n&#x3D;分区数&#x2F;消费者数量，m&#x3D;分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。 为了更加通俗的讲解RangeAssignor策略，我们不妨再举一些示例。假设消费组内有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有4个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为： 12消费者C0：t0p0、t0p1、t1p0、t1p1消费者C1：t0p2、t0p3、t1p2、t1p3 这样分配的很均匀，那么此种分配策略能够一直保持这种良好的特性呢？我们再来看下另外一种情况。假设上面例子中2个主题都只有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 12消费者C0：t0p0、t0p1、t1p0、t1p1消费者C1：t0p2、t1p2 可以明显的看到这样的分配并不均匀，如果将类似的情形扩大，有可能会出现部分消费者过载的情况。对此我们再来看下另一种RoundRobinAssignor策略的分配效果如何。 RoundRobinAssignorRoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。RoundRobinAssignor策略对应的partition.assignment.strategy参数值为：org.apache.kafka.clients.consumer.RoundRobinAssignor。 如果同一个消费组内所有的消费者的订阅信息都是相同的，那么RoundRobinAssignor策略的分区分配会是均匀的。举例，假设消费组中有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 12消费者C0：t0p0、t0p2、t1p1消费者C1：t0p1、t1p0、t1p2 如果同一个消费组内的消费者所订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个topic，那么在分配分区的时候此消费者将分配不到这个topic的任何分区。 举例，假设消费组内有3个消费者C0、C1和C2，它们共订阅了3个主题：t0、t1、t2，这3个主题分别有1、2、3个分区，即整个消费组订阅了t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。具体而言，消费者C0订阅的是主题t0，消费者C1订阅的是主题t0和t1，消费者C2订阅的是主题t0、t1和t2，那么最终的分配结果为： 123消费者C0：t0p0消费者C1：t1p0消费者C2：t1p1、t2p0、t2p1、t2p2 可以看到RoundRobinAssignor策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区t1p1分配给消费者C1。 StickyAssignor我们再来看一下StickyAssignor策略，“sticky”这个单词可以翻译为“粘性的”，Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的： 分区的分配要尽可能的均匀； 分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。我们举例来看一下StickyAssignor策略的实际效果。 假设消费组内有3个消费者：C0、C1和C2，它们都订阅了4个主题：t0、t1、t2、t3，并且每个主题有2个分区，也就是说整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下： 123消费者C0：t0p0、t1p1、t3p0消费者C1：t0p1、t2p0、t3p1消费者C2：t1p0、t2p1 这样初看上去似乎与采用RoundRobinAssignor策略所分配的结果相同，但事实是否真的如此呢？再假设此时消费者C1脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor策略，那么此时的分配结果如下： 12消费者C0：t0p0、t1p0、t2p0、t3p0消费者C2：t0p1、t1p1、t2p1、t3p1 如分配结果所示，RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配。而如果此时使用的是StickyAssignor策略，那么分配结果为： 12消费者C0：t0p0、t1p1、t3p0、t2p0消费者C2：t1p0、t2p1、t0p1、t3p1 可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果，并将原来消费者C1的“负担”分配给了剩余的两个消费者C0和C2，最终C0和C2的分配还保持了均衡。 如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor策略如同其名称中的“sticky”一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。 到目前为止所分析的都是消费者的订阅信息都是相同的情况，我们来看一下订阅信息不同的情况下的处理。 举例，同样消费组内有3个消费者：C0、C1和C2，集群中有3个主题：t0、t1和t2，这3个主题分别有1、2、3个分区，也就是说集群中有t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。消费者C0订阅了主题t0，消费者C1订阅了主题t0和t1，消费者C2订阅了主题t0、t1和t2。 如果此时采用RoundRobinAssignor策略，那么最终的分配结果如下所示（和讲述RoundRobinAssignor策略时的一样，这样不妨赘述一下）： 1234【分配结果集1】消费者C0：t0p0消费者C1：t1p0消费者C2：t1p1、t2p0、t2p1、t2p2 如果此时采用的是StickyAssignor策略，那么最终的分配结果为： 1234【分配结果集2】消费者C0：t0p0消费者C1：t1p0、t1p1消费者C2：t2p0、t2p1、t2p2 可以看到这是一个最优解（消费者C0没有订阅主题t1和t2，所以不能分配主题t1和t2中的任何分区给它，对于消费者C1也可同理推断）。假如此时消费者C0脱离了消费组，那么RoundRobinAssignor策略的分配结果为： 12消费者C1：t0p0、t1p1消费者C2：t1p0、t2p0、t2p1、t2p2 可以看到RoundRobinAssignor策略保留了消费者C1和C2中原有的3个分区的分配：t2p0、t2p1和t2p2（针对结果集1）。而如果采用的是StickyAssignor策略，那么分配结果为： 12消费者C1：t1p0、t1p1、t0p0消费者C2：t2p0、t2p1、t2p2 可以看到StickyAssignor策略保留了消费者C1和C2中原有的5个分区的分配：t1p0、t1p1、t2p0、t2p1、t2p2。 从结果上看StickyAssignor策略比另外两者分配策略而言显得更加的优异，这个策略的代码实现也是异常复杂。 自定义分区分配策略kafka 处理支持默认提供的三种分区分配算法，还支持用户自定义分区分配算法，自定义的分配策略必须要实现org.apache.kafka.clients.consumer.internals.PartitionAssignor接口。PartitionAssignor接口的定义如下： 123456789101112131415Subscription subscription(Set&lt;String&gt; topics);String name();Map&lt;String, Assignment&gt; assign(Cluster metadata, Map&lt;String, Subscription&gt; subscriptions);void onAssignment(Assignment assignment);class Subscription &#123; private final List&lt;String&gt; topics; private final ByteBuffer userData;（省略若干方法……）&#125;class Assignment &#123; private final List&lt;TopicPartition&gt; partitions; private final ByteBuffer userData;（省略若干方法……）&#125; PartitionAssignor接口中定义了两个内部类：Subscription和Assignment。 Subscription类用来表示消费者的订阅信息，类中有两个属性：topics和userData，分别表示消费者所订阅topic列表和用户自定义信息。PartitionAssignor接口通过subscription()方法来设置消费者自身相关的Subscription信息，注意到此方法中只有一个参数topics，与Subscription类中的topics的相互呼应，但是并没有有关userData的参数体现。为了增强用户对分配结果的控制，可以在subscription()方法内部添加一些影响分配的用户自定义信息赋予userData，比如：权重、ip地址、host或者机架（rack）等等。 举例，在subscription()这个方法中提供机架信息，标识此消费者所部署的机架位置，在分区分配时可以根据分区的leader副本所在的机架位置来实施具体的分配，这样可以让消费者与所需拉取消息的broker节点处于同一机架。参考下图，消费者consumer1和broker1都部署在机架rack1上，消费者consumer2和broker2都部署在机架rack2上。如果分区的分配不是机架感知的，那么有可能与图（上部分）中的分配结果一样，consumer1消费broker2中的分区，而consumer2消费broker1中的分区；如果分区的分配是机架感知的，那么就会出现图（下部分）的分配结果，consumer1消费broker1中的分区，而consumer2消费broker2中的分区，这样相比于前一种情形而言，既可以减少消费延迟又可以减少跨机架带宽的占用。 再来说一下Assignment类，它是用来表示分配结果信息的，类中也有两个属性：partitions和userData，分别表示所分配到的分区集合和用户自定义的数据。可以通过PartitionAssignor接口中的onAssignment()方法是在每个消费者收到消费组leader分配结果时的回调函数，例如在StickyAssignor策略中就是通过这个方法保存当前的分配方案，以备在下次消费组再平衡（rebalance）时可以提供分配参考依据。 接口中的name()方法用来提供分配策略的名称，对于Kafka提供的3种分配策略而言，RangeAssignor对应的protocol_name为“range”，RoundRobinAssignor对应的protocol_name为“roundrobin”，StickyAssignor对应的protocol_name为“sticky”，所以自定义的分配策略中要注意命名的时候不要与已存在的分配策略发生冲突。这个命名用来标识分配策略的名称，在后面所描述的加入消费组以及选举消费组leader的时候会有涉及。 真正的分区分配方案的实现是在assign()方法中，方法中的参数metadata表示集群的元数据信息，而subscriptions表示消费组内各个消费者成员的订阅信息，最终方法返回各个消费者的分配信息。 Kafka中还提供了一个抽象类org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor，它可以简化PartitionAssignor接口的实现，对assign()方法进行了实现，其中会将Subscription中的userData信息去掉后，在进行分配。Kafka提供的3种分配策略都是继承自这个抽象类。如果开发人员在自定义分区分配策略时需要使用userData信息来控制分区分配的结果，那么就不能直接继承AbstractPartitionAssignor这个抽象类，而需要直接实现PartitionAssignor接口。 下面代码参考Kafka中的RangeAssignor策略来自定义一个随机的分配策略，这里笔者称之为RandomAssignor，具体代码实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package org.apache.kafka.clients.consumer;import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor;import org.apache.kafka.common.TopicPartition;import java.util.*;public class RandomAssignor extends AbstractPartitionAssignor &#123; @Override public String name() &#123; return &quot;random&quot;; &#125; @Override public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign( Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions); Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) &#123; assignment.put(memberId, new ArrayList&lt;&gt;()); &#125; // 针对每一个topic进行分区分配 for (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123; String topic = topicEntry.getKey(); List&lt;String&gt; consumersForTopic = topicEntry.getValue(); int consumerSize = consumersForTopic.size(); Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic == null) &#123; continue; &#125; // 当前topic下的所有分区 List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic); // 将每个分区随机分配给一个消费者 for (TopicPartition partition : partitions) &#123; int rand = new Random().nextInt(consumerSize); String randomConsumer = consumersForTopic.get(rand); assignment.get(randomConsumer).add(partition); &#125; &#125; return assignment; &#125; // 获取每个topic所对应的消费者列表，即：[topic, List[consumer]] private Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) &#123; Map&lt;String, List&lt;String&gt;&gt; res = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) &#123; String consumerId = subscriptionEntry.getKey(); for (String topic : subscriptionEntry.getValue().topics()) put(res, topic, consumerId); &#125; return res; &#125;&#125; 在使用时，消费者客户端需要添加相应的Properties参数，示例如下： 12properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RandomAssignor.class.getName()); 分配的实施我们了解了Kafka中消费者的分区分配策略之后是否会有这样的疑问：如果消费者客户端中配置了两个分配策略，那么以哪个为准？如果有多个消费者，彼此所配置的分配策略并不完全相同，那么以哪个为准？多个消费者之间的分区分配是需要协同的，那么这个协同的过程又是怎样？ 在kafka中有一个组协调器（GroupCoordinator）负责来协调消费组内各个消费者的分区分配，对于每一个消费组而言，在kafka服务端都会有其对应的一个组协调器。具体的协调分区分配的过程如下：1.首先各个消费者向GroupCoordinator提案各自的分配策略。如下图所示，各个消费者提案的分配策略和订阅信息都包含在JoinGroupRequest请求中。2.GroupCoordinator收集各个消费者的提案，然后执行以下两个步骤：一、选举消费组的leader；二、选举消费组的分区分配策略。 选举消费组的分区分配策略比较好理解，为什么这里还要选举消费组的leader，因为最终的分区分配策略的实施需要有一个成员来执行，而这个leader消费者正好扮演了这一个角色。在Kafka中把具体的分区分配策略的具体执行权交给了消费者客户端，这样可以提供更高的灵活性。比如需要变更分配策略，那么只需修改消费者客户端就醒来，而不必要修改并重启Kafka服务端。 怎么选举消费组的leader? 这个分两种情况分析：如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader；如果某一时刻leader消费者由于某些原因退出了消费组，那么就会重新选举一个新的leader，这个重新选举leader的过程又更为“随意”了，相关代码如下： 123//scala code.private val members = new mutable.HashMap[String, MemberMetadata]var leaderId = members.keys.head 解释一下这2行代码：在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的名称，而value是消费者相关的元数据信息。leaderId表示leader消费者的名称，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机挑选无异。总体上来说，消费组的leader选举过程是很随意的。 怎么选举消费组的分配策略？投票决定。每个消费者都可以设置自己的分区分配策略，对于消费组而言需要从各个消费者所呈报上来的各个分配策略中选举一个彼此都“信服”的策略来进行整体上的分区分配。这个分区分配的选举并非由leader消费者来决定，而是根据消费组内的各个消费者投票来决定。这里所说的“根据组内的各个消费者投票来决定”不是指GroupCoordinator还要与各个消费者进行进一步交互来实施，而是根据各个消费者所呈报的分配策略来实施。最终所选举的分配策略基本上可以看做是被各个消费者所支持的最多的策略，具体的选举过程如下： 收集各个消费者所支持的所有分配策略，组成候选集candidates。每个消费者从候选集candidates中找出第一个自身所支持的策略，为这个策略投上一票。计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。如果某个消费者并不支持所选举出的分配策略，那么就会报错。3.GroupCoordinator发送回执给各个消费者，并交由leader消费者执行具体的分区分配。 如上图所示，JoinGroupResponse回执中包含有GroupCoordinator中投票选举出的分配策略的信息。并且，只有leader消费者的回执中包含各个消费者的订阅信息，因为只需要leader消费者根据订阅信息来执行具体的分配，其余的消费并不需要。 4.leader消费者在整理出具体的分区分配方案后通过SyncGroupRequest请求提交给GroupCoordinator，然后GroupCoordinator为每个消费者挑选出各自的分配结果并通过SyncGroupResponse回执以告知它们。 broker端的分区分配生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区，而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。分区分配是否均衡会影响到Kafka整体的负载均衡，具体还会牵涉到优先副本等概念。 在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。如果集群中所有的broker节点都没有配置broker.rack参数，或者使用disable-rack-aware参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。","tags":["kafka","分区分配"],"categories":["kafka"]},{"title":"Kafka 系列(九)：Kafka 是如何实现精确一次（exactly once）语义的？","path":"//blog/kafka/exactly_once/","content":"本文主要讲述了 Kafka 消息交付可靠性保障以及精确处理一次语义的实现，具体包括幂等生产者和事务生产者。 1. 概述所谓的消息交付可靠性保障，是指 Kafka 对 Producer 和 Consumer 要处理的消息提供什么样的承诺。常见的承诺有以下三种： 最多一次（at most once）：消息可能会丢失，但绝不会被重复发送。 至少一次（at least once）：消息不会丢失，但有可能被重复发送。 精确一次（exactly once）：消息不会丢失，也不会被重复发送。 目前，Kafka 默认提供的交付可靠性保障是第二种，即至少一次。 这样虽然不出丢失消息，但是会导致消息重复发送。 Kafka 也可以提供最多一次交付保障，只需要让 Producer 禁止重试即可。 这样一来肯定不会重复发送，但是可能会丢失消息。 无论是至少一次还是最多一次，都不如精确一次来得有吸引力。大部分用户还是希望消息只会被交付一次，这样的话，消息既不会丢失，也不会被重复处理。 Kafka 分别通过 幂等性（Idempotence）和事务（Transaction）这两种机制实现了 精确一次（exactly once）语义。 2. 幂等性（Idempotence）幂等这个词原是数学领域中的概念，指的是某些操作或函数能够被执行多次，但每次得到的结果都是不变的。 幂等性最大的优势在于我们可以安全地重试任何幂等性操作，反正它们也不会破坏我们的系统状态。 在 Kafka 中，Producer 默认不是幂等性的，但我们可以创建幂等性 Producer。它其实是 0.11.0.0 版本引入的新功能。指定 Producer 幂等性的方法很简单，仅需要设置一个参数即可，即 props.put(“enable.idempotence”, ture)，或 props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG， true)。 enable.idempotence 被设置成 true 后，Producer 自动升级成幂等性 Producer，其他所有的代码逻辑都不需要改变。Kafka 自动帮你做消息的重复去重。 底层具体的原理很简单，就是经典的用空间去换时间的优化思路，即在 Broker 端多保存一些字段。当 Producer 发送了具有相同字段值的消息后，Broker 能够自动知晓这些消息已经重复了，于是可以在后台默默地把它们 “丢弃” 掉。 当然，实际的实现原理并没有这么简单，但你大致可以这么理解。详细分析可参考：Kafka 系列(五)：幂等实现剖析 Kafka 为了实现幂等性，它在底层设计架构中引入了 ProducerID 和 SequenceNumber。 Producer 需要做的只有两件事： 1）初始化时像向 Broker 申请一个 ProducerID 2）为每条消息绑定一个 SequenceNumber Kafka Broker 收到消息后会以 ProducerID 为单位存储 SequenceNumber，也就是说即时 Producer 重复发送了， Broker 端也会将其过滤掉。 实现比较简单，同样的限制也比较大： 首先，它只能保证单分区上的幂等性。即一个幂等性 Producer 能够保证某个主题的一个分区上不出现重复消息，它无法实现多个分区的幂等性。 因为 SequenceNumber 是以 Topic + Partition 为单位单调递增的，如果一条消息被发送到了多个分区必然会分配到不同的 SequenceNumber , 导致重复问题。 其次，它只能实现单会话上的幂等性。不能实现跨会话的幂等性。当你重启 Producer 进程之后，这种幂等性保证就丧失了。 重启 Producer 后会分配一个新的 ProducerID，相当于之前保存的 SequenceNumber 就丢失了。 3. 事务（Transaction）Kafka 的事务概念类似于我们熟知的数据库提供的事务。 Kafka 自 0.11 版本开始也提供了对事务的支持，目前主要是在 read committed 隔离级别上做事情。它能保证多条消息原子性地写入到目标分区，同时也能保证 Consumer 只能看到事务成功提交的消息。 事务型 Producer 能够保证将消息原子性地写入到多个分区中。这批消息要么全部写入成功，要么全部失败。另外，事务型 Producer 也不惧进程的重启。Producer 重启回来后，Kafka 依然保证它们发送消息的精确一次处理。 设置事务型 Producer 的方法也很简单，满足两个要求即可： 和幂等性 Producer 一样，开启 enable.idempotence &#x3D; true。 设置 Producer 端参数 transactional. id。最好为其设置一个有意义的名字。 此外，你还需要在 Producer 代码中做一些调整，如这段代码所示： 123456789producer.initTransactions();try &#123; producer.beginTransaction(); producer.send(record1); producer.send(record2); producer.commitTransaction();&#125; catch (KafkaException e) &#123; producer.abortTransaction();&#125; 和普通 Producer 代码相比，事务型 Producer 的显著特点是调用了一些事务 API，如 initTransaction、beginTransaction、commitTransaction 和 abortTransaction，它们分别对应事务的初始化、事务开始、事务提交以及事务终止。 这段代码能够保证 Record1 和 Record2 被当作一个事务统一提交到 Kafka，要么它们全部提交成功，要么全部写入失败。 实际上即使写入失败，Kafka 也会把它们写入到底层的日志中，也就是说 Consumer 还是会看到这些消息。因此在 Consumer 端，读取事务型 Producer 发送的消息也是需要一些变更的。修改起来也很简单，设置 isolation.level 参数的值即可。当前这个参数有两个取值： read_uncommitted：这是默认值，表明 Consumer 能够读取到 Kafka 写入的任何消息，不论事务型 Producer 提交事务还是终止事务，其写入的消息都可以读取。 很显然，如果你用了事务型 Producer，那么对应的 Consumer 就不要使用这个值。 read_committed：表明 Consumer 只会读取事务型 Producer 成功提交事务写入的消息。 当然了，它也能看到非事务型 Producer 写入的所有消息。 4. 小结幂等性 Producer 和事务型 Producer 都是 Kafka 社区力图为 Kafka 实现精确一次处理语义所提供的工具，只是它们的作用范围是不同的。 幂等性 Producer 只能保证单分区、单会话上的消息幂等性； 而事务能够保证跨分区、跨会话间的幂等性。 从交付语义上来看，自然是事务型 Producer 能做的更多。天下没有免费的午餐。比起幂等性 Producer，事务型 Producer 的性能要更差，在实际使用过程中，我们需要仔细评估引入事务的开销，切不可无脑地启用事务。 最后还是建议实际使用时在 Consumer 端也要进行去重，防止重复消费，这样比较稳妥。","tags":["kafka"],"categories":["kafka"]},{"title":"Kafka 系列(八)：如何避免消息丢失?","path":"//blog/kafka/lost_messages/","content":"本文主要从 Producer、Broker、Consumer 等 3 个方面分析了 Kafka 应该如何配置才能避免消息丢失。 1. 概述在使用 MQ 的时候最大的问题就是消息丢失，常见的丢失情况如下： 1）Producer 端丢失 2）Broker 端丢失 3）Consumer 端丢失 一条消息从生产到消费一共要经过以下 3 个流程： 1）Producer 发送到 Broker 2）Broker 保存消息 (持久化) 3）Consumer 消费消息 3 个步骤分别对应了上述的 3 种消息丢失场景。 接下来以 Kafka 为例分析该如何避免这些问题。 2. Kafka 消息持久化保障一句话概括，Kafka 只对 “已提交” 的消息（committed message）做有限度的持久化保证。 其他 MQ 也类似。 第一个核心要素是已提交的消息。 什么是已提交的消息？当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时，这条消息在 Kafka 看来就正式变为 “已提交” 消息了。 那为什么是若干个 Broker 呢？这取决于你对 “已提交” 的定义。你可以选择只要有一个 Broker 成功保存该消息就算是已提交，也可以是令所有 Broker 都成功保存该消息才算是已提交。不论哪种情况，Kafka 只对已提交的消息做持久化保证这件事情是不变的。 第二个核心要素就是有限度的持久化保证。 也就是说 Kafka 不可能保证在任何情况下都做到不丢失消息。 举个极端点的例子，如果地球都不存在了，Kafka 还能保存任何消息吗？显然不能！ 有限度其实就是说 Kafka 不丢消息是有前提条件的。假如你的消息保存在 N 个 Kafka Broker 上，那么这个前提条件就是这 N 个 Broker 中至少有 1 个存活。只要这个条件成立，Kafka 就能保证你的这条消息永远不会丢失。 3. 具体场景分析3.1 Producer 端丢失Producer 端丢消息更多是因为消息根本没有提交到 Kafka。 目前 Kafka Producer 是异步发送消息的，也就是说如果你调用的是 producer.send(msg) 这个 API，那么它通常会立即返回，但此时你不能认为消息发送已成功完成。 这种发送方式有个有趣的名字，叫 “fire and forget”，翻译一下就是 “发射后不管”。如果出现消息丢失，我们是无法知晓的。这个发送方式挺不靠谱, 非常不建议使用。 导致消息没有发送成功的因素也有很多： 1）例如网络抖动，导致消息压根就没有发送到 Broker 端； 2）或者消息本身不合格导致 Broker 拒绝接收（比如消息太大了，超过了 Broker 的承受能力）等。 Kafka 不认为消息是已提交的，因此也就没有 Kafka 丢失消息这一说了。 解决方案也很简单：**Producer 永远要使用带有回调通知的发送 API，也就是说不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)**。 通过回调，一旦出现消息提交失败的情况，你就可以有针对性地进行处理。 举例来说： 如果是因为那些瞬时错误，那么仅仅让 Producer 重试就可以了； 如果是消息不合格造成的，那么可以调整消息格式后再次发送。 总之，处理发送失败的责任在 Producer 端而非 Broker 端。 3.2 Broker 端丢失Broker 丢失消息是由 Kafka 自身原因造成的。Kafka 为了提高吞吐量和性能，采用异步批量的刷盘策略，也就是按照一定的消息量和间隔时间进行刷盘。 Broker 端丢失消息才真的是因为 Kafka 造成的。 Kafka 收到消息后会先存储在也缓存中 (Page Cache) 中，之后由操作系统根据自己的策略进行刷盘或者通过 fsync 命令强制刷盘。如果系统挂掉，在 PageCache 中的数据就会丢失。 Kafka 没有提供同步刷盘的方式，也就是说单个 Broker 丢失消息是必定会出现的。 为了解决单个 broker 数据丢失问题，Kafka 通过 producer 和 broker 协同处理单个 broker 丢失参数的情况： acks&#x3D;0，producer 不等待 broker 的响应，效率最高，但是消息很可能会丢。 acks&#x3D;1，leader broker 收到消息后，不等待其他 follower 的响应，即返回 ack。也可以理解为 ack 数为 1。 此时，如果 follower 还没有收到 leader 同步的消息 leader 就挂了，那么消息会丢失。 acks&#x3D;-1(-1 等效于 all) ，leader broker 收到消息后，挂起，等待所有 ISR 列表中的 follower 返回结果后，再返回 ack。 这种配置下，如果 Leader 刚收到消息就断电，producer 可以知道消息没有被发送成功，将会重新发送。 如果在 follower 收到数据以后，成功返回 ack，leader 断电，数据将存在于原来的 follower 中。在重新选举以后，新的 leader 会持有该部分数据。 在配置为 all 或者 -1 的时候，只要 Producer 收到 Broker 的响应就可以理解为消息已经持久化了。 虽然可能只是刚写入了 PageCache，但是刷盘也就是迟早的事，除非刚好刷盘之前多个 Broker 同时挂了，那确实是没办法了。 建议根据实际情况设置： 如果要严格保证消息不丢失，请设置为 all 或 -1； 如果允许存在丢失，建议设置为 1； 一般不建议设为 0，除非无所谓消息丢不丢失。 3.3 Consumer 端丢失Consumer 端丢失数据主要体现在 Consumer 端要消费的消息不见了。 出现该情况的唯一原因就是：Consumer 没有正确消费消息，就把位移提交了，导致 Kafka 认为该消息已经被消费了，从而导致消息丢失。 可以看出这其实也不是 Kafka 的问题，毕竟 Kafka 也不知道究竟消费没有，只能以 Consumer 提交的位移为依据。 场景 1：获取到消息后直接提交位移了，然后再处理消息。 这样在提交位移后，处理完消息前，如果程序挂掉，这部分消息就算是丢失了。 场景 2：多线程并发消费消息，且开启了自动提交，导致消费完成之前程序就自动提交了位移，如果程序挂掉也会出现消息丢失。 解决方案也很简单：确定消费完成后才提交消息，如果是多线程异步处理消费消息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移。 4. 最佳实践以下为一些常见的 Kafka 无消息丢失的配置： 避免 Producer 端丢失 1）不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一定要使用带有回调通知的 send 方法。 2）设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries &gt; 0 的 Producer 能够自动重试消息发送，避免消息丢失。 避免 Broker 端丢失 3）设置 acks &#x3D; all。acks 是 Producer 的一个参数，代表了你对 “已提交” 消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是 “已提交”。这是最高等级的“已提交” 定义。 4）设置 unclean.leader.election.enable &#x3D; false。这是 Broker 端的参数，它控制的是哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false，即不允许这种情况的发生。 5）设置 replication.factor &gt;&#x3D; 3。这也是 Broker 端的参数。其实这里想表述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余。 6）设置 min.insync.replicas &gt; 1。这依然是 Broker 端参数，控制的是消息至少要被写入到多少个副本才算是 “已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。 7）确保 replication.factor &gt; min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor &#x3D; min.insync.replicas + 1。 避免 Consumer 端丢失 8）确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程处理的场景而言是至关重要的。 5. 小结消息生命周期中的 3 个地方都可能会出现消息丢失情况： 1）Producer 端：通过回调确保消息成功发送到 Kafka 了 2）Broker 端：通过多 Broker 以及 Producer 端设置 acks&#x3D;all 降低消息丢失概率 3）Consumer 端：一定要在消息处理完成后再提交位移 需要应用程序和 Kafka 一起配合才能保证消息不丢失。","tags":["kafka"],"categories":["kafka"]},{"title":"Kafka 系列(七)：kafka 对性能的优化","path":"//blog/kafka/kafka-optimization/","content":"性能问题一般常出现在三个地方: 网络 磁盘 复杂度 在 kafka 中性能的优化主要体现在三个方面： Producer Consumer Borker kafka 作为一个分布式队列，网络和磁盘更是优化的重中之重。kafka 中的优化手段主要有以下几种： 压缩 缓存 批量 并发 算法 顺序写一般来说，完成一次磁盘IO，需要经过 寻道、旋转和数据传输 三个步骤。 影响磁盘 IO 性能的因素也就发生在上面三个步骤上，因此主要花费的时间就是： 寻道时间：Tseek 是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I&#x2F;O 操作越快，目前磁盘的平均寻道时间一般在 3-15ms。 旋转延迟：Trotation 是指盘片旋转将请求数据所在的扇区移动到读写磁盘下方所需要的时间。旋转延迟取决于磁盘转速，通常用磁盘旋转一周所需时间的 1&#x2F;2 表示。比如：7200rpm 的磁盘平均旋转延迟大约为 60*1000&#x2F;7200&#x2F;2 &#x3D; 4.17ms，而转速为 15000rpm 的磁盘其平均旋转延迟为 2ms。 数据传输时间：Ttransfer 是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前 IDE&#x2F;ATA 能达到 133MB&#x2F;s，SATA II 可达到 300MB&#x2F;s 的接口数据传输率，数据传输时间通常远小于前两部分消耗时间。简单计算时可忽略。 因此，如果在写磁盘的时候省去寻道、旋转可以极大地提高磁盘读写的性能。 Kafka 采用顺序写文件的方式来提高磁盘写入性能。顺序写文件，基本减少了磁盘寻道和旋转的次数。磁头再也不用在磁道上乱舞了，而是一路向前飞速前行。 Kafka 中每个分区是一个有序的，不可变的消息序列，新的消息不断追加到 Partition 的末尾，在 Kafka 中 Partition 只是一个逻辑概念，Kafka 将 Partition 划分为多个 Segment，每个 Segment 对应一个物理文件，Kafka 对 segment 文件追加写，这就是顺序写文件。 为什么 Kafka 可以使用追加写的方式呢？ 简单来说，kafka 数据存储在队列中，队列是 FIFO 先进先出模型，保证了数据的有序（同一partition）。正是由于kafka 这种不可变性、有序性使得 kafka 可以使用追加写的方式写文件。 零拷贝零拷贝的核心思想是：尽量去减少数据的拷贝次数，从而减少拷贝的 CPU 开销，以及用户态和内核态的上下文切换次数，从而优化数据传输的性能。 有关零拷贝的详细介绍请参考： 零拷贝 PageCacheproducer 发送消息到 Broker 时，Broker 会使用 write() 系统调用 (对应到 Java NIO 的 FileChannel.write() API)，按偏移量写入数据，此时数据都会先写入page cache。consumer 消费消息时，Broker 使用 sendfile() 系统调用 (对应 FileChannel.transferTo() API)，以零拷贝 的方式将数据从 page cache 传输到 broker 的 Socket buffer，然后再通过网络传输。 leader 与 follower 之间的同步，与上面 consumer 消费数据的过程是同理的。 page cache中的数据会随着内核中 flusher 线程的调度以及对 sync()&#x2F;fsync() 的调用写回到磁盘，就算进程崩溃，也不用担心数据丢失。另外，如果 consumer 要消费的消息不在page cache里，才会去磁盘读取，并且会顺便预读出一些相邻的块放入 page cache，以方便下一次读取。 因此如果 Kafka producer 的生产速率与 consumer 的消费速率相差不大，那么就能几乎只靠对 broker page cache 的读写完成整个生产 - 消费过程，磁盘访问非常少。 网络模型Kafka 自己实现了网络模型做 RPC。底层基于 Java NIO，采用和 Netty 一样的 Reactor 线程模型。 这部分暂时还未细究。。。 批量和压缩Kafka Producer 向 Broker 发送消息不是一条消息一条消息的发送。Producer 有两个重要的参数：batch.size和linger.ms。这两个参数就和 Producer 的批量发送有关。 在 producer 端，消息在经过拦截器、序列化器、分区器之后会缓存到消息累加器（RecordAccumulator）中，消息累加器主要用来缓存消息，以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。 Kafka 支持多种压缩算法：lz4、snappy、gzip。Kafka 2.1.0 正式支持 ZStandard —— ZStandard 是 Facebook 开源的压缩算法，旨在提供超高的压缩比 Producer、Broker 和 Consumer 使用相同的压缩算法，在 producer 向 Broker 写入数据，Consumer 向 Broker 读取数据时甚至可以不用解压缩，最终在 Consumer Poll 到消息时才解压，这样节省了大量的网络和磁盘开销。 分区并发Kafka 的 Topic 可以分成多个 Partition，每个 Paritition 类似于一个队列，保证数据有序。同一个 Consumer Group 下的不同 Consumer 并发消费 Paritition，分区实际上是调优 Kafka 并行度的最小单元，因此，可以说，每增加一个 Paritition 就增加了一个消费并发。 Kafka 具有优秀的分区分配算法——StickyAssignor，可以保证分区的分配尽量地均衡，且每一次重分配的结果尽量与上一次分配结果保持一致。这样，整个集群的分区尽量地均衡，各个 Broker 和 Consumer 的处理不至于出现太大的倾斜。 分区数越多越好？ 不是 越多的分区需要打开更多的文件句柄 在 kafka 的 broker 中，每个分区都会对照着文件系统的一个目录。在 kafka 的数据日志文件目录中，每个日志数据段都会分配两个文件，一个索引文件和一个数据文件。因此，随着 partition 的增多，需要的文件句柄数急剧增加，必要时需要调整操作系统允许打开的文件句柄数。 客户端 &#x2F; 服务器端需要使用的内存就越多 客户端 producer 有个参数 batch.size，默认是 16KB。它会为每个分区缓存消息，一旦满了就打包将消息批量发出。看上去这是个能够提升性能的设计。不过很显然，因为这个参数是分区级别的，如果分区数越多，这部分缓存所需的内存占用也会更多。 降低高可用性 分区越多，每个 Broker 上分配的分区也就越多，当一个发生 Broker 宕机，那么恢复时间将很长。 高效的文件数据结构Kafka 消息是以 Topic 为单位进行归类，各个 Topic 之间彼此独立，互不影响。每个 Topic 又可以分为一个或多个分区。每个分区各自存在一个记录消息数据的日志文件。 Kafka 每个分区日志在物理上实际按大小被分成多个 Segment。 segment file 组成：每个LogSegment 对应于磁盘上的一个日志文件和两个索引文件，后缀 .index、 .timeindex 、.log 分别表示为 segment 索引文件、数据文件。 segment 文件命名规则：partion 全局的第一个 segment 从 0 开始，后续每个 segment 文件名为上一个 segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用 0 填充。 Kafka 中的索引文件以稀疏索引（sparse index）的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引项。每当写入一定量（由 broker 端参数 log.index.interval.bytes指定，默认值为4096，即4KB）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes的值，对应地可以增加或缩小索引项的密度。 稀疏索引通过 mmap 的方式，将 index 文件映射到内存，这样对 index 的操作就不需要操作磁盘 IO，以加快索引的查询速度。mmap的 Java 实现对应 MappedByteBuffer 。 mmap 是一种内存映射文件的方法。即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用 read,write 等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。mmap 详情可参考 系统调用 mmap 偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。 时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。 稀疏索引的方式是在磁盘空间、内存空间、查找时间等多方面之间的一个折中。 按照二分法找到小于 offset 的 segment 的.log 和.index 用目标 offset 减去文件名中的 offset 得到消息在这个 segment 中的偏移量。 再次用二分法在 index 文件中找到对应的索引。 到 log 文件中，顺序查找，直到找到 offset 对应的消息。 参考文章： https://mp.weixin.qq.com/s/9i4lDtWqOrzJbzN1I1yAwg","tags":["kafka","性能优化"],"categories":["kafka"]},{"title":"Kafka 系列(六)：幂等实现剖析","path":"//blog/kafka/kafka-idempotence/","content":"什么是幂等幂等 这个词原是数学领域中的概念，指的是某些操作或函数能够被执行多次，但每次得到的结果都是不变的。 下面通过几个简单的例子说明一下。 比如在乘法运算中，让数字乘以 1 就是一个幂等操作，因为不管你执行多少次这样的运算，结果都是相同的。再比如，取整函数（floor 和 ceiling）是幂等函数，那么运行 1 次 floor(3.4) 和 100 次 floor(3.4)，结果是一样的，都是 3。相反地，让一个数加 1 这个操作就不是幂等的，因为执行一次和执行多次的结果必然不同。 在计算机领域中，幂等性的含义稍微有一些不同： 在命令式编程语言（比如 C）中，若一个子程序是幂等的，那它必然不能修改系统状态。这样不管运行这个子程序多少次，与该子程序关联的那部分系统状态保持不变。 在函数式编程语言（比如 Scala 或 Haskell）中，很多纯函数（pure function）天然就是幂等的，它们不执行任何的 side effect。 幂等性有很多好处，其最大的优势在于我们可以安全地重试任何幂等性操作，反正它们也不会破坏我们的系统状态。如果是非幂等性操作，我们还需要担心某些操作执行多次对状态的影响，但对于幂等性操作而言，我们根本无需担心此事。 Producer 幂等性Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是 Kafka 所提供的幂等性是有条件的： kafka 中的幂等性只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）; kafka 中的幂等性不能跨多个 TopicPartition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。 如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。 Producer 幂等性使用在 Kafka 中，Producer 默认不是幂等性的，但我们可以创建幂等性 Producer。 指定 Producer 幂等性的方法很简单，仅需要设置一个参数即可，即 props.put(&quot;enable.idempotence&quot;, ture)，或 props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG， true)。 12345678910Properties props = new Properties();props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, &quot;true&quot;);props.put(&quot;acks&quot;, &quot;all&quot;); // 当 enable.idempotence 为 true，这里默认为 allprops.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);KafkaProducer producer = new KafkaProducer(props);producer.send(new ProducerRecord(topic, &quot;test&quot;); Prodcuer 幂等性对外保留的接口非常简单，其底层的实现对上层应用做了很好的封装，应用层并不需要去关心具体的实现细节，对用户非常友好。 幂等性要解决的问题一般来说，消息可靠性交付保障，提供三种级别： 最多一次（at most once）：消息可能会丢失，但绝不会被重复发送。 至少一次（at least once）：消息不会丢失，但有可能被重复发送。 精确一次（exactly once）：消息不会丢失，也不会被重复发送。 kafka 默认提供的就是第二种，即至少一次。 在 kafka 中，消息已提交的含义，通常是Broker 成功接收到消息，并且 Producer 接到 Broker 的应答才会认为该消息成功发送。不过倘若消息成功“提交”，但 Broker 的应答没有成功发送回 Producer 端（比如网络出现瞬时抖动），那么 Producer 就无法确定消息是否真的提交成功了。因此，它只能选择重试，也就是再次发送相同的消息。这就是 Kafka 默认提供至少一次可靠性保障的原因，不过这会导致消息重复发送。 Kafka 也可以提供最多一次交付保障，只需要让 Producer 禁止重试即可。这样一来，消息要么写入成功，要么写入失败，但绝不会重复发送。我们通常不会希望出现消息丢失的情况，但一些场景里偶发的消息丢失其实是被允许的，相反，消息重复是绝对要避免的。此时，使用最多一次交付保障就是最恰当的。 对于大多数应用而言，数据保证不丢是可以满足其需求的，但是对于一些其他的应用场景（比如支付数据等），它们是要求精确计数的，这时候如果上游数据有重复，下游应用只能在消费数据时进行相应的去重操作，应用在去重时，最常用的手段就是根据唯一 id 键做 check 去重。 在这种场景下，因为上游生产导致的数据重复问题，会导致所有有精确计数需求的下游应用都需要做这种复杂的、重复的去重处理。试想一下：如果在发送时，系统就能保证 exactly once，这对下游将是多么大的解脱。这就是幂等性要解决的问题，主要是解决数据重复的问题，正如前面所述，数据重复问题，通用的解决方案就是加唯一 id，然后根据 id 判断数据是否重复，Producer 的幂等性也是这样实现的，这一小节就让我们看下 Kafka 的 Producer 如何保证数据的 exactly once 的。 Producer 幂等性实现原理正如前面所述，幂等性要解决的问题是：Producer 设置 at least once 时，由于异常触发重试机制导致数据重复，幂等性的目的就是为了解决这个数据重复的问题，简单来说就是： at least once + 幂等 = exactly once kafka Producer 在实现时有两个重要机制： PID（Producer ID），用来标识每个 producer client； sequence numbers，client 发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复。 PID每个 Producer 在初始化时都会被分配一个唯一的 PID，这个 PID 对应用是透明的，完全没有暴露给用户。对于一个给定的 PID，sequence number 将会从0开始自增，每个 Topic-Partition 都会有一个独立的 sequence number。Producer 在发送数据时，将会给每条 msg 标识一个 sequence number，Server 也就是通过这个来验证数据是否重复。 这里的 PID 是全局唯一的，Producer 故障后重新启动后会被分配一个新的 PID，这也是幂等性无法做到跨会话的一个原因。 PID 申请下面我们看下 ProducerId 是如何获取的。 KafkaProducer 中的 Sender 线程在执行发送逻辑之前，会先判断判断是否需要一个新的 ProducerID 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void runOnce() &#123; // 如果开启了幂等或事务，需要多一些检查 if (transactionManager != null) &#123; try &#123; transactionManager.maybeResolveSequences(); // do not continue sending if the transaction manager is in a failed state if (transactionManager.hasFatalError()) &#123; RuntimeException lastError = transactionManager.lastError(); if (lastError != null) maybeAbortBatches(lastError); client.poll(retryBackoffMs, time.milliseconds()); return; &#125; // 判断是否需要一个新的 ProducerId transactionManager.bumpIdempotentEpochAndResetIdIfNeeded(); if (maybeSendAndPollTransactionalRequest()) &#123; return; &#125; &#125; catch (AuthenticationException e) &#123; // This is already logged as error, but propagated here to perform any clean ups. log.trace(&quot;Authentication exception while processing transactional request&quot;, e); transactionManager.authenticationFailed(e); &#125; &#125; long currentTimeMs = time.milliseconds(); long pollTimeout = sendProducerData(currentTimeMs); client.poll(pollTimeout, currentTimeMs);&#125;synchronized void bumpIdempotentEpochAndResetIdIfNeeded() &#123; if (!isTransactional()) &#123; if (epochBumpRequired) &#123; bumpIdempotentProducerEpoch(); &#125; // 当前不处于初始化状态，并且没有 ProducerId if (currentState != State.INITIALIZING &amp;&amp; !hasProducerId()) &#123; transitionTo(State.INITIALIZING); InitProducerIdRequestData requestData = new InitProducerIdRequestData() .setTransactionalId(null) .setTransactionTimeoutMs(Integer.MAX_VALUE); // 构建初始化ProducerId请求，放入请求队列中 InitProducerIdHandler handler = new InitProducerIdHandler(new InitProducerIdRequest.Builder(requestData), false); enqueueRequest(handler); &#125; &#125;&#125; 之后请求会被发送到服务端（Broker）, 服务端处理该请求的入口是 KafkaApis 中的 handleInitProducerIdRequest() 123456789101112131415161718192021222324252627282930313233343536373839404142434445def handleInitProducerIdRequest(request: RequestChannel.Request, requestLocal: RequestLocal): Unit = &#123; val initProducerIdRequest = request.body[InitProducerIdRequest] val transactionalId = initProducerIdRequest.data.transactionalId // 权限校验 if (transactionalId != null) &#123; if (!authHelper.authorize(request.context, WRITE, TRANSACTIONAL_ID, transactionalId)) &#123; requestHelper.sendErrorResponseMaybeThrottle(request, Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED.exception) return &#125; &#125; else if (!authHelper.authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME, true, false) &amp;&amp; !authHelper.authorizeByResourceType(request.context, AclOperation.WRITE, ResourceType.TOPIC)) &#123; requestHelper.sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception) return &#125; // 此处省略部分代码 def sendResponseCallback(result: InitProducerIdResult): Unit = &#123;...&#125; val producerIdAndEpoch = (initProducerIdRequest.data.producerId, initProducerIdRequest.data.producerEpoch) match &#123; // 初始化的是否都是 -1（具体可以看 InitProducerIdRequest 的构造方法），所以进入第一个 case case (RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH) =&gt; Right(None) case (RecordBatch.NO_PRODUCER_ID, _) | (_, RecordBatch.NO_PRODUCER_EPOCH) =&gt; Left(Errors.INVALID_REQUEST) case (_, _) =&gt; Right(Some(new ProducerIdAndEpoch(initProducerIdRequest.data.producerId, initProducerIdRequest.data.producerEpoch))) &#125; producerIdAndEpoch match &#123; // 初始化 ProducerId case Right(producerIdAndEpoch) =&gt; txnCoordinator.handleInitProducerId(transactionalId, initProducerIdRequest.data.transactionTimeoutMs, producerIdAndEpoch, sendResponseCallback, requestLocal) case Left(error) =&gt; requestHelper.sendErrorResponseMaybeThrottle(request, error.exception) &#125;&#125; def handleInitProducerId(transactionalId: String, transactionTimeoutMs: Int, expectedProducerIdAndEpoch: Option[ProducerIdAndEpoch], responseCallback: InitProducerIdCallback, requestLocal: RequestLocal = RequestLocal.NoCaching): Unit = &#123; if (transactionalId == null) &#123; // 最终可以发现，producerId 是由 producerIdManager 来管理的。 val producerId = producerIdManager.generateProducerId() responseCallback(InitProducerIdResult(producerId, producerEpoch = 0, Errors.NONE)) &#125; else if (transactionalId.isEmpty) &#123; ... 看代码可以发现 ProducerIdManager 是一个接口，它有两个实现类 ZkProducerIdManager RPCProducerIdManager ZkProducerIdManager 是通过 zk 来管理 producerId。 PID 端申请是向 ZooKeeper 申请，zk 中有一个 latest_producer_id_block 节点，每个 Broker 向 zk 申请一个 PID 段(默认情况下，每次申请 1000 个 PID)后，都会把自己申请的 PID 段信息写入到这个节点，这样当其他 Broker 再申请 PID 段时，会首先读写这个节点的信息，然后根据 block_end 选择一个 PID 段，最后再把信息写会到 zk 的这个节点，这个节点信息格式如下所示： 1&#123;&quot;version&quot;:1,&quot;broker&quot;:35,&quot;block_start&quot;:&quot;4000&quot;,&quot;block_end&quot;:&quot;4999&quot;&#125; ProducerIdManager 申请 PID 段的流程如下： 先从 zk 的 latest_producer_id_block 节点读取最新已经分配的 PID 段信息； 如果该节点不存在，直接从 0 开始分配，选择 0~1000 的 PID 段（ProducerIdManager 的 PidBlockSize 默认为 1000，即是每次申请的 PID 段大小）； 如果该节点存在，读取其中数据，根据 block_end 选择 这个 PID 段（如果 PID 段超过 Long 类型的最大值，这里会直接返回一个异常）； 在选择了相应的 PID 段后，将这个 PID 段信息写回到 zk 的这个节点中，如果写入成功，那么 PID 段就证明申请成功，如果写入失败（写入时会判断当前节点的 zkVersion 是否与步骤1获取的 zkVersion 相同，如果相同，那么可以成功写入，否则写入就会失败，证明这个节点被修改过），证明此时可能其他的 Broker 已经更新了这个节点（当前的 PID 段可能已经被其他 Broker 申请），那么从步骤 1 重新开始，直到写入成功。 RPCProducerIdManager 是最新版本新实现的一个功能，新版本的kafka 移除zookeeper之后，producerId 将在控制器上分配。 Sequence Numbers有了PID之后，在 PID+Partition 级别上再加上 sequence numbers 信息，就可以实现Producer的幂等性了。 ProducerBatch也提供了setProducerState() 方法（具体执行时机是在 RecordAccumulator 中的 drain 方法中），它可以给一个 batch 添加一些 meta 信息（pid、baseSequence、isTransactional），这些信息是会伴随着 ProduceRequest 发到 Server 端，Server 端也正是通过这些 meta 来做相应的判断。 发送流程客户端发送逻辑当开通幂等功能之后，producer 的发送流程如下： 客户端通过 KafkaProducer 的 send() 方法将数据添加到 RecordAccumulator 中，添加时会判断是否需要新建一个 ProducerBatch，这时这个 ProducerBatch 还是没有 PID 和 sequence number 信息的； Producer 后台发送线程 Sender，在 run() 方法中，会先根据 TransactionManager 的 maybeResolveSequences() 方法判断当前的 PID 是否需要重置，重置的原因是因为：如果有 topic-partition 的 batch 重试多次失败最后因为超时而被移除，这时 sequence number 将无法做到连续，因为 sequence number 有部分已经分配出去，这时系统依赖自身的机制无法继续进行下去（因为幂等性是要保证不丢不重的），相当于程序遇到了一个 fatal 异常，PID 会进行重置，TransactionManager 相关的缓存信息被清空（Producer 不会重启），只是保存状态信息的 TransactionManager 做了 clear+new 操作，遇到这个问题时是无法保证 exactly once 的（有数据已经发送失败了，并且超过了重试次数）； Sender 线程通过 bumpIdempotentEpochAndResetIdIfNeeded() 方法判断是否需要申请 PID，如果需要的话，会想服务端发送 InitProducerIdRequest Sender 线程通过 sendProducerData() 方法发送数据，整体流程与之前的 Producer 流程相似，不同的地方是在 RecordAccumulator 的 drain() 方法中，在加了幂等性之后，drain() 方法多了如下几步判断： 常规的判断：判断这个 topic-partition 是否可以继续发送（如果出现前面2中的情况是不允许发送的）、判断 PID 是否有效、如果这个 batch 是重试的 batch，那么需要判断这个 batch 之前是否还有 batch 没有发送完成，如果有，这里会先跳过这个 Topic-Partition 的发送，直到前面的 batch 发送完成，最坏情况下，这个 Topic-Partition 的 in-flight request 将会减少到1（这个涉及也是考虑到 server 端的一个设置，文章下面会详细分析）； 如果这个 ProducerBatch 还没有这个相应的 PID 和 sequence number 信息，会在这里进行相应的设置； 最后 Sender 线程再调用 sendProduceRequests() 方法发送 ProduceRequest 请求，后面的就跟之前正常的流程保持一致了。 服务端处理逻辑当 Broker 收到 ProduceRequest 请求之后，会通过 KafkaApis.handleProduceRequest() 做相应的处理，其处理流程如下（这里只讲述关于幂等性相关的内容）： 先进行权限校验（这里还不是太理解校验权限的目的） 如果请求是事务请求，检查是否对 TXN.id 有 Write 权限，没有的话返回 TRANSACTIONAL_ID_AUTHORIZATION_FAILED； 如果请求设置了幂等性，检查是否对 ClusterResource 有 IdempotentWrite 权限，没有的话返回 CLUSTER_AUTHORIZATION_FAILED； 验证对 topic 是否有 Write 权限以及 Topic 是否存在，否则返回 TOPIC_AUTHORIZATION_FAILED 或 UNKNOWN_TOPIC_OR_PARTITION 异常； 检查是否有 PID 信息，没有的话走正常的写入流程； UnifiedLog 对象会在 analyzeAndValidateProducerState() 方法先根据 batch 的 sequence number 信息检查这个 batch 是否重复（server 端会缓存 PID 对应这个 Topic-Partition 的最近5个 batch 信息），如果有重复，这里当做写入成功返回（不更新 LOG 对象中相应的状态信息，比如这个 replica 的 the end offset 等）； 有了 PID 信息，并且不是重复 batch 时，在更新 producer 信息时，会做以下校验： 检查该 PID 是否已经缓存中存在 如果不存在，那么判断 sequence number 是否 从0 开始，是的话，在缓存中记录 PID 的 meta（PID，epoch， sequence number），并执行写入操作，否则返回 UnknownProducerIdException（PID 在 server 端已经过期或者这个 PID 写的数据都已经过期了，但是 Client 还在接着上次的 sequence number 发送数据）； 如果该 PID 存在，先检查 PID epoch 与 server 端记录的是否相同； 如果不同并且 sequence number 不从 0 开始，那么返回 OutOfOrderSequenceException 异常； 如果不同并且 sequence number 从 0 开始，那么正常写入； 如果相同，那么根据缓存中记录的最近一次 sequence number（currentLastSeq）检查是否为连续（会区分为 0、Int.MaxValue 等情况），不连续的情况下返回 OutOfOrderSequenceException 异常。 下面与正常写入相同。 幂等性时，Broker 在处理 ProduceRequest 请求时，多了一些校验操作，这里重点看一下其中一些重要实现，先看下 analyzeAndValidateProducerState() 方法的实现，如下所示： analyzeAndValidateProducerState() 到达路径： KafkaApis.handleProduceRequest() ReplicaManager.appendRecords() -&gt; appendToLocalLog() -&gt; appendRecordsToLeader() UnifiedLog.appendAsLeader() -&gt; append() -&gt; analyzeAndValidateProducerState analyzeAndValidateProducerState()1234567891011121314151617181920212223242526272829303132333435private def analyzeAndValidateProducerState(appendOffsetMetadata: LogOffsetMetadata, records: MemoryRecords, origin: AppendOrigin):(mutable.Map[Long, ProducerAppendInfo], List[CompletedTxn], Option[BatchMetadata]) = &#123; val updatedProducers = mutable.Map.empty[Long, ProducerAppendInfo] val completedTxns = ListBuffer.empty[CompletedTxn] var relativePositionInSegment = appendOffsetMetadata.relativePositionInSegment records.batches.forEach &#123; batch =&gt; if (batch.hasProducerId) &#123; // if this is a client produce request, there will be up to 5 batches which could have been duplicated. // If we find a duplicate, we return the metadata of the appended batch to the client. if (origin == AppendOrigin.Client) &#123; val maybeLastEntry = producerStateManager.lastEntry(batch.producerId) maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach &#123; duplicate =&gt; return (updatedProducers, completedTxns.toList, Some(duplicate)) &#125; &#125; // We cache offset metadata for the start of each transaction. This allows us to // compute the last stable offset without relying on additional index lookups. val firstOffsetMetadata = if (batch.isTransactional) Some(LogOffsetMetadata(batch.baseOffset, appendOffsetMetadata.segmentBaseOffset, relativePositionInSegment)) else None val maybeCompletedTxn = updateProducers(producerStateManager, batch, updatedProducers, firstOffsetMetadata, origin) maybeCompletedTxn.foreach(completedTxns += _) &#125; relativePositionInSegment += batch.sizeInBytes &#125; (updatedProducers, completedTxns.toList, None)&#125; 如果这个 batch 有 PID 信息，会首先检查这个 batch 是否为重复的 batch 数据，其实现如下，batchMetadata 会缓存最新 5个 batch 的数据（如果超过5个，添加时会进行删除，这个也是幂等性要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5 的原因，与这个值的设置有关），根据 batchMetadata 缓存的 batch 数据来判断这个 batch 是否为重复的数据。 1234567891011121314151617181920def findDuplicateBatch(batch: RecordBatch): Option[BatchMetadata] = &#123; if (batch.producerEpoch != producerEpoch) None else batchWithSequenceRange(batch.baseSequence, batch.lastSequence)&#125;// Return the batch metadata of the cached batch having the exact sequence range, if any.def batchWithSequenceRange(firstSeq: Int, lastSeq: Int): Option[BatchMetadata] = &#123; val duplicate = batchMetadata.filter &#123; metadata =&gt; firstSeq == metadata.firstSeq &amp;&amp; lastSeq == metadata.lastSeq &#125; duplicate.headOption&#125;private def addBatchMetadata(batch: BatchMetadata): Unit = &#123; if (batchMetadata.size == ProducerStateEntry.NumBatchesToRetain) batchMetadata.dequeue() //note: 只会保留最近 5 个 batch 的记录 batchMetadata.enqueue(batch) //note: 添加到 batchMetadata 中记录，便于后续根据 seq id 判断是否重复&#125; 如果 batch 不是重复的数据，analyzeAndValidateProducerState() 会通过 updateProducers() 更新 producer 的相应记录，在更新的过程中，会做一步校验，校验方法如下所示： 123456789101112131415161718192021222324252627282930313233//note: 检查 seq numberprivate def checkSequence(producerEpoch: Short, appendFirstSeq: Int): Unit = &#123; if (producerEpoch != updatedEntry.producerEpoch) &#123; //note: epoch 不同时 if (appendFirstSeq != 0) &#123; //note: 此时要求 seq number 必须从0开始（如果不是的话，pid 可能是新建的或者 PID 在 Server 端已经过期） //note: pid 已经过期（updatedEntry.producerEpoch 不是-1，证明时原来的 pid 过期了） if (updatedEntry.producerEpoch != RecordBatch.NO_PRODUCER_EPOCH) &#123; throw new OutOfOrderSequenceException(s&quot;Invalid sequence number for new epoch: $producerEpoch &quot; + s&quot;(request epoch), $appendFirstSeq (seq. number)&quot;) &#125; else &#123; //note: pid 已经过期（updatedEntry.producerEpoch 为-1，证明 server 端 meta 新建的，PID 在 server 端已经过期，client 还在接着上次的 seq 发数据） throw new UnknownProducerIdException(s&quot;Found no record of producerId=$producerId on the broker. It is possible &quot; + s&quot;that the last message with t（）he producerId=$producerId has been removed due to hitting the retention limit.&quot;) &#125; &#125; &#125; else &#123; val currentLastSeq = if (!updatedEntry.isEmpty) updatedEntry.lastSeq else if (producerEpoch == currentEntry.producerEpoch) currentEntry.lastSeq else RecordBatch.NO_SEQUENCE if (currentLastSeq == RecordBatch.NO_SEQUENCE &amp;&amp; appendFirstSeq != 0) &#123; //note: 此时期望的 seq number 是从 0 开始,因为 currentLastSeq 是 -1,也就意味着这个 pid 还没有写入过数据 // the epoch was bumped by a control record, so we expect the sequence number to be reset throw new OutOfOrderSequenceException(s&quot;Out of order sequence number for producerId $producerId: found $appendFirstSeq &quot; + s&quot;(incoming seq. number), but expected 0&quot;) &#125; else if (!inSequence(currentLastSeq, appendFirstSeq)) &#123; //note: 判断是否连续 throw new OutOfOrderSequenceException(s&quot;Out of order sequence number for producerId $producerId: $appendFirstSeq &quot; + s&quot;(incoming seq. number), $currentLastSeq (current end sequence number)&quot;) &#125; &#125;&#125; 思考题 Producer 在设置幂等性时，为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5，如果设置大于 5（不考虑 Producer 端参数校验的报错），会带来什么后果？ Producer 在设置幂等性时，如果我们设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，那么是否可以保证有序，如果可以，是怎么做到的？ 为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5之所以要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是： Server 端的 ProducerStateManager 实例会缓存每个 PID 在 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档，忘记在哪了），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。 假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力 当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序先来分析一下，在什么情况下 Producer 会出现乱序的问题？ 没有幂等性时，乱序的问题是在重试时出现的，举个例子：client 依然发送了 6 个请求 1、2、3、4、5、6（它们分别对应了一个 batch），这 6 个请求只有 2-6 成功 ack 了，1 失败了，这时候需要重试，重试时就会把 batch 1 的数据添加到待发送的数据列队中），那么下次再发送时，batch 1 的数据将会被发送，这时候数据就已经出现了乱序，因为 batch 1 的数据已经晚于了 batch 2-6。 当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 1 时，是可以解决这个问题的，因为同时只允许一个请求正在发送，只有当前的请求发送完成（成功 ack 后），才能继续下一条请求的发送，类似单线程处理这种模式，每次请求发送时都会等待上次的完成，效率非常差，但是可以解决乱序的问题（当然这里有序只是针对单 client 情况，多 client 并发写是无法做到的）。 系统能提供的方案，基本上就是有序性与性能之间二选一，无法做到兼容，实际上系统出现请求重试的几率是很小的（一般都是网络问题触发的），可能连 0.1% 的时间都不到，但是就是为了这 0.1% 时间都不到的情况，应用需要牺牲性能问题来解决，在大数据场景下，我们是希望有更友好的方式来解决这个问题。简单来说，就是当出现重试时，max-in-flight-request 可以动态减少到 1，在正常情况下还是按 5 （5是举例说明）来处理，这有点类似于分布式系统 CAP 理论中关于 P 的考虑，当出现问题时，可以容忍性能变差，但是其他的情况下，我们希望的是能拥有原来的性能，而不是一刀切。令人高兴的，在 Kafka 2.0.0 版本中，如果 Producer 开始了幂等性，Kafka 是可以做到这一点的，如果不开启幂等性，是无法做到的，因为它的实现是依赖了 sequence number。 当请求出现重试时，batch 会重新添加到队列中，这时候是根据 sequence number 添加到队列的合适位置（有些 batch 如果还没有 sequence number，那么就保持其相对位置不变），也就是队列中排在这个 batch 前面的 batch，其 sequence number 都比这个 batch 的 sequence number 小，其实现如下，这个方法保证了在重试时，其 batch 会被放到合适的位置： 12345678910111213/** * Re-enqueue the given record batch in the accumulator to retry */public void reenqueue(ProducerBatch batch, long now) &#123; batch.reenqueued(now); //note: 重试,更新相应的 meta Deque&lt;ProducerBatch&gt; deque = getOrCreateDeque(batch.topicPartition); synchronized (deque) &#123; if (transactionManager != null) insertInSequenceOrder(deque, batch); //note: 将 batch 添加到队列的合适位置（根据 seq num 信息） else deque.addFirst(batch); &#125;&#125; 另外 Sender 在发送请求时，会首先通过 RecordAccumulator 的 drain() 方法获取其发送的数据，在遍历 Topic-Partition 对应的 queue 中的 batch 时，如果发现 batch 已经有了 sequence number 的话，则证明这个 batch 是重试的 batch，因为没有重试的 batch 其 sequence number 还没有设置，这时候会做一个判断，会等待其 in-flight-requests 中请求发送完成，才允许再次发送这个 Topic-Partition 的数据，其判断实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243private boolean shouldStopDrainBatchesForPartition(ProducerBatch first, TopicPartition tp) &#123; ProducerIdAndEpoch producerIdAndEpoch = null; if (transactionManager != null) &#123; if (!transactionManager.isSendToPartitionAllowed(tp)) return true; producerIdAndEpoch = transactionManager.producerIdAndEpoch(); if (!producerIdAndEpoch.isValid()) // we cannot send the batch until we have refreshed the producer id return true; if (!first.hasSequence()) &#123; if (transactionManager.hasInflightBatches(tp) &amp;&amp; transactionManager.hasStaleProducerIdAndEpoch(tp)) &#123; // Don&#x27;t drain any new batches while the partition has in-flight batches with a different epoch // and/or producer ID. Otherwise, a batch with a new epoch and sequence number // 0 could be written before earlier batches complete, which would cause out of sequence errors return true; &#125; if (transactionManager.hasUnresolvedSequence(first.topicPartition)) // Don&#x27;t drain any new batches while the state of previous sequence numbers // is unknown. The previous batches would be unknown if they were aborted // on the client after being sent to the broker at least once. return true; &#125; // 获取 inFlightBatches 中第一个 batch 的 baseSequence, inFlightBatches 为 null 的话返回 RecordBatch.NO_SEQUENCE int firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition); if (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence() &amp;&amp; first.baseSequence() != firstInFlightSequence) //重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight //queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）, //会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition） //这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义 //这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送 // If the queued batch already has an assigned sequence, then it is being retried. // In this case, we wait until the next immediate batch is ready and drain that. // We only move on when the next in line batch is complete (either successfully or due to // a fatal broker error). This effectively reduces our in flight request count to 1. return true; &#125; return false;&#125; 仅有 client 端这两个机制还不够，Server 端在处理 ProduceRequest 请求时，还会检查 batch 的 sequence number 值，它会要求这个值必须是连续的，如果不连续都会返回异常，Client 会进行相应的重试，举个栗子：假设 Client 发送的请求顺序是 1、2、3、4、5（分别对应了一个 batch），如果中间的请求 2 出现了异常，那么会导致 3、4、5 都返回异常进行重试（因为 sequence number 不连续），也就是说此时 2、3、4、5 都会进行重试操作添加到对应的 queue 中。 Producer 的 TransactionManager 实例的 TopicPartitionEntry.inflightBatchesBySequence 成员变量会维护这个 Topic-Partition 与目前正在发送的 batch 的对应关系（通过 addInFlightBatch() 方法添加 batch 记录），只有这个 batch 成功 ack 后，才会通过 removeInFlightBatch() 方法将这个 batch 从 inflightBatchesBySequence 中移除。 接着前面的例子，此时 inflightBatchesBySequence 中还有 2、3、4、5 这几个 batch（有顺序的，2 在前面），根据前面的 RecordAccumulator 的 drain() 方法可以知道只有这个 Topic-Partition 下次要发送的 batch 是 batch 2（跟 transactionManager 的这个 firstInFlightSequence() 方法获取 inFlightBatches 中第一个 batch 的 baseSequence 来判断） 时，才可以发送，否则会直接 break，跳过这个 Topic-Partition 的数据发送。这里相当于有一个等待，等待 batch 2 重新加入到 queue 中，才可以发送，不能跳过 batch 2，直接重试 batch 3、4、5，这是不允许的。 简单来说，其实现机制概括为： Server 端验证 batch 的 sequence number 值，不连续时，直接返回异常； Client 端请求重试时，batch 在 reenqueue 时会根据 sequence number 值放到合适的位置（有序保证之一）； Sender 线程发送时，在遍历 queue 中的 batch 时，会检查这个 batch 是否是重试的 batch，如果是的话，只有这个 batch 是最旧的那个需要重试的 batch，才允许发送，否则本次发送跳过这个 Topic-Partition 数据的发送等待下次发送。","tags":["kafka","幂等"],"categories":["kafka"]},{"title":"Kafka 系列(五)：如何保障数据可靠性？","path":"//blog/kafka/kafka-reliability/","content":"Kafka 中采用了多副本的机制，这是大多数分布式系统中惯用的手法，以此来实现水平扩展、提供容灾能力、提升可用性和可靠性等。 我们对此可以引申出一系列的疑问： Kafka 多副本之间如何进行数据同步，尤其是在发生异常时候的处理机制又是什么？ 多副本间的数据一致性如何解决，基于的一致性协议又是什么？ 如何确保Kafka 的可靠性？ Kafka 中的可靠性和可用性之间的关系又如何？ 下面从副本的角度切入来看看Kafka如何保障数据一致性、数据可靠性等问题，主要包括副本剖析、日志同步机制和可靠性分析等内容。 副本剖析副本（Replica）是分布式系统中常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。在常见的分布式系统中，为了对外提供可用的服务，我们往往会对数据和服务进行副本处理。数据副本是指在不同的节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取该数据，这是解决分布式系统数据丢失问题最有效的手段。另一类副本是服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。 Kafka从0.8版本开始为分区引入了多副本机制，通过增加副本数量来提升数据容灾能力。同时，Kafka通过多副本机制实现故障自动转移，在Kafka集群中某个broker节点失效的情况下仍然保证服务可用。 下面的内容会涉及到AR、ISR、HW等基础概念，下面我们先简要回顾一下，详情请参考 &lt;Kafka中的基本概念&gt; 副本是相对于分区而言的，即副本是特定分区的副本。 一个分区中包含一个或多个副本，其中一个为leader副本，其余为follower副本，各个副本位于不同的broker节点中。只有leader副本对外提供服务，follower副本只负责数据同步。 分区中的所有副本统称为 AR，而ISR 是指与leader 副本保持同步状态的副本集合，当然leader副本本身也是这个集合中的一员。 LEO标识每个分区中最后一条消息的下一个位置，分区的每个副本都有自己的LEO，ISR中最小的LEO即为HW，俗称高水位，消费者只能拉取到HW之前的消息。 从生产者发出的一条消息首先会被写入分区的leader副本，不过还需要等待ISR集合中的所有 follower 副本都同步完之后才能被认为已经提交，之后才会更新分区的 HW，进而消费者可以消费到这条消息。 失效副本正常情况下，分区的所有副本都处于ISR集合中，但是难免会有异常情况发生，从而某些副本被剥离出ISR集合中。在ISR集合之外，也就是处于同步失效或功能失效（比如副本处于非存活状态）的副本统称为失效副本，失效副本对应的分区也就称为同步失效分区（under-replicated分区）。 可以通过 kafka-topic.sh 脚本的 under-replicated-partitions 参数来显示主题中包含失效副本的分区。 失效副本不仅是指处于功能失效状态的副本，处于同步失效状态的副本也可以看作失效副本。 怎么判定一个分区是否有副本处于同步失效的状态呢？ Kafka 从 0.9.x 版本开始就通过唯一的broker端参数 replica.lag.time.max.ms 来抉择，当ISR集合中的一个follower副本滞后leader副本的时间超过此参数指定的值时则判定为同步失败，需要将此follower副本剔除出ISR集合，replica.lag.time.max.ms 参数的默认值为10000。 具体的实现原理也很容易理解，当follower副本将leader副本LEO（LogEndOffset）之前的日志全部同步时，则认为该 follower 副本已经追赶上leader 副本，此时更新该副本的lastCaughtUpTimeMs 标识。 注意： 千万不要错误的以为，只要 follower 副本拉取 leader 副本的数据就会更新 lastCaughtUpTimeMs 。 当leader 副本中消息的流入速度大于 follower 副本的拉取速度时，就算 follower 副本一直拉取，也不会和 leader 副本保持同步。如果还将该 follower 副本放入 ISR 集合中，就有可能造成消息丢失。 Kafka 的副本管理器会启动一个副本过期检测的定时任务，而这个定时任务会定时检查当前时间与副本的 lastCaughtUpTimeMs 差值是否大于参数replica.lag.time.max.ms 指定的值。 什么情况会导致副本失效？ follower副本进程卡住，在一段时间内根本没有向leader副本发起同步请求，比如频繁的Full GC。 follower副本进程同步过慢，在一段时间内都无法追赶上leader副本，比如I&#x2F;O开销过大。 当通过脚本工具增加了副本因子，新增加的副本因子在赶上leader之前都处于失效状态 ISR的伸缩Kafka 在启动的时候会开启两个与 ISR 相关的定时任务，名称分别为 isr-expiration 和isr-change-propagation isr-expiration 任务会周期性地检测每个分区是否需要缩减其ISR集合。 这个周期和 replica.lag.time.max.ms 参数有关，大小是这个参数值的一半，默认值为5000ms。 当检测到ISR集合中有失效副本时，就会收缩ISR集合。如果某个分区的ISR集合发生变更，则会将变更后的数据记录到 ZooKeeper 对应的 &#x2F;brokers&#x2F;topics&#x2F;partition&#x2F;state 节点中。节点中的数据示例如下： 1&#123;&quot;controller epoch&quot;: 26, &quot;leader&quot;: 0, &quot;version&quot;: 1, &quot;leader epoch&quot;: 2, &quot;isr&quot;: [0, 1]&#125; controller_epoch 表示当前Kafka控制器的epoch leader 表示当前分区的leader副本所在的broker的id编号 version表示版本号（当前版本固定为1） leader_epoch表示当前分区的leader纪元 isr表示变更后的ISR列表。 当 ISR 集合发生变更时还会将变更后的记录缓存到 isrChangeSet 中，isr-change-propagation 任务会周期性（固定值为 2500ms）地检查isrChangeSet，如果发现isrChangeSet中有ISR集合的变更记录，那么它会在ZooKeeper的&#x2F;isr_change_notification路径下创建一个以 isr_change_开头的持久顺序节点（比如&#x2F;isr_change_notification&#x2F;isr_change_0000000000），并将isrChangeSet中的信息保存到这个节点中。 Kafka控制器为&#x2F;isr_change_notification添加了一个Watcher，当这个节点中有子节点发生变化时会触发Watcher的动作，以此通知控制器更新相关元数据信息并向它管理的broker节点发送更新元数据的请求，最后删除&#x2F;isr_change_notification路径下已经处理过的节点。 频繁地触发Watcher会影响Kafka控制器、ZooKeeper甚至其他broker节点的性能。为了避免这种情况，Kafka添加了限定条件，当检测到分区的ISR集合发生变化时，还需要检查以下两个条件： 上一次ISR集合发生变化距离现在已经超过5s。 上一次写入ZooKeeper的时间距离现在已经超过60s。 满足以上两个条件之一才可以将ISR集合的变化写入目标节点。 有缩减对应就会有扩充，那么Kafka又是何时扩充ISR的呢？ 随着follower副本不断与leader副本进行消息同步，follower副本的LEO也会逐渐后移，并最终追赶上leader副本，此时该follower副本就有资格进入ISR集合。 追赶上leader副本的判定准则是此副本的LEO是否不小于leader副本的HW，注意这里并不是和leader副本的LEO相比。ISR扩充之后同样会更新ZooKeeper中的&#x2F;brokers&#x2F;topics&#x2F;partition&#x2F;state节点和isrChangeSet，之后的步骤就和ISR收缩时的相同。 当ISR集合发生增减时，或者ISR集合中任一副本的LEO发生变化时，都可能会影响整个分区的HW。 例如，leader副本的LEO为9，follower1副本的LEO为7，而follower2副本的LEO为6，如果判定这3个副本都处于ISR集合中，那么这个分区的HW为6；如果follower3已经被判定为失效副本被剥离出ISR集合，那么此时分区的HW为leader副本和follower1副本中LEO的最小值，即为7。 LEO 和 HW这两个概念可以参考：&lt;Kafka中的基本概念&gt; 对于副本而言，还有两个概念：本地副本（Local Replica）和远程副本（RemoteReplica）。 本地副本是指对应的Log分配在当前的broker节点上，远程副本是指对应的Log分配在其他的broker节点上。在Kafka中，同一个分区的信息会存在多个broker节点上，并被其上的副本管理器所管理，这样在逻辑层面每个broker节点上的分区就有了多个副本，但是只有本地副本才有对应的日志。 整个消息追加的过程可以概括如下： 生产者客户端发送消息至leader副本（副本1）中。 消息被追加到leader副本的本地日志，并且会更新日志的偏移量。 follower副本（副本2和副本3）向leader副本请求同步数据。 leader副本所在的服务器读取本地日志，并更新对应拉取的follower副本的信息。 leader副本所在的服务器将拉取结果返回给follower副本。 follower副本收到leader副本返回的拉取结果，将消息追加到本地日志中，并更新日志的偏移量信息。 了解了这些内容后，我们再来分析在这个过程中各个副本LEO和HW的变化情况。下面的示例，生产者一直在往leader副本中写入消息。某一时刻，leader副本的LEO增加至5，并且所有副本的HW还都为0。之后follower副本向leader副本拉取消息，在拉取的请求中会带有自身的LEO信息，这个LEO信息对应的是FetchRequest请求中的fetch_offset。leader副本返回给follower副本相应的消息，并且还带有自身的HW信息，这个HW信息对应的是FetchResponse中的high_watermark。 此时两个follower副本各自拉取到了消息，并更新各自的LEO为3和4。与此同时，follower副本还会更新自己的HW，更新HW的算法是比较当前LEO和leader副本中传送过来的HW的值，取较小值作为自己的HW值。当前两个follower副本的HW都等于0（min（0，0）&#x3D;0）。 接下来follower副本再次请求拉取leader副本中的消息。 此时leader副本收到来自follower副本的FetchRequest请求，其中带有LEO的相关信息，选取其中的最小值作为新的HW，即min（15，3，4）&#x3D;3。然后连同消息和HW一起返回FetchResponse给follower副本。注意leader副本的HW是一个很重要的东西，因为它直接影响了分区数据对消费者的可见性。 两个follower副本在收到新的消息之后更新LEO并且更新自己的HW为3（min（LEO，3）&#x3D;3）。 在一个分区中，leader副本所在的节点会记录所有副本的LEO，而follower副本所在的节点只会记录自身的LEO，而不会记录其他副本的LEO。对HW而言，各个副本所在的节点都只记录它自身的HW。leader 副本收到 follower副本的FetchRequest请求之后，它首先会从自己的日志文件中读取数据，然后在返回给follower副本数据前先更新follower副本的LEO。 Kafka 的根目录下有 cleaner-offset-checkpoint、log-start-offset-checkpoint、recovery-point-offset-checkpoint和replication-offset-checkpoint四个检查点文件 recovery-point-offset-checkpoint 和replication-offset-checkpoint 这两个文件分别对应了 LEO和 HW。 Kafka 中会有一个定时任务负责将所有分区的 LEO 刷写到恢复点文件 recovery-point-offset-checkpoint 中，定时周期由 broker 端参数 log.flush.offset.checkpoint.interval.ms来配置，默认值为60000。 还有一个定时任务负责将所有分区的HW刷写到复制点文件replication-offset-checkpoint中，定时周期由broker端参数replica.high.watermark.checkpoint.interval.ms来配置，默认值为5000。 log-start-offset-checkpoint文件对应logStartOffset（注意不能缩写为LSO，因为在Kafka中LSO是LastStableOffset的缩写），在FetchRequest和FetchResponse中也有它的身影，它用来标识日志的起始偏移量。各个副本在变动 LEO 和 HW 的过程中，logStartOffset 也有可能随之而动。Kafka 也有一个定时任务来负责将所有分区的 logStartOffset书写到起始点文件log-start-offset-checkpoint中，定时周期由broker端参数log.flush.start.offset.checkpoint.interval.ms来配置，默认值为60000。 Leader Epoch上述过程是在正常情况下的leader副本与follower副本之间的同步过程。 如果leader副本发生切换，那么同步过程又该如何处理呢？ 在0.11.0.0版本之前，Kafka使用的是基于HW的同步机制，但这样有可能出现数据丢失或leader副本和follower副本数据不一致的问题 数据丢失下图中，Replica B 是当前的leader副本（用L标记），Replica A是follower副本。 在某一时刻，B中有2条消息 m1 和 m2，A从B中同步了这两条消息，此时A和B的LEO都为2，同时HW都为1； 之后A再向B中发送请求以拉取消息，FetchRequest请求中带上了A的LEO信息，B在收到请求之后更新了自己的HW为2； B中虽然没有更多的消息，但还是会返回FetchResponse，并在其中包含了HW信息；最后A根据FetchResponse中的HW信息更新自己的HW为2。 可以看到整个过程中两者之间的HW同步有一个间隙，在A写入消息m2之后（LEO更新为2）需要再一轮的FetchRequest&#x2F;FetchResponse才能更新自身的HW为2。 下图中，如果在这个时候A宕机了，那么在A重启之后会根据之前HW位置（这个值会存入本地的复制点文件replication-offset-checkpoint）进行日志截断，这样便会将m2这条消息删除，此时A只剩下m1这一条消息，之后A再向B发送FetchRequest请求拉取消息。 此时若B 再宕机，那么 A 就会被选举为新的leader，如下图。B 恢复之后会成为follower，由于follower副本HW不能比leader副本的HW高，所以还会做一次日志截断，以此将HW调整为1。这样一来m2这条消息就丢失了（就算B不能恢复，这条消息也同样丢失）。 数据不一致对于上面的情况，也有一些解决方法，比如等待所有follower副本都更新完自身的HW之后再更新leader副本的HW，这样会增加多一轮的FetchRequest&#x2F;FetchResponse延迟，自然不够妥当。 还有一种方法就是follower副本恢复之后，在收到leader副本的FetchResponse前不要截断follower副本（follower副本恢复之后会做两件事情：截断自身和向leader发送FetchRequest请求），不过这样也避免不了数据不一致的问题。 例如下图中，当前leader副本为A，follower副本为B，A中有2条消息m1和m2，并且HW和LEO都为2，B中有1条消息m1，并且HW和LEO都为1。假设A和B同时“挂掉”，然后B第一个恢复过来并成为leader 之后B写入消息m3，并将LEO和HW更新至2（假设所有场景中的min.insync.replicas参数配置为1）。此时A也恢复过来了，根据前面数据丢失场景中的介绍可知它会被赋予follower的角色，并且需要根据HW截断日志及发送FetchRequest至B，不过此时A的HW正好也为2，那么就可以不做任何调整了，如下图 如此一来A中保留了m2而B中没有，B中新增了m3而A也同步不到，这样A和B就出现了数据不一致的情形。 Leader Epoch为了解决上述两种问题，Kafka从0.11.0.0开始引入了 leader epoch 的概念，在需要截断数据的时候使用leader epoch作为参考依据而不是原本的HW。 leader epoch代表leader的纪元信息（epoch），初始值为0。每当leader变更一次，leader epoch 的值就会加1，相当于为leader增设了一个版本号。 与此同时，每个副本中还会增设一个矢量 &lt;LeaderEpoch –&gt; ，其中StartOffset表示当前Leader Epoch下写入的第一条消息的偏移量。 每个副本的Log下都有一个leader-epoch-checkpoint文件，在发生leader epoch变更时，会将对应的矢量对追加到这个文件中. 解决数据丢失问题 下图中 LE（LeaderEpoch的缩写，当前A和B中的LE都为0）。 同样 A 发生重启，之后 A 不是先忙着截断日志而是先发送 OffsetsForLeaderEpochRequest 请求给 B（OffsetsForLeaderEpochRequest 请求体结构如下图所示，其中包含 A 当前的LeaderEpoch值），B作为目前的leader在收到请求之后会返回当前的LEO（LogEndOffset），与请求对应的响应为OffsetsForLeaderEpochResponse 如果A中的LeaderEpoch（假设为LE_A）和B中的不相同，那么B此时会查找LeaderEpoch为 LE_A+1 对应的 StartOffset 并返回给 A，也就是 LE_A 对应的 LEO，所以我们可以将OffsetsForLeaderEpochRequest的请求看作用来查找follower副本当前LeaderEpoch的LEO。 如下图，A 在收到2之后发现和目前的LEO相同，也就不需要截断日志了。B发生了宕机，A成为新的leader，那么对应的LE&#x3D;0也变成了LE&#x3D;1，对应的消息m2此时就得到了保留，之后不管B有没有恢复，后续的消息都可以以LE1为LeaderEpoch陆续追加到A中。 解决数据不一致问题 下面我们再来看一下leader epoch如何应对数据不一致的场景。 如下图所示，当前A为leader，B为follower，A中有2条消息m1和m2，而B中有1条消息m1。假设A和B同时“挂掉”，然后B第一个恢复过来并成为新的leader。 之后B写入消息m3，并将LEO和HW更新至2。（注意此时的LeaderEpoch已经从LE0增至LE1了。） 紧接着A也恢复过来成为follower，并向B发送OffsetsForLeaderEpochRequest请求，此时A的LeaderEpoch为LE0。B根据LE0查询到对应的offset为1并返回给A，A就截断日志并删除了消息m2。 之后A发送FetchRequest至B请求来同步数据，最终A和B中都有两条消息m1和m3，HW和LEO都为2，并且LeaderEpoch都为LE1，如此便解决了数据不一致的问题。 日志同步机制在分布式系统中，日志同步机制既要保证数据的一致性，也要保证数据的顺序性。虽然有许多方式可以实现这些功能，但最简单高效的方式还是从集群中选出一个leader来负责处理数据写入的顺序性。只要leader还处于存活状态，那么follower只需按照leader中的写入顺序来进行同步即可。 通常情况下，只要leader不宕机我们就不需要关心follower的同步问题。 不过当leader宕机时，我们就要从follower中选举出一个新的leader。follower的同步状态可能落后leader很多，甚至还可能处于宕机状态，所以必须确保选择具有最新日志消息的follower作为新的leader。 kafka 在执行日志同步时，在Kafka中动态维护着一个ISR集合，处于ISR集合内的节点保持与leader相同的高水位（HW），只有位列其中的副本（unclean.leader.election.enable配置为false）才有资格被选为新的 leader。 写入消息时只有等到所有 ISR 集合中的副本都确认收到之后才能被认为已经提交。位于 ISR 中的任何副本节点都有资格成为leader，选举过程简单、开销低，这也是Kafka选用此模型的重要因素。Kafka中包含大量的分区，leader副本的均衡保障了整体负载的均衡，所以这一因素也极大地影响Kafka的性能指标。 日志同步机制的一个基本原则就是：如果告知客户端已经成功提交了某条消息，那么即使 leader宕机，也要保证新选举出来的leader中能够包含这条消息。 这里就有一个需要权衡（tradeoff）的地方，如果leader在消息被提交前需要等待更多的follower确认，那么在它宕机之后就可以有更多的follower替代它，不过这也会造成性能的下降。 对于这种tradeoff，一种常见的做法是“少数服从多数”，“少数服从多数”的方式有一个很大的优势，系统的延迟取决于最快的几个节点，比如副本数为3，那么延迟就取决于最快的那个follower而不是最慢的那个（除了leader，只需要另一个follower确认即可）。 不过它也有一些劣势，为了保证leader选举的正常进行，它所能容忍的失败follower数比较少，如果要容忍1个follower失败，那么至少要有3个副本，如果要容忍2个follower失败，必须要有5个副本。也就是说，在生产环境下为了保证较高的容错率，必须要有大量的副本，而大量的副本又会在大数据量下导致性能的急剧下降。这也就是“少数服从多数”的这种Quorum模型常被用作共享集群配置（比如ZooKeeper），而很少用于主流的数据存储中的原因。 在采用ISR模型和（f+1）个副本数的配置下，一个Kafka分区能够容忍最大f个节点失败，相比于“少数服从多数”的方式所需的节点数大幅减少。实际上，为了能够容忍f个节点失败，“少数服从多数”的方式和ISR的方式都需要相同数量副本的确认信息才能提交消息。比如，为了容忍1个节点失败，“少数服从多数”需要3个副本和1个follower的确认信息，采用ISR的方式需要2个副本和1个follower的确认信息。在需要相同确认信息数的情况下，采用ISR的方式所需要的副本总数变少，复制带来的集群开销也就更低，“少数服从多数”的优势在于它可以绕开最慢副本的确认信息，降低提交的延迟，而对Kafka而言，这种能力可以交由客户端自己去选择。 总结kafka对可靠性的保障体现在多个方面，消息发送阶段、消息存储阶段以及消费消息阶段均有涉及。 消息发送阶段消息发送的3种模式，即发后即忘、同步和异步。 对于发后即忘的模式，不管消息有没有被成功写入，生产者都不会收到通知，那么即使消息写入失败也无从得知，因此发后即忘的模式不适合高可靠性要求的场景。 如果要提升可靠性，那么生产者可以采用同步或异步的模式，在出现异常情况时可以及时获得通知，以便可以做相应的补救措施，比如选择重试发送（可能会引起消息重复）。 有些发送异常属于可重试异常，比如 NetworkException，这个可能是由瞬时的网络故障而导致的，一般通过重试就可以解决。对于这类异常，客户端内部本身提供了重试机制来应对这种类型的异常，通过 retries 参数即可配置。默认情况下，retries参数设置为0，即不进行重试，对于高可靠性要求的场景，需要将这个值设置为大于 0 的值，与 retries 参数相关的还有一个retry.backoff.ms参数，它用来设定两次重试之间的时间间隔，以此避免无效的频繁重试。 如果配置的retries参数值大于0，则可能引起一些负面的影响。由于默认的max.in.flight.requests.per.connection参数值为5，这样可能会影响消息的顺序性。对此要么放弃客户端内部的重试功能，要么将max.in.flight.requests.per.connection参数设置为1，这样也就放弃了吞吐。 生产者客户端参数 acks 也是用来支撑可靠性的。该参数有三个可选项：0、1、-1 对于acks&#x3D;1的配置，生产者将消息发送到leader副本，leader副本在成功写入本地日志之后会告知生产者已经成功提交。（如果此时ISR集合的follower副本还没来得及拉取到leader中新写入的消息，leader就宕机了，那么此次发送的消息就会丢失。） 对于ack&#x3D;-1的配置，生产者将消息发送到leader副本，leader副本在成功写入本地日志之后还要等待 ISR 中的 follower 副本全部同步完成才能够告知生产者已经成功提交，即使此时leader副本宕机，消息也不会丢失 在acks&#x3D;-1的情形中，它要求ISR中所有的副本都收到相关的消息之后才能够告知生产者已经成功提交。试想一下这样的情形，leader 副本的消息流入速度很快，而follower副本的同步速度很慢，在某个临界点时所有的follower副本都被剔除出了ISR集合，那么ISR中只有一个leader副本，最终acks&#x3D;-1演变为acks&#x3D;1的情形，如此也就加大了消息丢失的风险。 Kafka也考虑到了这种情况，并为此提供了min.insync.replicas参数（默认值为1）来作为辅助（配合acks&#x3D;-1来使用），这个参数指定了ISR集合中最小的副本数，如果不满足条件就会抛出NotEnoughReplicasException或NotEnoughReplicasAfterAppendException。 在正常的配置下，需要满足副本数 &gt; min.insync.replicas参数的值。 一个典型的配置方案为：副本数配置为 3，min.insync.replicas 参数值配置为 2。注意min.insync.replicas参数在提升可靠性的时候会从侧面影响可用性。（试想如果ISR中只有一个leader副本，那么最起码还可以使用，而此时如果配置min.insync.replicas &gt; 1，则会使消息无法写入。） 与可靠性和ISR集合有关的还有一个参数—unclean.leader.election.enable。这个参数的默认值为false，如果设置为true就意味着当leader下线时候可以从非ISR集合中选举出新的 leader，这样有可能造成数据的丢失。如果这个参数设置为false，那么也会影响可用性，非ISR集合中的副本虽然没能及时同步所有的消息，但最起码还是存活的可用副本。 从0.11.0.0 版本开始，unclean.leader.election.enable 的默认值由原来的 true 改为了false，可以看出Kafka的设计者愈发地偏向于可靠性的提升。 消息存储阶段存储消息阶段需要在消息刷盘之后再给生产者响应，假设消息写入缓存中就返回响应，那么机器突然断电这消息就没了，而生产者以为已经发送成功了。 如果Broker是集群部署，有多副本机制，即消息不仅仅要写入当前Broker,还需要写入副本机中。那配置成至少写入两台机子后再给生产者响应。这样基本上就能保证存储的可靠了。一台挂了还有一台还在呢。 那假如来个地震机房机子都挂了呢？emmmmmm…大公司基本上都有异地多活。 就Kafka而言，越多的副本数越能够保证数据的可靠性，副本数可以在创建主题时配置，也可以在后期修改，不过副本数越多也会引起磁盘、网络带宽的浪费，同时会引起性能的下降。 一般而言，设置副本数为3即可满足绝大多数场景对可靠性的要求，而对可靠性要求更高的场景下，可以适当增大这个数值，比如国内部分银行在使用 Kafka 时就会设置副本数为 5。 在broker端还有两个参数log.flush.interval.messages 和 log.flush.interval.ms，用来调整同步刷盘的策略，默认是不做控制而交由操作系统本身来进行处理。同步刷盘是增强一个组件可靠性的有效方式，不过这种方式极其损耗性能，最好还是采用多副本的机制来保障。 消息消费阶段消费者需要真正执行完业务逻辑之后，再发送给Broker消费成功，这才是真正的消费了。 所以只要我们在消息业务逻辑处理完成之后再给Broker响应，那么消费阶段消息就不会丢失。 在kafka中，消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset（enable.auto.commit 参数的默认值为 true）。虽然这种方式非常简便，但它会带来重复消费和消息丢失的问题，对于高可靠性要求的应用来说显然不可取，所以需要将 enable.auto.commit 参数设置为 false 来执行手动位移提交。 在执行手动位移提交的时候也要遵循一个原则：如果消息没有被成功消费，那么就不能提交所对应的消费位移。对于高可靠要求的应用来说，宁愿重复消费也不应该因为消费异常而导致消息丢失。 对于消费端，Kafka 还提供了一个可以兜底的功能，即回溯消费，通过这个功能可以让我们能够有机会对漏掉的消息相应地进行回补，进而可以进一步提高可靠性。","tags":["kafka","可靠性"],"categories":["kafka"]},{"title":"Kafka 系列(四)：Kafka 消费者","path":"//blog/kafka/kafka-consumer/","content":"1. 传统消息模型传统消息模型一般分为消息队列模型和发布订阅模型： 1）消息队列模型的缺陷在于消息一旦被消费，就会从队列中被删除，而且只能被下游的一个 Consumer 消费。这种模型的伸缩性（scalability）很差，因为下游的多个 Consumer 都要“抢”这个共享消息队列的消息。 2）发布 &#x2F; 订阅模型倒是允许消息被多个 Consumer 消费，但它的问题也是伸缩性不高，因为每个订阅者都必须要订阅主题的所有分区。这种全量订阅的方式既不灵活，也会影响消息的真实投递效果。 Kafka 的 Consumer Group 机制正好避开这两种模型的缺陷，又兼具它们的优点。 Kafka 仅仅使用 Consumer Group 这一种机制，却同时实现了传统消息引擎系统的两大模型： 如果所有实例都属于同一个 Group，那么它实现的就是消息队列模型； 如果所有实例分别属于不同的 Group，那么它实现的就是发布 &#x2F; 订阅模型。 2. Consumer GroupConsumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制。 组内可以有多个消费者或消费者实例（Consumer Instance），它们共享一个公共的 ID，这个 ID 被称为 Group ID。组内的所有消费者协调在一起来消费订阅主题（Subscribed Topics）的所有分区（Partition）。当然，每个分区只能由同一个消费者组内的一个 Consumer 实例来消费。 Consumer Group 下可以有一个或多个 Consumer 实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。 在实际场景中，使用进程更为常见一些。 Group ID 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。 Consumer Group 下所有实例订阅的主题的单个分区，只能分配给组内的某个 Consumer 实例消费。 这个分区当然也可以被其他的 Group 消费。 理想情况下，Consumer 实例的数量应该等于该 Group 订阅主题的分区总数，这样能最大限度地实现高伸缩性。 注意：Consumer Group 中的实例是以 分区 为单位进行消费的，如果实例数大于分区数就会导致有的实例无法消费到任何消息。 假如有 6 个分区，Consumer Group 中却有 8 个实例，那么有两个实例将不会被分配任何分区，它们永远处于空闲状态。 因此，在实际使用过程中一般不推荐设置大于总分区数的 Consumer 实例。 下面的例子中，主题中共有4个分区（Partition）：P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。 每一个分区只能被一个消费组中的一个消费者所消费。 假设某一时刻某消费组内只有一个消费者 C0，订阅了一个主题，这个主题包含 7 个分区：P0、P1、P2、P3、P4、P5、P6。也就是说，这个消费者C0订阅了7个分区。 此时消费组内又加入了一个新的消费者C1，按照既定的逻辑，需要将原来消费者C0的部分分区分配给消费者C1消费。消费者C0和C1各自负责消费所分配到的分区，彼此之间并无逻辑上的干扰。 紧接着消费组内又加入了一个新的消费者C2 消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。假设一共有8个消费者，7个分区，那么最后的消费者C7由于分配不到任何分区而无法消费任何消息。 以上分配逻辑都是基于默认的分区分配策略进行分析的，可以通过消费者客户端参数partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略，有关分区分配的更多细节可以参考 Kafka有哪几处地方有分区分配的概念？ 对于消息中间件而言，一般有两种消息投递模式： 点对点（P2P，Point-to-Point）模式 发布&#x2F;订阅（Pub&#x2F;Sub）模式 点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。 发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题（Topic），主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息。主题使得消息的订阅者和发布者互相保持独立，不需要进行接触即可保证消息的传递，发布&#x2F;订阅模式在消息的一对多广播时采用。Kafka 同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合：· 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布&#x2F;订阅模式的应用。 消费组是一个逻辑上的概念，它将旗下的消费者归为一类，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id来配置，默认值为空字符串。 消费者并非逻辑上的概念，它是实际的应用实例，它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。 3. 消费者客户端一个正常的消费逻辑需要具备以下几个步骤： 配置消费者客户端参数及创建相应的消费者实例。 订阅主题。 拉取消息并消费。 提交消费位移。 关闭消费者实例。 3.1 主要参数 bootstrap.servers：用来指定连接 Kafka 集群所需的 broker 地址清单，具体内容形式为host1：port1，host2：post group.id：消费者隶属的消费组的名称，默认值为“”。如果设置为空，会抛 InvalidGroupIdException。一般而言，这个参数需要设置成具有一定的业务意义的名称。 key.deserializer 和 value.deserializer：消费者从broker端获取的消息格式都是字节数组（byte[]）类型，所以需要执行相应的反序列化操作才能还原成原有的对象格式。 client.id：用来设定KafkaConsumer对应的客户端id，默认值也为“”。如果客户端不设置，则KafkaConsumer会自动生成一个非空字符串，内容形式如“consumer-1” 3.2 订阅主题kafka 提供两种方式来订阅主题： subscribe（） assign （） 一个消费者可以订阅一个或多个主题，可以使用 subscribe（）方法订阅了一个主题，对于这个方法而言，既可以以集合的形式订阅多个主题，也可以以正则表达式的形式订阅特定模式的主题。 对于消费者使用集合的方式（subscribe（Collection））来订阅主题而言，比较容易理解，订阅了什么主题就消费什么主题中的消息。如果前后两次订阅了不同的主题，那么消费者以最后一次的为准。 如果消费者采用的是正则表达式的方式（subscribe（Pattern））订阅，在之后的过程中，如果有人又创建了新的主题，并且主题的名字与正则表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。在Kafka 和其他系统之间进行数据复制时，这种正则表达式的方式就显得很常见。 消费者不仅可以通过KafkaConsumer.subscribe（）方法订阅主题，还可以直接订阅某些主题的特定分区，在KafkaConsumer中还提供了一个assign（）方法来实现这些功能。这个方法只接受一个参数 Collection&lt;TopicPartition&gt; partitions ，用来指定需要订阅的分区集合。 既然有订阅，那么就有取消订阅，取消订阅的方式有如下几种 consumer.unsubscribe(); consumer.subscribe(new ArrayList&lt;&gt;()); consumer.assgin(new ArrayList&lt;&gt;()); 通过 subscribe（）方法订阅主题具有消费者自动再均衡的功能，在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组内的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。而通过assign（）方法订阅分区时，是不具备消费者自动均衡的功能的，其实这一点从assign（）方法的参数中就可以看出端倪，两种类型的subscribe（）都有ConsumerRebalanceListener类型参数的方法，而assign（）方法却没有。 再均衡监听器 再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。 不过在再均衡发生期间，消费组内的消费者是无法读取消息的。也就是说，在再均衡发生期间的这一小段时间内，消费组会变得不可用。 另外，当一个分区被重新分配给另一个消费者时，消费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。一般情况下，应尽量避免不必要的再均衡的发生。 在subscribe（Collection＜String＞ topics，ConsumerRebalanceListener listener） 和 subscribe（Pattern pattern，ConsumerRebalanceListener listener）方法中可以指定再均衡监听器ConsumerRebalanceListener。再均衡监听器用来设定发生再均衡动作前后的一些准备或收尾的动作。ConsumerRebalanceListener 是一个接口，包含2 个方法， 12void onPartitionsRevoked(Collection＜TopicPartition＞partitions)void onPartitionsAssigned(Collection＜TopicPartition＞partitions) onPartitionsRevoked 方法会在再均衡开始之前和消费者停止读取消息之后被调用。可以通过这个回调方法来处理消费位移的提交，以此来避免一些不必要的重复消费现象的发生。参数partitions表示再均衡前所分配到的分区。 onPartitionsAssigned方法会在重新分配分区之后和消费者开始读取消费之前被调用。参数partitions表示再均衡后所分配到的分区。 3.3 消息消费Kafka中的消费是基于拉模式的。 消息的消费一般有两种模式：推模式和拉模式。 推模式是服务端主动将消息推送给消费者 拉模式是消费者主动向服务端发起请求来拉取消息。 Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用poll（）方法，而poll（）方法返回的是所订阅的主题（分区）上的一组消息。 对于poll（）方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空；如果订阅的所有分区中都没有可供消费的消息，那么poll（）方法返回为空的消息集合。 poll（）方法里还有一个超时时间参数timeout，用来控制poll（）方法的阻塞时间，在消费者的缓冲区里没有可用数据时会发生阻塞。 timeout的设置取决于应用程序对响应速度的要求，比如需要在多长时间内将控制权移交给执行轮询的应用线程。可以直接将timeout设置为0，这样poll（）方法会立刻返回，而不管是否已经拉取到了消息。如果应用线程唯一的工作就是从Kafka中拉取并消费消息，则可以将这个参数设置为最大值Long.MAX_VALUE。 poll () 方法的返回值为 ConsumerRecords （每条消息为ConsumerRecord）。在 ConsumerRecords 类中还提供了几个方法来方便开发人员对消息集进行处理：count（）方法用来计算出消息集中的消息个数，返回类型是int；isEmpty（）方法用来判断消息集是否为空，返回类型是boolean；empty（）方法用来获取一个空的消息集，返回类型是ConsumerRecord＜K，V＞。 3.4 位移提交对于Kafka中的分区而言，它的每条消息都有唯一的offset，用来表示消息在分区中对应的位置。对于消费者而言，它也有一个offset的概念，消费者使用offset来表示消费到分区中某个消息所在的位置。 在每次调用poll（）方法时，它返回的是还没有被消费过的消息集（当然这个前提是消息已经存储在Kafka 中了，并且暂不考虑异常情况的发生），要做到这一点，就需要记录上一次消费时的消费位移。并且这个消费位移必须做持久化保存，而不是单单保存在内存中，否则消费者重启之后就无法知晓之前的消费位移。再考虑一种情况，当有新的消费者加入时，那么必然会有再均衡的动作，对于同一分区而言，它可能在再均衡动作之后分配给新的消费者，如果不持久化保存消费位移，那么这个新的消费者也无法知晓之前的消费位移。 在旧消费者客户端中，消费位移是存储在ZooKeeper中的。而在新消费者客户端中，消费位移存储在Kafka内部的主题__consumer_offsets中。这里把将消费位移存储起来（持久化）的动作称为“提交”，消费者在消费完消息之后需要执行消费位移的提交。 如下图，x表示某一次拉取操作中此分区消息的最大偏移量，假设当前消费者已经消费了 x 位置的消息，那么我们就可以说消费者的消费位移为 x，图中也用了lastConsumedOffset这个单词来标识它。 不过需要非常明确的是，当前消费者需要提交的消费位移并不是 x，而是 x+1。 对应于图中的position，它表示下一条需要拉取的消息的位置。 在消费者中还有一个committed offset的概念，它表示已经提交过的消费位移。 KafkaConsumer 类提供了 position（TopicPartition）和 committed（TopicPartition）两个方法来分别获取上面所说的position和committed offset的值。 12public long position(TopicPartition partition)public OffsetAndMetadata committed(TopicPartition partition) 下面来做个小实验，验证下lastConsumedOffset、committed offset和position之间的关系。 123456789101112131415161718192021222324TopicPartition tp = new TopicPartition(topic, 0);consumer.assign(Arrays.asList(tp));long lastConsumedOffset = -1; // 当前消费到的位移while(true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); if (records.isEmpty()) &#123; break; &#125; List&lt;consumerRecord&lt;string, string&gt;&gt; partitionRecords = records.records(tp); lastConsumedoffset = partitionRecords.get(partitionRecords.size() - 1).offset(); consumer.commitsync(); //同步提交消费位移 System.out.println(comsumed offset is&quot;+ lastconsumedoffset): offsetAndMetadata offsetAndMetadata = consumer.committed(tp); System.out.println(&quot;commited offset is &quot; + offsetAndMetadata.offset()); long posititon = consumer.position(tp); System.out.println(&quot;the offset of the next record is &quot; + posititon);&#125;Output:consumed offset is 377commited offset is 378the offset of the next record is 378 可以看出，消费者消费到此分区消息的最大偏移量为377，对应的消费位移lastConsumedOffset也就是377。在消费完之后就执行同步提交，但是最终结果显示所提交的位移committed offset为 378，并且下一次所要拉取的消息的起始偏移量 position 也为 378。 对于位移提交的具体时机的把握也很有讲究，有可能会造成重复消费和消息丢失的现象。 当前一次poll（）操作所拉取的消息集为[x+2，x+7]，x+2代表上一次提交的消费位移，说明已经完成了x+1之前（包括x+1在内）的所有消息的消费，x+5表示当前正在处理的位置。如果拉取到消息之后就进行了位移提交，即提交了x+8，那么当前消费x+5的时候遇到了异常，在故障恢复之后，我们重新拉取的消息是从x+8开始的。也就是说，x+5至x+7之间的消息并未能被消费，如此便发生了消息丢失的现象。 再考虑另外一种情形，位移提交的动作是在消费完所有拉取到的消息之后才执行的，那么当消费x+5的时候遇到了异常，在故障恢复之后，我们重新拉取的消息是从x+2开始的。也就是说，x+2至x+4之间的消息又重新消费了一遍，故而又发生了重复消费的现象。 在 Kafka 中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数enable.auto.commit 配置，默认值为 true。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数auto.commit.interval.ms配置，默认值为5秒，此参数生效的前提是enable.auto.commit参数为true。 在默认的方式下，消费者每隔5秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在poll（）方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。 自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象（对于再均衡的情况同样适用）。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。 在Kafka中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。很多时候并不是说拉取到消息就算消费完成，而是需要将消息写入数据库、写入本地缓存，或者是更加复杂的业务处理。在这些场景下，所有的业务处理完成才能认为消息被成功消费，手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。开启手动提交功能的前提是消费者客户端参数enable.auto.commit配置为false。 手动提交可以细分为同步提交和异步提交，对应于 KafkaConsumer 中的commitSync（）和commitAsync（）两种类型的方法。 3.5 指定位移消费在 Kafka 中每当消费者查找不到所记录的消费位移时，就会根据消费者客户端参数auto.offset.reset的配置来决定从何处开始进行消费，这个参数的默认值为“latest”，表示从分区末尾开始消费消息 如果将auto.offset.reset参数配置为“earliest”，那么消费者会从起始处，也就是0开始消费。 auto.offset.reset参数还有一个可配置的值—“none”，配置为此值就意味着出现查到不到消费位移的时候，既不从最新的消息位置处开始消费，也不从最早的消息位置处开始消费，此时会报出NoOffsetForPartitionException异常 如果能够找到消费位移，那么配置为“none”不会出现任何异常。如果配置的不是“latest”、“earliest”和“none”，则会报出ConfigException异常 seek() 方法： 提供的auto.offset.reset 参数也只能在找不到消费位移或位移越界的情况下粗粒度地从开头或末尾开始消费。有些时候，我们需要一种更细粒度的掌控，可以让我们从特定的位移处开始拉取消息，而 KafkaConsumer 中的 seek（）方法正好提供了这个功能，让我们得以追前消费或回溯消费。 1public void seek(TopicPartition partition, long offset); seek（）方法中的参数partition表示分区，而offset参数用来指定从分区的哪个位置开始消费。seek（）方法只能重置消费者分配到的分区的消费位置，而分区的分配是在 poll（）方法的调用过程中实现的。也就是说，在执行seek（）方法之前需要先执行一次poll（）方法，等到分配到分区之后才可以重置消费位置。 3.6 关闭消费KafkaConsumer 提供了对消费速度进行控制的方法，在有些应用场景下我们可能需要暂停某些分区的消费而先消费其他分区，当达到一定条件时再恢复这些分区的消费。KafkaConsumer中使用pause（）和resume（）方法来分别实现暂停某些分区在拉取操作时返回数据给客户端和恢复某些分区向客户端返回数据的操作。 KafkaConsumer还提供了一个无参的paused（）方法来返回被暂停的分区集合。 3.7 消费者拦截器消费者拦截器主要在消费到消息或在提交消费位移时进行一些定制化的操作。 与生产者拦截器对应的，消费者拦截器需要自定义实现org.apache.kafka.clients.consumer.ConsumerInterceptor接口。ConsumerInterceptor接口包含3个方法： 123public ConsumerRecords＜K,V＞ onConsume(ConsumerRecords＜K,V＞records);public void onCommit(Map＜TopicPartition,OffsetAndMetadata＞offsets);public void close(). KafkaConsumer会在poll（）方法返回之前调用拦截器的onConsume（）方法来对消息进行相应的定制化操作，比如修改返回的消息内容、按照某种规则过滤消息（可能会减少poll（）方法返回的消息的个数）。如果 onConsume（）方法中抛出异常，那么会被捕获并记录到日志中，但是异常不会再向上传递。 KafkaConsumer会在提交完消费位移之后调用拦截器的onCommit（）方法，可以使用这个方法来记录跟踪所提交的位移信息，比如当消费者使用commitSync的无参方法时，我们不知道提交的消费位移的具体细节，而使用拦截器的onCommit（）方法却可以做到这一点。 在消费者中也有拦截链的概念，和生产者的拦截链一样，也是按照interceptor.classes参数配置的拦截器的顺序来一一执行的（配置的时候，各个拦截器之间使用逗号隔开）。同样也要提防“副作用”的发生。如果在拦截链中某个拦截器执行失败，那么下一个拦截器会接着从上一个执行成功的拦截器继续执行。 4. 多线程实现KafkaProducer是线程安全的，然而KafkaConsumer却是非线程安全的。KafkaConsumer中定义了一个 acquire（）方法，用来检测当前是否只有一个线程在操作，若有其他线程正在操作则会抛出ConcurrentModifcationException异常 KafkaConsumer中的每个公用方法在执行所要执行的动作之前都会调用这个acquire（）方法，只有wakeup（）方法是个例外 12345678910private final AtomicLong currentThread = new AtomicLong(NO_CURRENT_THREAD); // kafkaConsumer 中的成员变量private void acquire() &#123; long threadId = Thread.currentThread().getId(); if (threadId != currentThread.get() &amp;&amp; !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId))&#123; throw new ConcurrentModificationException(&quot;KafkaConsumer is not safe for multi-threaded access&quot;); &#125; refcount.incrementAndGet();&#125; acquire（）方法和我们通常所说的锁（synchronized、Lock等）不同，它不会造成阻塞等待，我们可以将其看作一个轻量级锁，它仅通过线程操作计数标记的方式来检测线程是否发生了并发操作，以此保证只有一个线程在操作。acquire（）方法和release（）方法成对出现，表示相应的加锁和解锁操作。 12345private void release()&#123; if (refcount.decrementAndGet() == 0) &#123; currentThread.set(NO_CURRENT_THREAD); &#125;&#125; acquire（）方法和release（）方法都是私有方法，因此在实际应用中不需要我们显式地调用，但了解其内部的机理之后可以促使我们正确、有效地编写相应的程序逻辑。 KafkaConsumer 非线程安全并不意味着我们在消费消息的时候只能以单线程的方式执行。 如果生产者发送消息的速度大于消费者处理消息的速度，那么就会有越来越多的消息得不到及时的消费，造成了一定的延迟。除此之外，由于Kafka 中消息保留机制的作用，有些消息有可能在被消费之前就被清理了，从而造成消息的丢失。我们可以通过多线程的方式来实现消息消费，多线程的目的就是为了提高整体的消费能力。 多线程的实现方式大致有如下三种方式： 4.1 线程封闭第一种也是最常见的方式：线程封闭，即为每个线程实例化一个KafkaConsumer对象 一个线程对应一个KafkaConsumer实例，我们可以称之为消费线程。一个消费线程可以消费一个或多个分区中的消息，所有的消费线程都隶属于同一个消费组。这种实现方式的并发度受限于分区的实际个数。 内部类KafkaConsumerThread代表消费线程，其内部包裹着一个独立的KafkaConsumer实例。通过外部类的main（）方法来启动多个消费线程，消费线程的数量由consumerThreadNum变量指定。一般一个主题的分区数事先可以知晓，可以将consumerThreadNum设置成不大于分区数的值，如果不知道主题的分区数，那么也可以通过KafkaConsumer类的partitionsFor（）方法来间接获取，进而再设置合理的consumerThreadNum值。 上面这种多线程的实现方式和开启多个消费进程的方式没有本质上的区别，它的优点是每个线程可以按顺序消费各个分区中的消息。缺点也很明显，每个消费线程都要维护一个独立的TCP连接，如果分区数和consumerThreadNum的值都很大，那么会造成不小的系统开销。 4.2 指定分区消费第一种方案中，由于消费者与分区数的关系，当消费线程的个数大于分区数时，就有部分消费线程一直处于空闲的状态。 第二种方案是多个消费线程同时消费同一个分区，这个通过 assign（）、seek（）等方法实现，这样可以打破原有的消费线程的个数不能超过分区数的限制，进一步提高了消费的能力。不过这种实现方式对于位移提交和顺序控制的处理就会变得非常复杂，实际应用中使用得极少，并不推荐。 一般而言，分区是消费线程的最小划分单位。 4.3 改造消息处理模块在第一种方案的具体实现的第①行，如果这里对消息的处理非常迅速，那么 poll（）拉取的频次也会更高，进而整体消费的性能也会提升；相反，如果在这里对消息的处理缓慢，比如进行一个事务性操作，或者等待一个RPC的同步响应，那么poll（）拉取的频次也会随之下降，进而造成整体消费性能的下降。一般而言，poll（）拉取消息的速度是相当快的，而整体消费的瓶颈也正是在处理消息这一块，如果我们通过一定的方式来改进这一部分，那么我们就能带动整体消费性能的提升。 第三种方案，可以尝试将处理消息模块改成多线程的实现方式，来提升性能。 第三种实现方式相比第一种实现方式而言，除了横向扩展的能力，还可以减少TCP连接对系统资源的消耗，不过缺点就是对于消息的顺序处理就比较困难了。","tags":["kafka","consumer"],"categories":["kafka"]},{"title":"Kafka 系列(三)：Producer 源码篇","path":"//blog/kafka/kafka-producer-source-code/","content":"Kafka 系列文章列表Kafka 版本：2021-11 trunk 分支（当前最新版本3.0.0） 在 Kafka 中，客户端由 Java 语言实现，服务端由 Scala 语言来实现的，在使用 Kafka 时，客户端是用户最先接触到部分，因此，kafka系列文章的源码分析也会从客户端端开始，今天讲的是 Producer 端发送模型的实现。 在看这篇源码分析之前，不了解发送端原理的同学，可以先看下上篇博文Kafka Producer 原理篇 Producer 的使用在分析 Producer 发送模型之前，先看一下用户是如何使用 Producer 向 Kafka 写数据的，下面是一个关于 Producer 最简单的应用示例。 12345678910111213141516171819202122232425262728import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.Producer;import java.util.Properties;public class ProducerDemo &#123; private static String topicName; private static int msgNum; private static int key; public static void main(String[] args) &#123; Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); topicName = &quot;test&quot;; msgNum = 10; // 发送的消息数 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; msgNum; i++) &#123; String msg = i + &quot; Kafka Producer Demo.&quot;; producer.send(new ProducerRecord&lt;String, String&gt;(topicName, msg)); &#125; producer.close(); &#125;&#125; 从上面的代码可以看出 Kafka 为用户提供了非常简单的 API，在使用时，只需要如下两步： 初始化 KafkaProducer 实例； 调用 send 接口发送数据。 本文主要是围绕着 Producer 在内部是如何实现 send 接口而展开的。 发送流程下面通过对 send 源码分析来一步步剖析 Producer 数据的发送流程。 send()用户是直接使用 producer.send() 发送的数据，先看一下 send() 接口的实现 12345678910111213// 异步发送一条记录到 topic，等同于 send(record, null)@Overridepublic Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123; return send(record, null);&#125;// 异步发送一条记录到主题，并在确认发送后调用提供的回调。@Overridepublic Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // 在发送消息之前，先经过拦截器处理（），这个方法不会抛出异常 ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; 数据发送的最终实现还是调用了 Producer 的 doSend() 接口。 doSend()doSend() 源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112// 异步发送记录到主题private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; // 1. 检查 producer 是否被关闭 throwIfProducerClosed(); // 2. 确认数据要发送到的 topic 的 metadata 是可用的 long nowMs = time.milliseconds(); ClusterAndWaitTime clusterAndWaitTime; try &#123; clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs); &#125; catch (KafkaException e) &#123; if (metadata.isClosed()) throw new KafkaException(&quot;Producer closed while send in progress&quot;, e); throw e; &#125; nowMs += clusterAndWaitTime.waitedOnMetadataMs; long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; // 3. 序列化 record 的 key 和 value byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert key of class &quot; + record.key().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in key.serializer&quot;, cce); &#125; byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert value of class &quot; + record.value().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in value.serializer&quot;, cce); &#125; // 4. 获取该 record 的 partition 的值（可以指定,也可以根据算法计算） int partition = partition(record, serializedKey, serializedValue, cluster); tp = new TopicPartition(record.topic(), partition); setReadOnly(record.headers()); Header[] headers = record.headers().toArray(); int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(), compressionType, serializedKey, serializedValue, headers); ensureValidRecordSize(serializedSize); long timestamp = record.timestamp() == null ? nowMs : record.timestamp(); if (log.isTraceEnabled()) &#123; log.trace(&quot;Attempting to append record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;&quot;, record, callback, record.topic(), partition); &#125; // producer callback will make sure to call both &#x27;callback&#x27; and interceptor callback Callback interceptCallback = new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); if (transactionManager != null &amp;&amp; transactionManager.isTransactional()) &#123; transactionManager.failIfNotReadyForSend(); &#125; // 5. 向 accumulator 中追加数据 RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs, true, nowMs); // 6. 如果最近一个 ProducerBatch 满了，重新创建一个 ProducerBatch 进行追加 if (result.abortForNewBatch) &#123; int prevPartition = partition; partitioner.onNewBatch(record.topic(), cluster, prevPartition); partition = partition(record, serializedKey, serializedValue, cluster); tp = new TopicPartition(record.topic(), partition); if (log.isTraceEnabled()) &#123; log.trace(&quot;Retrying append due to new batch creation for topic &#123;&#125; partition &#123;&#125;. The old partition was &#123;&#125;&quot;, record.topic(), partition, prevPartition); &#125; // producer callback will make sure to call both &#x27;callback&#x27; and interceptor callback interceptCallback = new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs, false, nowMs); &#125; if (transactionManager != null &amp;&amp; transactionManager.isTransactional()) transactionManager.maybeAddPartitionToTransaction(tp); // 7. 如果 batch 已经满了,唤醒 sender 线程发送数据 if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(&quot;Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch&quot;, record.topic(), partition); this.sender.wakeup(); &#125; return result.future; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly &#125; catch (ApiException e) &#123; log.debug(&quot;Exception occurred during message send:&quot;, e); if (callback != null) callback.onCompletion(null, e); this.errors.record(); this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (KafkaException e) &#123; this.errors.record(); this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; // we notify interceptor about all exceptions, since onSend is called before anything else in this method this.interceptors.onSendError(record, tp, e); throw e; &#125;&#125; 在 dosend() 方法的实现上，一条 Record 数据的发送，可以分为以下几步： 确认数据要发送到的 topic 的 metadata 是可用的（如果该 partition 的 leader 存在则是可用的），如果没有 topic 的 metadata 信息，就需要获取相应的 metadata 序列化 record 的 key 和 value； 确定 record 要发送到的 partition（可以指定，也可以根据算法计算）； 向 accumulator 中追加 record 数据，数据会先进行缓存； 如果追加完数据后，对应的 RecordBatch 已经达到了 batch.size 的大小（或者batch 的剩余空间不足以添加下一条 Record），则唤醒 sender 线程发送数据。 下面会对这几部分的具体实现进行详细分析。 发送流程详解获取 topic 的 metadata 信息在数据发送前，需要先确定 topic 是可用的。这部分具体实现在 KafkaProducer 中的 waitOnMetadata() 。 序列化 key 和 valueProducer 端对 record 的 key 和 value 值进行序列化操作，在 Consumer 端再进行相应的反序列化。 kafka 提供了一部分通用的序列化实现（所有默认提供的序列化实现均在 org.apache.kafka.common.serialization），当然我们也是可以自定义序列化的具体实现。 确定分区关于 partition 值的计算，分为三种情况： 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 具体实现如下： 12345678// 为给定的记录计算分区。如果记录有分区直接返回，否则调用配置的分区算法来计算分区。private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition : partitioner.partition( record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; Producer 默认使用的 partitioner 是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，DefaultPartitioner 默认采用的是 sticky partitioning （黏性分区分配）。 12345678public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster, int numPartitions) &#123; if (keyBytes == null) &#123; return stickyPartitionCache.partition(topic, cluster); &#125; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;&#125; 关于 sticky partitioning 的具体实现，可以查看专题文章。 用户也可以自定义 partition 的策略。 向 accumulator 追加数据Producer 会先将 record 写入到 buffer 中，当达到一个 batch.size 的大小时，再唤起 sender 线程去发送 RecordBatch，这里先详细分析一下 Producer 是如何向 buffer 中写入数据的。 Producer 是通过 RecordAccumulator 实例追加数据，其中一个比较重要的变量就是 ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches ，每个 TopicPartition 都会对应一个 Deque&lt;RecordBatch&gt; ，当添加数据时，会向其 TopicPartition 对应的 queue 中尾部最后一个 RecordBatch 中添加 record，而发送数据时，则会先从 queue 头部的 RecordBatch 开始发送。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// org.apache.kafka.clients.producer.internals.RecordAccumulator// 向累加器中添加一条记录，返回追加结果public RecordAppendResult append(TopicPartition tp, long timestamp, byte[] key, byte[] value, Header[] headers, Callback callback, long maxTimeToBlock, boolean abortOnNewBatch, long nowMs) throws InterruptedException &#123; // We keep track of the number of appending thread to make sure we do not miss batches in // abortIncompleteBatches(). appendsInProgress.incrementAndGet(); ByteBuffer buffer = null; if (headers == null) headers = Record.EMPTY_HEADERS; try &#123; // check if we have an in-progress batch // 每个 topicPartition 对应一个 queue Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp); // 在对一个 queue 进行操作时,会保证线程安全 synchronized (dq) &#123; if (closed) throw new KafkaException(&quot;Producer closed while send in progress&quot;); // 向队列中最后一个 batch 中追加 RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs); if (appendResult != null) return appendResult; &#125; // 没有一个正在进行的记录批次，尝试分配一个新的批次 if (abortOnNewBatch) &#123; // Return a result that will cause another call to append. return new RecordAppendResult(null, false, false, true); &#125; // 为 topic-partition 创建一个新的 RecordBatch, 需要初始化相应的 RecordBatch，要为其分配的大小是: max（batch.size, 加上头文件的本条消息的大小） byte maxUsableMagic = apiVersions.maxUsableProduceMagic(); int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers)); log.trace(&quot;Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125; with remaining timeout &#123;&#125;ms&quot;, size, tp.topic(), tp.partition(), maxTimeToBlock); // 为新的 RecordBatch 分配 buffer buffer = free.allocate(size, maxTimeToBlock); // Update the current time in case the buffer allocation blocked above. nowMs = time.milliseconds(); synchronized (dq) &#123; // Need to check if producer is closed again after grabbing the dequeue lock. if (closed) throw new KafkaException(&quot;Producer closed while send in progress&quot;); RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs); if (appendResult != null) &#123; // Somebody else found us a batch, return the one we waited for! Hopefully this doesn&#x27;t happen often... return appendResult; &#125; // 给 topic-partition 创建一个 RecordBatch MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, nowMs); // 向新的 RecordBatch 中追加数据 FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers, callback, nowMs)); // 将 RecordBatch 添加到对应的 queue 中 dq.addLast(batch); // 向未 ack 的 batch 集合添加这个 batch incomplete.add(batch); // Don&#x27;t deallocate this buffer in the finally block as it&#x27;s being used in the record batch buffer = null; // 如果 dp.size()&gt;1 就证明这个 queue 有一个 batch 是可以发送了 return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true, false); &#125; &#125; finally &#123; if (buffer != null) free.deallocate(buffer); appendsInProgress.decrementAndGet(); &#125;&#125; 获取该 TopicPartition 对应的 queue，没有的话会创建一个空的 queue； 向 queue 中追加数据，先获取 queue 中最新加入的那个 RecordBatch，如果不存在或者存在但剩余空余不足以添加本条 record 则新创建一个，成功写入的话直接返回结果，写入成功； 创建一个新的 RecordBatch，初始化内存大小根据 Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers)) 来确定（防止单条 record 过大的情况）； 向新建的 RecordBatch 写入 record，并将 RecordBatch 添加到 queue 中，返回结果，写入成功。 发送 RecordBatch当 record 写入成功后，如果发现 RecordBatch 已满足发送的条件（通常是 queue 中有多个 batch，那么最先添加的那些 batch 肯定是可以发送了），那么就会唤醒 sender 线程，发送 RecordBatch。 sender 线程对 RecordBatch 的处理是在 sendProducerData() 方法中进行的，该方法具体实现如下： sendProducerData() 源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879private long sendProducerData(long now) &#123; Cluster cluster = metadata.fetch(); // 获取那些已经可以发送的 RecordBatch 对应的 nodes RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now); // 如果有 topic-partition 的 leader 是未知的,就强制 metadata 更新 if (!result.unknownLeaderTopics.isEmpty()) &#123; // The set of topics with unknown leader contains topics with leader election pending as well as // topics which may have expired. Add the topic again to metadata to ensure it is included // and request metadata update, since there are messages to send to the topic. for (String topic : result.unknownLeaderTopics) this.metadata.add(topic, now); log.debug(&quot;Requesting metadata update due to unknown leader topics from the batched records: &#123;&#125;&quot;, result.unknownLeaderTopics); this.metadata.requestUpdate(); &#125; // 如果与node 没有连接（如果可以连接,同时初始化该连接）,就证明该 node 暂时不能发送数据,暂时移除该 node Iterator&lt;Node&gt; iter = result.readyNodes.iterator(); long notReadyTimeout = Long.MAX_VALUE; while (iter.hasNext()) &#123; Node node = iter.next(); if (!this.client.ready(node, now)) &#123; iter.remove(); notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now)); &#125; &#125; // 返回该 node 对应的所有可以发送的 RecordBatch 组成的 batches（key 是 node.id）,并将 RecordBatch 从对应的 queue 中移除 Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now); addToInflightBatches(batches); if (guaranteeMessageOrder) &#123; // Mute all the partitions drained for (List&lt;ProducerBatch&gt; batchList : batches.values()) &#123; for (ProducerBatch batch : batchList) this.accumulator.mutePartition(batch.topicPartition); &#125; &#125; // 移除超时的 RecordBatch accumulator.resetNextBatchExpiryTime(); List&lt;ProducerBatch&gt; expiredInflightBatches = getExpiredInflightBatches(now); List&lt;ProducerBatch&gt; expiredBatches = this.accumulator.expiredBatches(now); expiredBatches.addAll(expiredInflightBatches); // 这一块暂时还没太搞懂，不过不影响理解发送的流程 if (!expiredBatches.isEmpty()) log.trace(&quot;Expired &#123;&#125; batches in accumulator&quot;, expiredBatches.size()); for (ProducerBatch expiredBatch : expiredBatches) &#123; String errorMessage = &quot;Expiring &quot; + expiredBatch.recordCount + &quot; record(s) for &quot; + expiredBatch.topicPartition + &quot;:&quot; + (now - expiredBatch.createdMs) + &quot; ms has passed since batch creation&quot;; failBatch(expiredBatch, new TimeoutException(errorMessage), false); if (transactionManager != null &amp;&amp; expiredBatch.inRetry()) &#123; // This ensures that no new batches are drained until the current in flight batches are fully resolved. transactionManager.markSequenceUnresolved(expiredBatch); &#125; &#125; sensors.updateProduceRequestMetrics(batches); // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry // time, and the delay time for checking data availability. Note that the nodes may have data that isn&#x27;t yet // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data // that aren&#x27;t ready to send since they would cause busy looping. long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout); pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now); pollTimeout = Math.max(pollTimeout, 0); if (!result.readyNodes.isEmpty()) &#123; log.trace(&quot;Nodes with data ready to send: &#123;&#125;&quot;, result.readyNodes); // if some partitions are already ready to be sent, the select time would be 0; // otherwise if some partition already has some data accumulated but not ready yet, // the select time will be the time difference between now and its linger expiry time; // otherwise the select time will be the time difference between now and the metadata expiry time; pollTimeout = 0; &#125; sendProduceRequests(batches, now); return pollTimeout;&#125; 这段代码前面有很多是其他的逻辑处理，如：移除暂时不可用的 node、处理超时的 RecordBatch，真正进行发送发送 RecordBatch 的是 sendProduceRequests(batches, now) 这个方法。里面的实现逻辑主要就是将 batches 中 leader 为同一个 node 的所有 RecordBatch 放在一个请求中进行发送。 以上就是 kafka producer 客户端发送的整个流程，如有问题，欢迎在评论区交流。","tags":["kafka","producer"],"categories":["kafka"]},{"title":"Kafka 系列(二)：Producer 原理篇","path":"//blog/kafka/kafka-producer/","content":"Kafka 系列文章列表Kafka 版本：2021-11 trunk 分支（当前最新版本3.0.0） 一个正常的生产者逻辑需要具备以下几个步骤： 配置生产者客户端参数以及创建相应的生产者实例 构建待发送的消息 发送消息 关闭生产者实例 序列化器生产者在发送消息时，需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给kafka，同样的消费者需要用反序列化器（Deserializer）把从Kafka中收到的字节数组转换成相应的对象。 生产者使用的序列化器和消费者使用的反序列化器需要一一对应。 kafka 客户端提供了多种序列化器，同时也支持自定义实现序列化器（可以选择使用如 Avro、JSON、Thrift、ProtoBuf、Protostuff等通用的序列化工具来实现）。 分区器消息在通过 send() 方法发往 broke 的过程中，有可能需要经过拦截器（Interceptor）、序列化器（Serializer）和分区器（Partitioner）的一些列作用之后才能被真正的发往kafka。拦截器一般不是必须的，而序列化器是必需的。 消息经过序列化器之后就需要确定他发往的分区，如果消息 ProducerRecord 中指定了具体的 partition 字段，那么就不需要分区器的作用，因为partition 就代表了要发往的分区号。 如果消息没有指定分区号，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。分区器的作用就是为消息分配分区。 kafka 中提供的默认分区器的计算逻辑是： 如果 key 不为 null，则对 key 进行 哈希 （采用 MurmurHash2 算法），最终根据到的哈希值计算分区号，拥有相同key的消息会被发送到同一个分区 如果 key 为 null，消息将会以轮训的方式发往主题内的各个可用分区 在不改变主题分区的数量下，key 和分区之间的映射关系可以保持不变，一旦主题的分区数量发生变化（增加分区），那么映射关系就很难保证了。 除了 kafka 默认提供的分区器外，还可以使用自定义的分区器，只需要同 DefaultPartitioner 一样实现 Partitioner 接口即可。 实现自定义分区器之后，需要通过配置参数显示指定自定义分区器。 拦截器kafka 一共有两种拦截器：生产者拦截器和消费者拦截器。 生产者拦截器既可以在 消息发送前 做一些准备工作，比如按照某个规则过滤不符合条件的消息、修改消息的内容等，也可以在 发送回调逻辑前 做一些定制化的需求，比如统计类的工作。 实现方式：实现 ProducerInterceptor 接口，该接口提供如下三个方法 onSend onAcknowledgement close KafkaProducer 再将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。（一般来说最好不要修改 ProducerRecord 的 topic key partition 等信息，不然有可能影响分区的计算以及broker端日志的压缩功能） KafkaProducer 会在消息被应答之前和消息发送失败时调用生产者拦截器的 onAcknowledgement() 方法，优先于用户设定的 Callback 之前执行。这个方法运行在 Producer 的 IO 线程中，所以逻辑越简单越好，否则会影响消息发送的速度。 close() 方法主要用于在关闭拦截器时，执行一些资源的清理工作。 这三个方法中抛出的异常会被捕获并记录到日志中，并不会向上传递。 整体架构kafka 中同个生产者客户端由两个线程协调运行，分别为：主线程和 Sender 线程（发送线程）。 在主线程中由 KafkaProducer 创建消息，然后通过 可能的 拦截器、序列化器、分区器的作用之后，缓存到消息累加器 RecordAccumulator 中。Sender 线程负责从消息累加器中获取消息并将其发送到 kafka 中。 消息累加器 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。消息累加器缓存的大小可以通过配置 buffer.memory 指定（默认为 33554432 B – 32MB）。如果发送速度超过了发送到服务器的速度，则会导致生产者空间不足，这时候 send() 方法调用要么被阻塞，要么抛出异常，这取决于 max.block.ms 的配置，默认为 60000 – 60s。 主线程中发送过来的消息都会被追加到 RecordAccumulator 的某个双端队列（Deque）中，在 RecordAccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch ，即 Deque&lt;ProducerBatch&gt; 。消息写入缓存时，追加到双端队列的尾部； Sender 读取消息时，从双端队列的头部读取。注意ProducerBatch不是ProducerRecord，ProducerBatch中可以包含一至多个ProducerRecord。通俗地说，ProducerRecord 是生产者中创建的消息，而ProducerBatch是指一个消息批次，ProducerRecord会被包含在ProducerBatch中，这样可以使字节的使用更加紧凑。与此同时，将较小的ProducerRecord拼凑成一个较大的ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量。如果生产者客户端需要向很多分区发送消息，则可以将buffer.memory参数适当调大以增加整体的吞吐量。 消息在网络上都是以字节（Byte）的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过java.io.ByteBuffer实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在RecordAccumulator的内部还有一个BufferPool，它主要用来实现ByteBuffer的复用，以实现缓存的高效利用。不过BufferPool只针对特定大小的ByteBuffer进行管理，而其他大小的ByteBuffer不会缓存进BufferPool中，这个特定的大小由batch.size参数来指定，默认值为16384B，即16KB。我们可以适当地调大batch.size参数以便多缓存一些消息。 ProducerBatch的大小和batch.size参数也有着密切的关系。当一条消息（ProducerRecord）流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端队列的尾部获取一个 ProducerBatch（如果没有则新建），查看 ProducerBatch 中是否还可以写入这个ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的ProducerBatch。在新建ProducerBatch时评估这条消息的大小是否超过batch.size参数的大小，如果不超过，那么就以 batch.size 参数的大小来创建ProducerBatch，这样在使用完这段内存区域之后，可以通过BufferPool 的管理来进行复用；如果超过，那么就以评估的大小来创建ProducerBatch，这段内存区域不会被复用。 Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本 &lt;分区，Deque&lt;ProducerBatch&gt;&gt; 的保存形式转变成 &lt;Node，List&lt;ProducerBatch&gt;&gt; 的形式，其中Node表示Kafka集群的broker节点。对于网络连接来说，生产者客户端是与具体的broker节点建立的连接，也就是向具体的broker 节点发送消息，而并不关心消息属于哪一个分区；而对于 KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I&#x2F;O层面的转换。 在转换成 &lt;Node，List&lt;ProducerBatch&gt;&gt; 的形式之后，Sender 还会进一步封装成 &lt;Node，Request&gt; 的形式，这样就可以将Request请求发往各个Node了，这里的Request是指Kafka的各种协议请求，对于消息发送而言就是指具体的ProduceRequest 请求在从Sender线程发往Kafka之前还会保存到InFlightRequests中，InFlightRequests保存对象的具体形式为 Map&lt;NodeId，Deque&lt;Request&gt;&gt; ，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个String 类型，表示节点的 id 编号）。与此同时，InFlightRequests还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接（也就是客户端与Node之间的连接）最多缓存的请求数。这个配置参数为max.in.flight.requests.per.connection，默认值为 5，即每个连接最多只能缓存 5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response）。通过比较 Deque&lt;Request&gt; 的size与这个参数的大小来判断对应的Node中是否已经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继续向其发送请求会增大请求超时的可能。 leastLoadedNode InFlightRequests还可以获得leastLoadedNode，即所有Node中负载最小的那一个。这里的负载最小是通过每个Node在InFlightRequests中还未确认的请求决定的，未确认的请求越多则认为负载越大。 图中展示了三个节点Node0、Node1和Node2，很明显Node1的负载最小。也就是说，Node1为当前的leastLoadedNode。选择leastLoadedNode发送请求可以使它能够尽快发出，避免因网络拥塞等异常而影响整体的进度。leastLoadedNode的概念可以用于多个应用场合，比如元数据请求、消费者组播协议的交互。 当客户端中没有需要使用的元数据信息时，比如没有指定的主题信息，或者超过metadata.max.age.ms 时间没有更新元数据都会引起元数据的更新操作。客户端参数metadata.max.age.ms的默认值为300000，即5分钟。元数据的更新操作是在客户端内部进行的，对客户端的外部使用者不可见。当需要更新元数据时，会先挑选出leastLoadedNode，然后向这个Node发送MetadataRequest请求来获取具体的元数据信息。这个更新操作是由Sender线程发起的，在创建完MetadataRequest之后同样会存入InFlightRequests，之后的步骤就和发送消息时的类似。元数据虽然由Sender线程负责更新，但是主线程也需要读取这些信息，这里的数据同步通过synchronized和final关键字来保障。","tags":["kafka","producer"],"categories":["kafka"]},{"title":"Kafka 系列(一)：基本概念","path":"//blog/kafka/kafka-concept/","content":"本文主要介绍一下kafka中的基本概念，主要包括：Producer、Consumer、topic、partition、offset、broker、ISR、AR、HW、LEO等。 Consumer &amp; Producer一个典型的 Kafka 体系架构包括若干 Producer、若干 Broker、若干Consumer，以及一个ZooKeeper集群。其中ZooKeeper是Kafka用来负责集群元数据的管理、控制器的选举等操作的。Producer将消息发送到Broker，Broker负责将收到的消息存储到磁盘中，而Consumer负责从Broker订阅并消费消息。 Producer：生产者，也就是发送消息的一方。生产者负责创建消息，然后将其投递到Kafka中。 Consumer：消费者，也就是接收消息的一方。消费者连接到Kafka上并接收消息，进而进行相应的业务逻辑处理。 Broker：服务代理节点。对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务节点或Kafka服务实例。大多数情况下也可以将Broker看作一台Kafka服务器，前提是这台服务器上只部署了一个Kafka实例。一个或多个Broker组成了一个Kafka集群。 消费者（Consumer）负责订阅Kafka中的主题（Topic），并且从订阅的主题上拉取消息。与其他一些消息中间件不同的是：在Kafka的消费理念中还有一层消费组（Consumer Group）的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。 例如，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。 有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。 按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。 换言之，每一个分区只能被一个消费组中的一个消费者所消费。 消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。 对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。 以上分配逻辑都是基于默认的分区分配策略进行分析的，可以通过消费者客户端参数 partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。 对于消息中间件而言，一般有两种消息投递模式： 点对点（P2P，Point-to-Point）模式 发布&#x2F;订阅（Pub&#x2F;Sub）模式 点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。 发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题（Topic），主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息。 主题使得消息的订阅者和发布者互相保持独立，不需要进行接触即可保证消息的传递，发布&#x2F;订阅模式在消息的一对多广播时采用。Kafka 同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合： 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布&#x2F;订阅模式的应用。 Topic &amp; Partition在Kafka中还有两个特别重要的概念 主题（Topic） 分区（Partition） Kafka中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题，而消费者负责订阅主题并进行消费。 主题是一个逻辑上的概念，它还可以细分为多个分区，一个分区只属于单个主题，很多时候也会把分区称为主题分区（Topic-Partition）。 同一主题下的不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）。offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。 Kafka中的分区可以分布在不同的服务器（broker）上，也就是说，一个主题可以横跨多个broker，以此来提供比单个broker更强大的性能。 每一条消息被发送到broker之前，会根据分区规则选择存储到哪个具体的分区。如果分区规则设定得合理，所有的消息都可以均匀地分配到不同的分区中。 如果一个主题只对应一个文件，那么这个文件所在的机器 I&#x2F;O 将会成为这个主题的性能瓶颈，而分区解决了这个问题。在创建主题的时候可以通过指定的参数来设置分区的个数，当然也可以在主题创建完成之后去修改分区的数量，通过增加分区的数量可以实现水平扩展。 Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样），副本之间是“一主多从”的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。 副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。Kafka通过多副本机制实现了故障的自动转移，当Kafka集群中某个broker失效时仍然能保证服务可用。 Kafka 消费端也具备一定的容灾能力。Consumer 使用拉（Pull）模式从服务端拉取消息，并且保存消费的具体位置，当消费者宕机后恢复上线时可以根据之前保存的消费位置重新拉取需要的消息进行消费，这样就不会造成消息丢失。 AR &amp; ISR分区中的所有副本统称为AR（Assigned Replicas）。所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。 消息会先发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步，同步期间内follower副本相对于leader副本而言会有一定程度的滞后。前面所说的“一定程度的同步”是指可忍受的滞后范围，这个范围可以通过参数进行配置。 与leader副本同步滞后过多的副本（不包括leader副本）组成OSR（Out-of-Sync Replicas），由此可见，AR&#x3D;ISR+OSR。在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，即AR&#x3D;ISR，OSR集合为空。 leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从ISR集合中剔除。如果OSR集合中有follower副本“追上”了leader副本，那么leader副本会把它从OSR集合转移至ISR集合。 默认情况下，当leader副本发生故障时，只有在ISR集合中的副本才有资格被选举为新的leader，而在OSR集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。 HW &amp; LEOHW 、 LEO 和上面提到的 ISR有着紧密的关系。 HW （High Watermark）俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。 下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最后一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。 LEO （Log End Offset），标识当前日志文件中下一条待写入的消息的offset。上图中offset为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的offset值加1. 分区 ISR 集合中的每个副本都会维护自身的 LEO ，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息。 LW是Low Watermark的缩写，俗称“低水位”，代表AR集合中最小的logStartOffset值。副本的拉取请求（FetchRequest，它有可能触发新建日志分段而旧的被清理，进而导致logStartOffset的增加）和删除消息请求（DeleteRecordRequest）都有可能促使LW的增长。 下面具体分析一下 ISR 集合和 HW、LEO的关系。 假设某分区的 ISR 集合中有 3 个副本，即一个 leader 副本和 2 个 follower 副本，此时分区的 LEO 和 HW 都分别为 3 。消息3和消息4从生产者出发之后先被存入leader副本。 在消息被写入leader副本之后，follower副本会发送拉取请求来拉取消息3和消息4进行消息同步。 在同步过程中不同的副本同步的效率不尽相同，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO 为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset0至3之间的消息。 当所有副本都成功写入消息3和消息4之后，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。 由此可见kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。 事实上，同步复制要求所有能工作的follower副本都复制完，这条消息才会被确认已成功提交，这种复制方式极大的影响了性能。而在异步复制的方式下，follower副本异步的从leader副本中复制数据，数据只要被leader副本写入就会被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，然后leader副本宕机，则会造成数据丢失。kafka使用这种ISR的方式有效的权衡了数据可靠性和性能之间的关系。","tags":["kafka"],"categories":["kafka"]},{"title":"分布式ID","path":"//blog/framework/unique-id/","content":"概述唯一id是我们在设计阶段常常遇到的问题。 在复杂的分布式系统中，几乎都需要对大量的数据和消息进行唯一标识。在设计初期，我们需要考虑日后数据量的级别，如果可能会对数据进行分库分表，那么就需要有一个全局唯一id来标识一条数据或记录。 生成唯一id的策略有多种，但是每种策略都有它的适用场景、优点以及局限性。 唯一ID的特点全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。 趋势递增：最好趋势递增，这个要求就得看具体业务场景了，一般不严格要求。 单调递增：保证下一个ID一定大于上一个ID，例如事务版本号、排序等特殊需求。 高可用：100%的可用性是骗人的，但是也要无限接近于100%的可用性 高可用性：同时除了对ID号码自身的要求，业务还对ID号生成系统的可用性要求极高，想象一下，如果ID生成系统瘫痪，这就会带来一场灾难。所以不能有单点故障； 信息安全：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则； 分片支持：可以控制ShardingId。比如某一个用户的文章要放在同一个分片内，这样查询效率高，修改也容易； 长度适中。 常见方案数据库自增ID基于数据库的auto_increment自增ID完全可以充当分布式ID，具体实现：需要一个单独的MySQL实例用来生成ID，建表结构如下： 123456CREATE DATABASE `SEQ_ID`;CREATE TABLE SEQID.SEQUENCE_ID ( id bigint(20) unsigned NOT NULL auto_increment, value char(10) NOT NULL default &#x27;&#x27;, PRIMARY KEY (id),) ENGINE=MyISAM; 当我们需要一个ID的时候，向表中插入一条记录返回主键ID，但这种方式有一个比较致命的缺点，访问量激增时MySQL本身就是系统的瓶颈，用它来实现分布式服务风险比较大. 1insert into SEQUENCE_ID(value) VALUES (&#x27;values&#x27;); 优点 使用简单。 利用现有数据库系统的功能实现，成本小，代码简单，性能可以接受。 ID号单调递增。数值类型查询速度快。 缺点 强依赖DB。不同数据库语法和实现不同，数据库迁移的时候、多数据库版本支持的时候、或分表分库的时候需要处理，会比较麻烦。当DB异常时整个系统不可用，属于致命问题。 单点故障。在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。 数据一致性问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。 难于扩展。在性能达不到要求的情况下，比较难于扩展。ID发号性能瓶颈限制在单台MySQL的读写性能。 优化实现 针对主库单点， 如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。 1234567-- Mysql 1set @@auto_increment_offset = 1; -- 起始值set @@auto_increment_increment = 2; -- 步长--Mysql 2set @@auto_increment_offset = 2; -- 起始值set @@auto_increment_increment = 2; -- 步长 这样两个MySQL实例的自增ID分别就是： Mysql 1 : 1、3、5、7、9 Mysql 2 : 2、4、6、8、10 这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。 使用集群之后性能依旧扛不住高并发时，就需要进行扩容。这时会比较麻烦。 水平扩展的数据库集群，有利于解决数据库单点压力的问题，同时为了ID生成特性，将自增步长按照机器数量来设置。 增加第三台MySQL实例需要人工修改一、二两台MySQL实例的起始值和步长，把第三台机器的ID起始生成位置设定在比现有最大自增ID的位置远一些，但必须在一、二两台MySQL实例ID还没有增长到第三台MySQL实例的起始ID值的时候，否则自增ID就要出现重复了，必要时可能还需要停机修改。 优点 解决DB单点问题 缺点 不利于后续扩容，而且实际上单个数据库自身压力还是大，依旧无法满足高并发场景。 🔥 数据库号段模式号段模式是当下分布式ID生成器的主流实现方式之一，号段模式可以理解为从数据库批量的获取自增ID，每次从数据库取出一个号段范围，例如 (1,1000] 代表1000个ID，具体的业务服务将本号段，生成1~1000的自增ID并加载到内存。表结构如下： 12345678CREATE TABLE id_generator ( id int(10) NOT NULL, max_id bigint(20) NOT NULL COMMENT &#x27;当前最大id&#x27;, step int(20) NOT NULL COMMENT &#x27;号段的布长&#x27;, biz_type int(20) NOT NULL COMMENT &#x27;业务类型&#x27;, version int(20) NOT NULL COMMENT &#x27;版本号&#x27;, PRIMARY KEY (`id`)) biz_type ：代表不同业务类型 max_id ：当前最大的可用id step ：代表号段的长度 version ：是一个乐观锁，每次都更新version，保证并发时数据的正确性 id biz_type max_id step version 1 101 1000 2000 0 等这批号段ID用完，再次向数据库申请新号段，对max_id字段做一次update操作，update max_id&#x3D; max_id + step，update成功则说明新号段获取成功，新的号段范围是(max_id ,max_id +step]。 1update id_generator set max_id = #&#123;max_id+step&#125;, version = version + 1 where version = # &#123;version&#125; and biz_type = XXX 由于多业务端可能同时操作，所以采用版本号version乐观锁方式更新，这种分布式ID生成方式不强依赖于数据库，不会频繁的访问数据库，对数据库的压力小很多。 UUIDUUID (Universally Unique Identifier) 的目的，是让分布式系统中的所有元素，都能有唯一的辨识资讯，而不需要透过中央控制端来做辨识资讯的指定。如此一来，每个人都可以建立不与其它人冲突的 UUID。在这样的情况下，就不需考虑数据库建立时的名称重复问题。 UUID的标准形式: 16字节128位，通常以36长度的字符串表示. 示例：550e8400-e29b-41d4-a716-446655440000，到目前为止业界一共有5种方式生成UUID。 在Java中我们可以直接使用下面的API生成UUID: 1UUID uuid = UUID.randomUUID(); String s = UUID.randomUUID().toString(); 优点 非常简单，本地生成，代码方便，API调用方便。 性能非高。生成的id性能非常好，没有网络消耗，基本不会有性能问题。 全球唯一。在数据库迁移、系统数据合并、或者数据库变更的情况下，可以 从容应对。 缺点 存储成本高。UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。如果是海量数据库，就需要考虑存储量的问题。 信息不安全。基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。 不适用作为主键，ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用。UUID往往是使用字符串存储，查询的效率比较低。 UUID是无序的。不是单调递增的，而现阶段主流的数据库主键索引都是选用的B+树索引，对于无序长度过长的主键插入效率比较低。 传输数据量大。 不可读。 优化方案 为了解决UUID不可读， 可以使用UUID to Int64的方法 。 为了解决UUID无序的问题，NHibernate在其主键生成方式中提供了Comb算法（combined guid&#x2F;timestamp）。保留GUID的10个字节，用另6个字节表示GUID生成的时间（DateTime）。 像用作订单号UUID这样的字符串没有丝毫的意义，看不出和订单相关的有用信息；而对于数据库来说用作业务主键ID，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作分布式ID。 Redis生成ID当使用数据库来生成ID性能不够要求的时候，我们可以尝试使用Redis来生成ID。这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。 1234127.0.0.1:6379&gt; set seq_id 1 // 初始化自增ID为1OK127.0.0.1:6379&gt; incr seq_id // 增加1，并返回递增后的数值(integer) 2 可以使用Redis集群来获取更高的吞吐量。假如一个集群中有5台Redis。可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5。各个Redis生成的ID为： 12345A：1,6,11,16,21B：2,7,12,17,22C：3,8,13,18,23D：4,9,14,19,24E：5,10,15,20,25 这个负载到哪台机器上需要提前设定好，未来很难做修改。但是3-5台服务器基本能够满足，都可以获得不同的ID。步长和初始值一定需要事先设定好。使用Redis集群也可以防止单点故障的问题。 比较适合使用Redis来生成日切流水号。比如订单号&#x3D;日期+当日自增长号。可以每天在Redis中生成一个Key，使用INCR进行累加。 用redis实现需要注意一点，要考虑到redis持久化的问题。 优点 不依赖于数据库，灵活方便，且性能优于数据库。。 数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点 如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。。 需要编码和配置的工作量比较大。 Redis单点故障，影响序列服务的可用性。 zookeeper生成IDzookeeper主要通过其znode数据版本来生成序列号，可以生成32位和64位的数据版本号，客户端可以使用这个版本号来作为唯一的序列号。 很少会使用zookeeper来生成唯一ID。主要是由于需要依赖zookeeper，并且是多步调用API，如果在竞争较大的情况下，需要考虑使用分布式锁。因此，性能在高并发的分布式环境下，也不甚理想。 Twitter的snowflake算法snowflake(雪花算法)是Twitter开源的分布式ID生成算法，结果是一个long型的ID。这种方案把64-bit分别划分成多段，分开来标示机器、时间、序列号等。 Snowflake ID组成结构：正数位（占1比特）+ 时间戳（占41比特）+ 机器ID（占5比特）+ 数据中心（占5比特）+ 自增值（占12比特），总共64比特组成的一个Long类型。 第一个bit位（1bit）：Java中long的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。 时间戳部分（41bit）：毫秒级的时间，不建议存当前时间戳，而是用（当前时间戳 - 固定开始时间戳）的差值，可以使产生的ID从更小的值开始；41位的时间戳可以使用69年，(1L &lt;&lt; 41) &#x2F; (1000L * 60 * 60 * 24 * 365) &#x3D; 69年 工作机器id（10bit）：也被叫做workId，这个可以灵活配置，机房或者机器号组合都可以。 序列号部分（12bit），自增值支持同一毫秒内同一个节点可以生成4096个ID 具体实现的代码可以参看如下： twitter-archive/snowflake Twitter的雪花算法SnowFlakehttps://github.com/twitter-archive/snowflake beyondfengyu/SnowFlake Twitter的雪花算法SnowFlake，使用Java语言实现。https://github.com/beyondfengyu/SnowFlake snowflake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。 优点 稳定性高，不依赖于数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。 灵活方便，可以根据自身业务特性分配bit位。 单机上ID单调自增，毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。 缺点 强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。 ID可能不是全局递增。在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。 百度 uid-generatoruid-generator是基于Snowflake算法实现的，与原始的snowflake算法不同在于，uid-generator支持自定义时间戳、工作机器ID和 序列号 等各部分的位数，而且uid-generator中采用用户自定义workId的生成策略。 uid-generator需要与数据库配合使用，需要新增一个WORKER_NODE表。当应用启动时会向数据库表中去插入一条数据，插入成功后返回的自增ID就是该机器的workId数据由host，port组成。 对于uid-generator ID组成结构： workId，占用了22个bit位，时间占用了28个bit位，序列化占用了13个bit位，需要注意的是，和原始的snowflake不太一样，时间的单位是秒，而不是毫秒，workId也不一样，而且同一应用每次重启就会消费一个workId。 uid-generator 百度 uid-generatorhttps://github.com/baidu/uid-generator 美团 LeafLeaf由美团开发，Leaf同时支持号段模式和snowflake算法模式，可以切换使用。 号段模式先导入源码 https://github.com/Meituan-Dianping/Leaf，再建一张表 leaf_alloc 12345678910DROP TABLE IF EXISTS `leaf_alloc`;CREATE TABLE `leaf_alloc` ( `biz_tag` varchar(128) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;业务key&#x27;, `max_id` bigint(20) NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;当前已经分配了的最大id&#x27;, `step` int(11) NOT NULL COMMENT &#x27;初始步长，也是动态调整的最小步长&#x27;, `description` varchar(256) DEFAULT NULL COMMENT &#x27;业务key的描述&#x27;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;数据库维护的更新时间&#x27;, PRIMARY KEY (`biz_tag`)) ENGINE=InnoDB; 然后在项目中开启号段模式，配置对应的数据库信息，并关闭 snowflake 模式 123456789leaf.name=com.sankuai.leaf.opensource.testleaf.segment.enable=trueleaf.jdbc.url=jdbc:mysql://localhost:3306/leaf_test?useUnicode=true&amp;characterEncoding=utf8&amp;characterSetResults=utf8leaf.jdbc.username=rootleaf.jdbc.password=rootleaf.snowflake.enable=false#leaf.snowflake.zk.address=#leaf.snowflake.port= 启动leaf-server 模块的 LeafServerApplication项目就跑起来了 号段模式获取分布式自增ID的测试url ：http：&#x2F;&#x2F;localhost：8080&#x2F;api&#x2F;segment&#x2F;get&#x2F;leaf-segment-test 监控号段模式：http://localhost:8080/cache snowflake模式Leaf的snowflake模式依赖于ZooKeeper，不同于原始snowflake算法也主要是在workId的生成上，Leaf中workId是基于ZooKeeper的顺序Id来生成的，每个应用在使用Leaf-snowflake时，启动时都会都在Zookeeper中生成一个顺序Id，相当于一台机器对应一个顺序节点，也就是一个workId。 123leaf.snowflake.enable=trueleaf.snowflake.zk.address=127.0.0.1leaf.snowflake.port=2181 snowflake模式获取分布式自增ID的测试url：http://localhost:8080/api/snowflake/get/test 滴滴 TinyidTinyid由滴滴开发，Github地址：https://github.com/didi/tinyid。 Tinyid是基于号段模式原理实现的与Leaf如出一辙，每个服务获取一个号段（1000,2000]、（2000,3000]、（3000,4000] Tinyid提供http和tinyid-client两种方式接入 Http 方式接入先导入 Tinyid 源码 https://github.com/didi/tinyid.git，再建一张表 tiny_id_info 123456789101112131415161718192021222324CREATE TABLE `tiny_id_info` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT &#x27;自增主键&#x27;, `biz_type` varchar(63) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;业务类型，唯一&#x27;, `begin_id` bigint(20) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;开始id，仅记录初始值，无其他含义。初始化时begin_id和max_id应相同&#x27;, `max_id` bigint(20) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;当前最大id&#x27;, `step` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;步长&#x27;, `delta` int(11) NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;每次id增量&#x27;, `remainder` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;余数&#x27;, `create_time` timestamp NOT NULL DEFAULT &#x27;2010-01-01 00:00:00&#x27; COMMENT &#x27;创建时间&#x27;, `update_time` timestamp NOT NULL DEFAULT &#x27;2010-01-01 00:00:00&#x27; COMMENT &#x27;更新时间&#x27;, `version` bigint(20) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;版本号&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uniq_biz_type` (`biz_type`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT &#x27;id信息表&#x27;;CREATE TABLE `tiny_id_token` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT &#x27;自增id&#x27;, `token` varchar(255) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;token&#x27;, `biz_type` varchar(63) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;此token可访问的业务类型标识&#x27;, `remark` varchar(255) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;备注&#x27;, `create_time` timestamp NOT NULL DEFAULT &#x27;2010-01-01 00:00:00&#x27; COMMENT &#x27;创建时间&#x27;, `update_time` timestamp NOT NULL DEFAULT &#x27;2010-01-01 00:00:00&#x27; COMMENT &#x27;更新时间&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT &#x27;token信息表&#x27;; 配置数据库 12345datasource.tinyid.names=primarydatasource.tinyid.primary.driver-class-name=com.mysql.jdbc.Driverdatasource.tinyid.primary.url=jdbc:mysql://ip:port/databaseName?autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=UTF-8datasource.tinyid.primary.username=rootdatasource.tinyid.primary.password=123456 测试 获取分布式自增ID: http://localhost:9999/tinyid/id/nextIdSimple?bizType=test&amp;token=0f673adf80504e2eaa552f5d791b644c‘ 批量获取分布式自增ID: http://localhost:9999/tinyid/id/nextIdSimple?bizType=test&amp;token=0f673adf80504e2eaa552f5d791b644c&amp;batchSize=10‘ Java客户端方式接入引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.xiaoju.uemc.tinyid&lt;/groupId&gt; &lt;artifactId&gt;tinyid-client&lt;/artifactId&gt; &lt;version&gt;$&#123;tinyid.version&#125;&lt;/version&gt;&lt;/dependency&gt; 配置文件 12tinyid.server =localhost:9999tinyid.token =0f673adf80504e2eaa552f5d791b644c 12345// 获取单个分布式自增IDLong id = TinyId.nextId(&quot;test&quot;);// 按需批量分布式自增IDList&lt;Long&gt; ids = TinyId.nextId(&quot;test&quot;, 10); test 是具体业务类型","tags":["分布式ID","全局唯一ID"],"categories":["分布式"]},{"title":"RTO & RPO","path":"//blog/design/rtorpo/","content":"RTO &amp; RPO在故障恢复方面，目前业界公认有三个目标值得努力。 恢复时间：企业能忍受多长时间没有 IT，处于停业状态。 网络多长时间能够恢复 业务层面的恢复 整个恢复过程中，最关键的衡量指标有两个：一个是 RTO，另一个是 RPO。 所谓 RTO，Recovery Time Objective，是指故障发生后，从 IT 系统宕机导致业务停顿之时开始，到 IT 系统恢复至可以支持各部门运作、恢复运营之时，此两点之间的时间段称为 RTO。 所谓 RPO，Recovery Point Objective，是指对系统和应用数据而言，要实现能够恢复至可以支持各部门业务运作，系统及生产数据应恢复到怎样的更新程度。这种更新程度可以是上一周的备份数据，也可以是上一次交易的实时数据。 选择标准对故障恢复而言，RTO 与 RPO 哪个衡量指标更合适呢？ 在考虑采用哪个指标之前，IT 人首先要弄清楚一个基本概念，企业的容灾系统预防的是什么灾害，是多少年一遇的，能忍受多少损失，需要算出一个大概的成本，当然不一定很精确。其次，无论企业容灾系统是采用冷备、热备、温备、还是磁盘备份，几分钟恢复业务和几天恢复业务效果是完全不一样的。企业需要明确对恢复时间的容忍底限是多少。 再从灾备本身的意义来讲，无论采用哪种衡量指标，最终目的是要能够很好地检验灾备系统的实用性能，否则就失去建立灾备的意义了。而灾备最核心的作用就是确保灾难发生后业务能够连续运行，交易中的数据完整保存，丢失越少越好。因此业务层面的恢复，企业要有一个底限。参考世界范围内一系列灾难恢复经验，国家之间的差别非常大。比如在美国，政府是第一位的，警察局对数据的恢复要求特别高。而在中国，无论什么性质，银行始终是排在第一位的。 综合平衡作为银行，除开展自身业务之外，更多数据来自上下级银行间的财务汇兑与结算。 站在管理者的位置上，一旦灾难发生，最重要的是在尽可能短的时间内排除障碍，恢复业务，保证系统做到连续运行。因此，从这个角度出发，银行容许系统停滞的时间应当越短越好。选择 RTO 刚好合适。 但是，RTO 对成本要求太高，与回报似乎不成正比。企业资金不可能无限制地投入到一个灾备系统中。对于银行证券这样的联机交易事故处理非常紧密的金融机构而言，可能每一笔、每一单、每一分钱都很重要，所以都需要恢复。RPO 显然更为合适。 许多时候进行选择并不意味着非此即彼，这与现实婚姻中一夫一妻的限制还是有差别的。RTO 和 RPO 对银行来讲都很重要。RTO 越短、RPO 越新，银行面临的损失就越小，但这也意味着系统开发成本将会急剧上升。许多时候，最佳的容灾解决方案却不一定是效益最好的。反之亦是。如何去平衡这中间的关系，不仅是门学问，更像是艺术。 根据国际经验，在选择“你”还是“她”的时候，企业应当考虑灾难发生后会在多大层面上冲击业务，这涉及到企业形象，商业机密，信誉评级，品牌竞争力等等方面，各个企业的情况不同，要根据自己的情况选择合适的“对象”。灾难恢复的目的是业务连续进行，因此无论采用 RTO 还是 RPO，都要朝着这个核心靠拢。","tags":["RTO","RPO"],"categories":["系统设计"]},{"title":"幂等性","path":"//blog/design/idempotent/","content":"什么是幂等性幂等性最早是数学里面的一个概念，后来被用于计算机领域，用于表示任意多次请求均与一次请求执行的结果相同，也就是说对于一个接口而言，无论调用了多少次，最终得到的结果都是一样的。比如以下代码： 123456789101112131415161718public class IdempotentExample &#123; // 变量 private static int count = 0; /** * 非幂等性方法 */ public static void addCount() &#123; count++; &#125; /** * 幂等性方法 */ public static void printCount() &#123; System.out.println(count); &#125;&#125; 对于变量 count 来说，如果重复调用 addCount() 方法的话，会一直累加 count 的值，因为 addCount() 方法就是非幂等性方法；而 printCount() 方法只是用来打印控制台信息的。因此，它无论调用多少次结果都是一样的，所以它是幂等性方法。 幂等性注意事项幂等性的实现与判断需要消耗一定的资源，因此不应该给每个接口都增加幂等性判断，要根据实际的业务情况和操作类型来进行区分。 例如，我们在进行查询操作和删除操作时就无须进行幂等性判断。查询操作查一次和查多次的结果都是一致的，因此我们无须进行幂等性判断。删除操作也是一样，删除一次和删除多次都是把相关的数据进行删除（这里的删除指的是条件删除而不是删除所有数据），因此也无须进行幂等性判断。 幂等性的关键步骤实现幂等性的关键步骤分为以下三个： 每个请求操作必须有唯一的 ID，而这个 ID 就是用来表示此业务是否被执行过的关键凭证，例如，订单支付业务的请求，就要使用订单的 ID 作为幂等性验证的 Key； 每次执行业务之前必须要先判断此业务是否已经被处理过； 第一次业务处理完成之后，要把此业务处理的状态进行保存，比如存储到 Redis 中或者是数据库中，这样才能防止业务被重复处理。 知道了幂等性的概念，那如何保证幂等性呢？ 如何保证接口的幂等性？幂等性的实现方案通常分为以下几类： 前端拦截 使用数据库实现幂等性 使用 JVM 锁实现幂等性 使用分布式锁实现幂等性 下面我们分别来看它们的具体实现过程。 前端拦截前端拦截是指通过 Web 站点的页面进行请求拦截，比如在用户点击完“提交”按钮后，我们可以把按钮设置为不可用或者隐藏状态，避免用户重复点击。 但前端拦截有一个致命的问题，如果是懂行的程序员或者黑客可以直接绕过页面的 JS 执行，直接模拟请求后端的接口，这样的话，我们前端的这些拦截就不能生效了。因此除了前端拦截一部分正常的误操作之外，后端的验证必不可少。 数据库实现数据库实现幂等性的方案有三个： 通过悲观锁来实现幂等性 通过唯一索引来实现幂等性 通过乐观锁来实现幂等性 悲观锁 使用悲观锁实现幂等性，一般是配合事务一起来实现，在没有使用悲观锁时，我们通常的执行过程是这样的，首先来判断数据的状态，执行 SQL 如下： 1select status from table_name where id=&#x27;xxx&#x27;; 然后再进行添加操作： 1insert into table_name (id) values (&#x27;xxx&#x27;); 最后再进行状态的修改： 1update table_name set status=&#x27;xxx&#x27;; 但这种情况因为是非原子操作，所以在高并发环境下可能会造成一个业务被执行两次的问题，当一个程序在执行中时，而另一个程序也开始状态判断的操作。因为第一个程序还未来得及更改状态，所以第二个程序也能执行成功，这就导致一个业务被执行了两次。 在这种情况下我们就可以使用悲观锁来避免问题的产生，实现 SQL 如下所示： 12345begin; # 1.开始事务select * from table_name where id=&#x27;xxx&#x27; for update; # 2.查询状态insert into table_name (id) values (&#x27;xxx&#x27;); # 3.添加操作update table_name set status=&#x27;xxx&#x27;; # 4.更改操作commit; # 5.提交事务 在实现的过程中需要注意以下两个问题： 如果使用的是 MySQL 数据库，必须选用 innodb 存储引擎，因为 innodb 支持事务； id 字段一定要是主键或者是唯一索引，不然会锁表，影响其他业务执行。 唯一索引 我们可以创建一个唯一索引的表来实现幂等性，在每次执行业务之前，先执行插入操作，因为唯一字段就是业务的 ID，因此如果重复插入的话会触发唯一约束而导致插入失败。在这种情况下（插入失败）我们就可以判定它为重复提交的请求。 唯一索引表的创建示例如下： 123456CREATE TABLE `table_name` ( `id` int NOT NULL AUTO_INCREMENT, `orderid` varchar(32) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;唯一id&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uq_orderid` (`orderid`) COMMENT &#x27;唯一约束&#x27;) ENGINE=InnoDB; 乐观锁 乐观锁是指在执行数据操作时（更改或添加）进行加锁操作，其他时间不加锁，因此相比于整个执行过程都加锁的悲观锁来说，它的执行效率要高很多。 乐观锁可以通过版本号来实现，例如以下 SQL： 1update table_name set version=version+1 where version=0; JVM 锁实现JVM 锁实现是指通过 JVM 提供的内置锁如 Lock 或者是 synchronized 来实现幂等性。使用 JVM 锁来实现幂等性的一般流程为：首先通过 Lock 对代码段进行加锁操作，然后再判断此订单是否已经被处理过，如果未处理则开启事务执行订单处理，处理完成之后提交事务并释放锁，执行流程如下图所示： JVM 锁存在的最大问题在于，它只能应用于单机环境，因为 Lock 本身为单机锁，所以它就不适应于分布式多机环境。 分布式锁实现分布式锁实现幂等性的逻辑是，在每次执行方法之前先判断是否可以获取到分布式锁，如果可以，则表示为第一次执行方法，否则直接舍弃请求即可，执行流程如下图所示： 需要注意的是分布式锁的 key 必须为业务的唯一标识，我们通常使用 Redis 或者 ZooKeeper 来实现分布式锁；如果使用 Redis 的话，则用 set 命令来创建和获取分布式锁，执行示例如下： 12127.0.0.1:6379&gt; set lock true ex 30 nxOK # 创建锁成功 其中，ex 是用来设置超时时间的；而 nx 是 not exists 的意思，用来判断键是否存在。如果返回的结果为“OK”，则表示创建锁成功，否则表示重复请求，应该舍弃。","tags":["幂等性","系统设计"],"categories":["系统设计"]},{"title":"RPC vs REST","path":"//blog/framework/rpc-rest/","content":"RPCRPC 是指远程服务调用（Remote Procedure Call） 也就是说两台服务器 A 和B。一个应用部署在 A 服务器上，想要调用 B 服务器上应用提供的函数 &#x2F; 方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。 最终解决的问题：让分布式或者微服务系统中不同服务之间的调用像本地调用一样简单。 RPC 要解决的三个基本问题： 如何表示数据：这里数据包括了传递给方法的参数，以及方法执行后的返回值。 进程内的方法调用，使用自定义的数据类型，就很容易解决数据表示问题，远程方法调用则完全可能面临交互双方各自使用不同程序语言的情况； 即使只支持一种程序语言的 RPC 协议，在不同硬件指令集、不同操作系统下，同样的数据类型也完全可能有不一样表现细节，譬如数据宽度、字节序的差异等等。 有效的做法是交互双方约定一种序列化和反序列化协议，将所涉及的数据转换为某种事先约定好的中立数据流格式来进行传输。 每种 RPC 协议都应该要有对应的序列化协议 如何传输数据：准确地说，是指如何通过网络，在两个服务的 Endpoint 之间相互操作、交换数据。 这里“交换数据”通常指的是应用层协议，实际传输一般是基于标准的 TCP、UDP 等标准的传输层协议来完成的。 两个服务交互不是只扔个序列化数据流来表示参数和结果就行的，许多在此之外信息，譬如异常、超时、安全、认证、授权、事务，等等，都可能产生双方需要交换信息的需求。 如果要求足够简单，双方都是 HTTP Endpoint，直接使用 HTTP 协议也是可以的 如何确定方法：这在本地方法调用中并不是太大的问题，编译器或者解释器会根据语言规范，将调用的方法签名转换为进程空间中子过程入口位置的指针。 不过一旦要考虑不同语言，事情又立刻麻烦起来，每门语言的方法签名都可能有所差别，所以“如何表示同一个方法”，“如何找到对应的方法”还是得弄个跨语言的统一的标准才行。 为什么用 RPC，不用 HTTPRPC 是一种设计，就是为了解决不同服务之间的调用问题，完整的 RPC 实现一般会包含有 传输协议 和 序列化协议 这两个。 而 HTTP 是一种传输协议，RPC 框架完全可以使用 HTTP 作为传输协议，也可以直接使用 TCP，使用不同的协议一般也是为了适应不同的场景。 使用 TCP 和使用 HTTP 各有优势： 传输效率： TCP，通常自定义上层协议，可以让请求报文体积更小 HTTP：如果是基于HTTP 1.1 的协议，请求中会包含很多无用的内容 性能消耗，主要在于序列化和反序列化的耗时 TCP，可以基于各种序列化框架进行，效率比较高 HTTP，大部分是通过 json 来实现的，字节大小和序列化耗时都要更消耗性能 跨平台： TCP：通常要求客户端和服务器为统一平台 HTTP：可以在各种异构系统上运行 RESTREST 无论是在思想上、概念上，还是使用范围上，与 RPC 都不尽相同，充其量只能算是有一些相似，应用会有一部分重合之处，但本质上并不是同一类型的东西。 REST 与 RPC 在思想上差异的核心是抽象的目标不一样，即面向资源的编程思想与面向过程的编程思想两者之间的区别。 概念上的不同是指 REST 并不是一种远程服务调用协议(它不是一种协议, 更像是一种设计风格)。协议都带有一定的规范性和强制性，最起码也该有个规约文档，譬如 JSON-RPC，它哪怕再简单，也要有个《JSON-RPC Specification》来规定协议的格式细节、异常、响应码等信息，但是 REST 并没有定义这些内容，尽管有一些指导原则，但实际上并不受任何强制的约束。 REST，即“表征状态转移”的缩写。 下面通过一个具体事例来理解什么是“表征”以及 REST 中其他关键概念： 资源（Resource）：譬如你现在正在阅读一篇名为《REST 设计风格》的文章，这篇文章的内容本身称之为“资源”。无论你是购买的书籍、是在浏览器看的网页、是打印出来看的文稿、是在电脑屏幕上阅读抑或是手机上浏览，尽管呈现的样子各不相同，但其中的信息是不变的，你所阅读的仍是同一份“资源”。 表征（Representation）：当你通过电脑浏览器阅读此文章时，浏览器向服务端发出请求“我需要这个资源的 HTML 格式”，服务端向浏览器返回的这个 HTML 就被称之为“表征”，你可能通过其他方式拿到本文的 PDF、Markdown、RSS 等其他形式的版本，它们也同样是一个资源的多种表征。 状态（State）：当你读完了这篇文章，想看后面是什么内容时，你向服务器发出请求“给我下一篇文章”。但是“下一篇”是个相对概念，必须依赖“当前你正在阅读的文章是哪一篇”才能正确回应，这类在特定语境中才能产生的上下文信息即被称为“状态”。我们所说的有状态（Stateful）抑或是无状态（Stateless），都是只相对于服务端来说的，服务器要完成“取下一篇”的请求，要么自己记住用户的状态：这个用户现在阅读的是哪一篇文章，这称为有状态；要么客户端来记住状态，在请求的时候明确告诉服务器：我正在阅读某某文章，现在要读它的下一篇，这称为无状态。 转移（Transfer）：无论状态是由服务端还是客户端来提供的，“取下一篇文章”这个行为逻辑必然只能由服务端来提供，因为只有服务端拥有该资源及其表征形式。服务器通过某种方式，把“用户当前阅读的文章”转变成“下一篇文章”，这就被称为“表征状态转移”。 RESTful 的系统一套理想的、完全满足 REST 风格的系统应该满足以下六大原则。 服务端与客户端分离（Client-Server） 无状态（Stateless） 无状态是 REST 的一条核心原则。 REST 希望服务器不要去负责维护状态，每一次从客户端发送的请求中，应包括所有的必要的上下文信息，会话信息也由客户端负责保存维护，服务端依据客户端传递的状态来执行业务处理逻辑，驱动整个应用的状态变迁。 客户端承担状态维护职责以后，会产生一些新的问题，譬如身份认证、授权等可信问题。 但必须承认的现状是，目前大多数的系统都达不到这个要求，往往越复杂、越大型的系统越是如此。服务端无状态可以在分布式计算中获得非常高价值的好处，但大型系统的上下文状态数量完全可能膨胀到让客户端在每次请求时提供变得不切实际的程度，在服务端的内存、会话、数据库或者缓存等地方持有一定的状态成为一种是事实上存在，并将长期存在、被广泛使用的主流的方案。 可缓存（Cacheability） 无状态服务虽然提升了系统的可见性、可靠性和可伸缩性，但降低了系统的网络性。 “降低网络性”的通俗解释是某个功能如果使用有状态的设计只需要一次（或少量）请求就能完成，使用无状态的设计则可能会需要多次请求，或者在请求中带有额外冗余的信息。 为了缓解这个矛盾，REST 希望软件系统能够如同万维网一样，允许客户端和中间的通讯传递者（譬如代理）将部分服务端的应答缓存起来。 当然，为了缓存能够正确地运作，服务端的应答中必须明确地或者间接地表明本身是否可以进行缓存、可以缓存多长时间，以避免客户端在将来进行请求的时候得到过时的数据。 运作良好的缓存机制可以减少客户端、服务器之间的交互，甚至有些场景中可以完全避免交互，这就进一步提了高性能。 分层系统（Layered System） 这里所指的并不是表示层、服务层、持久层这种意义上的分层。而是指客户端一般不需要知道是否直接连接到了最终的服务器，抑或连接到路径上的中间服务器。 中间服务器可以通过负载均衡和共享缓存的机制提高系统的可扩展性，这样也便于缓存、伸缩和安全策略的部署。 该原则的典型的应用是内容分发网络（Content Distribution Network，CDN）。如果你是通过网站浏览到这篇文章的话，你所发出的请求一般并不是直接访问位于 GitHub Pages 的源服务器，而是访问了位于国内的 CDN 服务器，但作为用户，你完全不需要感知到这一点。 统一接口（Uniform Interface） 这是 REST 的另一条核心原则。REST 希望开发者面向资源编程，希望软件系统设计的重点放在抽象系统该有哪些资源上，而不是抽象系统该有哪些行为（服务）上。 这条原则你可以类比计算机中对文件管理的操作来理解，管理文件可能会进行创建、修改、删除、移动等操作，这些操作数量是可数的，而且对所有文件都是固定的、统一的。如果面向资源来设计系统，同样会具有类似的操作特征，由于 REST 并没有设计新的协议，所以这些操作都借用了 HTTP 协议中固有的操作命令来完成。 按需代码 REST 的优势REST 的基本思想是面向资源来抽象问题，它与此前流行的编程思想——面向过程的编程在抽象主体上有本质的差别。 在 REST 提出以前，人们设计分布式系统服务的唯一方案就只有 RPC，RPC 是将本地的方法调用思路迁移到远程方法调用上，开发者是围绕着“远程方法”去设计两个系统间交互的。 这样做的坏处不仅是“如何在异构系统间表示一个方法”、“如何获得接口能够提供的方法清单”都成了需要专门协议去解决的问题（RPC 的三大基本问题之一），更在于服务的每个方法都是完全独立的，服务使用者必须逐个学习才能正确地使用它们。 REST 提出以资源为主体进行服务设计的风格，能为它带来不少好处，譬如： 降低的服务接口的学习成本。统一接口（Uniform Interface）是 REST 的重要标志，将对资源的标准操作都映射到了标准的 HTTP 方法上去，这些方法对于每个资源的用法都是一致的，语义都是类似的，不需要刻意去学习，更不需要有什么 Interface Description Language 之类的协议存在。 资源天然具有集合与层次结构。以方法为中心抽象的接口，由于方法是动词，逻辑上决定了每个接口都是互相独立的；但以资源为中心抽象的接口，由于资源是名词，天然就可以产生集合与层次结构。 REST 绑定于 HTTP 协议。面向资源编程不是必须构筑在 HTTP 之上，但 REST 是，这是缺点，也是优点。 因为 HTTP 本来就是面向资源而设计的网络协议，纯粹只用 HTTP（而不是 SOAP over HTTP 那样在再构筑协议）带来的好处是 RPC 中的 Wire Protocol 问题就无需再多考虑了，REST 将复用 HTTP 协议中已经定义的概念和相关基础支持来解决问题。HTTP 协议已经有效运作了三十年，其相关的技术基础设施已是千锤百炼，无比成熟。而坏处自然是，当你想去考虑那些 HTTP 不提供的特性时，便会彻底地束手无策。 REST 的不足 面向资源的编程思想只适合做 CRUD，面向过程、面向对象编程才能处理真正复杂的业务逻辑 REST 与 HTTP 完全绑定，不适合应用于要求高性能传输的场景中 REST 没有传输可靠性支持 REST 缺乏对资源进行“部分”和“批量”的处理能力 RMM 成熟度模型《RESTful Web APIs》和《RESTful Web Services》的作者 Leonard Richardson 曾提出过一个衡量“服务有多么 REST”的 Richardson 成熟度模型（Richardson Maturity Model），便于那些原本不使用 REST 的系统，能够逐步地导入 REST。Richardson 将服务接口“REST 的程度”从低到高，分为 0 至 3 级： The Swamp of Plain Old XML：完全不 REST。另外，关于 Plain Old XML 这说法，SOAP 表示感觉有被冒犯到。 Resources：开始引入资源的概念。 HTTP Verbs：引入统一接口，映射到 HTTP 协议的方法上。 Hypermedia Controls：超媒体控制在本文里面的说法是“超文本驱动”，在 Fielding 论文里的说法是“Hypertext As The Engine Of Application State，HATEOAS”，其实都是指同一件事情。","tags":["RPC","REST"],"categories":["RPC","REST"]},{"title":"常见限流算法","path":"//blog/design/limiting/","content":"限流工程上的限流是什么呢？ 限制的是 「流」，在不同场景下「流」的定义不同，可以是每秒请求数、每秒事务处理数、网络流量等等。 通常我们说的限流指代的是 限制到达系统的并发请求数，使得系统能够正常的处理 部分 用户的请求，来保证系统的稳定性。 限流不可避免的会造成用户的请求变慢或者被拒的情况，从而会影响用户体验。因此限流是需要在用户体验和系统稳定性之间做平衡的，即我们常说的 trade off。 限流也称流控（流量控制）。 为什么需要限流限流是为了保证系统的稳定性。 限流的本质是因为后端处理能力有限，需要截掉超过处理能力之外的请求，亦或是为了均衡客户端对服务端资源的公平调用，防止一些客户端饿死。 常见的限流算法计数限流、滑动窗口限流、漏桶限流、令牌桶限流。 计数限流计数限流是指限制某一个接口或者某一行为单位时间内的响应次数。设置一个计数器，对某一时间段内的请求进行计数，当请求超过设置的阈值之后则触发饱和策略（可以选择拒绝&#x2F;阻塞请求）。 1234567boolean tryAcquire() &#123; if (counter &lt; threshold) &#123; counter ++; return true; &#125; return false;&#125; 优点：简单粗暴，单机在 Java 中可用 Atomic 等原子类、分布式就 Redis incr。 缺点： 对突增流量处理不优化，有可能被绕过限流策略 假设系统每秒允许 100 个请求，假设第一个时间窗口是 0-1s，在第 0.55s 处一下次涌入 100 个请求，过了 1 秒的时间窗口后计数清零，此时在 1.05 s 的时候又一下次涌入100个请求。 虽然窗口内的计数没超过阈值，但是全局来看在 0.55s-1.05s 这 0.1 秒内涌入了 200 个请求，这其实对于阈值是 100&#x2F;s 的系统来说是无法接受的。 滑动窗口限流滑动窗口限流解决了上面的问题，可以保证在任意时间窗口内都不会超过阈值。 滑动窗口除了需要引入计数器之外还需要记录时间窗口内每个请求到达的时间点，因此对内存的占用会比较多。 规则如下，假设时间窗口为 1 秒 记录每次请求的时间 统计每次请求的时间 至 往前推1秒这个时间窗口内请求数，并且 1 秒前的数据可以删除。 统计的请求数小于阈值就记录这个请求的时间，并允许通过，反之拒绝。 1234567891011boolean tryAcquire() &#123; // 获取当前时间 long now = currentTimeMuillis(); // 根据当前时间获取窗口内的计数 long counter = getCounterInTimeWindow(now); if (counter &lt; threshold) &#123; // 小于阈值 addToTimeWindow(now); return true; &#125; return false;&#125; 滑动窗口虽然解决了计数算法的临界值问题，但是对突增流量问题依旧不友好。 漏桶算法漏桶（Leaky Bucket）算法是限流方面比较经典的算法，该算法最早应用于网络拥塞控制方面。 理解该算法可以联想一个具体的漏桶模型，不管进水量有多大，漏桶始终以恒定的速率往外排水，如果桶被装满则后来涌入的水会漫出去。 对应接口限流来说，用户的请求可以看做是这里的水，不管用户的请求量有多大多不均衡，能够被处理的请求速率是恒定的，而且能够被接受的请求数也是有上限的，超出上限的请求会被拒绝，典型的我们可以采用队列作为这里的漏桶实现。 123456789101112131415boolean tryAcquire() &#123; // 获取当前时间 long now currentTimeMillis(); // （当前时间 - 上次注水时间）*流出速率 = 流出的水量 long consumeWater = (now - lastInjectTime) * rate; // 之前桶内的水量 - 这段时间流出的水量 long leftWater = max(0, leftWater - consumeWater); if (leftWater + 1 &lt;= capacity) &#123; lastInjectTime = now; leftWater ++; return true; &#125; else &#123; return false; &#125;&#125; 由上面的解释我们应该能够感觉到漏桶算法非常适用于秒杀系统的限流，漏桶在这种应用场景下可以起到一定的削峰填谷的作用，并且漏桶的设计从根本上能够应对集中访问的问题，同时具备平滑策略，但是始终恒定的处理速率有时候并不一定是好事情，对于突发的请求洪峰，在保证服务安全的前提下，应该尽最大努力去响应，这个时候漏桶算法显得有些呆滞。 令牌桶算法令牌桶（Token Bucket）算法可以看作是漏桶算法的逆过程。该算法要求系统以一定的速率发放访问令牌，用户的请求必须在持有合法令牌的前提下才能够被响应，我们可以按照权重设置一类请求被响应所需持有的令牌数，只有当桶中的令牌数目满足当前请求所需时才授予令牌，对于其他情况则拒绝该请求。 可以看出令牌桶在应对突发流量的时候，桶内假如有 100 个令牌，那么这 100 个令牌可以马上被取走，而不像漏桶那样匀速的消费。所以在应对突发流量的时候令牌桶表现的更佳。 虽然漏桶和令牌桶对比时间窗口对流量的限流效果更佳，流量更加得平滑，但是也有各自的缺点。 拿令牌桶来说，假设没预热，在刚上线时候桶里没令牌，这是就会存在误杀问题（系统明明没有负载）。 再比如说请求的访问其实是随机的，假设令牌桶每20ms放入一个令牌，桶内初始没令牌，这请求就刚好在第一个20ms内有两个请求，再过20ms里面没请求，其实从40ms来看只有2个请求，应该都放行的，而有一个请求就直接被拒了。这就有可能造成很多请求的误杀，但是如果看监控曲线的话，好像流量很平滑，峰值也控制的很好。 再拿漏桶来说，漏桶中请求是暂时存在桶内的。这其实不符合互联网业务低延迟的要求。 所以漏桶和令牌桶其实比较适合阻塞式限流场景，即没令牌我就等着，这就不会误杀了，而漏桶本就是等着。比较适合后台任务类的限流。而基于时间窗口的限流比较适合对时间敏感的场景，请求过不了您就快点儿告诉我，等的花儿都谢了。 限流的难点__限流的难点在于配置__，如何让限流在不误伤的前提下尽量发挥硬件的最大性能是一个富有经验的问题，而压测是一个基础且行之有效的途径。 限流组件 Google Guava - RateLimiter（基于令牌桶实现，并扩展了算法，支持预热功能） Alibaba - Sentinel （基于漏桶算法，匀速排队限流策略）","tags":["限流算法","系统设计"],"categories":["系统设计"]},{"title":"【转】如何设计一个可扩展的限流算法","path":"//blog/design/how-to-design-extensible-limiting-algorithm/","content":"限流（Rate Limiting，即速率限制）通过限制每个用户调用API的频率来防止API被过度使用，这可以防止他们因疏忽或恶意导致的API滥用。在没有速率限制的情况下，每个用户可以随心所欲地请求，这可能会导致“峰值”请求，从而导致其他用户得不到响应。在启用速率限制之后，它们的请求将被限制为每秒固定的数量。 在示例图表中，你可以看到速率限制如何在一段时间内阻塞请求。API最初每分钟接收4个请求，用绿色表示。当12:02启用速率限制时，以红色显示的其他请求将被拒绝。 速率限制对于公共API是非常重要的，因为你想要为每个消费者（API调用者）维护良好的服务质量，即使有些用户获取了超出其公平配额的服务。计算密集型的端点特别需要速率限制——特别是通过自动伸缩或AWS Lambda和OpenWhisk等按计算付费服务来提供服务时。你还可能希望对提供敏感数据的API进行评级，因为如果攻击者在某些不可预见的事件中获得访问权限，这可能会限制暴露的数据。 实际上有许多不同的方法来实现速率限制，我们将探讨不同速率限制算法的优缺点。我们还将探讨跨集群扩展时出现的问题。最后，我们将向你展示一个如何使用Kong快速设置速率限制的示例，Kong是最流行的开源API网关。 速度限制算法有各种各样的速率限制算法，每一种都有自己的优点和缺点。让我们回顾一下，这样你就可以根据自己的需要选择最好的限流算法。 漏桶算法漏桶算法（Leaky Bucket，与令牌桶密切相关）是这样一种算法，它提供了一种简单、直观的方法来通过队列限制速率，你可以将队列看作一个存储请求的桶。当一个请求被注册时，它被附加到队列的末尾。每隔一段时间处理队列上的第一项。这也称为先进先出（FIFO）队列。如果队列已满，则丢弃（或泄漏）其他请求。 这种算法的优点是它可以平滑请求的爆发，并以近似平均的速度处理它们。它也很容易在单个服务器或负载均衡器上实现，并且在有限的队列大小下对于每个用户都是内存有效的。 然而，突发的访问量会用旧的请求填满队列，并使最近的请求无法被处理。它也不能保证在固定的时间内处理请求。此外，如果为了容错或增加吞吐量而负载平衡服务器，则必须使用策略来协调和强制它们之间的限制。稍后我们将讨论分布式环境的挑战。 固定窗口算法在固定窗口（Fixed Window）算法中，使用n秒的窗口大小（通常使用对人类友好的值，如60秒或3600秒）来跟踪速率。每个传入的请求都会增加窗口的计数器。如果计数器超过阈值，则丢弃请求。窗口通常由当前时间戳的层定义，因此12:00:03的窗口长度为60秒，应该在12:00:00的窗口中。 这种算法的优点是，它可以确保处理更多最近的请求，而不会被旧的请求饿死。然而，发生在窗口边界附近的单个流量突发会导致处理请求的速度增加一倍，因为它允许在短时间内同时处理当前窗口和下一个窗口的请求。另外，如果许多消费者等待一个重置窗口，例如在一个小时的顶部，那么他们可能同时扰乱你的API。 滑动日志算法滑动日志（Sliding Log）速率限制涉及到跟踪每个使用者请求的时间戳日志。这些日志通常存储在按时间排序的散列集或表中。时间戳超过阈值的日志将被丢弃。当新请求出现时，我们计算日志的总和来确定请求率。如果请求将超过阈值速率，则保留该请求。 该算法的优点是不受固定窗口边界条件的限制，速率限制将严格执行。此外，因为滑动日志是针对每个消费者进行跟踪的，所以不会出现对固定窗口造成挑战的踩踏效应。但是，为每个请求存储无限数量的日志可能非常昂贵。它的计算成本也很高，因为每个请求都需要计算使用者先前请求的总和，这些请求可能跨越一个服务器集群。因此，它不能很好地处理大规模的流量突发或拒绝服务攻击。 滑动窗口这是一种将固定窗口算法的低处理成本与改进的滑动日志边界条件相结合的混合方法。与固定窗口算法一样，我们跟踪每个固定窗口的计数器。接下来，我们根据当前的时间戳计算前一个窗口请求率的加权值，以平滑突发的流量。例如，如果当前窗口通过了25%，那么我们将前一个窗口的计数加权为75%。跟踪每个键所需的相对较少的数据点允许我们在大型集群中扩展和分布。 我们推荐使用滑动窗口方法，因为它可以灵活地调整速率限制，并且具有良好的性能。速率窗口是它向API消费者提供速率限制数据的一种直观方式。它还避免了漏桶的饥饿问题和固定窗口实现的突发问题。 分布式系统中的速率限制同步策略如果希望在使用多个节点的集群时实施全局速率限制，则必须设置策略来实施该限制。如果每个节点都要跟踪自己的速率限制，那么当请求被发送到不同节点时，使用者可能会超过全局速率限制。实际上，节点的数量越大，用户越有可能超过全局限制。 执行此限制的最简单方法是在负载均衡器中设置粘性会话，以便每个使用者都被精确地发送到一个节点。缺点包括缺乏容错性和节点过载时的缩放问题。 允许更灵活的负载平衡规则的更好解决方案是使用集中的数据存储，如Redis或Cassandra。这将存储每个窗口和消费者的计数。这种方法的两个主要问题是增加了向数据存储发出请求的延迟，以及竞争条件（我们将在下面讨论）。 竞态条件集中式数据存储的最大问题之一是高并发请求模式中的竞争条件。当你使用一种简单的“get-then-set”方法时，就会发生这种情况，在这种方法中，你检索当前速率限制计数器，增加它的值，然后将其推回到数据存储中。这个模型的问题是，在执行一个完整的读递增存储周期时，可能会出现额外的请求，每个请求都试图用无效的（较低的）计数器值存储递增计数器。这允许使用者发送非常高的请求率来绕过速率限制控制。 避免这个问题的一种方法是在有问题的密钥周围放置一个“锁”，防止任何其他进程访问或写入计数器。这将很快成为一个主要的性能瓶颈，而且伸缩性不好，特别是在使用诸如Redis之类的远程服务器作为备份数据存储时。 更好的方法是使用“先设置后获取”的心态，依赖于原子操作符，它们以一种非常高性能的方式实现锁，允许你快速增加和检查计数器值，而不让原子操作成为障碍。 性能优化使用集中式数据存储的另一个缺点是，在检查速率限制计数器时增加了延迟。不幸的是，即使是检查像Redis这样的快速数据存储，也会导致每个请求增加毫秒的延迟。 为了以最小的延迟确定这些速率限制，有必要在内存中进行本地检查。这可以通过放松速率检查条件和使用最终一致的模型来实现。例如，每个节点可以创建一个数据同步周期，该周期将与中央数据存储同步。每个节点定期将每个使用者的计数器增量和它看到的窗口推送到数据存储，数据存储将自动更新这些值。然后，节点可以检索更新后的值来更新其内存版本。集群内节点之间的这种收敛→发散→再收敛的循环最终是一致的。 节点聚合的周期速率应该是可配置的。当流量分布在群集中的多个节点上时（例如，当坐在一个轮询调度平衡器后面时），较短的同步间隔将导致较少的数据点分散，而较长的同步间隔将对数据存储施加较小的读&#x2F;写压力，更少的开销在每个节点上获取新的同步值。 使用Kong快速设置速率限制Kong是一个开源的API网关，它使构建具有速率限制的可伸缩服务变得非常容易。它被全球超过300,000个活动实例使用。它可以完美地从单个的Kong节点扩展到大规模的、跨越全球的Kong集群。 Kong位于API前面，是上游API的主要入口。在处理请求和响应时，Kong将执行你决定添加到API中的任何插件。 Kong的速率限制插件是高度可配置的。它提供了为每个API和消费者定义多个速率限制窗口和速率的灵活性。它支持本地内存、Redis、Postgres和Cassandra备份数据存储。它还提供了各种数据同步选项，包括同步和最终一致的模型。 你可以在其中一台开发机器上快速安装Kong来测试它。我最喜欢的入门方式是使用AWS云形成模板，因为只需几次单击就可以获得预先配置好的开发机器。只需选择一个HVM选项，并将实例大小设置为使用t2.micro，这些对于测试都是负担得起的。然后ssh到新实例上的命令行进行下一步。 在Kong上添加API下一步是使用Kong的admin API在Kong上添加一个API。我们将使用httpbin作为示例，它是一个针对API的免费测试服务。get URL将把我的请求数据镜像成JSON。我们还假设Kong在本地系统上的默认端口上运行。 12345curl -i -X POST \\--url http://localhost:8001/apis/ \\--data &#x27;name=test&#x27; \\--data &#x27;uris=/test&#x27; \\--data &#x27;upstream_url=http://httpbin.org/get&#x27; 现在 Kong 意识到每个发送到 “&#x2F;test” 的请求都应该代理到 httpbin。我们可以向它的代理端口上的 Kong 发出以下请求来测试它： 12345678910111213curl http://localhost:8000/test&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;curl/7.51.0&quot;, &quot;X-Forwarded-Host&quot;: &quot;localhost&quot; &#125;, &quot;origin&quot;: &quot;localhost, 52.89.171.202&quot;, &quot;url&quot;: &quot;http://localhost/get&quot;&#125; 它还是好的！Kong 已经接收了请求并将其代理到 httpbin，httpbin 已将我的请求头和我的原始 IP 地址进行了镜像。 添加基本的速率限制让我们继续，通过使用社区版的限速插件 [1] 添加限速功能来保护它不受过多请求的影响，每个消费者每分钟只能发出 5 个请求： 123curl -i -X POST http://localhost:8001/apis/test/plugins/ \\-d &quot;name=rate-limiting&quot; \\-d &quot;config.minute=5&quot; 如果我们现在发出超过 5 个请求，Kong 会回复以下错误信息： 1234curl http://localhost:8000/test&#123; &quot;message&quot;:&quot;API rate limit exceeded&quot;&#125; 看上去不错！我们在 Kong 上添加了一个 API，并且仅在两个 HTTP 请求中向 Kong 的 admin API 添加了速率限制。 它默认使用固定的窗口来限制 IP 地址的速率，并使用默认的数据存储在集群中的所有节点之间进行同步。有关其他选项，包括每个用户的速率限制或使用其他数据存储（如 Redis），请参阅文档 [1]。 企业版 Kong，更好的性能企业版 [2] 的速率限制增加了对滑动窗口算法的支持，以更好地控制和性能。滑动窗口可以防止你的 API 在窗口边界附近重载，如上面的部分所述。对于低延迟，它使用计数器的内存表，可以使用异步或同步更新跨集群进行同步。这提供了本地阈值的延迟，并且可以跨整个集群扩展。 第一步是安装企业版的 Kong。然后，可以配置速率限制、以秒为单位的窗口大小和同步计数器值的频率。它真的很容易使用，你可以得到这个强大的控制与一个简单的 API 调用： 12345curl -i -X POST http://localhost:8001/apis/test/plugins \\-d &quot;name=rate-limiting&quot; \\-d &quot;config.limit=5&quot; \\-d &quot;config.window_size=60&quot; \\-d &quot;config.sync_rate=10&quot; 企业还增加了对 Redis Sentinel 的支持，这使得 Redis 高可用性和更强的容错能力。你可以阅读更多的企业速率限制插件文档。","tags":["限流算法"],"categories":["系统设计"]},{"title":"线程池详解","path":"//blog/concurrent/thread_pool/","content":"什么是线程池线程池（Thread Pool）是一种基于池化思想管理线程的工具，经常出现在多线程服务器中，如MySQL。 线程过多会带来额外的开销，其中包括创建销毁线程的开销、调度线程的开销等等，同时也降低了计算机的整体性能。线程池维护多个线程，等待监督管理者分配可并发执行的任务。 这种做法，一方面避免了处理任务时创建销毁线程开销的代价，另一方面避免了线程数量膨胀导致的过分调度问题，保证了对内核的充分利用。 使用线程池可以带来一系列好处： 降低资源消耗：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。 提高响应速度：任务到达时，无需等待线程创建即可立即执行。 提高线程的可管理性：线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控。 提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能。比如延时定时线程池ScheduledThreadPoolExecutor，就允许任务延期执行或定期执行。 线程池解决了什么问题线程池解决的核心是：资源管理问题 在并发环境下，系统无法确定在任意时刻，有多少任务需要执行，有多少资源需要投入。这种不确定性将带来以下若干问题： 频繁申请&#x2F;销毁资源和调度资源，将带来额外的消耗，可能会非常巨大。 对资源无限申请缺少抑制手段，易引发系统资源耗尽的风险。 系统无法合理管理内部的资源分布，会降低系统的稳定性。 为解决资源分配这个问题，线程池采用了“池化”（Pooling）思想。池化，顾名思义，是为了最大化收益并最小化风险，而将资源统一在一起管理的一种思想。“池化”思想不仅仅能应用在计算机领域，在金融、设备、人员管理、工作管理等领域也有相关的应用。 在计算机领域中的表现为：统一管理IT资源，包括服务器、存储、和网络资源等等。通过共享资源，使用户在低投入中获益。除去线程池，还有其他比较典型的几种使用策略包括： 1. 内存池(Memory Pooling)：预先申请内存，提升申请内存速度，减少内存碎片。 2. 连接池(Connection Pooling)：预先申请数据库连接，提升申请连接的速度，降低系统的开销。 3. 实例池(Object Pooling)：循环使用对象，减少资源在初始化和释放时的昂贵损耗。 使用线程池的风险虽然线程池是构建多线程应用程序的强大机制，但使用它并不是没有风险的。用线程池构建的应用程序容易遭受任何其它多线程应用程序容易遭受的所有并发风险，诸如同步错误和死锁，它还容易遭受特定于线程池的少数其它风险，诸如与池有关的死锁、资源不足和线程泄漏。 死锁任何多线程应用程序都有死锁风险。当一组进程或线程中的每一个都在等待一个只有该组中另一个进程才能引起的事件时，我们就说这组进程或线程 死锁 了。 死锁的最简单情形是：线程 A 持有对象 X 的独占锁，并且在等待对象 Y 的锁，而线程 B 持有对象 Y 的独占锁，却在等待对象 X 的锁。除非有某种方法来打破对锁的等待（Java 锁定不支持这种方法），否则死锁的线程将永远等下去。 虽然任何多线程程序中都有死锁的风险，但线程池却引入了另一种死锁可能，在那种情况下，所有池线程都在执行已阻塞的等待队列中另一任务的执行结果的任务，但这一任务却因为没有未被占用的线程而不能运行。当线程池被用来实现涉及许多交互对象的模拟，被模拟的对象可以相互发送查询，这些查询接下来作为排队的任务执行，查询对象又同步等待着响应时，会发生这种情况。 资源不足线程池的一个优点在于：相对于其它替代调度机制而言，它们通常执行得很好。但只有恰当地调整了线程池大小时才是这样的。线程消耗包括内存和其它系统资源在内的大量资源。除了 Thread 对象所需的内存之外，每个线程都需要两个可能很大的执行调用堆栈。除此以外，JVM 可能会为每个 Java 线程创建一个本机线程，这些本机线程将消耗额外的系统资源。最后，虽然线程之间切换的调度开销很小，但如果有很多线程，环境切换也可能严重地影响程序的性能。 如果线程池太大，那么被那些线程消耗的资源可能严重地影响系统性能。在线程之间进行切换将会浪费时间，而且使用超出比您实际需要的线程可能会引起资源匮乏问题，因为池线程正在消耗一些资源，而这些资源可能会被其它任务更有效地利用。除了线程自身所使用的资源以外，服务请求时所做的工作可能需要其它资源，例如 JDBC 连接、套接字或文件。这些也都是有限资源，有太多的并发请求也可能引起失效，例如不能分配 JDBC 连接。 并发错误线程池和其它排队机制依靠使用 wait() 和 notify() 方法，这两个方法都难于使用。如果编码不正确，那么可能丢失通知，导致线程保持空闲状态，尽管队列中有工作要处理。使用这些方法时，必须格外小心；即便是专家也可能在它们上面出错。而最好使用现有的、已经知道能工作的实现，例如在下面的 无须编写您自己的池中讨论的 util.concurrent 包。 线程泄漏各种类型的线程池中一个严重的风险是线程泄漏，当从池中除去一个线程以执行一项任务，而在任务完成后该线程却没有返回池时，会发生这种情况。发生线程泄漏的一种情形出现在任务抛出一个 RuntimeException 或一个 Error 时。如果池类没有捕捉到它们，那么线程只会退出而线程池的大小将会永久减少一个。当这种情况发生的次数足够多时，线程池最终就为空，而且系统将停止，因为没有可用的线程来处理任务。 有些任务可能会永远等待某些资源或来自用户的输入，而这些资源又不能保证变得可用，用户可能也已经回家了，诸如此类的任务会永久停止，而这些停止的任务也会引起和线程泄漏同样的问题。如果某个线程被这样一个任务永久地消耗着，那么它实际上就被从池除去了。对于这样的任务，应该要么只给予它们自己的线程，要么只让它们等待有限的时间。 请求过载仅仅是请求就压垮了服务器，这种情况是可能的。在这种情形下，我们可能不想将每个到来的请求都排队到我们的工作队列，因为排在队列中等待执行的任务可能会消耗太多的系统资源并引起资源缺乏。在这种情形下决定如何做取决于您自己；在某些情况下，您可以简单地抛弃请求，依靠更高级别的协议稍后重试请求，您也可以用一个指出服务器暂时很忙的响应来拒绝请求。 如何创建线程池构造方法1234567891011121314151617181920212223242526272829303132333435363738public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler)&#123;&#125; ExecutorsExecutor 框架的工具类 Executors 提供了几种默认的线程池。（方法内部实际上都是调用了ThreadPoolExecutor 的构造方法） FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 除了避免 OOM 的原因之外，不推荐使用 Executors 提供的两种快捷的线程池的原因还有： 实际使用中需要根据自己机器的性能、业务场景来手动配置线程池的参数比如核心线程数、使用的任务队列、饱和策略等等。 我们应该显示地给我们的线程池命名，这样有助于我们定位问题。 ThreadPoolExecutor 参数解析123456789101112131415161718192021222324 /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 参数解析 corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。 饱和策略如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 举个例子： Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。 核心设计 &amp; 实现以下分析基于 JDK 1.8 Java中的线程池核心实现类是ThreadPoolExecutor，首先来看一下ThreadPoolExecutor的UML类图，了解下ThreadPoolExecutor的继承关系。 ThreadPoolExecutor实现的顶层接口是Executor，顶层接口Executor提供了一种思想：将任务提交和任务执行进行解耦。用户无需关注如何创建线程，如何调度线程来执行任务，用户只需提供Runnable对象，将任务的运行逻辑提交到执行器(Executor)中，由Executor框架完成线程的调配和任务的执行部分。 ExecutorService接口增加了一些能力： 扩充执行任务的能力，补充可以为一个或一批异步任务生成Future的方法； 提供了管控线程池的方法，比如停止线程池的运行。 AbstractExecutorService则是上层的抽象类，将执行任务的流程串联了起来，保证下层的实现只需关注一个执行任务的方法即可。最下层的实现类ThreadPoolExecutor实现最复杂的运行部分，ThreadPoolExecutor将会一方面维护自身的生命周期，另一方面同时管理线程和任务，使两者良好的结合从而执行并行任务。 ThreadPoolExecutor是如何运行? 如何同时维护线程和执行任务的呢？其运行机制如下图所示： 线程池在内部实际上构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。线程池的运行主要分成两部分：任务管理、线程管理。任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转： 直接申请线程执行该任务； 缓冲到队列中等待线程执行； 拒绝该任务。 线程管理部分是消费者，它们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后则会继续获取新的任务去执行，最终当线程获取不到任务的时候，线程就会被回收。 接下来，我们会按照以下三个部分去详细讲解线程池运行机制： 线程池如何维护自身状态。 线程池如何管理任务。 线程池如何管理线程。 生命周期管理线程池运行的状态，并不是用户显式设置的，而是伴随着线程池的运行，由内部来维护。线程池内部使用一个变量维护两个值：运行状态(runState)和线程数量 (workerCount)。在具体实现中，线程池将运行状态(runState)、线程数量 (workerCount)两个关键参数的维护放在了一起，如下代码所示： 1private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); ctl这个AtomicInteger类型，是对线程池的运行状态和线程池中有效线程的数量进行控制的一个字段， 它同时包含两部分的信息：线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，高3位保存runState，低29位保存workerCount，两个变量之间互不干扰。 用一个变量去存储两个值，可避免在做相关决策时，出现不一致的情况，不必为了维护两者的一致，而占用锁资源。 通过阅读线程池源代码也可以发现，经常出现要同时判断线程池运行状态和线程数量的情况。线程池也提供了若干方法去供用户获得线程池当前的运行状态、线程个数。这里都使用的是位运算的方式，相比于基本运算，速度也会快很多。 关于内部封装的获取生命周期状态、获取线程池线程数量的计算方法如以下代码所示： 123456//计算当前运行状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125; //计算当前线程数量private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;//通过状态和线程数生成ctl private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; ThreadPoolExecutor的运行状态有5种，分别为： 其生命周期转换如下入所示： 任务管理任务调度 任务调度是线程池的主要入口，当用户提交了一个任务，接下来这个任务将如何执行都是由这个阶段决定的。了解这部分就相当于了解了线程池的核心运行机制。 首先，所有任务的调度都是由execute方法完成的，这部分完成的工作是：检查现在线程池的运行状态、运行线程数、运行策略，决定接下来执行的流程，是直接申请线程执行，或是缓冲到队列中执行，亦或是直接拒绝该任务。其执行过程如下： 首先检测线程池运行状态，如果不是RUNNING，则直接拒绝，线程池要保证在RUNNING的状态下执行任务。 如果workerCount &lt; corePoolSize，则创建并启动一个线程来执行新提交的任务。 如果workerCount &gt;&#x3D; corePoolSize，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中。 如果workerCount &gt;&#x3D; corePoolSize &amp;&amp; workerCount &lt; maximumPoolSize，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务。 如果workerCount &gt;&#x3D; maximumPoolSize，并且线程池内的阻塞队列已满, 则根据拒绝策略来处理该任务, 默认的处理方式是直接抛异常。 其执行流程如下图所示： 任务缓冲 任务缓冲模块是线程池能够管理任务的核心部分。线程池的本质是对任务和线程的管理，而做到这一点最关键的思想就是将任务和线程两者解耦，不让两者直接关联，才可以做后续的分配工作。线程池中是以生产者消费者模式，通过一个阻塞队列来实现的。阻塞队列缓存任务，工作线程从阻塞队列中获取任务。 阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 下图中展示了线程1往阻塞队列中添加元素，而线程2从阻塞队列中移除元素： 使用不同的队列可以实现不一样的任务存取策略。在这里，我们可以再介绍下阻塞队列的成员： 任务申请 由上文的任务分配部分可知，任务的执行有两种可能：一种是任务直接由新创建的线程执行。另一种是线程从任务队列中获取任务然后执行，执行完任务的空闲线程会再次去从队列中申请任务再去执行。第一种情况仅出现在线程初始创建的时候，第二种是线程获取任务绝大多数的情况。 线程需要从任务缓存模块中不断地取任务执行，帮助线程从阻塞队列中获取任务，实现线程管理模块和任务管理模块之间的通信。这部分策略由getTask方法实现，其执行流程如下图所示： getTask这部分进行了多次判断，为的是控制线程的数量，使其符合线程池的状态。如果线程池现在不应该持有那么多线程，则会返回null值。工作线程Worker会不断接收新任务去执行，而当工作线程Worker接收不到任务的时候，就会开始被回收。 任务拒绝 任务拒绝模块是线程池的保护部分，线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到maximumPoolSize时，就需要拒绝掉该任务，采取任务拒绝策略，保护线程池。 拒绝策略是一个接口，其设计如下： 123public interface RejectedExecutionHandler &#123; void rejectedExecution(Runnable r, ThreadPoolExecutor executor);&#125; 用户可以通过实现这个接口去定制拒绝策略，也可以选择JDK提供的四种已有拒绝策略，其特点如下： 线程管理Worker线程 线程池为了掌握线程的状态并维护线程的生命周期，设计了线程池内的工作线程Worker。我们来看一下它的部分代码： 1234private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; final Thread thread;//Worker持有的线程 Runnable firstTask;//初始化的任务，可以为null&#125; Worker这个工作线程，实现了Runnable接口，并持有一个线程thread，一个初始化的任务firstTask。thread是在调用构造方法时通过ThreadFactory来创建的线程，可以用来执行任务；firstTask用它来保存传入的第一个任务，这个任务可以有也可以为null。如果这个值是非空的，那么线程就会在启动初期立即执行这个任务，也就对应核心线程创建时的情况；如果这个值是null，那么就需要创建一个线程去执行任务列表（workQueue）中的任务，也就是非核心线程的创建。 Worker执行任务的模型如下图所示： 线程池需要管理线程的生命周期，需要在线程长时间不运行的时候进行回收。线程池使用一张Hash表去持有线程的引用，这样可以通过添加引用、移除引用这样的操作来控制线程的生命周期。这个时候重要的就是如何判断线程是否在运行。 Worker是通过继承AQS，使用AQS来实现独占锁这个功能。没有使用可重入锁ReentrantLock，而是使用AQS，为的就是实现不可重入的特性去反应线程现在的执行状态。 lock方法一旦获取了独占锁，表示当前线程正在执行任务中。 如果正在执行任务，则不应该中断线程。 如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断。 线程池在执行shutdown方法或tryTerminate方法时会调用interruptIdleWorkers方法来中断空闲的线程，interruptIdleWorkers方法会使用tryLock方法来判断线程池中的线程是否是空闲状态；如果线程是空闲状态则可以安全回收。 在线程回收过程中就使用到了这种特性，回收过程如下图所示： Worker线程增加 增加线程是通过线程池中的addWorker方法，该方法的功能就是增加一个线程，该方法不考虑线程池是在哪个阶段增加的该线程，这个分配线程的策略是在上个步骤完成的，该步骤仅仅完成增加线程，并使它运行，最后返回是否成功这个结果。addWorker方法有两个参数：firstTask、core。firstTask参数用于指定新增的线程执行的第一个任务，该参数可以为空；core参数为true表示在新增线程时会判断当前活动线程数是否少于corePoolSize，false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize，其执行流程如下图所示： Worker线程回收 线程池中线程的销毁依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程被JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可。Worker被创建出来后，就会不断地进行轮询，然后获取任务去执行，核心线程可以无限等待获取任务，非核心线程要限时获取任务。当Worker无法获取到任务，也就是获取的任务为空时，循环会结束，Worker会主动消除自身在线程池内的引用。 1234567try &#123; while (task != null || (task = getTask()) != null) &#123; //执行任务 &#125;&#125; finally &#123; processWorkerExit(w, completedAbruptly);//获取不到任务时，主动回收自己&#125; 线程回收的工作是在processWorkerExit方法完成的。 事实上，在这个方法中，将线程引用移出线程池就已经结束了线程销毁的部分。但由于引起线程销毁的可能性有很多，线程池还要判断是什么引发了这次销毁，是否要改变线程池的现阶段状态，是否要根据新状态，重新分配线程。 Worker线程执行任务 在Worker类中的run方法调用了runWorker方法来执行任务，runWorker方法的执行过程如下： 1.while循环不断地通过getTask()方法获取任务。 2.getTask()方法从阻塞队列中取任务。 3.如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态。 4.执行任务。 5.如果getTask结果为null则跳出循环，执行processWorkerExit()方法，销毁线程。 执行流程如下图所示： 经典问题execute() 和 submit()的区别 execute() 方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否。 submit() 方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 线程池的替代方案业务使用线程池是为了获取并发性，对于获取并发性，是否可以有什么其他的方案呢替代？ 如何确定线程池大小类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。 如果我们设置的线程池数量太小的话，如果同一时间有大量任务&#x2F;请求需要处理，可能会导致大量的请求&#x2F;任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务&#x2F;请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的！ CPU 根本没有得到充分利用。 但是，如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率。 经典回答 有一个简单并且适用面比较广的公式： CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I&#x2F;O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I&#x2F;O 交互，而线程在处理 I&#x2F;O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I&#x2F;O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 如何判断是 CPU 密集任务还是 IO 密集任务？ CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。单凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。 惊艳回答 线程池参数动态化。 具体实现请参考：https://copyfuture.com/blogs-details/20201209214330534lcv0q96xiv8nvv9 线程池被创建后里面有线程吗？如何预热？线程池被创建后如果没有任务过来，里面是不会有线程的。如果需要预热的话可以调用下面的两个方法： 全部启动 prestartAllCoreThreads 仅启动一个 prestartCoreThread 核心线程数会被回收吗？核心线程数默认是不会被回收的，如果需要回收核心线程数，需要调用下面的方法： allowCoreThreadTimeOut（默认为false） ScheduledThreadPoolExecutorScheduledThreadPoolExecutor 主要用来在给定的延迟后运行任务，或者定期执行任务。 这个在实际项目中基本不会被用到，因为有其他方案选择比如quartz。 ScheduledThreadPoolExecutor 使用的任务队列 DelayQueue 封装了一个 PriorityQueue，PriorityQueue 会对队列中的任务进行排序，执行所需时间短的放在前面先被执行(ScheduledFutureTask 的 time 变量小的先执行)，如果执行所需时间相同则先提交的任务将被先执行(ScheduledFutureTask 的 squenceNumber 变量小的先执行)。 ScheduledThreadPoolExecutor 和 Timer 的比较： Timer 对系统时钟的变化敏感，ScheduledThreadPoolExecutor不是； Timer 只有一个执行线程，因此长时间运行的任务可以延迟其他任务。 ScheduledThreadPoolExecutor 可以配置任意数量的线程。 此外，如果你想（通过提供 ThreadFactory），你可以完全控制创建的线程; 在TimerTask 中抛出的运行时异常会杀死一个线程，从而导致 Timer 死机:-( …即计划任务将不再运行。ScheduledThreadExecutor 不仅捕获运行时异常，还允许您在需要时处理它们（通过重写 afterExecute 方法ThreadPoolExecutor）。抛出异常的任务将被取消，但其他任务将继续运行。 综上，在 JDK1.5 之后，你没有理由再使用 Timer 进行任务调度了。 备注： Quartz 是一个由 java 编写的任务调度库，由 OpenSymphony 组织开源出来。在实际项目开发中使用 Quartz 的还是居多，比较推荐使用 Quartz。因为 Quartz 理论上能够同时对上万个任务进行调度，拥有丰富的功能特性，包括任务调度、任务持久化、可集群化、插件等等。 ScheduledThreadPoolExecutor 的执行主要分为两大部分： 当调用 ScheduledThreadPoolExecutor 的 scheduleAtFixedRate() 方法或者 scheduleWithFixedDelay() 方法时，会向 ScheduledThreadPoolExecutor 的 DelayQueue 添加一个实现了 RunnableScheduledFuture 接口的 ScheduledFutureTask 。 线程池中的线程从 DelayQueue 中获取 ScheduledFutureTask，然后执行任务。 ScheduledThreadPoolExecutor 为了实现周期性的执行任务，对 ThreadPoolExecutor做了如下修改： 使用 DelayQueue 作为任务队列； 获取任务的方不同 执行周期任务后，增加了额外的处理 ScheduledThreadPoolExecutor 执行周期任务的步骤 线程 1 从 DelayQueue 中获取已到期的 ScheduledFutureTask（DelayQueue.take()）。到期任务是指 ScheduledFutureTask的 time 大于等于当前系统的时间； 线程 1 执行这个 ScheduledFutureTask； 线程 1 修改 ScheduledFutureTask 的 time 变量为下次将要被执行的时间； 线程 1 把这个修改 time 之后的 ScheduledFutureTask 放回 DelayQueue 中（DelayQueue.add())。","tags":["线程池","ThreadPool"],"categories":["并发","线程池"]},{"title":"Actor Model","path":"//blog/concurrent/actor/","content":"Actors模型(Actor model)首先是由Carl Hewitt在1973定义， 由Erlang OTP (Open Telecom Platform) 推广，其消息传递更加符合面向对象的原始意图。 Actors属于并发组件模型 ，通过组件方式定义并发编程范式的高级阶段，避免使用者直接接触多线程并发或线程池等基础概念。 在 Actor 模型中，一切都是 actor，actor 通过消息传递的方式与外界通信。消息传递是异步的。每个actor都有一个邮箱（Mailbox），该邮箱接收并缓存其他actor发过来的消息，actor一次只能同步处理一个消息，处理消息过程中，除了可以接收消息，不能做任何其他操作。 使用 Actor 模型需要遵循以下几个基本原则： 所有计算都是在 actor 内执行的 actor 之间只能通过消息进行通信交流 为了响应消息，actor 可以进行如下操作 更改状态或行为 发消息给其他 actor 创建有限数量的子 actor 特性解读所有计算都在一个 actor 中进行 actor 是最基本的计算单元，在使用 Actor 模型构建系统时，一切都是actor。无论是计算斐波那契序列还是维护系统中用户的状态，都可以在一个或多个 actor 中进行。 Actor 模型中的 actor 不仅有状态，还有行为，和OOP有点像。不同的是OOP将对象（类实例）作为基本的计算单元。 Actor 模型中另一个对高并发应用有帮助的是隔离actor状态的思想。actor 的状态永远不会直接暴露在外面，也无法直接被其他 actor 查看或修改，除非通过消息机制间接地进行。这种机制同样适用于 actor 的行为。actor内部的方法同样也不会直接暴露给其他 actor 。事实上，在 actor 内部，状态和行为可以被视为相同的因素。 actor 之间只能通过消息进行通信 在 Actor 模型中，所有的通信都是基于消息机制进行的，这也是 actor 之间进行通信的方法。 每个 actor 在创建的时候都会获得一个地址，该地址是与该 actor 通信的入口。不能通过这个地址直接访问该 actor，但是可以通过这个地址发消息给它。 发送给 actor 的消息是不可变的数据。这些消息会被发送到目标 actor 提供的地址并被存在邮箱（Mailbox）中。Actor 模型提供了最多投递一次的消息机制，这意味着有可能发生投递失败的情况，如果想要保证每次都投递成功，需要使用其他工具来辅助。 akka 实现了最少投递一次的机制，更进一步提供了更加强大的顺序保证机制，可以确保正在通信的 actor 之间的消息都是按照顺序被送达的。也就是说，一个actor 发给另一个 actor多个消息的时候，可以确保消息被送达的顺序和发送顺序是一致的。 消息被投递到邮箱（Mailbox）之后，actor 可以接受并处理这些消息，但是每次只能处理一条消息。所以actor 可以很自由的修改自己的内部状态，而不用担心是否会有其他线程也在操作该状态。 actor 可以创建子 actor 在 actor 模型中，一切都是 actor ，而且 actor 之间只能通过消息机制进行通信，但是 actor 还需要知道其他 actor 的存在。 当 actor 接收到消息之后，可以进行的操作之一是创建有限数量的子 actor。之后父节点就会知道它的所有子节点的存在，并可以访问子节点的地址。 除了通过创建子节点来获取其他 actor 的方法，一个 actor 还可以把地址信息通过消息机制发送给其他 actor。这样父节点便可以把他知道的所有actor 的地址信息通知给子 actor。 actor 的这种层级结构意味着，除根节点外的所有节点都将拥有一个父节点，同时任何节点都可以拥有一个或多个子节点。这样从根节点开始遍历整棵树的actor集合被称为一个 actor 系统。 actor系统中的每个 actor 总是可以通过其他地址被唯一标识。地址命名不需要遵循任何特定的规范，只要地址唯一即可。 akka 使用层级结构的模式来命名，就像目录结构一样。或者可以使用随机生成的唯一键。 优势更加面向对象 Actor类似面向对象编程（OOP）中的对象，每个Actor实例封装了自己相关的状态，并且和其他Actor处于物理隔离状态。 举个游戏玩家的例子，每个玩家在Actor系统中是Player 这个Actor的一个实例，每个player都有自己的属性，比如Id，昵称，攻击力等，体现到代码级别其实和我们OO的代码并无多大区别，在系统内存级别也是出现了多个OO的实例 1234class PlayerActor &#123; public int Id &#123; get; set; &#125; public string Name &#123; get; set; &#125; &#125; 无锁 Actor 模型内部的状态由它自己维护，即它内部数据只能由它自己修改(通过消息传递来进行状态修改)，所以使用Actors模型进行并发编程可以很好地避免这些问题。 异步 每个Actor都有一个专用的MailBox来接收消息，这也是Actor实现异步的基础。当一个Actor实例向另外一个Actor发消息的时候，并非直接调用Actor的方法，而是把消息传递到对应的MailBox里，就好像邮递员，并不是把邮件直接送到收信人手里，而是放进每家的邮箱，这样邮递员就可以快速的进行下一项工作。所以在Actor系统里，Actor发送一条消息是非常快的。 这样的设计主要优势就是解耦了Actor，数万个Actor并发的运行，每个actor都以自己的步调运行，且发送消息，接收消息都不会被阻塞。 隔离 每个Actor的实例都维护着自己的状态，与其他Actor实例处于物理隔离状态，并非像 多线程+锁 模式那样基于共享数据。 天生分布式 每个Actor实例的位置透明，无论Actor地址是在本地还是在远程机器上对于代码来说都是一样的。每个Actor的实例非常小，最多几百字节，所以单机几十万的Actor的实例很轻松。由于位置透明性，所以Actor系统可以随意的横向扩展来应对并发，对于调用者来说，调用的Actor的位置就在本地，当然这也得益于Actor系统强大的路由系统。 容错 传统的编程方式都是在将来可能出现异常的地方去捕获异常来保证系统的稳定性，这就是所谓的防御式编程。但是防御式编程也有自己的缺点，类似于现实，防御的一方永远不能100%的防御住所有将来可能出现代码缺陷的地方。比如在java代码中很多地方充斥着判断变量是否为nil，这些就属于防御式编码最典型的案例。 但是Actor模型的程序并不进行防御式编程，而是遵循“任其崩溃”的哲学，让Actor的管理者们来处理这些崩溃问题。比如一个Actor崩溃之后，管理者可以选择创建新的实例或者记录日志。每个Actor的崩溃或者异常信息都可以反馈到管理者那里，这就保证了Actor系统在管理每个Actor实例的灵活性。 劣势由于同一类型的Actor对象是分散在多个宿主之中，所以取多个Actor的集合是个软肋。比如在电商系统中，商品作为一类Actor，查询一个商品的列表在多数情况下经过以下过程：首先根据查询条件筛选出一系列商品id，根据商品id分别取商品Actor列表（很可能会产生一个商品搜索的服务，无论是用es或者其他搜索引擎）。如果量非常大的话，有产生网络风暴的危险（虽然几率非常小）。在实时性要求不是太高的情况下，其实也可以独立出来商品Actor的列表，利用MQ接收商品信息修改的信号来处理数据一致性的问题。 在很多情况下基于Actor模型的分布式系统，缓存很有可能是进程内缓存，也就是说每个Actor其实都在进程内保存了自己的状态信息，业内通常把这种服务成为有状态服务。但是每个Actor又有自己的生命周期，会产生问题吗？还是拿商品作为例子， 如果环境是非Actor并发模型，商品的缓存可以利用LRU策略来淘汰非活跃的商品缓存，来保证内存不会使用过量，如果是基于Actor模型的进程内缓存呢，每个actor其实就是缓存本身，就不那么容易利用LRU策略来保证内存使用量了，因为Actor的活跃状态对于你来说是未知的。 分布式事物问题，其实这是所有分布式模型都面临的问题，非由于Actor而存在。还是以商品Actor为例，添加一个商品的时候，商品Actor和统计商品的Actor（很多情况下确实被设计为两类Actor服务）需要保证事物的完整性，数据的一致性。在很多的情况下可以牺牲实时一致性用最终一致性来保证。 每个Actor的mailBox有可能会出现堆积或者满的情况，当这种情况发生，新消息的处理方式是被抛弃还是等待呢，所以当设计一个Actor系统的时候mailBox的设计需要注意。 升华一下 通过以上介绍，既然Actor对于位置是透明的，任何Actor对于其他Actor就好像在本地一样。基于这个特性我们可以做很多事情了，以前传统的分布式系统，A服务器如果想和B服务器通信，要么RPC的调用（http调用不太常用），要么通过MQ系统。但是在Actor系统中，服务器之间的通信都变的很优雅了，虽然本质上也属于RPC调用，但是对于编码者来说就好像在调用本地函数一样。其实现在比较时兴的是Streaming方式。 由于Actor系统的执行模型是单线程，并且异步，所以凡是有资源竞争的类似功能都非常适合Actor模型，比如秒杀活动。 基于以上的介绍，Actor模型在设计层面天生就支持了负载均衡，而且对于水平扩容支持的非常好。当然Actor的分布式系统也是需要服务注册中心的。 虽然Actor是单线程执行模型，并不意味着每个Actor都需要占用一个线程，其实Actor上执行的任务就像Golang的goroutine一样，完全可以是一个轻量级的东西，而且一个宿主上所有的Actor可以共享一个线程池，这就保证了在使用最少线程资源的情况下，最大量化业务代码。","tags":["Actor 模型","并发模型"],"categories":["并发"]},{"title":"Token、Cookie、Session傻傻分不清楚？","path":"//blog/network/token_cookie_session/","content":"Cookiecookie 是一个非常具体的东西，指的就是浏览器里面能永久存储的一种数据，仅仅是浏览器实现的一种数据存储功能。 cookie由服务器生成，发送给浏览器，浏览器把cookie以 kv 形式保存到某个目录下的文本文件内，下一次请求同一网站时会把该cookie发送给服务器。 由于cookie是存在客户端上的，所以浏览器加入了一些限制确保cookie不会被恶意使用，同时不会占据太多磁盘空间，所以每个域的cookie数量是有限的。 Sessionsession 从字面上讲，就是会话。这个就类似于你和一个人交谈，你怎么知道当前和你交谈的是张三而不是李四呢？对方肯定有某种特征（长相等）表明他就是张三。 session 也是类似的道理，服务器要知道当前发请求给自己的是谁。为了做这种区分，服务器就要给每个客户端分配不同的“身份标识”，然后客户端每次向服务器发请求的时候，都带上这个“身份标识”，服务器就知道这个请求来自于谁了。至于客户端怎么保存这个“身份标识”，可以有很多种方式，对于浏览器客户端，大家都默认采用 cookie 的方式。 服务器使用session把用户的信息临时保存在了服务器上，用户离开网站后session会被销毁。这种用户信息存储方式相对cookie来说更安全，可是session有一个缺陷：如果web服务器做了负载均衡，那么下一个操作请求到了另一台服务器的时候session会丢失。 TokenToken的引入：Token是在客户端频繁向服务端请求数据，服务端频繁的去数据库查询用户名和密码并进行对比，判断用户名和密码正确与否，并作出相应提示，在这样的背景下，Token便应运而生。 Token的定义：Token是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。 最简单的token组成:uid(用户唯一的身份标识)、time(当前时间的时间戳)、sign(签名，由token的前几位+盐以哈希算法压缩成一定长的十六进制字符串，可以防止恶意第三方拼接token请求服务器)。 Token的目的：Token的目的是为了减轻服务器的压力，减少频繁的查询数据库，使服务器更加健壮。 区别cookie 和 session 的区别 cookie数据存放在客户端上，session数据放在服务器上。 cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗, 考虑到安全应当使用session。 session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能。考虑到减轻服务器性能方面，应当使用COOKIE。 单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。 session 和 token 的区别Session 是一种HTTP存储机制，目的是为无状态的HTTP提供的持久机制。所谓Session 认证只是简单的把User 信息存储到Session 里。Session只提供一种简单的认证，只要有 Session ID，即认为有此 User的全部权利。是需要严格保密的，这个数据应该只保存在站方，不应该共享给其它网站或者第三方App。 Token ，如果指的是OAuth Token 或类似的机制的话，提供的是 认证 和 授权 ，认证是针对用户，授权是针对App 。其目的是让 某App有权利访问 某用户 的信息。这里的 Token是唯一的。不可以转移到其它 App上，也不可以转到其它 用户 上。 所以简单来说，如果你的用户数据可能需要和第三方共享，或者允许第三方调用 API 接口，用 Token 。如果永远只是自己的网站，自己的 App，用什么就无所谓了。 只要关闭浏览器 ，session就消失了？对session来说，除非程序通知服务器删除一个session，否则服务器会一直保留，程序一般都是在用户做log off的时候发个指令去删除session。 然而浏览器从来不会主动在关闭之前通知服务器它将要关闭，因此服务器根本不会有机会知道浏览器已经关闭，之所以会有这种错觉，是大部分session机制都使用会话cookie来保存session id，而关闭浏览器后这个session id就消失了，再次连接服务器时也就无法找到原来的session。如果服务器设置的cookie被保存在硬盘上，或者使用某种手段改写浏览器发出的HTTP请求头，把原来的session id发送给服务器，则再次打开浏览器仍然能够打开原来的session. 恰恰是由于关闭浏览器不会导致session被删除，迫使服务器为session设置了一个失效时间，当距离客户端上一次使用session的时间超过这个失效时间时，服务器就可以以为客户端已经停止了活动，才会把session删除以节省存储空间。","tags":["网络"],"categories":["网络"]},{"title":"从输入 URL 到页面展示到底发生了什么？","path":"//blog/network/what_happen_after_enter_url/","content":"首先打开 Google Chrome ，然后在 URL 地址栏中输入了 maps.google.com 然后 …… 查找DNSDNS(Domain Name System) 是一个分布式的数据库，它用于维护网址 URL 到其 IP 地址的映射关系。在互联网中，IP 地址是计算机所能够理解的一种地址，而 DNS 的这种别名地址是我们人类能够理解和记忆的地址，DNS 就负责把人类记忆的地址映射成计算机能够理解的地址，每个 URL 都有唯一的 IP 地址进行对应。 举个例子，google 的官网是 www.google.com ，而 google 的 ip 地址是 216.58.200.228 ，这两个地址你在 URL 上输入哪个都能访问，但是 IP 地址不好记忆，而 google.com 简单明了。 浏览器在这个阶段会检查四个地方是否存在缓存。 浏览器缓存，这个缓存就是 DNS 记录。 浏览器会为你访问过的网站在固定期限内维护 DNS 记录。因此，它是第一个运行 DNS 查询的地方。 浏览器首先会检查这个网址在浏览器中是否有一条对应的 DNS 记录，用来找到目标网址的 IP 地址。 Windows 中可以使用 chrome:&#x2F;&#x2F;net-internals&#x2F;#dns 找到对应的 IP 地址Mac 中可以使用 nslookup 命令来查找 操作系统缓存。 如果 DNS 记录不在浏览器缓存中，那么浏览器将对操作系统发起系统调用，Windows 下就是 getHostName。 在 Linux 和大部分 UNIX 系统上，除非安装了 nscd，否则操作系统可能没有 DNS 缓存。nscd 是 Linux 系统上的一种名称服务缓存程序。 路由器缓存 如果 DNS 记录不在自己电脑上的话，浏览器就会和与之相连的路由器共同维护 DNS 记录。 ISP 缓存 如果与之相连的路由器也没有 DNS 记录的话，浏览器就会检查 ISP 中是否有缓存。ISP 缓存就是你本地通信服务商的缓存，因为 ISP 维护着自己的 DNS 服务器，它缓存 DNS 记录的本质也是为了降低请求时间，达到快速响应的效果。一旦你访问过某些网站，你的 ISP 可能就会缓存这些页面，以便下次快速访问。 所以，上面涉及到 DNS 缓存的查询过程如下。 如果上面四个步骤中都不存在 DNS 记录，那么就表示不存在 DNS 缓存，这个时候就需要发起 DNS 查询，以查找目标网址（本示例中是 maps.google.com）的 IP 地址。 发起 DNS 查询如上所述，如果想要使我的计算机和 maps.google.com 建立连接并进行通信的话，我需要知道 maps.google.com 的 IP 地址，由于 DNS 的设计原因，本地 DNS 可能无法给我提供正确的 IP 地址，那么它就需要在互联网上搜索多个 DNS 服务器，来找到网站的正确 IP 地址。 这里有个疑问，为什么我需要搜索多个 DNS 服务器的来找到网站的 IP 地址呢？一台服务器不行吗？ 因为 DNS 是分布式域名服务器，每台服务器只维护一部分 IP 地址到网络地址的映射，没有任何一台服务器能够维持全部的映射关系。 在 DNS 的早期设计中只有一台 DNS 服务器。这台服务器会包含所有的 DNS 映射。这是一种集中式的设计，这种设计并不适用于当今的互联网，因为互联网有着数量巨大并且持续增长的主机，这种集中式的设计会存在以下几个问题 单点故障(a single point of failure)，如果 DNS 服务器崩溃，那么整个网络随之瘫痪。 通信容量(traaffic volume)，单个 DNS 服务器不得不处理所有的 DNS 查询，这种查询级别可能是上百万上千万级，一台服务器很难满足。 远距离集中式数据库(distant centralized database)，单个 DNS 服务器不可能 邻近 所有的用户，假设在美国的 DNS 服务器不可能临近让澳大利亚的查询使用，其中查询请求势必会经过低速和拥堵的链路，造成严重的时延。 维护(maintenance)，维护成本巨大，而且还需要频繁更新。 所以在当今网络情况下 DNS 不可能集中式设计，因为它完全没有可扩展能力，所以采用分布式设计，这种设计的特点如下 分布式、层次数据库 首先分布式设计首先解决的问题就是 DNS 服务器的扩展性问题，因此 DNS 使用了大量的 DNS 服务器，它们的组织模式一般是层次方式，并且分布在全世界范围内。没有一台 DNS 服务器能够拥有因特网上所有主机的映射。相反，这些映射分布在所有的 DNS 服务器上。 大致来说有三种 DNS 服务器： 根 DNS 服务器、 顶级域(Top-Level Domain, TLD) DNS 服务器 权威 DNS 服务器 。 这些服务器的层次模型如下图所示 根 DNS 服务器 ，有 400 多个根域名服务器遍及全世界，这些根域名服务器由 13 个不同的组织管理。根域名服务器的清单和组织机构可以在 https://root-servers.org/ 中找到，根域名服务器提供 TLD 服务器的 IP 地址。 顶级域 DNS 服务器，对于每个顶级域名比如 com、org、net、edu 和 gov 和所有的国家级域名 uk、fr、ca 和 jp 都有 TLD 服务器或服务器集群。所有的顶级域列表参见 https://tld-list.com/ 。TDL 服务器提供了权威 DNS 服务器的 IP 地址。 权威 DNS 服务器，在因特网上具有公共可访问的主机，如 Web 服务器和邮件服务器，这些主机的组织机构必须提供可供访问的 DNS 记录，这些记录将这些主机的名字映射为 IP 地址。一个组织机构的权威 DNS 服务器收藏了这些 DNS 记录。 在了解了 DNS 服务器的设计理念之后，我们回到 DNS 查找的步骤上来，DNS 的查询方式主要分为三种 通过组合使用这些查询，优化的 DNS 解析过程可缩短传输距离。在理想情况下，可以使用缓存的记录数据，从而使 DNS 域名服务器能够直接使用非递归查询。 递归查询：一般发生在 Client 请求 DNS Server。Client 发出一个域名解析的请求，DNS Server 必须返回对应的 IP 地址，或者返回找不到的错误。 迭代查询：一般发生在 DNS Server 之间，当 Client 发出域名解析的请求后，DNS Server 需要给予最佳答案，这个最佳答案可能是”距离最近”的顶级域名服务器，也能是权威域名服务器。无论如何，Client 需要对返回结果再次发起请求，知道获得最终结果。 非递归查询：一般发生在 Client 和 DNS Server 之间，指的是，请求的 DNS Server 已经知道答案，直接返回。这里可能有两种情况，一种是 DNS Server 本机缓存了对应的 IP，或者是缓存了对应的域名的权威服务器。第二种情况只需要再发一次请求，即可拿到结果返回。 详情可参考：https://juejin.cn/post/6844903900982558734#heading-9 上面负责开始 DNS 查找的介质就是 DNS 解析器，它一般是 ISP 维护的 DNS 服务器，它的主要职责就是通过向网络中其他 DNS 服务器询问正确的 IP 地址。 所以对于 maps.google.com 这个域名来说，如果 ISP 维护的服务器没有 DNS 缓存记录，它就会向 DNS 根服务器地址发起查询，根名称服务器会将其重定向到 .com 顶级域名服务器。 .com 顶级域名服务器会将其重定向到google.com 权威服务器。google.com 名称服务器将在其 DNS 记录中找到 maps.google.com 匹配的 IP 地址，并将其返回给您的 DNS 解析器，然后将其发送回你的浏览器。 ARP 请求ARP 协议的全称是 Address Resolution Protocol(地址解析协议)，它是一个通过用于实现从 IP 地址到 MAC 地址的映射，即询问目标 IP 对应的 MAC 地址 的一种协议。 ARP 就是一种解决地址问题的协议，它以 IP 地址为线索，定位下一个应该接收数据分包的主机 MAC 地址。如果目标主机不在同一个链路上，那么会查找下一跳路由器的 MAC 地址。 关于为什么有了 IP 地址，还要有 MAC 地址概述可以参看知乎这个回答 https://www.zhihu.com/question/21546408 如果 DNS 服务器和我们的主机在同一个子网内，系统会按照下面的 ARP 过程对 DNS 服务器进行 ARP 查询 如果 DNS 服务器和我们的主机在不同的子网，系统会按照下面的 ARP 过程对默认网关进行查询 ARP 的大致工作流程如下 假设 A 和 B 位于同一链路，不需要经过路由器的转换，主机 A 向主机 B 发送一个 IP 分组，主机 A 的地址是 192.168.1.2 ，主机 B 的地址是 192.168.1.3，它们都不知道对方的 MAC 地址是啥，主机 C 和 主机 D 是同一链路的其他主机。 主机 A 想要获取主机 B 的 MAC 地址，通过主机 A 会通过广播 的方式向以太网上的所有主机发送一个 ARP 请求包，这个 ARP 请求包中包含了主机 A 想要知道的主机 B 的 IP 地址的 MAC 地址。 主机 A 发送的 ARP 请求包会被同一链路上的所有主机&#x2F;路由器接收并进行解析。每个主机&#x2F;路由器都会检查 ARP 请求包中的信息，如果 ARP 请求包中的目标 IP 地址 和自己的相同，就会将自己主机的 MAC 地址写入响应包返回主机 A 由此，可以通过 ARP 从 IP 地址获取 MAC 地址，实现同一链路内的通信。 ARP 维护每个主机和路由器上的 ARP 缓存(或表)。这个缓存维护着每个 IP 到 MAC 地址的映射关系。通过把第一次 ARP 获取到的 MAC 地址作为 IP 对 MAC 的映射关系到一个 ARP 缓存表中，下一次再向这个地址发送数据报时就不再需要重新发送 ARP 请求了，而是直接使用这个缓存表中的 MAC 地址进行数据报的发送。每发送一次 ARP 请求，缓存表中对应的映射关系都会被清除。 通过 ARP 缓存，降低了网络流量的使用，在一定程度上防止了 ARP 的大量广播。 一般来说，发送过一次 ARP 请求后，再次发送相同请求的几率比较大，因此使用 ARP 缓存能够减少 ARP 包的发送，除此之外，不仅仅 ARP 请求的发送方能够缓存 ARP 接收方的 MAC 地址，接收方也能够缓存 ARP 请求方的 IP 和 MAC 地址，如下所示 不过，MAC 地址的缓存有一定期限，超过这个期限后，缓存的内容会被清除。 所以，浏览器会首先查询 ARP 缓存，如果缓存命中，我们返回结果：目标 IP &#x3D; MAC。 如果缓存没有命中： 查看路由表，看看目标 IP 地址是不是在本地路由表中的某个子网内。是的话，使用跟那个子网相连的接口，否则使用与默认网关相连的接口。 查询选择的网络接口的 MAC 地址 我们发送一个数据链路层的 ARP 请求： 封装 TCP 数据包浏览器得到目标服务器的 IP 地址后，根据 URL 中的端口可以知道端口号 （http 协议默认端口号是 80， https 默认端口号是 443），会准备 TCP 数据包。数据包的封装会经过下面的层层处理，数据到达目标主机后，目标主机会解析数据包，完整的请求和解析过程如下。 在经过上述 DNS 和 ARP 查找流程后，浏览器就会收到一个目标服务器的 IP 和 MAC地址，然后浏览器将会和目标服务器建立连接来传输信息。这里可以使用很多种 Internet 协议，但是 HTTP 协议建立连接所使用的运输层协议是 TCP 协议。所以这一步骤是浏览器与目标服务器建立 TCP 连接的过程。 TCP 的连接建立需要经过 TCP&#x2F;IP 的三次握手，三次握手的过程其实就是浏览器和服务器交换 SYN 同步和 ACK 确认消息的过程。 假设图中左端是客户端主机，右端是服务端主机，一开始，两端都处于CLOSED（关闭）状态。 浏览器发送 HTTP 请求到 web 服务器一旦 TCP 连接建立完成后，就开始直接传输数据办正事了！此时浏览器会发送 GET 请求，要求目标服务器提供 maps.google.com 的网页，如果你填写的是表单，则发起的是 POST 请求，在 HTTP 中，GET 请求和 POST 请求是最常见的两种请求，基本上占据了所有 HTTP 请求的九成以上。 除了请求类型外，HTTP 请求还包含很多很多信息，最常见的有 Host、Connection 、User-agent、Accept-language 等 服务器处理请求并发回一个响应这个服务器包含一个 Web 服务器，也就是 Apache 服务器，服务器会从浏览器接收请求并将其传递给请求处理程序并生成响应。 服务器发送回一个 HTTP 响应服务器响应包含你请求的网页以及状态代码，压缩类型（Content-Encoding），如何缓存页面（Cache-Control），要设置的 cookie，隐私信息等。 浏览器显示 HTML 的相关内容浏览器会分阶段显示 HTML 内容。 首先，它将渲染裸露的 HTML 骨架。 然后它将检查 HTML 标记并发送 GET 请求以获取网页上的其他元素，例如图像，CSS 样式表，JavaScript 文件等。这些静态文件由浏览器缓存，因此你再次访问该页面时，不用重新再请求一次。最后，您会看到 maps.google.com 显示的内容出现在你的浏览器中。","tags":["网络"],"categories":["网络"]},{"title":"动态代理","path":"//blog/java/dynamic-proxy/","content":"代理模式代理模式是一种比较好理解的设计模式。 简单来说就是：我们使用代理对象来代替对真实对象(real object)的访问，这样就可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的能力，实现功能的增强。 生活中的代购、租房中介、售票黄牛、婚介、经纪人、快递、事务代理、非侵入式日志监听等，都是代理 模式的实际体现。 使用代理模式主要有两个目的： 一是保护目标对象 二是增强目标对象功能（比如说在目标对象的某个方法执行前后增加一些自定义的操作） 代理模式一般包含三种角色： 抽象角色（Subject）：抽象角色的主要职责是声明真实类与代理类的共同接口方法，该类可以是接口也可以是抽象类 真实角色（RealSubject）：该角色也被称为被代理类，该类定义了代理所表示的真实对象，是负责执行系统真正的逻辑业务对象 代理角色（Proxy）：也被称为代理类，其内部持有 Real Subject 的引用，因此具备完全的对 RealSubject 的代理权。客户端调用代理对象的方法，同时也调用被代理对象的方法，但是会在代理对象前后增加一些处理代码。 一般代理会被理解为代码增强，实际上就是在原代码逻辑前后增加一些代码逻辑，而使调用者无感知。代理模式属于结构型模式，分为静态代理和动态代理。 静态代理静态代理是代理类在编译期间就创建好的，不是编译器生成的代理类，而是手动创建的类。在编译时就已经将接口，被代理类，代理类等确定下来。软件设计中所指的代理一般是指静态代理，也就是在代码中显式指定的代理。 下面我们通过一个简单的案例，来了解下静态代理。 Cat.java123456789/** * 静态代理类接口, 委托类和代理类都需要实现的接口规范。 * 定义了一个猫科动物的两个行为接口，吃东西，奔跑。 * 作为代理类 和委托类之间的约束接口 */public interface Cat &#123; public String eatFood(String foodName); public boolean running();&#125; Lion.java1234567891011121314151617181920212223242526272829303132/** * 狮子 实现了猫科动物接口Cat， 并实现了具体的行为。作为委托类实现 */public class Lion implements Cat &#123; private String name; private int runningSpeed; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getRunningSpeed() &#123; return runningSpeed; &#125; public void setRunningSpeed(int runningSpeed) &#123; this.runningSpeed = runningSpeed; &#125; public Lion() &#123; &#125; @Override public String eatFood(String foodName) &#123; String eat = this.name + &quot; Lion eat food. foodName = &quot; + foodName; System.out.println(eat); return eat; &#125; @Override public boolean running() &#123; System.out.println(this.name + &quot; Lion is running . Speed :&quot; + this.runningSpeed); return false; &#125;&#125; 代理类角色(FeederProxy) FeederProxy.java12345678910111213141516171819202122232425262728/** * 饲养员 实现Cat接口，作为静态代理类实现。代理狮子的行为。 * 代理类中可以新增一些其他行为，在实践中主要做的是参数校验的功能。 */public class FeederProxy implements Cat &#123; private Cat cat; public FeederProxy()&#123;&#125; public FeederProxy(Cat cat) &#123; if (cat instanceof Cat) &#123; this.cat = cat; &#125; &#125; public void setCat(Cat cat) &#123; if (cat instanceof Cat) &#123; this.cat = cat; &#125; &#125; @Override public String eatFood(String foodName) &#123; System.out.println(&quot;proxy Lion exec eatFood &quot;); return cat.eatFood(foodName); &#125; @Override public boolean running() &#123; System.out.println(&quot;proxy Lion exec running.&quot;); return cat.running(); &#125;&#125; 静态代理类测试 staticProxyTest.java12345678910111213141516/** * 静态代理类测试 */public class staticProxyTest &#123; public static void main(String[] args) &#123; Lion lion = new Lion(); lion.setName(&quot;狮子 小王&quot;); lion.setRunningSpeed(100); /** * new 静态代理类，静态代理类在编译前已经创建好了，和动态代理的最大区别点 */ Cat proxy = new FeederProxy(lion); System.out.println(Thread.currentThread().getName()+&quot; -- &quot; + proxy.eatFood(&quot;水牛&quot;)); proxy.running(); &#125;&#125; 静态代理很好的诠释了代理设计模式，代理模式最主要的就是有一个公共接口（Cat），一个委托类（Lion），一个代理类（FeederProxy）,代理类持有委托类的实例，代为执行具体类实例方法。 代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。这里的间接性就是指客户端不直接调用实际对象的方法，客户端依赖公共接口并使用代理类。 那么我们在代理过程中就可以加上一些其他用途。 就这个例子来说在 eatFood 方法调用中，代理类在调用具体实现类之前添加System.out.println(“proxy Lion exec eatFood “);语句 就是添加间接性带来的收益。代理类存在的意义是为了增加一些公共的逻辑代码。 静态代理的缺陷 代理类和委托类实现了相同的接口，代理类通过委托类实现了相同的方法。这样就出现了大量的代码重复。如果接口增加一个方法，除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度。 代理对象只服务于一种类型的对象，如果要服务多类型的对象。势必要为每一种对象都进行代理，静态代理在程序规模稍大时就无法胜任了。 静态代理一个代理只能代理一种类型，而且是在编译器就已经确定被代理的对象。 动态代理jdk动态代理模式是利用java中的反射技术，在运行时动态创建代理类。 JDK 动态代理基于 JDK 的动态代理涉及到两个核心的类：Proxy类 和 InvocationHandler接口 JDK动态代理的写法比较固定，需要先定义一个接口和接口的实现类，然后再定义一个实现了InvocationHandler接口的实现类。最终调用Proxy类的newInstance()方法即可。示例代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 先定义一个接口：UserService，接口中有两个方法public interface UserService &#123; int insert(); String query();&#125;// 再定义一个UserService接口的实现类：UserServiceImplpublic class UserServiceImpl implements UserService&#123; @Override public int insert() &#123; System.out.println(&quot;insert&quot;); return 0; &#125; @Override public String query() &#123; System.out.println(&quot;query&quot;); return null; &#125;&#125;// 再定义一个InvocationHandler接口的实现类：UserServiceInvocationHandler。在自定义的InvocationHandler中，定义了一个属性：target，定义这个属性的目的是为了在InvocationHandler中持有对目标对象的引用，target属性的初始化是在构造器中进行初始化的。public class UserServiceInvocationHandler implements InvocationHandler &#123; // 持有目标对象 private Object target; public UserServiceInvocationHandler(Object target)&#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;invocation handler&quot;); // 通过反射调用目标对象的方法 return method.invoke(target,args); &#125;&#125;// 通过Proxy.newProxyInstance()方法创建代理对象public class MainApplication &#123; public static void main(String[] args) &#123; // 指明一个类加载器，要操作class文件，怎么少得了类加载器呢 ClassLoader classLoader = MainApplication.class.getClassLoader(); // 为代理对象指定要是实现哪些接口，这里我们要为UserServiceImpl这个目标对象创建动态代理，所以需要为代理对象指定实现UserService接口 Class[] classes = new Class[]&#123;UserService.class&#125;; // 初始化一个InvocationHandler，并初始化InvocationHandler中的目标对象 InvocationHandler invocationHandler = new UserServiceInvocationHandler(new UserServiceImpl()); // 创建动态代理 UserService userService = (UserService) Proxy.newProxyInstance(classLoader, classes, invocationHandler); // 执行代理对象的方法，通过观察控制台的结果，判断我们是否对目标对象(UserServiceImpl)的方法进行了增强 userService.insert(); &#125;&#125;// 控制台打印结果如下，从打印结果来看，已经完成了对目标对象UserServiceImpl的代理（增强）。Output:invocation handlerinsert jdk实现动态代理可以分为以下几个步骤： 先检查委托类是否实现了相应接口，保证被访问方法在接口中也要有定义 创建一个实现InvocationHandler接口的类 在类中定义一个被代理对象的成员属性，为了扩展方便可以直接使用Object类，也可以根据需求定义相应的接口 在invoke方法中实现对委托对象的调用，根据需求对方法进行增强 使用Proxy.newProxyInstance(…)方法创建代理对象，并提供要给获取代理对象的方法 下面我们通过一些手段，来看下jdk动态代理生成的代理类长什么样子。 手段一： 自定义的一个小工具类将动态生成的代理类保存到本地 12345678910111213141516171819202122232425262728/** * 将生成的代理类保存为.class文件的工具类 */public class ProxyUtils &#123; /** * 将代理类保存到指定路径 * * @param path 保存到的路径 * @param proxyClassName 代理类的Class名称 * @param interfaces 代理类接口 * @return */ public static boolean saveProxyClass(String path, String proxyClassName, Class[] interfaces)&#123; if (proxyClassName == null || path == null) &#123; return false; &#125; // 获取文件字节码，然后输出到目标文件中 byte[] classFile = ProxyGenerator.generateProxyClass(proxyClassName, interfaces); try (FileOutputStream out = new FileOutputStream(path)) &#123; out.write(classFile); out.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); return false; &#125; return true; &#125;&#125; 手段二： 在代码中添加一行代码：System.getProperties().put(&quot;sun.misc.ProxyGenerator.saveGeneratedFiles&quot;, &quot;true&quot;)，就能实现将程序运行过程中产生的动态代理对象的class文件写入到磁盘。如下示例： 运行程序，最终发现在项目的根目录下出现了一个包：com.sun.proxy。包下有一个文件$Proxy0.class。在idea打开，发现就是所产生代理类的源代码。 12345678910111213141516public class MainApplication &#123; public static void main(String[] args) &#123; // 让代理对象的class文件写入到磁盘 System.getProperties().put(&quot;sun.misc.ProxyGenerator.saveGeneratedFiles&quot;, &quot;true&quot;); // 指明一个类加载器，要操作class文件，怎么少得了类加载器呢 ClassLoader classLoader = MainApplication.class.getClassLoader(); // 为代理对象指定要是实现哪些接口，这里我们要为UserServiceImpl这个目标对象创建动态代理，所以需要为代理对象指定实现UserService接口 Class[] classes = new Class[]&#123;UserService.class&#125;; // 初始化一个InvocationHandler，并初始化InvocationHandler中的目标对象 InvocationHandler invocationHandler = new UserServiceInvocationHandler(new UserServiceImpl()); // 创建动态代理 UserService userService = (UserService) Proxy.newProxyInstance(classLoader, classes, invocationHandler); // 执行代理对象的方法，通过观察控制台的结果，判断我们是否对目标对象(UserServiceImpl)的方法进行了增强 userService.insert(); &#125;&#125; 通过上面两种方法获取到代理对象的源代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.sun.proxy;import com.tiantang.study.UserService;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;public final class $Proxy0 extends Proxy implements UserService &#123; private static Method m1; private static Method m3; private static Method m4; private static Method m2; private static Method m0; public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return (Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final int insert() throws &#123; try &#123; return (Integer)super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final String query() throws &#123; try &#123; return (String)super.h.invoke(this, m4, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return (Integer)super.h.invoke(this, m0, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m3 = Class.forName(&quot;com.tiantang.study.UserService&quot;).getMethod(&quot;insert&quot;); m4 = Class.forName(&quot;com.tiantang.study.UserService&quot;).getMethod(&quot;query&quot;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 通过源码我们发现，$Proxy0类继承了Proxy类，同时实现了UserService接口。到这里，我们的问题一就能解释了，为什么JDK的动态代理只能基于接口实现，不能基于继承来实现？ 因为Java中不支持多继承，而JDK的动态代理在创建代理对象时，默认让代理对象继承了Proxy类，所以JDK只能通过接口去实现动态代理。 $Proxy0实现了UserService接口，所以重写了接口中的两个方法（$Proxy0同时还重写了Object类中的几个方法）。所以当我们调用query()方法时，先是调用到$Proxy0.query()方法，在这个方法中，直接调用了super.h.invoke()方法，父类是Proxy，父类中的h就是我们定义的InvocationHandler，所以这儿会调用到UserServiceInvocationHandler.invoke()方法。因此当我们通过代理对象去执行目标对象的方法时，会先经过InvocationHandler的invoke()方法，然后在通过反射method.invoke()去调用目标对象的方法，因此每次都会先打印invocation handler这句话。 CGLIB 动态代理cglib动态代理和jdk动态代理类似，也是采用操作字节码机制，在运行时生成代理类。cglib 动态代理采取的是创建目标类的子类的方式，因为是子类化，我们可以达到近似使用被调用者本身的效果。 字节码处理机制-指得是ASM来转换字节码并生成新的类 注：spring中有完整的cglib相关的依赖，所以以下代码基于spring官方下载的demo中直接进行编写的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 1. 订单服务-委托类，不需要再实现接口 **/public class OrderService &#123; /** * 保存订单接口 */ public void saveOrder(String orderInfo) throws InterruptedException &#123; // 随机休眠，模拟订单保存需要的时间 Thread.sleep(System.currentTimeMillis() &amp; 100); System.out.println(&quot;订单：&quot; + orderInfo + &quot; 保存成功&quot;); &#125;&#125;/** * cglib动态代理工厂 * * @author cruder * @date 2019-11-23 18:36 **/public class ProxyFactory implements MethodInterceptor &#123; /** * 委托对象， 即被代理对象 */ private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; /** * 返回一个代理对象 * @return */ public Object getProxyInstance()&#123; // 1. 创建一个工具类 Enhancer enhancer = new Enhancer(); // 2. 设置父类 enhancer.setSuperclass(target.getClass()); // 3. 设置回调函数 enhancer.setCallback(this); // 4.创建子类对象，即代理对象 return enhancer.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; long start = System.currentTimeMillis(); Object result = method.invoke(target, args); System.out.println(&quot;cglib代理：保存订单用时: &quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;); return result; &#125;&#125;/** * 使用cglib代理类来保存订单 **/public class Client &#123; public static void main(String[] args) throws InterruptedException &#123; // 1. 创建委托对象 OrderService orderService = new OrderService(); // 2. 获取代理对象 OrderService orderServiceProxy = (OrderService) new ProxyFactory(orderService).getProxyInstance(); String saveFileName = &quot;CglibOrderServiceDynamicProxy.class&quot;; ProxyUtils.saveProxyClass(saveFileName, orderService.getClass().getSimpleName(), new Class[]&#123;IOrderService.class&#125;); orderServiceProxy.saveOrder(&quot; cruder 新买的花裤衩 &quot;); &#125;&#125; cglib动态代理实现步骤和jdk及其相似，可以分为以下几个步骤： 创建一个实现MethodInterceptor接口的类 在类中定义一个被代理对象的成员属性，为了扩展方便可以直接使用Object类，也可以根据需求定义相应的接口 在invoke方法中实现对委托对象的调用，根据需求对方法进行增强 使用Enhancer创建生成代理对象，并提供要给获取代理对象的方法 cglib动态代理生成的代理类和jdk动态代理代码格式上几乎没有什么区别，唯一的区别在于cglib生成的代理类继承了仅仅Proxy类，而jdk动态代理生成的代理类继承了Proxy类的同时也实现了一个接口。代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 生成一个Proxy的子类public final class OrderService extends Proxy &#123; private static Method m1; private static Method m2; private static Method m0; public OrderService(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return (Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return (Integer)super.h.invoke(this, m0, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; jdk proxy vs cglibJDK Proxy 的优势： 最小化依赖关系，减少依赖意味着简化开发和维护，JDK 本身的支持，可能比 cglib 更加可靠。 平滑进行 JDK 版本升级，而字节码类库通常需要进行更新以保证在新版 Java 上能够使用。 代码实现简单。 cglib 优势： 有的时候调用目标可能不便实现额外接口，从某种角度看，限定调用者实现接口是有些侵入性的实践，类似 cglib 动态代理就没有这种限制。 只操作我们关心的类，而不必为其他相关类增加工作量。 代理模式: 为其他对象提供一种代理以控制（隔离，使用接口）对这个对象的访问。 jdk动态代理生成的代理类继承了Proxy类并实现了被代理的接口；而cglib生成的代理类则仅继承了Proxy类。 jdk动态代理最大缺点：只能代理接口，既委托类必须实现相应的接口 cglib缺点：由于是通过“子类化”的方式， 所以不能代理final的委托类或者普通委托类的final修饰的方法。 动态代理导致spring事务失效的场景： https://www.jianshu.com/p/3dd79531fe41","tags":["Java","设计","动态代理"],"categories":["Java"]},{"title":"接口 vs 抽象类的区别？","path":"//blog/java/abstract_interface/","content":"在面向对象编程中，抽象类和接口是两个经常被用到的语法概念，是面向对象四大特性，以及很多设计模式、设计思想、设计原则编程实现的基础。 比如：我们可以使用接口来实现面向对象的抽象特性、多态特性和基于接口而非实现的设计原则，使用抽象类来实现面向对象的继承特性和模板设计模式等。 什么是抽象类和接口？ 区别在哪里？不同的编程语言对接口和抽象类的定义方式可能有些差别，但是差别并不大。本文使用 Java 语言。 抽象类下面我们通过一个例子来看一个典型的抽象类的使用场景。 Logger 是一个记录日志的抽象类，FileLogger 和 MessageQueueLogger 继承Logger，分别实现两种不同的日志记录方式： 记录日志到文件中 记录日志到消息队列中 FileLogger 和 MessageQueuLogger 两个子类复用了父类 Logger 中的name、enabled 以及 minPermittedLevel 属性和 log 方法，但是因为两个子类写日志的方式不同，他们又各自重写了父类中的doLog方法。 父类 1234567891011121314151617181920212223242526import java.util.logging.Level;/** * 抽象父类 * @author yanliang * @date 9/27/2020 5:59 PM */public abstract class Logger &#123; private String name; private boolean enabled; private Level minPermittedLevel; public Logger(String name, boolean enabled, Level minPermittedLevel) &#123; this.name = name; this.enabled = enabled; this.minPermittedLevel = minPermittedLevel; &#125; public void log(Level level, String message) &#123; boolean loggable = enabled &amp;&amp; (minPermittedLevel.intValue() &lt;= level.intValue()); if(!loggable) return; doLog(level, message); &#125; protected abstract void doLog(Level level, String message);&#125; FileLogger 12345678910111213141516171819202122232425import java.io.FileWriter;import java.io.IOException;import java.io.Writer;import java.util.logging.Level;/** * 抽象类Logger的子类：输出日志到文件中 * @author yanliang * @date 9/28/2020 4:44 PM */public class FileLogger extends Logger &#123; private Writer fileWriter; public FileLogger(String name, boolean enabled, Level minPermittedLevel, String filePath) throws IOException &#123; super(name, enabled, minPermittedLevel); this.fileWriter = new FileWriter(filePath); &#125; @Override protected void doLog(Level level, String message) &#123; // 格式化level 和 message，输出到日志文件 fileWriter.write(...); &#125;&#125; MessageQueuLogger 12345678910111213141516171819202122import java.util.logging.Level;/** * 抽象类Logger的子类：输出日志到消息队列中 * @author yanliang * @date 9/28/2020 6:39 PM */public class MessageQueueLogger extends Logger &#123; private MessageQueueClient messageQueueClient; public MessageQueueLogger(String name, boolean enabled, Level minPermittedLevel, MessageQueueClient messageQueueClient) &#123; super(name, enabled, minPermittedLevel); this.messageQueueClient = messageQueueClient; &#125; @Override protected void doLog(Level level, String message) &#123; // 格式化level 和 message，输出到消息队列中 messageQueueClient.send(...) &#125;&#125; 通过上面的例子，我们来看下抽象类有哪些特性。 抽象类不能被实例化，只能被继承。（new 一个抽象类，会报编译错误） 抽象类可以包含属性和方法。方法既可以包含实现，也可以不包含实现。不包含实现的方法叫做抽象方法 子类继承抽象类，必须实现抽象类中的所有抽象方法。 接口同样的，下面我们通过一个例子来看下接口的使用场景。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 过滤器接口 * @author yanliang * @date 9/28/2020 6:46 PM */public interface Filter &#123; void doFilter(RpcRequest req) throws RpcException;&#125;/** * 接口实现类：鉴权过滤器 * @author yanliang * @date 9/28/2020 6:48 PM */public class AuthencationFilter implements Filter &#123; @Override public void doFilter(RpcRequest req) throws RpcException &#123; // 鉴权逻辑 &#125;&#125;/** * 接口实现类：限流过滤器 * @author yanliang * @date 9/28/2020 6:48 PM */public class RateLimitFilter implements Filter&#123; @Override public void doFilter(RpcRequest req) throws RpcException &#123; // 限流逻辑 &#125;&#125;/** * 过滤器使用demo * @author yanliang * @date 9/28/2020 6:48 PM */public class Application &#123; // 过滤器列表 private List&lt;Filter&gt; filters = new ArrayList&lt;&gt;(); filters.add(new AuthencationFilter()); filters.add(new RateLimitFilter()); public void handleRpcRequest(RpcRequest req) &#123; try &#123; for (Filter filter : filters) &#123; filter.doFilter(req); &#125; &#125; catch (RpcException e) &#123; // 处理过滤结果 &#125; // ... &#125;&#125; 上面的案例是一个典型的接口使用场景。通过Java中的 interface 关键字定义了一个Filter 接口，AuthencationFilter 和 RetaLimitFilter 是接口的两个实现类，分别实现了对Rpc请求的鉴权和限流的过滤功能。 下面我们来看下接口的特性： JDK 1.8允许给接口添加非抽象的方法实现，但必须使用default关键字修饰；定义了default的方法可以不被实现子类所实现，但只能被实现子类的对象调用；如果子类实现了多个接口，并且这些接口包含一样的默认方法，则子类必须重写默认方法； JDK 1.8中允许使用static关键字修饰一个方法，并提供实现，称为接口静态方法。接口静态方法只能通过接口调用（接口名.静态方法名）。 类实现接口时，必须实现接口中定义的所有方法。 除了语法特性的不同外，从设计的角度，这两者也有较大区别。抽象类本质上就是类，只不过是一种特殊的类，这种类不能被实例化，只能被子类继承。属于is-a的关系。接口则是 has-a 的关系，表示具有某些功能。对于接口，有一个更形象的叫法：协议（contract） 抽象类和接口解决了什么问题？下面我们先来思考一个问题~ 抽象类的存在意义是为了解决代码复用的问题（多个子类可以继承抽象类中定义的属性哈方法，避免在子类中，重复编写相同的代码）。 那么，既然继承本身就能达到代码复用的目的，而且继承也不一定非要求是抽象类。我们不适用抽象类，貌似也可以实现继承和复用。从这个角度上讲，我们好像并不需要抽象类这种语法呀。那抽象类除了解决代码复用的问题，还有其他存在的意义吗？ 这里大家可以先思考一下哈~ 我们还是借用上面Logger的例子，首先对上面的案例实现做一些改造。在改造之后的实现中，Logger不再是抽象类，只是一个普通的父类，删除了Logger中的两个方法，新增了 isLoggable()方法。FileLogger 和 MessageQueueLogger 还是继承Logger父类已达到代码复用的目的。具体代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 父类：非抽象类，就是普通的类 * @author yanliang * @date 9/27/2020 5:59 PM */public class Logger &#123; private String name; private boolean enabled; private Level minPermittedLevel; public Logger(String name, boolean enabled, Level minPermittedLevel) &#123; this.name = name; this.enabled = enabled; this.minPermittedLevel = minPermittedLevel; &#125; public boolean isLoggable(Level level) &#123; return enabled &amp;&amp; (minPermittedLevel.intValue() &lt;= level.intValue()); &#125; &#125;/** * 抽象类Logger的子类：输出日志到文件中 * @author yanliang * @date 9/28/2020 4:44 PM */public class FileLogger extends Logger &#123; private Writer fileWriter; public FileLogger(String name, boolean enabled, Level minPermittedLevel, String filePath) throws IOException &#123; super(name, enabled, minPermittedLevel); this.fileWriter = new FileWriter(filePath); &#125; protected void log(Level level, String message) &#123; if (!isLoggable(level)) return ; // 格式化level 和 message，输出到日志文件 fileWriter.write(...); &#125;&#125;package com.yanliang.note.java.abstract_demo;import java.util.logging.Level;/** * 抽象类Logger的子类：输出日志到消息队列中 * @author yanliang * @date 9/28/2020 6:39 PM */public class MessageQueueLogger extends Logger &#123; private MessageQueueClient messageQueueClient; public MessageQueueLogger(String name, boolean enabled, Level minPermittedLevel, MessageQueueClient messageQueueClient) &#123; super(name, enabled, minPermittedLevel); this.messageQueueClient = messageQueueClient; &#125; protected void log(Level level, String message) &#123; if (!isLoggable(level)) return ; // 格式化level 和 message，输出到消息队列中 messageQueueClient.send(...) &#125;&#125; 以上实现虽然达到了代码复用的目的（复用了父类中的属性），但是却无法使用多态的特性了。 像下面这样编写代码就会出现编译错误，因为Logger中并没有定义log（）方法。 12Logger logger = new FileLogger(&quot;access-log&quot;, true, Level.WARN, &quot;/user/log&quot;);logger.log(Level.ERROR, &quot;This is a test log message.&quot;); 如果我们在父类中，定义一个空的log（）方法，让子类重写父类的log（）方法，实现自己的记录日志逻辑。使用这种方式是否能够解决上面的问题呢？ 大家可以先思考下~ 这个思路可以用使用，但是并不优雅，主要有一下几点原因： 在Logger中定义一个空的方法，会影响代码的可读性。如果不熟悉Logger背后的设计思想，又没有代码注释的话，在阅读Logger代码时就会感到疑惑（为什么这里会存在一个空的log（）方法） 当创建一个新的子类继承Logger父类时，有时可能会忘记重新实现log方法。之前是基于抽象类的设计思想，编译器会强制要求子类重写父类的log方法，否则就会报编译错误。 Logger可以被实例化，这也就意味着这个空的log方法有可能会被调用。这就增加了类被误用的风险。当然，这个问题 可以通过设置私有的构造函数的方式来解决，但是不如抽象类优雅。 抽象类更多是为了代码复用，而接口更侧重于解耦。接口是对行为的一种抽象，相当于一组协议或者契约（可类比API接口）。调用者只需要关心抽象的接口，不需要了解具体的实现，具体的实现代码对调用者透明。接口实现了约定和实现相分离，可以降低代码间的耦合，提高代码的可扩展性。 实际上，接口是一个比抽象类应用更加广泛、更加重要的知识点。比如，我们经常提到的 ”基于接口而非实现编程“ ,就是一条几乎天天会用到的，并且能极大的提高代码的灵活性、扩展性的设计思想。 如何模拟抽象类和接口在前面列举的例子中，我们使用Java的接口实现了Filter过滤器。不过，在 C++ 中只提供了抽象类，并没有提供接口，那从代码的角度上说，是不是就无法实现 Filter 的设计思路了呢？ 大家可以先思考下 🤔 ~ 我们先会议下接口的定义：接口中没有成员变量，只有方法声明，没有方法实现，实现接口的类必须实现接口中的所有方法。主要满足以上几点从设计的角度上来说，我们就可以把他叫做接口。 实际上，要满足接口的这些特性并不难。下面我们来看下实现： 1234567class Strategy &#123; public: -Strategy(); virtual void algorithm()=0; protected: Strategy();&#125; 抽象类 Strategy 没有定义任何属性，并且所有的方法都声明为 virtual 类型（等同于Java中的abstract关键字），这样，所有的方法都不能有代码实现，并且所有继承了这个抽象类的子类，都要实现这些方法。从语法特性上看，这个抽象类就相当于一个接口。 处理用抽象类来模拟接口外，我们还可以用普通类来模拟接口。具体的Java实现如下所示： 123456public class MockInterface &#123; protected MockInteface(); public void funcA() &#123; throw new MethodUnSupportedException(); &#125;&#125; 我们知道类中的方法必须包含实现，这个不符合接口的定义。但是，我们可以让类中的方法抛出 MethodUnSupportedException 异常，来模拟不包含实现的接口，并且强迫子类来继承这个父类的时候，都主动实现父类的方法，否则就会在运行时抛出异常。 那又如何避免这个类被实例化呢？ 实际上很简单，我们只需要将这个类的构造函数声明为 protected 访问权限就可以了。 如何决定该用抽象还是接口？上面的讲解可能偏理论，现在我们就从真实项目开发的角度来看下。在代码设计&#x2F;编程时，什么时候该用接口？什么时候该用抽象类？ 实际上，判断的标准很简单。如果我们需要一种is-a关系，并且是为了解决代码复用的问题，就用抽象类。如果我们需要的是一种has-a关系，并且是为了解决抽象而非代码复用问题，我们就用接口。 从类的继承层次来看，抽象类是一种自下而上的设计思路，先有子类的代码复用，然后再抽象成上层的父类（也就是抽象类）。而接口则相反，它是一种自上而下的设计思路，我们在编程的时候，一般都是先设计接口，再去思考具体实现。 好了，你是否掌握了上面的内容呢。你可以通过一下几个维度来回顾自检一下： 抽象类和接口的语法特性 抽象类和接口存在的意义 抽象类和接口的应用场景有哪些","tags":["Java","接口","抽象"],"categories":["Java"]},{"title":"类加载器","path":"//blog/java/jvm/jvm-class-loader2/","content":"类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。 注意：JVM主要在程序第一次主动使用类的时候，才会去加载该类，也就是说，JVM并不是在一开始就把一个程序就所有的类都加载到内存中，而是到不得不用的时候才把它加载进来，而且只加载一次。 类加载器jvm支持两种类型的加载器，分别是引导类加载器和 自定义加载器 引导类加载器是由c&#x2F;c++实现的 自定义加载器是由java实现的。 jvm规范定义自定义加载器是指派生于抽象类ClassLoder的类加载器。按照这样的加载器的类型划分，在程序中我们最常见的类加载器是：引导类加载器BootStrapClassLoader、自定义类加载器(Extension Class Loader、System Class Loader、User-Defined ClassLoader） 上图中的加载器划分为包含关系而并非继承关系 启动类加载器这个类加载器使用c&#x2F;c++实现，嵌套再jvm内部，用来加载Java的核心类库（JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;rt.jar、resource.jar或sun.boot.class.path路径下的内容），用于提供JVM自身需要的类。并不继承自java.lang.ClassLoader，没有父加载器 扩展类加载器java语言编写，由sun.misc.Launcher$ExtClassLoader实现。从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre&#x2F;lib&#x2F;ext 子目录（扩展目录）下加载类库。如果用户创建的JAR 放在此目录下，也会自动由扩展类加载器加载；派生于 ClassLoader。 父类加载器为启动类加载器 系统类加载器java语言编写，由 sun.misc.Lanucher$AppClassLoader 实现。该类加载是程序中默认的类加载器，一般来说，Java应用的类都是由它来完成加载的，它负责加载环境变量classpath或系统属性java.class.path 指定路径下的类库；派生于 ClassLoader 。父类加载器为扩展类加载器 。通过ClassLoader#getSystemClassLoader() 方法可以获取到该类加载器。 1234567891011121314151617181920212223public class ClassLoaderTest &#123; public static void main(String[] args) &#123; // 获取系统类加载器 ClassLoader systemClassLoader = ClassLoader.getSystemClassLoader(); System.out.println(systemClassLoader); // sun.misc.Launcher$AppClassLoader@18b4aac2 // 获取系统类加载器的父类加载器，扩展类加载器 ClassLoader extClassLoader = systemClassLoader.getParent(); System.out.println(extClassLoader); // sun.misc.Launcher$ExtClassLoader@1b6d3586 // 获取扩展类加载器的上层启动类加载器，这里获取不到 ClassLoader bootstrapClassLoader = extClassLoader.getParent(); System.out.println(bootstrapClassLoader); // null // 获取用户自定义类的加载器，classLoader的打印结果和systemClassLoader的结果完全一致 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); System.out.println(classLoader); // sun.misc.Launcher$AppClassLoader@18b4aac2 // 核心类库使用的是启动类加载器，以String为例 ClassLoader stringClassLoader = String.class.getClassLoader(); System.out.println(stringClassLoader); // null &#125;&#125; 双亲委派模型双亲委派模型工作过程是：如果一个类加载器收到类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器完成。每个类加载器都是如此，只有当父加载器在自己的搜索范围内找不到指定的类时（即 ClassNotFoundException ），子加载器才会尝试自己去加载。 为什么需要双亲委派模型？假设没有双亲委派模型，试想一个场景： 黑客自定义一个 java.lang.String 类，该 String 类具有系统的 String 类一样的功能，只是在某个函数稍作修改。比如 equals 函数，这个函数经常使用，如果在这这个函数中，黑客加入一些“病毒代码”。并且通过自定义类加载器加入到 JVM 中。此时，如果没有双亲委派模型，那么 JVM 就可能误以为黑客自定义的java.lang.String 类是系统的 String 类，导致“病毒代码”被执行。 而有了双亲委派模型，黑客自定义的 java.lang.String 类永远都不会被加载进内存。因为首先是最顶端的类加载器加载系统的 java.lang.String 类，最终自定义的类加载器无法加载 java.lang.String 类。 或许你会想，我在自定义的类加载器里面强制加载自定义的 java.lang.String 类，不去通过调用父加载器不就好了吗?确实，这样是可行。但是，在 JVM 中，判断一个对象是否是某个类型时，如果该对象的实际类型与待比较的类型的类加载器不同，那么会返回false。 举个简单例子： ClassLoader1 ClassLoader2 都加载 java.lang.String 类，对应Class1、Class2对象。那么 Class1对象不属于 ClassLoad2 对象加载的 java.lang.String 类型。 如何实现双亲委派模型?双亲委派模型的原理很简单，实现也简单。每次通过先委托父类加载器加载，当父类加载器无法加载时，再自己加载。其实 ClassLoader 类默认的 loadClass 方法已经帮我们写好了，我们无需去写。 几个重要函数 loadClass 默认实现如下： 123public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false);&#125; 再看看 loadClass(String name, boolean resolve) 函数： 123456789101112131415161718192021222324252627282930313233protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 从上面代码可以明显看出， loadClass(String, boolean) 函数即实现了双亲委派模型！整个大致过程如下： 首先，检查一下指定名称的类是否已经加载过，如果加载过了，就不需要再加载，直接返回。 如果此类没有加载过，那么，再判断一下是否有父加载器；如果有父加载器，则由父加载器加载（即调用 parent.loadClass(name, false); ）.或者是调用 bootstrap 类加载器来加载。 如果父加载器及 bootstrap 类加载器都没有找到指定的类，那么调用当前类加载器的 findClass 方法来完成类加载。 换句话说，如果自定义类加载器，就必须重写 findClass 方法！ findClass 的默认实现如下： 123protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name);&#125; 可以看出，抽象类 ClassLoader 的 findClass 函数默认是抛出异常的。而前面我们知道， loadClass 在父加载器无法加载类的时候，就会调用我们自定义的类加载器中的 findeClass 函数，因此我们必须要在 loadClass 这个函数里面实现将一个指定类名称转换为 Class 对象. 如果是读取一个指定的名称的类为字节数组的话，这很好办。但是如何将字节数组转为 Class 对象呢？很简单，Java 提供了 defineClass 方法，通过这个方法，就可以把一个字节数组转为Class对象 defineClass 主要的功能是： 将一个字节数组转为 Class 对象，这个字节数组是 class 文件读取后最终的字节数组。如，假设 class 文件是加密过的，则需要解密后作为形参传入 defineClass 函数。 defineClass 默认实现如下： 1234protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len) throws ClassFormatError &#123; return defineClass(name, b, off, len, null);&#125; 自定义加类加载器为什么要自定义类加载器 隔离加载类 模块隔离,把类加载到不同的应用选中。比如tomcat这类web应用服务器，内部自定义了好几中类加载器，用于隔离web应用服务器上的不同应用程序。 修改类加载方式 除了Bootstrap加载器外，其他的加载并非一定要引入。根据实际情况在某个时间点按需进行动态加载。扩展加载源比如还可以从数据库、网络、或其他终端上加载 防止源码泄漏 java代码容易被编译和篡改，可以进行编译加密，类加载需要自定义还原加密字节码。 自定义类加载器实现实现方式: 所有用户自定义类加载器都应该继承ClassLoader类 在自定义ClassLoader的子类是,我们通常有两种做法: 重写loadClass方法(是实现双亲委派逻辑的地方,修改他会破坏双亲委派机制,不推荐) 重写findClass方法 (推荐) 首先，我们定义一个待加载的普通 Java 类: Test.java 。 1234567891011public class ClassLoaderTest &#123; public static void main(String[] args) &#123; MyClassLoader classLoader = new MyClassLoader(&quot;d:/&quot;); try &#123; Class&lt;?&gt; clazz = classLoader.loadClass(&quot;TestMain&quot;); System.out.println(&quot;我是由&quot;+clazz.getClassLoader().getClass().getName()+&quot;类加载器加载的&quot;); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 接下来就是自定义的类加载器： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.io.*;public class MyClassLoader extends ClassLoader&#123; private String codePath; public MyClassLoader(ClassLoader parent, String codePath) &#123; super(parent); this.codePath = codePath; &#125; public MyClassLoader(String codePath) &#123; this.codePath = codePath; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; BufferedInputStream bis = null; ByteArrayOutputStream baos = null; try &#123; //1.字节码路径 String fileName = codePath+name+&quot;.class&quot;; //2.获取输入流 bis = new BufferedInputStream(new FileInputStream(fileName)); //3.获取输出流 baos = new ByteArrayOutputStream(); //4.io读写 int len; byte[] data = new byte[1024]; while ((len = bis.read(data)) != -1)&#123; baos.write(data , 0 , len); &#125; //5.获取内存中字节数组 byte[] byteCode = baos.toByteArray(); //6.调用defineClass 将字节数组转成Class对象 Class&lt;?&gt; defineClass = defineClass(null, byteCode, 0, byteCode.length); return defineClass; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; bis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; baos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125;&#125; 最后运行结果如下： 1我是由 class Main$MyClassLoader 加载进来的","tags":["Java","JVM","类加载器"],"categories":["Java","JVM","类加载器"]},{"title":"JVM 类加载机制","path":"//blog/java/jvm/jvm-class-loader/","content":"类加载子系统主要包含如下几项功能： 负责从文件系统或是网络中加载.class文件，class文件在文件开头有特定的文件标识。 把加载后的class类信息存放于方法区，除了类信息之外，方法区还会存放运行时常量池信息，可能还包括字符串字面量和数字常量（这部分常量信息是Class文件中常量池部分的内存映射）。 ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定。 如果调用构造器实例化对象，则该对象存放在堆区。 虚拟机把描述类的数据从 Class 文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 类加载的执行过程类从被加载到虚拟机内存中开始，到卸载出内存，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initiallization）、使用（Using）和卸载（Unloading）这7个阶段。其中验证、准备、解析3个部分统称为连接（Linking），这七个阶段的发生顺序如下图： 上图中，加载、验证、准备、初始化、卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段不一定：它在某些情况下可以初始化阶段之后在开始，这是为了支持Java语言的运行时绑定（也称为动态绑定）。接下来讲解加载、验证、准备、解析、初始化五个步骤，这五个步骤组成了一个完整的类加载过程。使用没什么好说的，卸载属于GC的工作 。 加载加载是类加载的第一个阶段。有两种时机会触发类加载： 预加载 虚拟机启动时加载，加载的是JAVA_HOME&#x2F;lib&#x2F;下的rt.jar下的.class文件，这个jar包里面的内容是程序运行时非常常常用到的，像java.lang.*、java.util.、java.io. 等等，因此随着虚拟机一起加载。要证明这一点很简单，写一个空的main函数，设置虚拟机参数为”-XX:+TraceClassLoading”来获取类加载信息，运行一下： 12345678[Opened E:\\developer\\JDK8\\JDK\\jre\\lib\\rt.jar][Loaded java.lang.Object from E:\\developer\\JDK8\\JDK\\jre\\lib\\rt.jar][Loaded java.io.Serializable from E:\\developer\\JDK8\\JDK\\jre\\lib\\rt.jar][Loaded java.lang.Comparable from E:\\developer\\JDK8\\JDK\\jre\\lib\\rt.jar][Loaded java.lang.CharSequence from E:\\developer\\JDK8\\JDK\\jre\\lib\\rt.jar][Loaded java.lang.String from E:\\developer\\JDK8\\JDK\\jre\\lib\\rt.jar][Loaded java.lang.reflect.AnnotatedElement from E:\\developer\\JDK8\\JDK\\jre\\lib\\rt.jar]...... 运行时加载 虚拟机在用到一个.class文件的时候，会先去内存中查看一下这个.class文件有没有被加载，如果没有就会按照类的全限定名来加载这个类。 那么，加载阶段做了什么，其实加载阶段做了有三件事情： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个.class文件的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。一般这个Class是在堆里的，不过HotSpot虚拟机比较特殊，这个Class对象是放在方法区中的。 虚拟机规范对这三点的要求并不具体，因此虚拟机实现与具体应用的灵活度都是相当大的。例如第一条，根本没有指明二进制字节流要从哪里来、怎么来，因此单单就这一条，就能变出许多花样来： 从zip包中获取，这就是以后jar、ear、war格式的基础 从网络中获取，典型应用就是Applet 运行时计算生成，典型应用就是动态代理技术 由其他文件生成，典型应用就是JSP，即由JSP生成对应的.class文件 从数据库中读取，这种场景比较少见 总而言之，在类加载整个过程中，这部分是对于开发者来说可控性最强的一个阶段。 连接链接包含三个步骤： 分别是 验证Verification , 准备Preparation , 解析Resolution 三个过程。 验证 Verification 连接阶段的第一步，这一阶段的目的是为了确保.class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 Java语言本身是相对安全的语言（相对C&#x2F;C++来说），但是前面说过，.class文件未必要从Java源码编译而来，可以使用任何途径产生，甚至包括用十六进制编辑器直接编写来产生.class文件。在字节码语言层面上，Java代码至少从语义上是可以表达出来的。虚拟机如果不检查输入的字节流，对其完全信任的话，很可能会因为载入了有害的字节流而导致系统崩溃，所以验证是虚拟机对自身保护的一项重要工作。 验证阶段将做一下几个工作，具体就不细讲了（详情可参考《深入理解Java虚拟机》），这是虚拟机实现层面的问题： 文件格式验证 元数据验证 字节码验证 符号引用验证 准备Preparation 准备阶段是正式为类变量分配内存并设置其初始值的阶段，这些变量所使用的内存都将在方法区中分配。关于这点，有两个地方注意一下： 这时候进行内存分配的仅仅是类变量（被static修饰的变量），而不是实例变量，实例变量将会在对象实例化的时候随着对象一起分配在Java堆中 这个阶段赋初始值的变量指的是那些不被final修饰的static变量，比如”public static int value &#x3D; 123”，value在准备阶段过后是0而不是123，给value赋值为123的动作将在初始化阶段才进行；比如”public static final int value &#x3D;123;”就不一样了，在准备阶段，虚拟机就会给value赋值为123。 各个数据类型的零值如下表： 数据类型 零值 int 0 long 0L short (short)0 chart ‘\\u0000’ byte (byte)0 boolean false float 0.0f double 0.0d reference null 实战： 说出下面两段代码的表现 1234567891011121314151617code-snippet-1:public class A &#123; static int a ; public static void main(String[] args) &#123; System.out.println(a); &#125;&#125;code-snippet-2:public class B &#123; public static void main(String[] args) &#123; int a ; System.out.println(a); &#125;&#125; Answercode-snippet-1 将会输出 0code-snippet-2 将无法通过编译这是因为局部变量不像类变量那样存在准备阶段。类变量有两次赋初始值的过程，一次在准备阶段，赋予初始值（也可以是指定值）；另外一次在初始化阶段，赋予程序员定义的值。因此，即使程序员没有为类变量赋值也没有关系，它仍然有一个默认的初始值。但局部变量就不一样了，如果没有给它赋初始值，是不能使用的。 解析Resolution 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。来了解一下符号引用和直接引用有什么区别： 符号引用 符号引用是一种定义，可以是任何字面上的含义，而直接引用就是直接指向目标的指针、相对偏移量。 这个其实是属于编译原理方面的概念，符号引用包括了下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 这么说可能不太好理解，结合实际看一下，写一段很简单的代码： 123456789public class TestMain &#123; private static int i; private double d; public static void print() &#123;&#125; private boolean trueOrFalse()&#123; return false; &#125;&#125; 用javap把这段代码的.class反编译一下 1234567891011121314151617181920212223Constant pool:#1 = Class #2 // com/xrq/test6/TestMain#2 = Utf8 com/xrq/test6/TestMain#3 = Class #4 // java/lang/Object#4 = Utf8 java/lang/Object#5 = Utf8 i#6 = Utf8 I#7 = Utf8 d#8 = Utf8 D#9 = Utf8 &lt;init&gt;#10 = Utf8 ()V#11 = Utf8 Code#12 = Methodref #3.#13 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V#13 = NameAndType #9:#10 // &quot;&lt;init&gt;&quot;:()V#14 = Utf8 LineNumberTable#15 = Utf8 LocalVariableTable#16 = Utf8 this#17 = Utf8 Lcom/xrq/test6/TestMain;#18 = Utf8 print#19 = Utf8 trueOrFalse#20 = Utf8 ()Z#21 = Utf8 SourceFile#22 = Utf8 TestMain.java 看到Constant Pool也就是常量池中有22项内容，其中带”Utf8”的就是符号引用。比如#2，它的值是”com&#x2F;xrq&#x2F;test6&#x2F;TestMain”，表示的是这个类的全限定名；又比如#5为i，#6为I，它们是一对的，表示变量时Integer（int）类型的，名字叫做i；#6为D、#7为d也是一样，表示一个Double（double）类型的变量，名字为d； #18、#19表示的都是方法的名字。 总而言之，符号引用和我们上面讲的是一样的，是对于类、变量、方法的描述。符号引用和虚拟机的内存布局是没有关系的，引用的目标未必已经加载到内存中了。 直接引用 直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局相关的，同一个符号引用在不同的虚拟机示例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经存在在内存中了。 解析阶段负责把整个类激活，串成一个可以找到彼此的网，过程不可谓不重要。那这个阶段都做了哪些工作呢？大体可以分为： 类或接口的解析 类方法解析 接口方法解析 字段解析 初始化类的初始化阶段是类加载过程的最后一个步骤， 之前介绍的几个类加载的动作里， 除了在加载阶段用户应用程序可以通过自定义类加载器的方式局部参与外， 其余动作都完全由Java虚拟机来主导控制。 直到初始化段，Java 虚拟机才真正开始执行类中编写的Java程序代码， 将主导权移交给应用程序。 初始化阶段就是执行类构造器clinit()方法的过程。 clinit()并不是程序员在Java代码中直接编写的方法， 它是Javac编译器的自动生成物。 clinit()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块） 中的语句合并产生的， 编译器收集的顺序是由语句在源文件中出现的顺序决定的， 静态语句块中只能访问到定义在静态语句块之前的变量， 定义在它之后的变量，在前面的静态语句块可以赋值， 但是不能访问， 如下代码 1234567public class TestClinit &#123; static &#123; i = 0; // 给变量复制可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; clinit()方法与类的构造函数（即在虚拟机视角中的实例构造器()方法） 不同， 它不需要显式地调用父类构造器， Java虚拟机会保证在子类的clinit()方法执行前， 父类的clinit()方法已经执行 完毕。 因此在Java虚拟机中第一个被执行的clinit()方法的类型肯定是java.lang.Object。 由于父类的clinit()方法先执行， 也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作， 如下代码中， 字段B的值将会是2而不是1。方法执行顺序 123456789101112131415161718class TestClinit02 &#123; static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125; &#125; static class Sub extends Parent &#123; public static int B = A; &#125; public static void main(String[] args) &#123; System.out.println(Sub.B); &#125;&#125;output: 2 clinit()方法对于类或接口来说并不是必需的， 如果一个类中没有静态语句块， 也没有对变量的赋值操作， 那么编译器可以不为这个类生成clinit()方法。 接口中不能使用静态语句块， 但仍然有变量初始化的赋值操作， 因此接口与类一样都会生成 clinit()方法。 但接口与类不同的是， 执行接口的clinit()方法不需要先执行父接口的clinit()方法， 因为只有当父接口中定义的变量被使用时， 父接口才会被初始化。 此外， 接口的实现类在初始化时也一样不会执行接口的clinit()方法。 Java虚拟机必须保证一个类的clinit()方法在多线程环境中被正确地加锁同步， 如果多个线程同时去初始化一个类， 那么只会有其中一个线程去执行这个类的clinit()方法， 其他线程都需要阻塞等待， 直到活动线程执行完毕clinit()方法。 如果在一个类的clinit()方法中有耗时很长的操作， 那就可能造成多个进程阻塞， 在实际应用中这种阻塞往往是很隐蔽的。 12345678910111213141516171819202122232425class TestDeadLoop &#123; static class DeadLoopClass &#123; static &#123; // 如果不加上这个if语句， 编译器将提示“Initializer does not complete normally”拒绝编译 if (true) &#123; System.out.println(Thread.currentThread() + &quot;init DeadLoopClass&quot;); while (true) &#123; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; Runnable script = new Runnable() &#123; public void run() &#123; System.out.println(Thread.currentThread() + &quot;start&quot;); DeadLoopClass dlc = new DeadLoopClass(); System.out.println(Thread.currentThread() + &quot; run over&quot;); &#125; &#125;; Thread thread1 = new Thread(script); Thread thread2 = new Thread(script); thread1.start(); thread2.start(); &#125;&#125; cinit 和 init 的区别主要是为了弄明白类的初始化和对象的初始化之间的差别。 12345678910111213141516171819202122232425262728293031323334public class ParentA &#123; static &#123; System.out.println(&quot;1&quot;); &#125; public ParentA() &#123; System.out.println(&quot;2&quot;); &#125;&#125;class SonB extends ParentA &#123; static &#123; System.out.println(&quot;a&quot;); &#125; public SonB() &#123; System.out.println(&quot;b&quot;); &#125; public static void main(String[] args) &#123; ParentA ab = new SonB(); ab = new SonB(); &#125;&#125;Answer:1a2b2b 其中 static 字段和 static 代码块，是属于类的，在类的加载的初始化阶段就已经被执行。类信息会被存放在方法区，在同一个类加载器下，这些信息有一份就够了，所以上面的 static 代码块只会执行一次，它对应的是 cinit 方法。 所以，上面代码的 static 代码块只会执行一次，对象的构造方法执行两次。再加上继承关系的先后原则，不难分析出正确结果。 结论: 方法 cinit 的执行时期: 类初始化阶段(该方法只能被jvm调用, 专门承担类变量的初始化工作) ,只执行一次 方法 init 的执行时期: 对象的初始化阶段","tags":["Java","JVM","类加载机制"],"categories":["Java","JVM"]},{"title":"JVM 内存管理","path":"//blog/java/jvm/jvm-memory-manager/","content":"JVM 整体架构根据 JVM 规范，JVM 内存共分为: 程序计数器、虚拟机栈、本地方法栈、堆、方法区 这五个部分。 名 称 特征 作用 配置参数 异常 程 序 计 数 器 占用内存小，线程私有，生命周 期与线程相同 大致为字节码行号指示器 无 无 虚 拟 机 栈 线程私有，生命周期与线程相 同，使用连续的 内存空间 Java 方法执行的内存模 型，存储局部变量表、 操作栈、动态链接、方 法出口等信息 -Xss StackOverflowError&#x2F; OutOfMemoryError 本 地 方 法 栈 线程私有 为虚拟机使用到的 Native 方法服务 无 StackOverflowError&#x2F; OutOfMemoryError 堆 线程共享，生命周期与虚拟机相 同，可以不使用连续的内存地址 保存对象实例，所有对 象实例（包括数组）都 要在堆上分配 -Xms -Xsx -Xmn OutOfMemoryError 方 法 区 线程共享，生命周期与虚拟机相 同，可以不使用连续的内存地址 存储已被虚拟机加载的 类信息、常量、静态变 量、即时编译器编译后 的代码等数据 -XX:PermSize:16M-XX:MaxPermSize64M-XX:MetaspaceSize&#x3D;16M-XX:MaxMetaspaceSize&#x3D;64M OutOfMemoryError JVM分为五大模块： 类装载器子系统 、 运行时数据区 、 执行引擎 、 本地方法接口 和 垃圾收集模块 。 JVM 运行时内存Java 虚拟机有自动内存管理机制，如果出现面的问题，排查错误就必须要了解虚拟机是怎样使用内存的。 Java7和Java8内存结构的不同主要体现在方法区的实现，方法区是java虚拟机规范中定义的一种概念上的区域，不同的厂商可以对虚拟机进行不同的实现。我们通常使用的Java SE都是由Sun JDK和OpenJDK所提供，这也是应用最广泛的版本。而该版本使用的VM就是 HotSpot VM。通常情况下，我们所讲的java虚拟机指的就是HotSpot的版本。 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存(非运行时数据区的一部分) 对于Java8，HotSpots取消了永久代，那么是不是就没有方法区了呢？ 当然不是，方法区只是一个规范，只不过它的实现变了。在Java8中，元空间(Metaspace)登上舞台，方法区存在于元空间(Metaspace)。同时，元空间不再与堆连续，而且是存在于本地内存（Native memory）。 方法区Java8之后的变化 移除了永久代（PermGen），替换为元空间（Metaspace） 永久代中的class metadata（类元信息）转移到了native memory（本地内存，而不是虚拟机） 永久代中的interned Strings（字符串常量池） 和 class static variables（类静态变量）转移到了Java heap 永久代参数（PermSize MaxPermSize）-&gt; 元空间参数（MetaspaceSize MaxMetaspaceSize） Java8为什么要将永久代替换成Metaspace？ 字符串存在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。 永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。 Oracle 可能会将HotSpot 与 JRockit 合二为一，JRockit没有所谓的永久代。 程序计数器程序计数器（Program Counter Register）:也叫PC寄存器，是一块较小的内存空间，它可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令、分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 特点： 区别于计算机硬件的pc寄存器，两者不略有不同。计算机用pc寄存器来存放“伪指令”或地址，而相对于虚拟机，pc寄存器它表现为一块内存，虚拟机的pc寄存器的功能也是存放伪指令，更确切的说存放的是将要执行指令的地址。 当虚拟机正在执行的方法是一个本地（native）方法的时候，jvm的pc寄存器存储的值是undefined。 程序计数器是线程私有的，它的生命周期与线程相同，每个线程都有一个。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器只会执行一条线程中的指令。 因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 虚拟机栈Java虚拟机栈(Java Virtual Machine Stacks)也是线程私有的，即生命周期和线程相同。Java虚拟机栈和线程同时创建，用于存储栈帧。每个方法在执行时都会创建一个栈帧(Stack Frame)，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直到执行完成的过程就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 12345678910111213141516171819202122232425262728package com.lagou.unit;public class StackDemo &#123; public static void main(String[] args) &#123; StackDemo sd = new StackDemo(); sd.A(); &#125; public void A()&#123; int a = 10; System.out.println(&quot; method A start&quot;); System.out.println(a); B(); System.out.println(&quot;method A end&quot;); &#125; public void B()&#123; int b = 20; System.out.println(&quot; method B start&quot;); C(); System.out.println(&quot;method B end&quot;); &#125; private void C() &#123; int c = 30; System.out.println(&quot; method C start&quot;); System.out.println(&quot;method C end&quot;); &#125;&#125; 什么是栈帧？栈帧(Stack Frame)是用于支持虚拟机进行方法调用和方法执行的数据结构。栈帧存储了方法的局部变量表、操作数栈、动态连接和方法返回地址等信息。每一个方法从调用至执行完成的过程，都对应着一个栈帧在虚拟机栈里从入栈到出栈的过程。 设置虚拟机栈的大小-Xss 为jvm启动的每个线程分配的内存大小，默认JDK1.4中是256K，JDK1.5+中是1M - Linux/x64 (64-bit): 1024 KB - macOS (64-bit): 1024 KB - Oracle Solaris/x64 (64-bit): 1024 KB - Windows: The default value depends on virtual memory -Xss1m-Xss1024k-Xss1048576 局部变量表局部变量表(Local Variable Table)是一组变量值存储空间，用于存放方法参数和方法内定义的局部变量。包括8种基本数据类型、对象引用（reference类型）和returnAddress类型（指向一条字节码指令的地址）。其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。 操作数栈操作数栈(Operand Stack)也称作操作栈，是一个后入先出栈(LIFO)。随着方法执行和字节码指令的执行，会从局部变量表或对象实例的字段中复制常量或变量写入到操作数栈，再随着计算的进行将栈中元素出栈到局部变量表或者返回给方法调用者，也就是出栈&#x2F;入栈操作。 动态链接Java虚拟机栈中，每个栈帧都包含一个指向运行时常量池中该栈所属方法的符号引用，持有这个引用的目的是为了支持方法调用过程中的动态链接(Dynamic Linking)。 方法返回地址方法返回地址存放调用该方法的PC寄存器的值。一个方法的结束，有两种方式：正常地执行完成，出现未处理的异常非正常的退出。无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置。方法正常退出时，调用者的PC计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址。而通过异常退出的，返回地址是要通过异常表来确定，栈帧中一般不会保存这部分信息。无论方法是否正常完成，都需要返回到方法被调用的位置，程序才能继续进行。 本地方法栈本地方法栈（Native Method Stacks） 与虚拟机栈所发挥的作用是非常相似的， 其区别只是虚拟机栈为虚拟机执行 Java方法（也就是字节码） 服务， 而本地方法栈则是为虚拟机使用到的本地（Native） 方法服务。 特点： 本地方法栈加载native的但是方法, native类方法存在的意义当然是填补java代码不方便实现的缺陷而提出的。 虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则是为虚拟机使用到的Native方法服务。 是线程私有的，它的生命周期与线程相同，每个线程都有一个。 在Java虚拟机规范中，对本地方法栈这块区域，与Java虚拟机栈一样，规定了两种类型的异常： StackOverFlowError :线程请求的栈深度&gt;所允许的深度。 OutOfMemoryError：本地方法栈扩展时无法申请到足够的内存。 堆对于Java应用程序来说， Java堆（Java Heap） 是虚拟机所管理的内存中最大的一块。 Java堆是被所有线程共享的一块内存区域， 在虚拟机启动时创建。 此内存区域的唯一目的就是存放对象实例， Java 世界里“几乎”所有的对象实例都在这里分配内存。 “几乎”是指从实现角度来看， 随着Java语言的发展，现在已经能看到些许迹象表明日后可能出现值类型的支持， 即使只考虑现在， 由于即时编译技术的进步， 尤其是逃逸分析技术的日渐强大， 栈上分配、 标量替换优化手段已经导致一些微妙的变化悄然发生， 所以说Java对象实例都分配在堆上也渐渐变得不是那么绝对了。 特点： 是Java虚拟机所管理的内存中最大的一块。 堆是jvm所有线程共享的。堆中也包含私有的线程缓冲区 Thread Local Allocation Buffer (TLAB) 在虚拟机启动的时候创建。 唯一目的就是存放对象实例，几乎所有的对象实例以及数组都要在这里分配内存。 Java堆是垃圾收集器管理的主要区域。 因此很多时候java堆也被称为“GC堆”（Garbage Collected Heap）。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，所以Java堆还可以细分为：新生代和老年代；新生代又可以分为：Eden 空间、FromSurvivor空间、To Survivor空间。 java堆是计算机物理存储上不连续的、逻辑上是连续的，也是大小可调节的（通过-Xms和-Xmx控制）。 方法结束后,堆中对象不会马上移出仅仅在垃圾回收的时候时候才移除。 如果在堆中没有内存完成实例的分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常 设置堆空间大小 内存大小-Xmx&#x2F;-Xms 使用示例: -Xmx20m -Xms5m 说明： 当下Java应用最大可用内存为20M， 最小内存为5M 测试: 12345678910111213141516171819public class TestVm &#123; public static void main(String[] args) &#123; //补充 //byte[] b=new byte[5*1024*1024]; //System.out.println(&quot;分配了1M空间给数组&quot;); System.out.print(&quot;Xmx=&quot;); System.out.println(Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + &quot;M&quot;); System.out.print(&quot;free mem=&quot;); System.out.println(Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + &quot;M&quot;); System.out.print(&quot;total mem=&quot;); System.out.println(Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + &quot;M&quot;); &#125;&#125;Output:Xmx=20.0Mfree mem=4.1877593994140625Mtotal mem=6.0M 可以发现，打印出来的Xmx值和设置的值之间是由差异的，total Memory和最大的内存之间还是存在一定差异的，就是说JVM一般会尽量保持内存在一个尽可能底的层面，而非贪婪做法按照最大的内存来进行分配。 在测试代码中新增如下语句，申请内存分配： 12byte[] b=new byte[4*1024*1024];System.out.println(&quot;分配了1M空间给数组&quot;); 在申请分配了4m内存空间之后，total memory上升了，同时可用的内存也上升了，可以发现其实JVM在分配内存过程中是动态的， 按需来分配的。 堆的分类现在垃圾回收器都使用分代理论,堆空间也分类如下: 在Java7 Hotspot虚拟机中将Java堆内存分为3个部分： 青年代Young Generation 老年代Old Generation 永久代Permanent Generation 在Java8以后，由于方法区的内存不在分配在Java堆上，而是存储于本地内存元空间Metaspace中，所以永久代就不存在了，在几天前(2018年9约25日)Java11正式发布以后，我从官网上找到了关于Java11中垃圾收集器的官方文档， 文档中没有提到“永久代”，而只有青年代和老年代。 年轻代 &amp; 老年代JVM中存储java对象可以被分为两类: 年轻代(Young Gen)：年轻代主要存放新创建的对象，内存大小相对会比较小，垃圾回收会比较频繁。年轻代分成1个Eden Space和2个Suvivor Space（from 和to）。 年老代(Tenured Gen)：年老代主要存放JVM认为生命周期比较长的对象（经过几次的Young Gen的垃圾回收后仍然存在），内存大小相对会比较大，垃圾回收也相对没有那么频繁。 默认 -XX:NewRatio&#x3D;2 , 标识新生代占1 , 老年代占2 ,新生代占整个堆的1&#x2F;3修改占比 -XX:NewPatio&#x3D;4 , 标识新生代占1 , 老年代占4 , 新生代占整个堆的1&#x2F;5 Eden空间和另外两个Survivor空间占比分别为8:1:1。可以通过操作选项 -XX:SurvivorRatio 调整这个空间比例。 比如 -XX:SurvivorRatio&#x3D;8 几乎所有的java对象都在Eden区创建, 但80%的对象生命周期都很短,创建出来就会被销毁. 从图中可以看出： 堆大小 &#x3D; 新生代 + 老年代。其中，堆的大小可以通过参数 –Xms、-Xmx 来指定。 默认的，新生代 ( Young ) 与老年代 ( Old ) 的比例的值为 1:2 ( 该值可以通过参数 –XX:NewRatio 来指定 )，即：新生代 ( Young ) &#x3D; 1&#x2F;3 的堆空间大小。老年代 ( Old ) &#x3D; 2&#x2F;3 的堆空间大小。其中，新生代 ( Young ) 被细分为 Eden 和 两个Survivor 区域，这两个 Survivor 区域分别被命名为 from 和 to，以示区分。 默认的，Edem : from : to &#x3D; 8 : 1 : 1 ( 可以通过参数 –XX:SurvivorRatio 来设定 )，即： Eden &#x3D; 8&#x2F;10 的新生代空间大小，from &#x3D; to &#x3D; 1&#x2F;10 的新生代空间大小。 JVM 每次只会使用 Eden 和其中的一块 Survivor 区域来为对象服务，所以无论什么时候，总是有一块 Survivor 区域是空闲着的。因此，新生代实际可用的内存空间为 9&#x2F;10 ( 即90% )的新生代空间。 对象分配过程JVM设计者不仅需要考虑到内存如何分配，在哪里分配等问题，并且由于内存分配算法与内存回收算法密切相关，因此还需要考虑GC执行完内存回收后是否存在空间中间产生内存碎片。 分配过程： new的对象先放在伊甸园区。该区域有大小限制 当伊甸园区域填满时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园预期进行垃圾回收（Minor GC）,将伊甸园区域中不再被其他对象引用的额对象进行销毁，再加载新的对象放到伊甸园区 然后将伊甸园区中的剩余对象移动到幸存者0区 如果再次触发垃圾回收，此时上次幸存下来的放在幸存者0区的，如果没有回收，就会放到幸存者1区 如果再次经历垃圾回收，此时会重新返回幸存者0区，接着再去幸存者1区。 如果累计次数到达默认的15次，这会进入养老区。可以通过设置参数，调整阈值 -XX:MaxTenuringThreshold&#x3D;N 养老区内存不足时,会再次触发GC:Major GC 进行养老区的内存清理 如果养老区执行了Major GC后仍然没有办法进行对象的保存,就会报OOM异常. GC 堆Java 中的堆也是 GC 收集垃圾的主要区域。GC 分为两种：一种是部分收集器（Partial GC）另一类是整堆收集器（Fu’ll GC） 部分收集器: 不是完整收集java堆的的收集器,它又分为: 新生代收集（Minor GC &#x2F; Young GC）: 只是新生代的垃圾收集 老年代收集 （Major GC &#x2F; Old GC）: 只是老年代的垃圾收集 (CMS GC 单独回收老年代) 混合收集（Mixed GC）:收集整个新生代及老年代的垃圾收集 (G1 GC会混合回收, region区域回收) 整堆收集（Full GC）:收集整个java堆和方法区的垃圾收集器 年轻代GC触发条件: 年轻代空间不足,就会触发Minor GC， 这里年轻代指的是Eden代满，Survivor不满不会引发GCMinor GC会引发STW(stop the world) ,暂停其他用户的线程,等垃圾回收接收,用户的线程才恢复. 老年代GC (Major GC)触发条件： 老年代空间不足时,会尝试触发MinorGC. 如果空间还是不足,则触发Major GC。如果Major GC , 内存仍然不足,则报错OOM，Major GC的速度比Minor GC慢10倍以上. FullGC 触发条件: 调用System.gc() , 系统会执行Full GC ,不是立即执行. 老年代空间不足 方法区空间不足 通过Minor GC进入老年代平均大小大于老年代可用内存 元空间在JDK1.7之前，HotSpot 虚拟机把方法区当成永久代来进行垃圾回收。而从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。 HotSpots取消了永久代，那么是不是也就没有方法区了呢？当然不是，方法区是一个规范，规范没变，它就一直在，只不过取代永久代的是元空间（Metaspace）而已。 它和永久代有什么不同的？ 存储位置不同：永久代在物理上是堆的一部分，和新生代、老年代的地址是连续的，而元空间属于本地内存。 存储内容不同：在原来的永久代划分中，永久代用来存放类的元数据信息、静态变量以及常量池等。现在类的元信息存储在元空间中，静态变量和常量池等并入堆中，相当于原来的永久代中的数据，被元空间和堆内存给瓜分了。 为什么要废弃永久代，引入元空间？ 相比于之前的永久代划分，Oracle为什么要做这样的改进呢？ 在原来的永久代划分中，永久代需要存放类的元数据、静态变量和常量等。它的大小不容易确定，因为这其中有很多影响因素，比如类的总数，常量池的大小和方法数量等，-XX:MaxPermSize 指定太小很容易造成永久代内存溢出。 移除永久代是为融合HotSpot VM与 JRockit VM而做出的努力，因为JRockit没有永久代，不需要配置永久代 永久代会为GC带来不必要的复杂度，并且回收效率偏低。 废除永久代的好处 由于类的元数据分配在本地内存中，元空间的最大可分配空间就是系统可用内存空间。不会遇到永久代存在时的内存溢出错误。 将运行时常量池从PermGen分离出来，与类的元数据分开，提升类元数据的独立性。 将元数据从PermGen剥离出来到Metaspace，可以提升对元数据的管理同时提升GC效率。 Metaspace相关参数 -XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 -XX:MaxMetaspaceSize，最大空间，默认是没有限制的。如果没有使用该参数来设置类的元数据的大小，其最大可利用空间是整个系统内存的可用空间。JVM也可以增加本地内存空间来满足类元数据信息的存储。但是如果没有设置最大值，则可能存在bug导致Metaspace的空间在不停的扩展，会导致机器的内存不足；进而可能出现swap内存被耗尽；最终导致进程直接被系统直接kill掉。如果设置了该参数，当Metaspace剩余空间不足，会抛出：java.lang.OutOfMemoryError: Metaspace space -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集 -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 方法区方法区（Method Area） 与Java堆一样， 是各个线程共享的内存区域， 它用于存储已被虚拟机加载 的类型信息、常量、 静态变量、 即时编译器编译后的代码缓存等数据。 《Java虚拟机规范》中明确说明：“尽管所有的方法区在逻辑上是属于堆的一部分，但些简单的实现可能不会选择去进行垃圾收集或者进行压缩”。对HotSpot而言，方法区还有一个别名叫做Non-Heap（非堆），的就是要和堆分开。 元空间、永久代是方法区具体的落地实现。方法区看作是一块独立于Java堆的内存空间，它主要是用来存储所加载的类信息的 方法区的特点 方法区与堆一样是各个线程共享的内存区域 方法区在JVM启动的时候就会被创建并且它实例的物理内存空间和Java堆一样都可以不连续 方法区的大小跟堆空间一样 可以选择固定大小或者动态变化 方法区的对象决定了系统可以保存多少个类,如果系统定义了太多的类 导致方法区溢出虚拟机同样会跑出(OOM)异常(Java7之前是 PermGen Space (永久带) Java 8之后 是MetaSpace(元空间) ) 关闭JVM就会释放这个区域的内存 方法区结构 类加载器将Class文件加载到内存之后，将类的信息存储到方法区中。 方法区中存储的内容： 类型信息（域信息、方法信息） 运行时常量池 类型信息 对每个加载的类型（类Class、接口 interface、枚举enum、注解 annotation），JVM必须在方法区中存储以下类型信息： 这个类型的完整有效名称（全名 &#x3D; 包名.类名） 这个类型直接父类的完整有效名（对于 interface或是java.lang. Object，都没有父类） 这个类型的修饰符（ public, abstract，final的某个子集） 这个类型直接接口的一个有序列表 域信息 域信息，即为类的属性，成员变量。JVM必须在方法区中保存类所有的成员变量相关信息及声明顺序。 域的相关信息包括：域名称、域类型、域修饰符（pυblic、private、protected、static、final、volatile、transient的某个子集） 方法信息 JVM必须保存所有方法的以下信息，同域信息一样包括声明顺序： 方法名称方法的返回类型（或void） 方法参数的数量和类型（按顺序） 方法的修饰符public、private、protected、static、final、synchronized、native,、abstract的一个子集 方法的字节码bytecodes、操作数栈、局部变量表及大小（ abstract和native方法除外） 异常表（ abstract和 native方法除外）。每个异常处理的开始位置、结束位置、代码处理在程序计数器中的偏移地址、被捕获的异常类的常量池索引 方法区设置方法区的大小不必是固定的，JVM可以根据应用的需要动态调整。 jdk7及以前 通过-xx:Permsize来设置永久代初始分配空间。默认值是20.75M -XX:MaxPermsize来设定永久代最大可分配空间。32位机器默认是64M，64位机器模式是82M 当JVM加载的类信息容量超过了这个值，会报异常OutofMemoryError:PermGen space。 查看JDK PermSpace区域默认大小 123jps #是java提供的一个显示当前所有java进程pid的命令jinfo -flag PermSize 进程号 #查看进程的PermSize初始化空间大小jinfo -flag MaxPermSize 进程号 #查看PermSize最大空间 JDK8以后 元数据区大小可以使用参数 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize指定 默认值依赖于平台。windows下，-XX:MetaspaceSize是21M，-XX:MaxMetaspaceSize的值是-1，即没有限制。 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存。如果元数据区发生溢出，虚拟机一样会抛出异常OutOfMemoryError:Metaspace -XX:MetaspaceSize：设置初始的元空间大小。对于一个64位的服务器端JVM来说，其默认的-xx:MetaspaceSize值为21MB。这就是初始的高水位线，一旦触及这个水位线，FullGC将会被触发并卸载没用的类（即这些类对应的类加载器不再存活）然后这个高水位线将会置。新的高水位线的值取决于GC后释放了多少元空间。如果释放的空间不足，那么在不超过MaxMetaspaceSize时，适当提高该值。如果释放空间过多，则适当降低该值。 如果初始化的高水位线设置过低，上述高水位线调整情况会发生很多次。通过垃圾回收器的日志可以观察到FullGC多次调用。为了避免频繁地GC，建议将-XX:MetaspaceSize设置为一个相对较高的值。 123456jps #是java提供的一个显示当前所有java进程pid的命令jinfo -flag PermSize 进程号 #查看进程的PermSize初始化空间大小jinfo -flag MaxPermSize 进程号 #查看PermSize最大空间jps #查看进程号jinfo -flag MetaspaceSize 进程号 #查看Metaspace 最大分配内存空间jinfo -flag MaxMetaspaceSize 进程号 #查看Metaspace最大空间 运行时常量池字节码文件中，内部包含了常量池。方法区中，内部包含了运行时常量池 常量池：存放编译期间生成的各种字面量与符号引用 运行时常量池：常量池表在运行时的表现形式 编译后的字节码文件中包含了类型信息、域信息、方法信息等。通过ClassLoader将字节码文件的常量池中的信息加载到内存中，存储在了方法区的运行时常量池中。 可以理解为字节码中的常量池 Constant pool 只是文件信息，它想要执行就必须加载到内存中。而Java程序是靠JVM，更具体的来说是JVM的执行引擎来解释执行的。执行引擎在运行时常量池中取数据，被加载的字节码常量池中的信息是放到了方法区的运行时常量池中。它们不是一个概念，存放的位置是不同的。一个在字节码文件中，一个在方法区中。 对字节码文件反编译之后，查看常量池相关信息： 要弄清楚方法区的运行时常量池，需要理解清楚字节码中的常量池。 一个有效的字节码文件中除了包含类的版本信息、字段、方法以及接口等描述信息外，还包含一项信息那就是常量池表（ Constant pool table），包括各种字面量和对类型、域和方法的符号引用。常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名、方法名、参数类型、字面量等类型。 常量池表Constant pool table： 在方法中对常量池表的符号引用 为什么需要常量池？ 举例来说： 12345public class Solution &#123; public void method() &#123; System.out.println(&quot;are you ok&quot;); &#125;&#125; 这段代码很简单，但是里面却使用了 String、 System、 PrintStream及Object等结构。如果代码多，引用到的结构会更多！这里就需要常暈池，将这些引用转变为符号引用，具体用到时，采取加载。 直接内存直接内存（Direct Memory） 并不是虚拟机运行时数据区的一部分。 在JDK 1.4中新加入了NIO（New Input&#x2F;Output） 类， 引入了一种基于通道（Channel） 与缓冲区 （Buer） 的I&#x2F;O方式， 它可以使用Native函数库直接分配堆外内存， 然后通过一个存储在Java堆里面的 DirectByteBuer对象作为这块内存的引用进行操作。 这样能在一些场景中显著提高性能， 因为避免了 在Java堆和Native堆中来回复制数据。 NIO的Buer提供一个可以直接访问系统物理内存的类——DirectBuer。DirectBuer类继承自ByteBuer，但和普通的ByteBuer不同。普通的ByteBuer仍在JVM堆上分配内存，其最大内存受到最大堆内存的 限制。而DirectBuer直接分配在物理内存中，并不占用堆空间。在访问普通的ByteBuer时，系统总是会使用一个“内核缓冲区”进行操作。 而DirectBuer所处的位置，就相当于这个“内核缓冲区”。因此，使用DirectBuer是一种更加接近内存底层的方法，所以它的速度比普通的ByteBuer更快。 通过使用堆外内存，可以带来以下好处： 改善堆过大时垃圾回收效率，减少停顿。Full GC时会扫描堆内存，回收效率和堆大小成正比。Native的内存，由OS负责管理和回收。 减少内存在Native堆和JVM堆拷贝过程，避免拷贝损耗，降低内存使用。 可突破JVM内存大小限制。 OOM 常见异常Java堆溢出堆内存中主要存放对象、数组等，只要不断地创建这些对象，并且保证 GC Roots 到对象之间有可达路径来避免垃圾收集回收机制清除这些对象，当这些对象所占空间超过最大堆容量时，就会产生 OutOfMemoryError 的异常。堆内存异常示例如下： 12345678910111213/*** 设置最大堆最小堆：-Xms20m -Xmx20m*/public class HeapOOM &#123; static class OOMObject &#123; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; oomObjectList = new ArrayList&lt;&gt;(); while (true) &#123; oomObjectList.add(new OOMObject()); &#125; &#125;&#125; 运行后会报异常，在堆栈信息中可以看到 java.lang.OutOfMemoryError: Java heap space 的信息，说明在堆内存空间产生内存溢出的异常。 新产生的对象最初分配在新生代，新生代满后会进行一次 Minor GC ，如果 Minor GC 后空间不足会把该对象和新生代满足条件的对象放入老年代，老年代空间不足时会进行 Full GC ，之后如果空间还不足以存放新对象则抛出 OutOfMemoryError 异常。 常见原因： 内存中加载的数据过多，如一次从数据库中取出过多数据； 集合对对象引用过多且使用完后没有清空； 代码中存在死循环或循环产生过多重复对象； 堆内存分配不合理 虚拟机栈和本地方法栈溢出由于HotSpot虚拟机中并不区分虚拟机栈和本地方法栈， 因此对于HotSpot来说， -Xoss参数（设置本地方法栈大小） 虽然存在， 但实际上是没有任何效果的， 栈容量只能由-Xss参数来设定。 关于虚拟机栈和本地方法栈， 在《Java虚拟机规范》 中描述了两种异常： 如果线程请求的栈深度大于虚拟机所允许的最大深度， 将抛出StackOverflowError异常。 如果虚拟机的栈内存允许动态扩展， 当扩展栈容量无法申请到足够的内存时， 将抛出 OutOfMemoryError异常。 《Java虚拟机规范》 明确允许Java虚拟机实现自行选择是否支持栈的动态扩展， 而HotSpot虚拟机的选择是不支持扩展， 所以除非在创建线程申请内存时就因无法获得足够内存而出现 OutOfMemoryError异常， 否则在线程运行时是不会因为扩展而导致内存溢出的， 只会因为栈容量无法容纳新的栈帧而导致StackOverflowError异常。 运行时常量池和方法区溢出前面曾经提到HotSpot从JDK 7开始逐步“去永久代”的计划， 并在JDK 8中完全使用元空间来代替永久代的背景故事， 在此我们就以测试代码来观察一下， 使用“永久代”还是“元空间”来实现方法区， 对程序有什么 实际的影响。 String::intern()是一个本地方法， 它的作用是如果字符串常量池中已经包含一个等于此String对象的 字符串， 则返回代表池中这个字符串的String对象的引用； 否则， 会将此String对象包含的字符串添加到常量池中， 并且返回此String对象的引用。 在JDK 6或更早之前的HotSpot虚拟机中， 常量池都是分配在永久代中， 我们可以通过-XX：PermSize和-XX： MaxPermSize限制永久代的大小， 即可间接限制其中常量池的容量， 具体实现如代码清单 12345678910111213// OOM异常一：Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.base/java.lang.Integer.toString(Integer.java:440) at java.base/java.lang.String.valueOf(String.java:3058) at RuntimeConstantPoolOOM.main(RuntimeConstantPoolOOM.java:12)// OOM异常二：//根据Oracle官方文档，默认情况下，如果Java进程花费98%以上的时间执行GC，并且每次只有不到2%的堆被恢复，则JVM抛出此错误Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded at java.lang.Integer.toString(Integer.java:401) at java.lang.String.valueOf(String.java:3099) at com.lagou.unit.RuntimeConstantPoolOOM.main(RuntimeConstantPoolOOM.java:17) 方法区的其他部分的内容， 方法区的主要职责是用于存放类型的相关信息， 如类名、 访问修饰符、 常量池、 字段描述、 方法描述等。 对于这部分区域的测试， 基本的思路是运行时产 生大量的类去填满方法区， 直到溢出为止。 虽然直接使用Java SE API也可以动态产生类（如反射时的 GeneratedConstructorAccessor和动态代理等） ， 但在本次实验中操作起来比较麻烦。 在代码清单 借助CGLib使得方法区出现内存溢出异常 123456789101112131415161718192021/*** VM Args： -XX:PermSize=10M -XX:MaxPermSize=10M*/public class JavaMethodAreaOOM &#123; public static void main(String[] args) &#123; while (true) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() &#123; public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws ThrowException&#123; return proxy.invokeSuper(obj, args); &#125; &#125;); enhancer.create(); &#125; &#125; static class OOMObject &#123; &#125;&#125; 在JDK 6中的运行结果 12345Caused by: java.lang.OutOfMemoryError: PermGen spaceat java.lang.ClassLoader.defineClass1(Native Method)at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632)at java.lang.ClassLoader.defineClass(ClassLoader.java:616)... 8 more 方法区溢出也是一种常见的内存溢出异常， 一个类如果要被垃圾收集器回收， 要达成的条件是比较苛刻的。 在经常运行时生成大量动态类的应用场景里， 就应该特别关注这些类的回收状况。 这类场 景除了之前提到的程序使用了CGLib字节码增强和动态语言外， 常见的还有： 大量JSP或动态产生JSP 文件的应用（JSP第一次运行时需要编译为Java类） 、 基于OSGi的应用（即使是同一个类文件， 被不同 的加载器加载也会视为不同的类） 等。 在JDK 8以后， 永久代便完全退出了历史舞台， 元空间作为其替代者登场。 在默认设置下， 前面列举的那些正常的动态创建新类型的测试用例已经很难再迫使虚拟机产生方法区的溢出异常了。 不过 为了让使用者有预防实际应用里出现破坏性的操作， HotSpot还是提供了一 些参数作为元空间的防御措施， 主要包括： -XX： MaxMetaspaceSize： 设置元空间最大值， 默认是-1， 即不限制， 或者说只受限于本地内存 大小。 -XX： MetaspaceSize： 指定元空间的初始空间大小， 以字节为单位， 达到该值就会触发垃圾收集 进行类型卸载，同时收集器会对该值进行调整： 如果释放了大量的空间， 就适当降低该值； 如果释放 了很少的空间， 那么在不超过-XX： MaxMetaspaceSize（如果设置了的话） 的情况下， 适当提高该值。 -XX： MinMetaspaceFreeRatio： 作用是在垃圾收集之后控制最小的元空间剩余容量的百分比， 可 减少因为元空间不足导致的垃圾收集的频率。 类似的还有-XX： Max-MetaspaceFreeRatio， 用于控制最 大的元空间剩余容量的百分比。 直接内存溢出直接内存（Direct Memory） 的容量大小可通过-XX： MaxDirectMemorySize参数来指定， 如果不去指定， 则默认与Java堆最大值（由-Xmx指定） 一致， 越过了DirectByteBuer类直接通 过反射获取Unsafe实例进行内存分配（Unsafe类的getUnsafe()方法指定只有引导类加载器才会返回实例， 体现了设计者希望只有虚拟机标准类库里面的类才能使用Unsafe的功能， 在JDK 10时才将Unsafe 的部分功能通过VarHandle开放给外部使用） ， 因为虽然使用DirectByteBuer分配内存也会抛出内存溢出异常， 但它抛出异常时并没有真正向操作系统申请分配内存， 而是通过计算得知内存无法分配就会 在代码里手动抛出溢出异常， 真正申请分配内存的方法是Unsafe::allocateMemory() 1234567891011121314151617181920/*** VM Args： -Xmx20M -XX:MaxDirectMemorySize=10M*/public class DirectMemoryOOM &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) throws Exception &#123; Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) &#123; unsafe.allocateMemory(_1MB); &#125; &#125;&#125;Output:Exception in thread &quot;main&quot; java.lang.OutOfMemoryErrorat sun.misc.Unsafe.allocateMemory(Native Method)at org.fenixsoft.oom.DMOOM.main(DMOOM.java:20) 由直接内存导致的内存溢出， 一个明显的特征是在Heap Dump文件中不会看见有什么明显的异常 情况， 如果发现内存溢出之后产生的Dump文件很小， 而程序中又直接或间接使用了 DirectMemory（典型的间接使用就是NIO） ，那就可以考虑重点检查一下直接内存方面的原因了。","tags":["Java","JVM","Java 虚拟机","内存管理"],"categories":["Java","JVM"]},{"title":"JVM 基础知识","path":"//blog/java/jvm/jvm/","content":"什么是JVMJVM是Java Virtual Machine（Java虚拟机）的缩写，JVM是一种用于计算设备的规范，它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。 主流的JVM 虚拟机名称 介绍 HotSpot Oracle&#x2F;Sun JDK和OpenJDK都使用HotSPot VM的相同核心 J9 J9是IBM开发的高度模块化的JVM JRockit JRockit 与 HotSpot 同属于 Oracle，目前为止 Oracle 一直在推进 HotSpot 与 JRockit 两款各有优势的虚拟机进行融合互补 Zing 由Azul Systems根据HostPot为基础改进的高性能低延迟的JVM Dalvik Android上的Dalvik 虽然名字不叫JVM，但骨子里就是不折不扣的JVM JVM 与 操作系统的关系首选我们考虑这样一个问题，为什么要在程序和操作系统中间添加一个JVM？ Java 是一门抽象程度特别高的语言，提供了自动内存管理等一系列的特性。这些特性直接在操作系统上实现是不太可能的，所以就需要 JVM 进行一番转换。 从上图中可以看到，有了 JVM 这个抽象层之后，Java 就可以实现跨平台了。JVM 只需要保证能够正确执行 .class 文件，就可以运行在诸如 Linux、Windows、MacOS 等平台上了。 而 Java 跨平台的意义在于一次编译，处处运行，能够做到这一点 JVM 功不可没。比如我们在 Maven 仓库下载同一版本的 jar 包就可以到处运行，不需要在每个平台上再编译一次。 现在的一些 JVM 的扩展语言，比如 Clojure、JRuby、Groovy 等，编译到最后都是 .class 文件，Java 语言的维护者，只需要控制好 JVM 这个解析器，就可以将这些扩展语言无缝的运行在 JVM 之上了。 应用程序、JVM、操作系统之间的关系如下图所示： 我们用一句话概括 JVM 与操作系统之间的关系：JVM 上承开发语言，下接操作系统，它的中间接口就是字节码。 JVM JRE JDK的关系 JVM 是 Java 程序能够运行的核心。但是需要注意，JVM 自己什么也干不了，你需要给它提供生产原料（.class 文件）。仅仅是 JVM，是无法完成一次编译，处处运行的。它需要一个基本的类库，比如怎么操作文件、怎么连接网络等。 而 Java 体系很慷慨，会一次性将 JVM 运行所需的类库都传递给它。JVM 标准加上实现的一大堆基础类库，就组成了 Java 的运行时环境，也就是我们常说的 JRE（Java Runtime Environment）对于 JDK 来说，就更庞大了一些。除了 JRE，JDK 还提供了一些非常好用的小工具，比如 javac、java、jar 等。它是 Java 开发的核心，让外行也可以炼剑！ JVM、JRE、JDK 它们三者之间的关系，可以用一个包含关系表示。 java虚拟机规范和java语言规范的关系 左半部分是 Java 虚拟机规范，其实就是为输入和执行字节码提供一个运行环境。右半部分是我们常说的 Java 语法规范，比如 switch、for、泛型、lambda 等相关的程序，最终都会编译成字节码。而连接左右两部分的桥梁依然是Java 的字节码。 如果 .class 文件的规格是不变的，这两部分是可以独立进行优化的。但 Java 也会偶尔扩充一下 .class 文件的格式，增加一些字节码指令，以便支持更多的特性。 我们可以把 Java 虚拟机可以看作是一台抽象的计算机，它有自己的指令集以及各种运行时内存区域。 最后，我们简单看一下一个 Java 程序的执行过程，它到底是如何运行起来的。 这里的 Java 程序是文本格式的。比如下面这段 HelloWorld.java，它遵循的就是 Java 语言规范。其中，我们调用了System.out 等模块，也就是 JRE 里提供的类库。 12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println(&quot;Hello World&quot;); &#125;&#125; 使用 JDK 的工具 javac 进行编译后，会产生 HelloWorld 的字节码。 我们一直在说 Java 字节码是沟通 JVM 与 Java 程序的桥梁，下面使用 javap 来稍微看一下字节码到底长什么样子。 12345670 getstatic #2 &lt;java/lang/System.out&gt; // getstatic 获取静态字段的值3 ldc #3 &lt;Hello World&gt; // ldc 常量池中的常量值入栈5 invokevirtual #4 &lt;java/io/PrintStream.println&gt; // invokevirtual 运行时方法绑定调用方法8 return //void 函数返回 Java 虚拟机采用基于栈的架构，其指令由操作码和操作数组成。这些 字节码指令 ，就叫作 opcode。其中，getstatic、ldc、invokevirtual、return 等，就是 opcode，可以看到是比较容易理解的。 JVM 就是靠解析这些 opcode 和操作数来完成程序的执行的。当我们使用 Java 命令运行 .class 文件的时候，实际上就相当于启动了一个 JVM 进程。 然后 JVM 会翻译这些字节码，它有两种执行方式。常见的就是解释执行，将 opcode + 操作数翻译成机器代码；另外一种执行方式就是 JIT，也就是我们常说的即时编译，它会在一定条件下将字节码编译成机器码之后再执行。","tags":["Java","JVM","Java 虚拟机"],"categories":["Java","JVM"]},{"title":"撸一个JSON解析器","path":"//blog/JSONParser/","content":"JSON(JavaScript Object Notation, JS 对象简谱) 是一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。采用完全独立于语言的文本格式，但是也使用了类似于 C 语言家族的习惯（包括 C, C++, C#, Java, JavaScript, Perl, Python 等）。这些特性使 JSON 成为理想的数据交换语言。 JSON 与 JS 的区别以及和 XML 的区别具体请参考百度百科 JSON 有两种结构： 第一种：对象 “名称 &#x2F; 值” 对的集合不同的语言中，它被理解为对象（object），纪录（record），结构（struct），字典（dictionary），哈希表（hash table），有键列表（keyed list），或者关联数组 （associative array）。 对象是一个无序的 “‘名称 &#x2F; 值’对” 集合。一个对象以 “{”（左括号）开始，“}”（右括号）结束。每个“名称” 后跟一个 “:”（冒号）；“‘名称 &#x2F; 值’ 对” 之间使用“,”（逗号）分隔。 1&#123;&quot;姓名&quot;: &quot;张三&quot;, &quot;年龄&quot;: &quot;18&quot;&#125; 第二种：数组 值的有序列表（An ordered list of values）。在大部分语言中，它被理解为数组（array）。 数组是值（value）的有序集合。一个数组以 “[”（左中括号）开始，“]”（右中括号）结束。值之间使用 “,”（逗号）分隔。 值（value）可以是双引号括起来的字符串（string）、数值 (number)、true、false、 null、对象（object）或者数组（array）。这些结构可以嵌套。 12345678910111213[ &#123; &quot;姓名&quot;: &quot;张三&quot;, &quot;年龄&quot;:&quot;18&quot; &#125;, &#123; &quot;姓名&quot;: &quot;里斯&quot;, &quot;年龄&quot;:&quot;19&quot; &#125;] 通过上面的了解可以看出，JSON 存在以下几种数据类型（以 Java 做类比）： json java string Java 中的 String number Java 中的 Long 或 Double true&#x2F;false Java 中的 Boolean null Java 中的 null [array] Java 中的 List 或 Object[] {“key”:”value”} Java 中的 Map&lt;String, Object&gt; 解析 JSON输入一串 JSON 字符串，输出一个 JSON 对象。 步骤 JSON 解析的过程主要分以下两步： 第一步： 对于输入的一串 JSON 字符串我们需要将其解析成一组 token 流。 例如 JSON 字符串 {“姓名”: “张三”, “年龄”: “18”} 我们需要将它解析成 1&#123;、 姓名、 :、 张三、 ,、 年龄、 :、 18、 &#125; 这样一组 token 流 第二步：根据得到的 token 流将其解析成对应的 JSON 对象（JSONObject）或者 JSON 数组（JSONArray） 下面我们来详细分析下这两个步骤： 获取 token 流根据 JSON 格式的定义，token 可以分为以下几种类型 token 含义 NULL null NUMBER 数字 STRING 字符串 BOOLEAN true&#x2F;false SEP_COLON : SEP_COMMA , BEGIN_OBJECT { END_OBJECT } BEGIN_ARRAY [ END_ARRAY ] END_DOCUMENT 表示 JSON 数据结束 根据以上的 JSON 类型，我们可以将其封装成 enum 类型的 TokenType 123456789101112131415161718192021222324252627282930313233343536373839package com.json.demo.tokenizer;/** BEGIN_OBJECT（&#123;） END_OBJECT（&#125;） BEGIN_ARRAY（[） END_ARRAY（]） NULL（null） NUMBER（数字） STRING（字符串） BOOLEAN（true/false） SEP_COLON（:） SEP_COMMA（,） END_DOCUMENT（表示JSON文档结束） */public enum TokenType &#123; BEGIN_OBJECT(1), END_OBJECT(2), BEGIN_ARRAY(4), END_ARRAY(8), NULL(16), NUMBER(32), STRING(64), BOOLEAN(128), SEP_COLON(256), SEP_COMMA(512), END_DOCUMENT(1024); private int code; // 每个类型的编号 TokenType(int code) &#123; this.code = code; &#125; public int getTokenCode() &#123; return code; &#125;&#125; 在 TokenType 中我们为每一种类型都赋一个数字，目的是在 Parser 做一些优化操作（通过位运算来判断是否是期望出现的类型） 在进行第一步之前 JSON 串对计算机来说只是一串没有意义的字符而已。第一步的作用就是把这些无意义的字符串变成一个一个的 token，上面我们已经为每一种 token 定义了相应的类型和值。所以计算机能够区分不同的 token，并能以 token 为单位解读 JSON 数据。 下面我们封装一个 token 类来存储每一个 token 对应的值 12345678910111213141516171819202122232425262728293031323334353637383940package com.json.demo.tokenizer;/** * 存储对应类型的字面量 */public class Token &#123; private TokenType tokenType; private String value; public Token(TokenType tokenType, String value) &#123; this.tokenType = tokenType; this.value = value; &#125; public TokenType getTokenType() &#123; return tokenType; &#125; public void setTokenType(TokenType tokenType) &#123; this.tokenType = tokenType; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; @Override public String toString() &#123; return &quot;Token&#123;&quot; + &quot;tokenType=&quot; + tokenType + &quot;, value=&#x27;&quot; + value + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 在解析的过程中我们通过字符流来不断的读取字符，并且需要经常根据相应的字符来判断状态的跳转。所以我们需要自己封装一个 ReaderChar 类，以便我们更好的操作字符流。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.json.demo.tokenizer;import java.io.IOException;import java.io.Reader;public class ReaderChar &#123; private static final int BUFFER_SIZE = 1024; private Reader reader; private char[] buffer; private int index; // 下标 private int size; public ReaderChar(Reader reader) &#123; this.reader = reader; buffer = new char[BUFFER_SIZE]; &#125; /** * 返回 pos 下标处的字符，并返回 * @return */ public char peek() &#123; if (index - 1 &gt;= size) &#123; return (char) -1; &#125; return buffer[Math.max(0, index - 1)]; &#125; /** * 返回 pos 下标处的字符，并将 pos + 1，最后返回字符 * @return * @throws IOException */ public char next() throws IOException &#123; if (!hasMore()) &#123; return (char) -1; &#125; return buffer[index++]; &#125; /** * 下标回退 */ public void back() &#123; index = Math.max(0, --index); &#125; /** * 判断流是否结束 */ public boolean hasMore() throws IOException &#123; if (index &lt; size) &#123; return true; &#125; fillBuffer(); return index &lt; size; &#125; /** * 填充buffer数组 * @throws IOException */ void fillBuffer() throws IOException &#123; int n = reader.read(buffer); if (n == -1) &#123; return; &#125; index = 0; size = n; &#125;&#125; 另外我们还需要一个 TokenList 来存储解析出来的 token 流 12345678910111213141516171819202122232425262728293031323334353637383940package com.json.demo.tokenizer;import java.util.ArrayList;import java.util.List;/** * 存储词法解析所得的token流 */public class TokenList &#123; private List&lt;Token&gt; tokens = new ArrayList&lt;Token&gt;(); private int index = 0; public void add(Token token) &#123; tokens.add(token); &#125; public Token peek() &#123; return index &lt; tokens.size() ? tokens.get(index) : null; &#125; public Token peekPrevious() &#123; return index - 1 &lt; 0 ? null : tokens.get(index - 2); &#125; public Token next() &#123; return tokens.get(index++); &#125; public boolean hasMore() &#123; return index &lt; tokens.size(); &#125; @Override public String toString() &#123; return &quot;TokenList&#123;&quot; + &quot;tokens=&quot; + tokens + &#x27;&#125;&#x27;; &#125;&#125; JSON 解析比其他文本解析要简单的地方在于，我们只需要根据下一个字符就可知道接下来它所期望读取的到的内容是什么样的。如果满足期望了，则返回 Token，否则返回错误。 为了方便程序出错时更好的 debug，程序中自定义了两个 exception 类来处理错误信息。（具体实现参考 exception 包） 下面就是第一步中的重头戏（核心代码）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public TokenList getTokenStream(ReaderChar readerChar) throws IOException &#123; this.readerChar = readerChar; tokenList = new TokenList(); // 词法解析，获取token流 tokenizer(); return tokenList; &#125; /** * 将JSON文件解析成token流 * @throws IOException */ private void tokenizer() throws IOException &#123; Token token; do &#123; token = start(); tokenList.add(token); &#125; while (token.getTokenType() != TokenType.END_DOCUMENT); &#125; /** * 解析过程的具体实现方法 * @return * @throws IOException * @throws JsonParseException */ private Token start() throws IOException, JsonParseException &#123; char ch; while (true)&#123; //先读一个字符，若为空白符（ASCII码在[0, 20H]上）则接着读，直到刚读的字符非空白符 if (!readerChar.hasMore()) &#123; return new Token(TokenType.END_DOCUMENT, null); &#125; ch = readerChar.next(); if (!isWhiteSpace(ch)) &#123; break; &#125; &#125; switch (ch) &#123; case &#x27;&#123;&#x27;: return new Token(TokenType.BEGIN_OBJECT, String.valueOf(ch)); case &#x27;&#125;&#x27;: return new Token(TokenType.END_OBJECT, String.valueOf(ch)); case &#x27;[&#x27;: return new Token(TokenType.BEGIN_ARRAY, String.valueOf(ch)); case &#x27;]&#x27;: return new Token(TokenType.END_ARRAY, String.valueOf(ch)); case &#x27;,&#x27;: return new Token(TokenType.SEP_COMMA, String.valueOf(ch)); case &#x27;:&#x27;: return new Token(TokenType.SEP_COLON, String.valueOf(ch)); case &#x27;n&#x27;: return readNull(); case &#x27;t&#x27;: case &#x27;f&#x27;: return readBoolean(); case &#x27;&quot;&#x27;: return readString(); case &#x27;-&#x27;: return readNumber(); &#125; if (isDigit(ch)) &#123; return readNumber(); &#125; throw new JsonParseException(&quot;Illegal character&quot;); &#125; 在 start 方法中，我们将每个处理方法都封装成了单独的函数。主要思想就是通过一个死循环不停的读取字符，然后再根据字符的期待值，执行不同的处理函数。 下面我们详解分析几个处理函数： 12345678910111213141516171819202122232425262728293031private Token readString() throws IOException &#123; StringBuilder sb = new StringBuilder(); while(true) &#123; char ch = readerChar.next(); if (ch == &#x27;\\\\&#x27;) &#123; // 处理转义字符 if (!isEscape()) &#123; throw new JsonParseException(&quot;Invalid escape character&quot;); &#125; sb.append(&#x27;\\\\&#x27;); ch = readerChar.peek(); sb.append(ch); if (ch == &#x27;u&#x27;) &#123; // 处理 Unicode 编码，形如 \\u4e2d。且只支持 \\u0000 ~ \\uFFFF 范围内的编码 for (int i = 0; i &lt; 4; i++) &#123; ch = readerChar.next(); if (isHex(ch)) &#123; sb.append(ch); &#125; else &#123; throw new JsonParseException(&quot;Invalid character&quot;); &#125; &#125; &#125; &#125; else if (ch == &#x27;&quot;&#x27;) &#123; // 碰到另一个双引号，则认为字符串解析结束，返回 Token return new Token(TokenType.STRING, sb.toString()); &#125; else if (ch == &#x27;\\r&#x27; || ch == &#x27; &#x27;) &#123; // 传入的 JSON 字符串不允许换行 throw new JsonParseException(&quot;Invalid character&quot;); &#125; else &#123; sb.append(ch); &#125; &#125; &#125; 该方法也是通过一个死循环来读取字符，首先判断的是 JSON 中的转义字符。 JSON 中允许出现的有以下几种 12345678910\\&quot;\\\\\\b\\f \\r\\t\\u four-hex-digits\\/ 具体的处理方法封装在了 isEscape() 方法中，处理 Unicode 编码时要特别注意一下 u 的后面会出现四位十六进制数。当读取到一个双引号或者读取到了非法字符（’r’或’、’n’）循环退出。 判断数字的时候也要特别小心，注意负数，frac，exp 等等情况。 通过上面的解析，我们可以得到一组 token，接下来我们需要以这组 token 作为输入，解析出相应的 JSON 对象 解析出 JSON 对象解析之前我们需要定义出 JSON 对象（JSONObject）和 JSON 数组 (JSONArray) 的实体类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.json.demo.jsonstyle;import com.json.demo.exception.JsonTypeException;import com.json.demo.util.FormatUtil;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;/** * JSON的对象形式 * 对象是一个无序的“‘名称/值’对”集合。一个对象以“&#123;”（左括号）开始，“&#125;”（右括号）结束。每个“名称”后跟一个“:”（冒号）；“‘名称/值’ 对”之间使用“,”（逗号）分隔。 */public class JsonObject &#123; private Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); public void put(String key, Object value) &#123; map.put(key, value); &#125; public Object get(String key) &#123; return map.get(key); &#125; ... &#125;package com.json.demo.jsonstyle;import com.json.demo.exception.JsonTypeException;import com.json.demo.util.FormatUtil;import java.util.ArrayList;import java.util.Iterator;import java.util.List;/** * JSON的数组形式 * 数组是值（value）的有序集合。一个数组以“[”（左中括号）开始，“]”（右中括号）结束。值之间使用“,”（逗号）分隔。 */public class JsonArray &#123; private List list = new ArrayList(); public void add(Object obj) &#123; list.add(obj); &#125; public Object get(int index) &#123; return list.get(index); &#125; public int size() &#123; return list.size(); &#125; ...&#125; 之后我们就可以写解析类了，由于代码较长，这里就不展示了。有兴趣的可以去 GitHub 上下载。实现逻辑比较简单，也易于理解。 解析类中的 parse 方法首先根据第一个 token 的类型选择调用 parseJsonObject（）或者 parseJsonArray（），进而返回 JSON 对象或者 JSON 数组。上面的解析方法中利用位运算来判断字符的期待值既提高了程序的执行效率也有助于提高代码的 ke’du’xi 完成之后我们可以写一个测试类来验证下我们的解析器的运行情况。我们可以自己定义一组 JSON 串也可以通过 HttpUtil 工具类从网上获取。最后通过 FormatUtil 类来规范我们输出。 具体效果如下图所示： 参考文章 http://www.cnblogs.com/absfre... https://www.liaoxuefeng.com/a... https://segmentfault.com/a/11... http://json.org/json-zh.html","tags":["json","json 解析器"],"categories":["json"]},{"title":"常见排序算法的 Java 实现","path":"//blog/algorithm/sort/","content":"https://www.cnblogs.com/shixiangwan/p/6724292.html 本文主要记录了几种常见的排序算法的 Java 实现，如冒泡排序、快速排序、直接插入排序、希尔排序、选择排序等等。 1. 冒泡排序将序列中所有元素两两比较，将最大的放在最后面。 将剩余序列中所有元素两两比较，将最大的放在最后面。 重复第二步，直到只剩下一个数。 123456789101112131415161718/** * 冒泡排序：两两比较，大者交换位置,则每一轮循环结束后最大的数就会移动到最后. * 时间复杂度为O(n²) 空间复杂度为O(1) */private static void bubbleSort(int[] arr) &#123; //外层循环length-1次 for (int i = 0; i &lt; arr.length-1; i++) &#123; //外层每循环一次最后都会排好一个数 //所以内层循环length-1-i次 for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125;&#125; 2. 快速排序快速排序（Quicksort）是对冒泡排序的一种改进，借用了分治的思想，由 C. A. R. Hoare 在 1962 年提出。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列. 具体步骤 快速排序使用分治策略来把一个序列（list）分为两个子序列（sub-lists）。 ①. 从数列中挑出一个元素，称为” 基准”（pivot）。 ②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 ③. 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归到最底部时，数列的大小是零或一，也就是已经排序好了。这个算法一定会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 123456789101112131415161718192021222324252627282930313233/** * 快速排序 * 时间复杂度为O(nlogn) 空间复杂度为O(1) */ public static void quickSort(int[] arr, int start, int end) &#123; if (start &lt; end) &#123; int baseNum = arr[start];//选基准值 int midNum;//记录中间值 int left = start;//左指针 int right = end;//右指针 while(left&lt;right)&#123; while ((arr[left] &lt; baseNum) &amp;&amp; left &lt; end) &#123; left++; &#125; while ((arr[right] &gt; baseNum) &amp;&amp; right &gt; start) &#123; right--; &#125; if (left &lt;= right) &#123; midNum = arr[left]; arr[left] = arr[right]; arr[right] = midNum; left++; right--; &#125; &#125; if (start &lt; right) &#123; quickSort(arr, start, right); &#125; if (end &gt; left) &#123; quickSort(arr, left, end); &#125; &#125; &#125; 3. 插入排序直接插入排序（Straight Insertion Sorting）的基本思想：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过为止。 首先设定插入次数，即循环次数，for(int i&#x3D;1;i&lt;length;i++)，1 个数的那次不用插入。 设定插入数和得到已经排好序列的最后一个数的位数。insertNum 和 j&#x3D;i-1。 从最后一个数开始向前循环，如果插入数小于当前数，就将当前数向后移动一位。 将当前数放置到空着的位置，即 j+1。 12345678910111213141516/** * 直接插入排序 * 时间复杂度O(n²) 空间复杂度O(1) */public static void straightInsertion(int[] arr) &#123; int current;//要插入的数 for (int i = 1; i &lt; arr.length; i++) &#123; //从1开始 第一次一个数不需要排序 current = arr[i]; int j = i - 1;//序列元素个数 while (j &gt;= 0 &amp;&amp; arr[j] &gt; current) &#123;//从后往前循环，将大于当前插入数的向后移动 arr[j + 1] = arr[j];//元素向后移动 j--; &#125; arr[j + 1] = current;//找到位置，插入当前元素 &#125;&#125; 4. 希尔排序是插入排序的一种高速而稳定的改进版本。 希尔排序是先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录 “基本有序” 时，再对全体记录进行依次直接插入排序。 123456789101112131415161718192021/** * 希尔排序 * 时间复杂度O(n²) 空间复杂度O(1) */public static void shellSort(int[] arr) &#123; int gap = arr.length / 2; for (; gap &gt; 0; gap = gap / 2) &#123; //不断缩小gap，直到1为止 for (int j = 0; (j + gap) &lt; arr.length; j++) &#123; //使用当前gap进行组内插入排序 for (int k = 0; (k + gap) &lt; arr.length; k += gap) &#123; if (arr[k] &gt; arr[k + gap]) &#123; //交换操作 int temp = arr[k]; arr[k] = arr[k + gap]; arr[k + gap] = temp; &#125; &#125; &#125; &#125;&#125; 5. 选择排序遍历整个序列，将最小的数放在最前面。 遍历剩下的序列，将最小的数放在最前面。 重复第二步，直到只剩下一个数。 12345678910111213141516171819/** * 选择排序 * 时间复杂度O(n²) 空间复杂度O(1) */public static void selectSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; //循环次数 int min = arr[i];//等会用来放最小值 int index = i;//用来放最小值的索引 for (int j = i + 1; j &lt; arr.length; j++) &#123; //找到最小值 if (arr[j] &lt; min) &#123; min = arr[j]; index = j; &#125; &#125; //内层循环结束后进行交换 arr[index] = arr[i];//当前值放到最小值所在位置 arr[i] = min;//当前位置放最小值 &#125;&#125; 6. 堆排序对简单选择排序的优化。 将序列构建成大顶堆。 将根节点与最后一个节点交换，然后断开最后一个节点。 重复第一、二步，直到所有节点断开。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public void heapSort(int[] a)&#123; int len=a.length; //循环建堆 for(int i=0;i&lt;len-1;i++)&#123; //建堆 buildMaxHeap(a,len-1-i); //交换堆顶和最后一个元素 swap(a,0,len-1-i); &#125; &#125; //交换方法 private void swap(int[] data, int i, int j) &#123; int tmp=data[i]; data[i]=data[j]; data[j]=tmp; &#125; //对data数组从0到lastIndex建大顶堆 private void buildMaxHeap(int[] data, int lastIndex) &#123; //从lastIndex处节点（最后一个节点）的父节点开始 for(int i=(lastIndex-1)/2;i&gt;=0;i--)&#123; //k保存正在判断的节点 int k=i; //如果当前k节点的子节点存在 while(k*2+1&lt;=lastIndex)&#123; //k节点的左子节点的索引 int biggerIndex=2*k+1; //如果biggerIndex小于lastIndex，即biggerIndex+1代表的k节点的右子节点存在 if(biggerIndex&lt;lastIndex)&#123; //若果右子节点的值较大 if(data[biggerIndex]&lt;data[biggerIndex+1])&#123; //biggerIndex总是记录较大子节点的索引 biggerIndex++; &#125; &#125; //如果k节点的值小于其较大的子节点的值 if(data[k]&lt;data[biggerIndex])&#123; //交换他们 swap(data,k,biggerIndex); //将biggerIndex赋予k，开始while循环的下一次循环，重新保证k节点的值大于其左右子节点的值 k=biggerIndex; &#125;else&#123; break; &#125; &#125; &#125; &#125; 7. 归并排序速度仅次于快速排序，内存少的时候使用，可以进行并行计算的时候使用。 选择相邻两个数组成一个有序序列。 选择相邻的两个有序序列组成一个有序序列。 重复第二步，直到全部组成一个有序序列。 1234567891011121314151617181920212223242526272829303132333435363738public void mergeSort(int[] a, int left, int right) &#123; int t = 1;// 每组元素个数 int size = right - left + 1; while (t &lt; size) &#123; int s = t;// 本次循环每组元素个数 t = 2 * s; int i = left; while (i + (t - 1) &lt; size) &#123; merge(a, i, i + (s - 1), i + (t - 1)); i += t; &#125; if (i + (s - 1) &lt; right) merge(a, i, i + (s - 1), right); &#125; &#125; private static void merge(int[] data, int p, int q, int r) &#123; int[] B = new int[data.length]; int s = p; int t = q + 1; int k = p; while (s &lt;= q &amp;&amp; t &lt;= r) &#123; if (data[s] &lt;= data[t]) &#123; B[k] = data[s]; s++; &#125; else &#123; B[k] = data[t]; t++; &#125; k++; &#125; if (s == q + 1) B[k++] = data[t++]; else B[k++] = data[s++]; for (int i = p; i &lt;= r; i++) data[i] = B[i]; &#125; 8. 基数排序用于大量数，很长的数进行排序时。 将所有的数的个位数取出，按照个位数进行排序，构成一个序列。 将新构成的所有的数的十位数取出，按照十位数进行排序，构成一个序列。 123456789101112131415161718192021222324252627282930313233343536373839404142public void baseSort(int[] a) &#123; //首先确定排序的趟数; int max = a[0]; for (int i = 1; i &lt; a.length; i++) &#123; if (a[i] &gt; max) &#123; max = a[i]; &#125; &#125; int time = 0; //判断位数; while (max &gt; 0) &#123; max /= 10; time++; &#125; //建立10个队列; List&lt;ArrayList&lt;Integer&gt;&gt; queue = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; ArrayList&lt;Integer&gt; queue1 = new ArrayList&lt;Integer&gt;(); queue.add(queue1); &#125; //进行time次分配和收集; for (int i = 0; i &lt; time; i++) &#123; //分配数组元素; for (int j = 0; j &lt; a.length; j++) &#123; //得到数字的第time+1位数; int x = a[j] % (int) Math.pow(10, i + 1) / (int) Math.pow(10, i); ArrayList&lt;Integer&gt; queue2 = queue.get(x); queue2.add(a[j]); queue.set(x, queue2); &#125; int count = 0;//元素计数器; //收集队列元素; for (int k = 0; k &lt; 10; k++) &#123; while (queue.get(k).size() &gt; 0) &#123; ArrayList&lt;Integer&gt; queue3 = queue.get(k); a[count] = queue3.get(0); queue3.remove(0); count++; &#125; &#125; &#125; &#125; 排序法平均时间最小时间最大时间稳定度额外空间备注冒泡排序O(n2)O(n)O(n2)稳定O(1)n 小时较好选择排序O(n2)O(n2)O(n2)不稳定O(1)n 小时较好插入排序O(n2)O(n)O(n2)稳定O(1)大部分已排序时较好基数排序O(logRB)O(n)O(logRB)稳定O(n)B 是真数 (0-9)，R 是基数 (个十百)Shell 排序O(nlogn)- O(ns) 不稳定O(1)s 是所选分组快速排序O(nlogn)O(n2)O(n2)不稳定O(logn)n 大时较好归并排序O(nlogn)O(nlogn)O(nlogn)稳定O(n)要求稳定性时较好堆排序O(nlogn)O(nlogn)O(nlogn)不稳定O(1)n 大时较好","tags":["algorithm","sort","排序"],"categories":["algorithm"]},{"title":"常见思维题三","path":"//blog/algorithm/algo3/","content":"赛马问题一般有这么几种问法： 25匹马5条跑道找最快的3匹马，需要跑几次？ 7 25匹马5条跑道找最快的5匹马，需要跑几次？ 8&#x2F;9 64匹马8条跑道找最快的4匹马，需要跑几次？ 11 答案25匹马5条跑道找最快的3匹马，需要跑几次？将马分成A、B、C、D、E五组。第1-5次比赛：各组分别进行比赛，决出各组名次,取每组前三名12345A1、A2、A3，B1、B2、B3，C1、C2、C3，D1、D2、D3，E1、E2、E3。第6次比赛：A1、B1、C1、D1、E1，假设得到的结果是A1、B1、C1、D1、E1，A1是跑的最快的，那么分析A组A2、A3还有希望冲进前3，B组呢？只有B2还有希望冲进前3，C组的C1还有希望冲进前3，C2并没有希望冲进前3了，因为C1是比赛的名次是第3名了，D组E组都没有希望了。现在已经知道A1肯定是第一名，剩下A2、A3、B1、B2、C1是有希望冲进前三的。第7次比赛：A2、A3、B1、B2、C1比赛求出第2，第3即可。25匹马5条跑道找最快的5匹马，需要跑几次？(1) 首先将25匹马分成5组，并分别进行5场比赛之后得到的名次排列如下：A组： [A1 A2 A3 A4 A5]B组： [B1 B2 B3 B4 B5]C组： [C1 C2 C3 C4 C5]D组： [D1 D2 D3 D4 D5]E组： [E1 E2 E3 E4 E5]其中，每个小组最快的马为[A1、B1、C1、D1、E1]。(2) 将[A1、B1、C1、D1、E1]进行第6场，选出第1名的马，不妨设 A1&gt;B1&gt;C1&gt;D1&gt;E1. 此时第1名的马为A1。(3) 将[A2、B1、C1、D1、E1]进行第7场，此时选择出来的必定是第2名的马，不妨假设为B1。因为这5匹马是除去A1之外每个小组当前最快的马。(3) 进行第8场，选择[A2、B2、C1、D1、E1]角逐出第3名的马。(4) 依次类推，第9，10场可以分别决出第4，5名因此，依照这种竞标赛排序思想，需要10场比赛是一定可以取出前5名的。仔细想一下，如果需要减少比赛场次，就一定需要在某一次比赛中同时决出2个名次，而且每一场比赛之后，有一些不可能进入前5名的马可以提前出局。 当然要做到这一点，就必须小心选择每一场比赛的马匹。我们在上面的方法基础上进一步思考这个问题，希望能够得到解决。优化解法(1) 首先利用5场比赛角逐出每个小组的排名次序是绝对必要的。(2) 第6场比赛选出第1名的马也是必不可少的。假如仍然是A1马(A1&gt;B1&gt;C1&gt;D1&gt;E1)。那么此时我们可以得到一个重要的结论：有一些马在前6场比赛之后就决定出局的命运了(下面粉色字体标志出局)。A组： [A1 A2 A3 A4 A5]B组： [B1 B2 B3 B4 B5 ]C组： [C1 C2 C3 C4 C5 ]D组： [D1 D2 D3 D4 D5 ]E组： [E1 E2 E3 E4 E5 ](3) 第7场比赛是关键，能否同时决出第2，3名的马呢？我们首先做下分析：在上面的方法中，第7场比赛[A2、B1、C1、D1、E1]是为了决定第2名的马。但是在第6场比赛中我们已经得到(B1&gt;C1&gt;D1&gt;E1)，试问？有B1在的比赛，C1、D1、E1还有可能争夺第2名吗？ 当然不可能，也就是说第2名只能在A2、B1中出现。实际上只需要2条跑道就可以决出第2名，剩下C1、D1、E1的3条跑道都只能用来凑热闹的吗？能够优化的关键出来了，我们是否能够通过剩下的3个跑道来决出第3名呢？当然可以，我们来进一步分析第3名的情况？如果A2&gt;B1(即第2名为A2)，那么根据第6场比赛中的(B1&gt;C1&gt;D1&gt;E1)。 可以断定第3名只能在A3和B1中产生。如果B1&gt;A2(即第2名为B1)，那么可以断定的第3名只能在A2, B2,C1 中产生。好了，结论也出来了，只要我们把[A2、B1、A3、B2、C1]作为第7场比赛的马，那么这场比赛的第2，3名一定是整个25匹马中的第2，3名。我们在这里列举出第7场的2，3名次的所有可能情况：第2名&#x3D;A2，第3名&#x3D;A3第2名&#x3D;A2，第3名&#x3D;B1第2名&#x3D;B1，第3名&#x3D;A2第2名&#x3D;B1，第3名&#x3D;B2第2名&#x3D;B1，第3名&#x3D;C1(4) 第8场比赛很复杂，我们要根据第7场的所有可能的比赛情况进行分析。① 第2名&#x3D;A2，第3名&#x3D;A3。那么此种情况下第4名只能在A4和B1中产生。如果第4名&#x3D;A4，那么第5名只能在A5、B1中产生。如果第4名&#x3D;B1，那么第5名只能在A4、B2、C1中产生。不管结果如何，此种情况下，第4、5名都可以在第8场比赛中决出。其中比赛马匹为[A4、A5、B1、B2、C1]② 第2名&#x3D;A2，第3名&#x3D;B1。那么此种情况下第4名只能在A3、B2、C1中产生。如果第4名&#x3D;A3，那么第5名只能在A4、B2、C1中产生。如果第4名&#x3D;B2，那么第5名只能在A3、B3、C1中产生。如果第4名&#x3D;C1，那么第5名只能在A3、B2、C2、D1中产生。那么，第4、5名需要在马匹[A3、B2、B3、C1、A4、C2、D1]七匹马中产生，则必须比赛两场才行，也就是到第9场角逐出全部的前5名。③ 第2名&#x3D;B1，第3名&#x3D;A2。那么此种情况下第4名只能在A3、B2、C1中产生。情况和②一样，必须角逐第9场④ 第2名&#x3D;B1，第3名&#x3D;B2。 那么此种情况下第4名只能在A2、B3、C1中产生。如果第4名&#x3D;A2，那么第5名只能在A3、B3、C1中产生。如果第4名&#x3D;B3，那么第5名只能在A2、B4、C1中产生。如果第4名&#x3D;C1，那么第5名只能在A2、B3、C2、D1中产生。那么，第4、5名需要在马匹[A2、B3、B4、C1、A3、C2、D1]七匹马中产 生，则必须比赛两场才行，也就是到第9场角逐出全部的前5名。⑤ 第2名&#x3D;B1，第3名&#x3D;C1。那么此种情况下第4名只能在A2、B2、C2、D1中产生。如果第4名&#x3D;A2，那么第5名只能在A3、B2、C2、D1中产生。如果第4名&#x3D;B2，那么第5名只能在A2、B3、C2、D1中产生。如果第4名&#x3D;C2，那么第5名只能在A2、B2、C3、D1中产生。如果第4名&#x3D;D1，那么第5名只能在A2、B2、C2、D2、E2中产生。那么，第4、5名需要在马匹[A2、B2、C2、D1、A3、B3、C3、D2、E1]九匹马中 产 生，因此也必须比赛两场，也就是到第9长决出胜负。总结：最好情况可以在第8场角逐出前5名，最差也可以在第9场搞定。64匹马8条跑道找最快的4匹马，需要跑几次？第一步全部马分为8组，每组8匹，每组各跑一次，然后淘汰掉每组的后四名，如下图（需要比赛8场）第二步取每组第一名进行一次比赛，然后淘汰最后四名所在组的所有马，如下图（需要比赛1场）这个时候总冠军已经诞生，它就是A1，蓝域（它不需要比赛了），而其他可能跑得最快的三匹马只可能是下图中的黄域了（A2,A3,A4,B1,B2,B3,C1,C2,D1，共9匹马）第三步只要从上面的9匹马中找出跑得最快的三匹马就可以了，但是现在只要8个跑道，怎么办？那就随机选出8匹马进行一次比赛吧（需要比赛一场）第四步上面比赛完，选出了前三名，但是9匹马中还有一匹马没跑呢，它可能是一个潜力股啊，那就和前三名比一比吧，这四匹马比一场，选出前三名。最后加上总冠军，跑得最快的四匹马诞生了！！！（需要一场比赛）最后，一共需要比赛的场次：8 + 1 + 1 + 1 &#x3D; 11 场来源：https://blog.csdn.net/u013829973/article/details/80787928 砝码称轻重这一类的题目有很多 这里只举几个经典的： 有一个天平，九个砝码，其中一个砝码比另八个要轻一些，问至少要用天平称几次才能将轻的那个找出来？ 十组砝码每组十个，每个砝码都是10g重，但是现在其中有一组砝码每个都只有9g重，现有一个能显示克数的秤，最少称几次能找到轻的那组？ 答案问题一：至少2次：第一次，一边3个，哪边轻就在哪边，一样重就是剩余的3个；第二次，一边1个，哪边轻就是哪个，一样重就是剩余的那个；答：至少称2次．问题二：将砝码分组1~10，第一组拿一个，第二组拿两个以此类推。。第十组拿十个放到秤上称出克数x，则y &#x3D; 550 - x，第y组就是轻的那组 药瓶毒白鼠有1000个一模一样的瓶子，其中有999瓶是普通的水，有1瓶是毒药。任何喝下毒药的生命都会在一星期之后死亡。现在你只有10只小白鼠和1个星期的时间，如何检验出哪个瓶子有毒药？ 答案首先一共有1000瓶，2的10次方是1024，刚好大于1000，也就是说，1000瓶药品可以使用10位二进制数就可以表示。从第一个开始：第一瓶 ： 00 0000 0001第二瓶： 00 0000 0010第三瓶： 00 0000 0011……第999瓶： 11 1111 0010第1000瓶： 11 1111 0011需要十只老鼠，如果按顺序编号，ABCDEFGHIJ分别代表从低位到高位每一个位。 每只老鼠对应一个二进制位，如果该位上的数字为1，则给老鼠喝瓶里的药。观察，若死亡的老鼠编号为：ACFGJ，一共死去五只老鼠，则对应的编号为 10 0110 0101，则有毒的药品为该编号的药品，转为十进制数为：613号。 绳子两头烧现有若干不均匀的绳子，烧完这根绳子需要一个小时，问如何准确计时15分钟，30分钟，45分钟，75分钟。。。 答案15：对折之后两头烧(要求对折之后绑的够紧，否则看45分钟解法)30：两头烧45：两根，一根两头烧一根一头烧，两头烧完过了30分钟，立即将第二根另一头点燃，到烧完又过15分钟，加起来45分钟75：&#x3D; 30 + 45 犯人猜颜色一百个犯人站成一纵列，每人头上随机带上黑色或白色的帽子，各人不知道自己帽子的颜色，但是能看见自己前面所有人帽子的颜色． 然后从最后一个犯人开始，每人只能用同一种声调和音量说一个字：”黑”或”白”， 如果说中了自己帽子的颜色，就存活，说错了就拉出去斩了， 说的答案所有犯人都能听见， 是否说对，其他犯人不知道， 在这之前，所有犯人可以聚在一起商量策略， 问如果犯人都足够聪明而且反应足够快，100个人最大存活率是多少？ 答案1、最后一个人如果看到奇数顶黑帽子报“黑”否则报“白”，他可能死2、其他人记住这个值（实际是黑帽奇偶数），在此之后当再听到黑时，黑帽数量减一3、从倒数第二人开始，就有两个信息：记住的值与看到的值，相同报“白”，不同报“黑”99人能100%存活，1人50%能活除此以外，此题还有变种：每个犯人只能看见前面一个人帽子颜色又能最多存活多少人？答案：在上题基础上，限制了条件，这时上次的方法就不管用了，此时只能约定偶数位犯人说他前一个人的帽子颜色，奇数犯人获取信息100%存活，偶数犯人50几率存活。 猴子搬香蕉一个小猴子边上有100根香蕉，它要走过50米才能到家，每次它最多搬50根香蕉，（多了就被压死了），它每走 1 米就要吃掉一根，请问它最多能把多少根香蕉搬到家里。 （提示：他可以把香蕉放下往返的走，但是必须保证它每走一米都能有香蕉吃。也可以走到n米时，放下一些香蕉，拿着n根香蕉走回去重新搬50根。） 答案这种试题通常有一个迷惑点，让人看不懂题目的意图。此题迷惑点在于：走一米吃一根香蕉，一共走50米，那不是把50根香蕉吃完了吗？如果要回去搬另外50根香蕉，则往回走的时候也要吃香蕉，这样每走一米需要吃掉三根香蕉，走50米岂不是需要150根香蕉？其实不然，本题关键点在于：猴子搬箱子的过程其实分为两个阶段，第一阶段：来回搬，当香蕉数目大于50根时，猴子每搬一米需要吃掉三根香蕉。第二阶段：香蕉数《&#x3D;50，直接搬回去。每走一米吃掉1根。我们分析第一阶段：假如把100根香蕉分为两箱。一箱50根。第一步，把A箱搬一米，吃一根。第二步，往回走一米，吃一根。第三步，把B箱搬一米，吃一根。这样，把所有香蕉搬走一米需要吃掉三根香蕉。这样走到第几米的时候，香蕉数刚好小于50呢？100-(n*3)&lt;50 &amp;&amp; 100-(n-1*3)&gt;50走到16米的时候，吃掉48根香蕉，剩52根香蕉。这步很有意思，它可以直接搬50往前走，也可以再来回搬一次，但结果都是一样的。到17米的时候，猴子还有49根香蕉。这时猴子就轻松啦。直接背着走就行。第二阶段：走一米吃一根。把剩下的50-17&#x3D;33米走完。还剩49-33&#x3D;16根香蕉。 高楼扔鸡蛋有2个鸡蛋，从100层楼上往下扔，以此来测试鸡蛋的硬度。比如鸡蛋在第9层没有摔碎，在第10层摔碎了，那么鸡蛋不会摔碎的临界点就是9层。 问：如何用最少的尝试次数，测试出鸡蛋不会摔碎的临界点？ 答案暴力法举个栗子，最笨的测试方法，是什么样的呢？把其中一个鸡蛋，从第1层开始往下扔。如果在第1层没碎，换到第2层扔；如果在第2层没碎，换到第3层扔…….如果第59层没碎，换到第60层扔；如果第60层碎了，说明不会摔碎的临界点是第59层。在最坏情况下，这个方法需要扔100次。二分法采用类似于二分查找的方法，把鸡蛋从一半楼层（50层）往下扔。如果第一枚鸡蛋，在50层碎了，第二枚鸡蛋，就从第1层开始扔，一层一层增长，一直扔到第49层。如果第一枚鸡蛋在50层没碎了，则继续使用二分法，在剩余楼层的一半（75层）往下扔……这个方法在最坏情况下，需要尝试50次。均匀法如何让第一枚鸡蛋和第二枚鸡蛋的尝试次数，尽可能均衡呢？很简单，做一个平方根运算，100的平方根是10。因此，我们尝试每10层扔一次，第一次从10层扔，第二次从20层扔，第三次从30层……一直扔到100层。这样的最好情况是在第10层碎掉，尝试次数为 1 + 9 &#x3D; 10次。最坏的情况是在第100层碎掉，尝试次数为 10 + 9 &#x3D; 19次。不过，这里有一个小小的优化点，我们可以从15层开始扔，接下来从25层、35层扔……一直到95层。这样最坏情况是在第95层碎掉，尝试次数为 9 + 9 &#x3D; 18次。最优解法最优解法是反向思考的经典：如果最优解法在最坏情况下需要扔X次，那第一次在第几层扔最好呢？答案是：从X层扔假设最优的尝试次数的x次，为什么第一次扔就要选择第x层呢？这里的解释会有些烧脑，请小伙伴们坐稳扶好：假设第一次扔在第x+1层：如果第一个鸡蛋碎了，那么第二个鸡蛋只能从第1层开始一层一层扔，一直扔到第x层。这样一来，我们总共尝试了x+1次，和假设尝试x次相悖。由此可见，第一次扔的楼层必须小于x+1层。假设第一次扔在第x-1层：如果第一个鸡蛋碎了，那么第二个鸡蛋只能从第1层开始一层一层扔，一直扔到第x-2层。这样一来，我们总共尝试了x-2+1 &#x3D; x-1次，虽然没有超出假设次数，但似乎有些过于保守。假设第一次扔在第x层：如果第一个鸡蛋碎了，那么第二个鸡蛋只能从第1层开始一层一层扔，一直扔到第x-1层。这样一来，我们总共尝试了x-1+1 &#x3D; x次，刚刚好没有超出假设次数。因此，要想尽量楼层跨度大一些，又要保证不超过假设的尝试次数x，那么第一次扔鸡蛋的最优选择就是第x层。那么算最坏情况，第二次你只剩下x-1次机会，按照上面的说法，你第二次尝试的位置必然是X+（X-1）；以此类推我们可得：x + (x-1) + (x-2) + … + 1 &#x3D; 100这个方程式不难理解：左边的多项式是各次扔鸡蛋的楼层跨度之和。由于假设尝试x次，所以这个多项式共有x项。右边是总的楼层数100。下面我们来解这个方程：x + (x-1) + (x-2) + … + 1 &#x3D; 100 转化为(x+1)*x&#x2F;2 &#x3D; 100最终x向上取整，得到 x &#x3D; 14因此，最优解在最坏情况的尝试次数是14次，第一次扔鸡蛋的楼层也是14层。最后，让我们把第一个鸡蛋没碎的情况下，所尝试的楼层数完整列举出来：14，27， 39， 50， 60， 69， 77， 84， 90， 95， 99， 100举个栗子验证下：假如鸡蛋不会碎的临界点是65层，那么第一个鸡蛋扔出的楼层是14，27，50，60，69。这时候啪的一声碎了。第二个鸡蛋继续，从61层开始，61，62，63，64，65，66，啪的一声碎了。因此得到不会碎的临界点65层，总尝试次数是 6 + 6 &#x3D; 12 &lt; 14 。 轮流拿石子问题：一共有N颗石子（或者其他乱七八糟的东西），每次最多取M颗最少取1颗，A，B轮流取，谁最后会获胜？（假设他们每次都取最优解）。 答案：简单的巴什博奕：https://www.cnblogs.com/StrayWolf/p/5396427.html 问题：有若干堆石子，每堆石子的数量是有限的，二个人依次从这些石子堆中拿取任意的石子，至少一个（不能不取），最后一个拿光石子的人胜利。 答案：较复杂的尼姆博弈：https://blog.csdn.net/BBHHTT/article/details/80199541 蚂蚁走树枝放N只蚂蚁在一条长度为M树枝上，蚂蚁与蚂蚁之间碰到就各自往反方向走，问总距离或者时间。 答案：这个其实就一个诀窍：蚂蚁相碰就往反方向走，可以直接看做没有发生任何事：大家都相当于独立的 A蚂蚁与B蚂蚁相碰后你可以看做没有发生这次碰撞，这样无论是求时间还是距离都很简单了。 海盗分金币5个海盗抢到了100枚金币，每一颗都一样的大小和价值。 他们决定这么分： 抽签决定自己的号码（1，2，3，4，5） 首先，由1号提出分配方案，然后大家5人进行表决，当半数以上的人同意时（ 不包括半数，这是重点），按照他的提案进行分配，否则将被扔入大海喂鲨鱼。 如果1号死后，再由2号提出分配方案，然后大家4人进行表决，当且仅当半超过半数的人同意时，按照他的提案进行分配，否则将被扔入大海喂鲨鱼。 依次类推…… 假设每一位海盗都足够聪明，并且利益至上，能多分一枚金币绝不少分，那么1号海盗该怎么分金币才能使自己分到最多的金币呢？ 答案从后向前推，如果1至3号强盗都喂了鲨鱼，只剩4号和5号的话，5号一定投反对票让4号喂鲨鱼，以独吞全部金币。所以，4号惟有支持3号才能保命。3号知道这一点，就会提出“100，0，0”的分配方案，对4号、5号一毛不拔而将全部金币归为已有，因为他知道4号一无所获但还是会投赞成票，再加上自己一票，他的方案即可通过。不过，2号推知3号的方案，就会提出“98，0，1，1”的方案，即放弃3号，而给予4号和5号各一枚金币。由于该方案对于4号和5号来说比在3号分配时更为有利，他们将支持他而不希望他出局而由3号来分配。这样，2号将拿走98枚金币。同样，2号的方案也会被1号所洞悉，1号并将提出（97，0，1，2，0）或（97，0，1，0，2）的方案，即放弃2号，而给3号一枚金币，同时给4号（或5号）2枚金币。由于1号的这一方案对于3号和4号（或5号）来说，相比2号分配时更优，他们将投1号的赞成票，再加上1号自己的票，1号的方案可获通过，97枚金币可轻松落入囊中。这无疑是1号能够获取最大收益的方案了！答案是：1号强盗分给3号1枚金币，分给4号或5号强盗2枚，自己独得97枚。分配方案可写成（97，0，1，2，0）或（97，0，1，0，2）。此题还有变种： 就是只需要一半人同意即可，不需要一半人以上同意方案就可以通过，在其他条件不变的情况下，1号该怎么分配才能获得最多的金币？答案：类似的推理过程4号：4号提出的方案的时候肯定是最终方案，因为不管5号同意不同意都能通过，所以4号5号不必担心自己被投入大海。那此时5号获得的金币为0，4号获得的金币为100。5号：因为4号提方案的时候 ，自己获取的金币为0 。所以只要4号之前的人分配给自己的金币大于0就同意该方案。4号：如果3号提的方案一定能获得通过（原因：3号给5号的金币大于0， 5号就同意 因此就能通过），那自己获得的金币就为0，所以只要2号让自己获得的金币大于0就会同意。3号：因为到了自己提方案的时候可以给5号一金币，自己的方案就能通过，但考虑到2号提方案的时候给4号一个金币，2号的方案就会通过，那自己获得的金币就为0。所以只要1号让自己获得的金币大于0就会同意。2号：因为到了自己提方案的时候只要给4号一金币，就能获得通过，根本就不用顾及3 号 5号同意不同意，所以不管1号怎么提都不会同意。1号：2号肯定不会同意。但只要给3号一块金币，5号一块金币（因为5号如果不同意，那么4号分配的时候，他什么都拿不到）就能获得通过。所以答案是 98，0，1，0，1。类似的问题也可用类似的推理，并不难 三个火枪手彼此痛恨的甲、乙、丙三个枪手准备决斗。甲枪法最好，十发八中；乙枪法次之，十发六中；丙枪法最差，十发四中。如果三人同时开枪，并且每人每轮只发一枪；那么枪战后，谁活下来的机会大一些？ 答案一般人认为甲的枪法好，活下来的可能性大一些。但合乎推理的结论是，枪法最糟糕的丙活下来的几率最大。那么我们先来分析一下各个枪手的策略。如同田忌赛马一般，枪手甲一定要对枪手乙先开枪。因为乙对甲的威胁要比丙对甲的威胁更大，甲应该首先干掉乙，这是甲的最佳策略。同样的道理，枪手乙的最佳策略是第一枪瞄准甲。乙一旦将甲干掉，乙和丙进行对决，乙胜算的概率自然大很多。枪手丙的最佳策略也是先对甲开枪。乙的枪法毕竟比甲差一些，丙先把甲干掉再与乙进行对决，丙的存活概率还是要高一些。我们根据分析来计算一下三个枪手在上述情况下的存活几率：第一轮：甲射乙，乙射甲，丙射甲。甲的活率为24%（40% X 60%）乙的活率为20%(100% - 80%)丙的活率为100%（无人射丙）。由于丙100％存活率，因此根据上轮甲乙存活的情况来计算三人第二轮的存活几率：情况1：甲活乙死（24% X 80% &#x3D; 19.2%）甲射丙，丙射甲：甲的活率为60%，丙的活率为20%。情况2：乙活甲死（20% X 76% &#x3D; 15.2%）乙射丙，丙射乙：乙的活率为60%，丙的活率为40%。情况3：甲乙同活（24% X 20% &#x3D; 4.8%）重复第一轮。情况4：甲乙同死（76% X 80% &#x3D; 60.8%）枪战结束。据此来计算三人活率：甲的活率为(19.2% X 60%) + (4.8% X 24%) &#x3D; 12.672%乙的活率为(15.2% X 60%) + (4.8% X 20%) &#x3D; 10.08%丙的活率为(19.2% X 20%) + (15.2% X 40%) + (4.8% X 100%) + (60.8% X 100%) &#x3D; 75.52%通过对两轮枪战的详细概率计算，我们发现枪法最差的丙存活的几率最大，枪法较好的甲和乙的存活几率却远低于丙的存活几率。来自：https://www.zhihu.com/question/288093713/answer/482192781 囚犯拿豆子有5个囚犯被***，他们请求上诉，于是法官愿意给他们一个机会。 犯人抽签分好顺序，按序每人从100粒豆子中随意抓取，最多可以全抓，最少可以不抓，可以和别人抓的一样多。 最终，抓的最多的和最少的要被处死。 他们都是非常聪明且自私的人。 他们的原则是先求保命。如果不能保命，就拉人陪葬。 100颗不必都分完。 若有重复的情况，则也算最大或最小，一并处死（中间重复不算）。 假设每个犯人都足够聪明，但每个犯人并不知道其他犯人足够聪明。那么，谁活下来的可能性最大？ 答案不存在“谁活下来的可能性比较大”的问题。实际情况是5个人都要死。答案看起来很扯淡，但推理分析后却发现十分符合逻辑。根据题意，一号知道有五个人抓豆子，为保性命，他只要让豆子在20颗以内就可以了。但是他足够聪明的话他一定拿20颗，因为无论多拿一颗：2,3,4号的人一定会拿20颗最后死的人就会是最多的1号和最少的5号 还是少拿一颗：2,3,4号拿20个后，5号选择也拿20个拉上1234号垫背。（下面会说为什么多拿少拿也只会相差一颗）2号是知道1号抓了几颗豆子(20)的。那么，对于2号来说，只有2种选择：与1号一样多，或者不一样多。我们就从这里入手。情况一，假如2号选择与1号的豆子数不一样多，也就是说2号选择比1号多或者比1号少。我们先要证明，如果2号选择比1号多或者比1号少，那么他一定会选择比1号只多1颗或者只少1颗。要证明这个并不算太难。因为每个囚犯的第一选择是先求保命，要保命就要尽量使自己的豆子数既不是最多也不是最少。当2号决定选择比1号多的时候，他已经可以保证自己不是最少，为了尽量使自己不是最多，当然比1号多出来的数量越小越好。因为这个数量如果与一号相差大于1的话，那么3号就有机会抓到的居中数，相差越大，二号成为最多的可能性也就越大。反之，当2号决定选择比1号少的时候，也是同样的道理，他会选择只比1号少1颗。既然2号只会会选择比1号多1颗或者比1号少1颗，那么1、2号的豆子数一定是2个连续的自然数，和一定是2n+1（其中1个人是n,另1人是n+1）。轮到3号的时候，他可以从剩下的豆子数知道1、2号的数量和，也就不难计算出n的值。而3号也只有2个选择：n颗或者n+1颗。为什么呢？这与上面的证明是一样的道理，保命原则，取最接近的数量，这里不再赘述。不过，3号选择的时候会有一个特殊情况，在这一情况下，他一定会选择较小的n，而不是较大的n+1。这一特殊情况就是，当3号知道自己选择了n后(已保证自己不是最多)，剩下的豆子数由于数量有限，4、5号中一定有人比n要少，这样自己一定可以活下来。计算的话就是 [100-(3n+1)]&#x2F;2&lt;&#x3D;n ，不难算出，在这个特殊情况下，n&gt;&#x3D;20。也就是说，当1、2号选择了20或21颗的时候，3号只要选择20颗，就可以保证自己活下来。这样一来剩下的豆子只剩39颗，4、5号至少有一人少于20颗的（这个人当然是后选的5号），这样死的将是5号和1、2号中选21颗的那个人。当然，1号、2号肯定不会有人选择21这一“倒霉”的数字（因为他们都是聪明人），这样的话，上述“特殊情况（即3号选择n）”就不会发生了。综上所述，2345这四个人不难从剩下的豆子数知道前面几个人的数量总和，也就不难进而计算出n的值，而这样一来他们也只有n或者n+1这两种选择。最后的5号也是不难算出n的。在前4个人只选择了2个数字(n和n+1)的情况下，5号已是必死无疑，这时,根据“死也要拉几个垫背”的条件，5号会选择n或n+1，选择5个人一起完蛋。情况二，如果2号选择了与1号不一样多的话，最终结果是5个人一起死，那么2号只有选择与1号一样多了。那么1、2号的和就是2n，而3号如果选择n+1或者n -1的话，就又回到第一点的情况去了(前3个人的和是3m+1或3m+2)，于是3号也只能选择n ，当然，4号还是只能选n，最后的结果仍旧是5个人一起完蛋。“最后处死抓的最多和最少的囚犯”严格执行这句话的话，除非有人舍己为人，死二留三。但这是足够聪明且自私的囚犯，所以这五个聪明人的下场是全死，这道题只不过是找了一个处死所有人的借口罢了. . . . . .变种问题：如果每个囚犯都知道其他囚犯足够聪明，事情会怎么发展？答案：这样的情况下囚犯一也会像我们一样推导出前面的结论，那么根据自私的规定，他会直接拿完100个，大家一起完蛋(反正结局已定) 学生猜生日这种题目笔试中出现的次数比较多，用排除法比较好解决 小明和小强都是张老师的学生，张老师的生日是M月N日, 2人都知道张老师的生日是下列10组中的一天，张老师把M值告诉了小明, 把N值告诉了小强，张老师问他们知道他的生日是那一天吗? 3月4日 3月5日 3月8日 6月4日 6月7日 9月1日 9月5日 12月1日 12月2日 12月8日 小明说:如果我不知道的话，小强肯定也不知道. 小强说:本来我也不知道，但是现在我知道了. 小明说:哦，那我也知道了. 请根据以上对话推断出张老师的生日是哪一天? 答案排除法：1.小明肯定小强不知道是哪天，排除所有月份里有单独日的月份：6月和12月&lt;因为如果小强的M是2或者7的话，小强就知道了，所以把6月7日与12月2日排除&gt;，所以小明拿到的是3或者92.小强本来不知道，所以小强拿到的不是2或者7，但是小强现在知道了，说明把6月与12月排除后，小强拿到的是1,4,8中的一个&lt;这里小强肯定没拿到5，否则他不会知道是哪天的&gt;3.小明现在也知道了，说明小明拿到的不是3，否则他不会知道是3月4日还是3月8日的，所以小明拿到的是9才能唯一确定生日综上，答案是9月1日 小明和小强是赵老师的学生，张老师的生日是M月N日，张老师 把M值告诉小明，N值告诉小强 给他们六个选项 3月1日 3月3日 7月3日 7月5日 9月1日 11月7日 小明说:我猜不出来 小强说:本来我也猜不出来，但是现在我知道了 问:张老师生日多少 答案：3月1日 答案排除法：1.小明说猜不出来，说明小明拿到的不是单独出现的9或者11，说明老师生日只能是3月或者7月2.小强原本不知道，说明小强拿到的不是单独出现的5或者7，说明老是生日是1日或3日3.小强现在知道了，说明小强拿到的是1，因为如果拿到的是3，那么小强就不知道是3月3日还是7月3日了综上，老师生日是3月1日 参考文章： https://www.nowcoder.com/discuss/262595","tags":["智力题","algorithm"],"categories":["algorithm"]},{"title":"常见思维题二","path":"//blog/algorithm/algo2/","content":"轮流拿石子问题：一共有N颗石子（或者其他乱七八糟的东西），每次最多取M颗最少取1颗，A，B轮流取，谁最后会获胜？（假设他们每次都取最优解）。 答案：简单的巴什博奕：https://www.cnblogs.com/StrayWolf/p/5396427.html 问题：有若干堆石子，每堆石子的数量是有限的，二个人依次从这些石子堆中拿取任意的石子，至少一个（不能不取），最后一个拿光石子的人胜利。 答案：较复杂的尼姆博弈：https://blog.csdn.net/BBHHTT/article/details/80199541 蚂蚁走树枝放N只蚂蚁在一条长度为M树枝上，蚂蚁与蚂蚁之间碰到就各自往反方向走，问总距离或者时间。 答案：这个其实就一个诀窍：蚂蚁相碰就往反方向走，可以直接看做没有发生任何事：大家都相当于独立的 A蚂蚁与B蚂蚁相碰后你可以看做没有发生这次碰撞，这样无论是求时间还是距离都很简单了。 海盗分金币5个海盗抢到了100枚金币，每一颗都一样的大小和价值。 他们决定这么分： 抽签决定自己的号码（1，2，3，4，5） 首先，由1号提出分配方案，然后大家5人进行表决，当半数以上的人同意时（ 不包括半数，这是重点），按照他的提案进行分配，否则将被扔入大海喂鲨鱼。 如果1号死后，再由2号提出分配方案，然后大家4人进行表决，当且仅当半超过半数的人同意时，按照他的提案进行分配，否则将被扔入大海喂鲨鱼。 依次类推…… 假设每一位海盗都足够聪明，并且利益至上，能多分一枚金币绝不少分，那么1号海盗该怎么分金币才能使自己分到最多的金币呢？ 答案从后向前推，如果1至3号强盗都喂了鲨鱼，只剩4号和5号的话，5号一定投反对票让4号喂鲨鱼，以独吞全部金币。所以，4号惟有支持3号才能保命。3号知道这一点，就会提出“100，0，0”的分配方案，对4号、5号一毛不拔而将全部金币归为已有，因为他知道4号一无所获但还是会投赞成票，再加上自己一票，他的方案即可通过。不过，2号推知3号的方案，就会提出“98，0，1，1”的方案，即放弃3号，而给予4号和5号各一枚金币。由于该方案对于4号和5号来说比在3号分配时更为有利，他们将支持他而不希望他出局而由3号来分配。这样，2号将拿走98枚金币。同样，2号的方案也会被1号所洞悉，1号并将提出（97，0，1，2，0）或（97，0，1，0，2）的方案，即放弃2号，而给3号一枚金币，同时给4号（或5号）2枚金币。由于1号的这一方案对于3号和4号（或5号）来说，相比2号分配时更优，他们将投1号的赞成票，再加上1号自己的票，1号的方案可获通过，97枚金币可轻松落入囊中。这无疑是1号能够获取最大收益的方案了！答案是：1号强盗分给3号1枚金币，分给4号或5号强盗2枚，自己独得97枚。分配方案可写成（97，0，1，2，0）或（97，0，1，0，2）。此题还有变种： 就是只需要一半人同意即可，不需要一半人以上同意方案就可以通过，在其他条件不变的情况下，1号该怎么分配才能获得最多的金币？答案：类似的推理过程4号：4号提出的方案的时候肯定是最终方案，因为不管5号同意不同意都能通过，所以4号5号不必担心自己被投入大海。那此时5号获得的金币为0，4号获得的金币为100。5号：因为4号提方案的时候 ，自己获取的金币为0 。所以只要4号之前的人分配给自己的金币大于0就同意该方案。4号：如果3号提的方案一定能获得通过（原因：3号给5号的金币大于0， 5号就同意 因此就能通过），那自己获得的金币就为0，所以只要2号让自己获得的金币大于0就会同意。3号：因为到了自己提方案的时候可以给5号一金币，自己的方案就能通过，但考虑到2号提方案的时候给4号一个金币，2号的方案就会通过，那自己获得的金币就为0。所以只要1号让自己获得的金币大于0就会同意。2号：因为到了自己提方案的时候只要给4号一金币，就能获得通过，根本就不用顾及3 号 5号同意不同意，所以不管1号怎么提都不会同意。1号：2号肯定不会同意。但只要给3号一块金币，5号一块金币（因为5号如果不同意，那么4号分配的时候，他什么都拿不到）就能获得通过。所以答案是 98，0，1，0，1。类似的问题也可用类似的推理，并不难 三个火枪手彼此痛恨的甲、乙、丙三个枪手准备决斗。甲枪法最好，十发八中；乙枪法次之，十发六中；丙枪法最差，十发四中。如果三人同时开枪，并且每人每轮只发一枪；那么枪战后，谁活下来的机会大一些？ 答案一般人认为甲的枪法好，活下来的可能性大一些。但合乎推理的结论是，枪法最糟糕的丙活下来的几率最大。那么我们先来分析一下各个枪手的策略。如同田忌赛马一般，枪手甲一定要对枪手乙先开枪。因为乙对甲的威胁要比丙对甲的威胁更大，甲应该首先干掉乙，这是甲的最佳策略。同样的道理，枪手乙的最佳策略是第一枪瞄准甲。乙一旦将甲干掉，乙和丙进行对决，乙胜算的概率自然大很多。枪手丙的最佳策略也是先对甲开枪。乙的枪法毕竟比甲差一些，丙先把甲干掉再与乙进行对决，丙的存活概率还是要高一些。我们根据分析来计算一下三个枪手在上述情况下的存活几率：第一轮：甲射乙，乙射甲，丙射甲。甲的活率为24%（40% X 60%）乙的活率为20%(100% - 80%)丙的活率为100%（无人射丙）。由于丙100％存活率，因此根据上轮甲乙存活的情况来计算三人第二轮的存活几率：情况1：甲活乙死（24% X 80% &#x3D; 19.2%）甲射丙，丙射甲：甲的活率为60%，丙的活率为20%。情况2：乙活甲死（20% X 76% &#x3D; 15.2%）乙射丙，丙射乙：乙的活率为60%，丙的活率为40%。情况3：甲乙同活（24% X 20% &#x3D; 4.8%）重复第一轮。情况4：甲乙同死（76% X 80% &#x3D; 60.8%）枪战结束。据此来计算三人活率：甲的活率为(19.2% X 60%) + (4.8% X 24%) &#x3D; 12.672%乙的活率为(15.2% X 60%) + (4.8% X 20%) &#x3D; 10.08%丙的活率为(19.2% X 20%) + (15.2% X 40%) + (4.8% X 100%) + (60.8% X 100%) &#x3D; 75.52%通过对两轮枪战的详细概率计算，我们发现枪法最差的丙存活的几率最大，枪法较好的甲和乙的存活几率却远低于丙的存活几率。来自：https://www.zhihu.com/question/288093713/answer/482192781 囚犯拿豆子有5个囚犯被***，他们请求上诉，于是法官愿意给他们一个机会。 犯人抽签分好顺序，按序每人从100粒豆子中随意抓取，最多可以全抓，最少可以不抓，可以和别人抓的一样多。 最终，抓的最多的和最少的要被处死。 他们都是非常聪明且自私的人。 他们的原则是先求保命。如果不能保命，就拉人陪葬。 100颗不必都分完。 若有重复的情况，则也算最大或最小，一并处死（中间重复不算）。 假设每个犯人都足够聪明，但每个犯人并不知道其他犯人足够聪明。那么，谁活下来的可能性最大？ 答案不存在“谁活下来的可能性比较大”的问题。实际情况是5个人都要死。答案看起来很扯淡，但推理分析后却发现十分符合逻辑。根据题意，一号知道有五个人抓豆子，为保性命，他只要让豆子在20颗以内就可以了。但是他足够聪明的话他一定拿20颗，因为无论多拿一颗：2,3,4号的人一定会拿20颗最后死的人就会是最多的1号和最少的5号 还是少拿一颗：2,3,4号拿20个后，5号选择也拿20个拉上1234号垫背。（下面会说为什么多拿少拿也只会相差一颗）2号是知道1号抓了几颗豆子(20)的。那么，对于2号来说，只有2种选择：与1号一样多，或者不一样多。我们就从这里入手。情况一，假如2号选择与1号的豆子数不一样多，也就是说2号选择比1号多或者比1号少。我们先要证明，如果2号选择比1号多或者比1号少，那么他一定会选择比1号只多1颗或者只少1颗。要证明这个并不算太难。因为每个囚犯的第一选择是先求保命，要保命就要尽量使自己的豆子数既不是最多也不是最少。当2号决定选择比1号多的时候，他已经可以保证自己不是最少，为了尽量使自己不是最多，当然比1号多出来的数量越小越好。因为这个数量如果与一号相差大于1的话，那么3号就有机会抓到的居中数，相差越大，二号成为最多的可能性也就越大。反之，当2号决定选择比1号少的时候，也是同样的道理，他会选择只比1号少1颗。既然2号只会会选择比1号多1颗或者比1号少1颗，那么1、2号的豆子数一定是2个连续的自然数，和一定是2n+1（其中1个人是n,另1人是n+1）。轮到3号的时候，他可以从剩下的豆子数知道1、2号的数量和，也就不难计算出n的值。而3号也只有2个选择：n颗或者n+1颗。为什么呢？这与上面的证明是一样的道理，保命原则，取最接近的数量，这里不再赘述。不过，3号选择的时候会有一个特殊情况，在这一情况下，他一定会选择较小的n，而不是较大的n+1。这一特殊情况就是，当3号知道自己选择了n后(已保证自己不是最多)，剩下的豆子数由于数量有限，4、5号中一定有人比n要少，这样自己一定可以活下来。计算的话就是 [100-(3n+1)]&#x2F;2&lt;&#x3D;n ，不难算出，在这个特殊情况下，n&gt;&#x3D;20。也就是说，当1、2号选择了20或21颗的时候，3号只要选择20颗，就可以保证自己活下来。这样一来剩下的豆子只剩39颗，4、5号至少有一人少于20颗的（这个人当然是后选的5号），这样死的将是5号和1、2号中选21颗的那个人。当然，1号、2号肯定不会有人选择21这一“倒霉”的数字（因为他们都是聪明人），这样的话，上述“特殊情况（即3号选择n）”就不会发生了。综上所述，2345这四个人不难从剩下的豆子数知道前面几个人的数量总和，也就不难进而计算出n的值，而这样一来他们也只有n或者n+1这两种选择。最后的5号也是不难算出n的。在前4个人只选择了2个数字(n和n+1)的情况下，5号已是必死无疑，这时,根据“死也要拉几个垫背”的条件，5号会选择n或n+1，选择5个人一起完蛋。情况二，如果2号选择了与1号不一样多的话，最终结果是5个人一起死，那么2号只有选择与1号一样多了。那么1、2号的和就是2n，而3号如果选择n+1或者n -1的话，就又回到第一点的情况去了(前3个人的和是3m+1或3m+2)，于是3号也只能选择n ，当然，4号还是只能选n，最后的结果仍旧是5个人一起完蛋。“最后处死抓的最多和最少的囚犯”严格执行这句话的话，除非有人舍己为人，死二留三。但这是足够聪明且自私的囚犯，所以这五个聪明人的下场是全死，这道题只不过是找了一个处死所有人的借口罢了. . . . . .变种问题：如果每个囚犯都知道其他囚犯足够聪明，事情会怎么发展？答案：这样的情况下囚犯一也会像我们一样推导出前面的结论，那么根据自私的规定，他会直接拿完100个，大家一起完蛋(反正结局已定) 学生猜生日这种题目笔试中出现的次数比较多，用排除法比较好解决 小明和小强都是张老师的学生，张老师的生日是M月N日, 2人都知道张老师的生日是下列10组中的一天，张老师把M值告诉了小明, 把N值告诉了小强，张老师问他们知道他的生日是那一天吗? 3月4日 3月5日 3月8日 6月4日 6月7日 9月1日 9月5日 12月1日 12月2日 12月8日 小明说:如果我不知道的话，小强肯定也不知道. 小强说:本来我也不知道，但是现在我知道了. 小明说:哦，那我也知道了. 请根据以上对话推断出张老师的生日是哪一天? 答案排除法：1.小明肯定小强不知道是哪天，排除所有月份里有单独日的月份：6月和12月&lt;因为如果小强的M是2或者7的话，小强就知道了，所以把6月7日与12月2日排除&gt;，所以小明拿到的是3或者92.小强本来不知道，所以小强拿到的不是2或者7，但是小强现在知道了，说明把6月与12月排除后，小强拿到的是1,4,8中的一个&lt;这里小强肯定没拿到5，否则他不会知道是哪天的&gt;3.小明现在也知道了，说明小明拿到的不是3，否则他不会知道是3月4日还是3月8日的，所以小明拿到的是9才能唯一确定生日综上，答案是9月1日 小明和小强是赵老师的学生，张老师的生日是M月N日，张老师 把M值告诉小明，N值告诉小强 给他们六个选项 3月1日 3月3日 7月3日 7月5日 9月1日 11月7日 小明说:我猜不出来 小强说:本来我也猜不出来，但是现在我知道了 问:张老师生日多少 答案：3月1日 答案排除法：1.小明说猜不出来，说明小明拿到的不是单独出现的9或者11，说明老师生日只能是3月或者7月2.小强原本不知道，说明小强拿到的不是单独出现的5或者7，说明老是生日是1日或3日3.小强现在知道了，说明小强拿到的是1，因为如果拿到的是3，那么小强就不知道是3月3日还是7月3日了综上，老师生日是3月1日 参考文章： https://www.nowcoder.com/discuss/262595","tags":["智力题","algorithm"],"categories":["algorithm"]},{"title":"常见思维题一","path":"//blog/algorithm/algo1/","content":"倒水问题水无限。3L和5L水桶各一个，怎样取4L的水？ 答案 老虎吃羊问题岛上有100只老虎和1只羊，老虎可以吃草但是更愿意吃羊。 假设A: 每次老虎吃完羊之后自己就变成了羊 假设B: 所有老虎都很聪明也很理性，它们都想活下去 请问这只羊会被吃吗? 答案我们先从1只老虎开始分析，如果只有一只老虎，那它一定会吃羊。因为就算吃完变成羊它也不用担心自己被吃掉。如果岛上有两只老虎的话，那羊不会被吃掉，因为如果其中一只老虎吃掉羊之后自己就会变成羊被另一只老虎吃掉。如果岛上有三只老虎，羊会被吃掉。因为一旦有一只最聪明的老虎吃掉羊之后，那只老虎自己变成羊，就变成了刚才所分析的2虎1羊的局面，剩下的2只老虎不敢吃掉变成羊的那只老虎。如果岛上有4只老虎，羊不会被吃掉，因为一旦有一只虎吃掉羊，就会变成刚刚3虎1羊的局面，那只老虎变成的羊就会被吃掉。以此类推，如果老虎的数量是偶数，羊不会被吃掉，如果老虎的数量是奇数，羊就会被吃掉。 其他No.1给一个瞎子52张扑克牌，并告诉他里面恰好有10张牌是正面朝上的。要求这个瞎子把牌分成两堆，使得每堆牌里正面朝上的牌的张数一样多。瞎子应该怎么做？ 答案把扑克牌分成两堆，一堆10张，一堆42张。然后，把小的那一堆里的所有牌全部翻过来。 No.2如何用一枚硬币等概率地产生一个1到3之间的随机整数？如果这枚硬币是不公正的呢？ 答案如果是公正的硬币，则投掷两次，“正反”为1，“反正”为2，“正正”为3，“反反”重来。如果是不公正的硬币，注意到出现“正反”和“反正”的概率一样，因此令“正反反正”、“反正正反”、“正反正反”分别为1、2、3，其余情况重来。另一种更妙的办法是，投掷三次硬币，“正反反”为1，“反正反”为2，“反反正”为3，其余情况重来。 No.330枚面值不全相同的硬币摆成一排，甲、乙两个人轮流选择这排硬币的其中一端，并取走最外边的那枚硬币。如果你先取硬币，能保证得到的钱不会比对手少吗？ 答案先取者可以让自己总是取奇数位置上的硬币或者总是取偶数位置上的硬币。数一数是奇数位置上的面值总和多还是偶数位置上的面值总和多，然后总是取这些位置上的硬币就可以了。 No.4 一个环形轨道上有n个加油站，所有加油站的油量总和正好够车跑一圈。证明，总能找到其中一个加油站，使得初始时油箱为空的汽车从这里出发，能够顺利环行一圈回到起点。 答案总存在一个加油站，仅用它的油就足够跑到下一个加油站（否则所有加油站的油量加起来将不够全程）。把下一个加油站的所有油都提前搬到这个加 油站来，并把油已被搬走的加油站无视掉。在剩下的加油站中继续寻找油量足以到达下个加油站的地方，不断合并加油站，直到只剩一个加油站为止。显然从这里出发就能顺利跑完全程。另一种证明方法：先让汽车油箱里装好足够多的油，随便从哪个加油站出发试跑一圈。车每到一个加油站时，记录此时油箱里剩下的油量，然后把那个加油站的油全部装上。试跑完一圈后，检查刚才路上到哪个加油站时剩的油量最少，那么空着油箱从那里出发显然一定能跑完全程。 No.5初始时，两个口袋里各有一个球。把后面的n-2个球依次放入口袋，放进哪个口袋其概率与各口袋已有的球数成正比。这样下来，球数较少的那个口袋平均期望有多少个球？ 答案先考虑一个看似无关的问题——怎样产生一个1到n的随机排列。首先，在纸上写下数字1；然后，把2写在1的左边或者右边；然后，把3写在最 左边，最右边，或者插进1和2之间……总之，把数字i等概率地放进由前面i-1个数产生的（包括最左端和最右端在内的）共i个空位中的一个。这样生成的显 然是一个完全随机的排列。我们换一个角度来看题目描述的过程：假想用一根绳子把两个球拴在一起，把这根绳子标号为1。接下来，把其中一个小球分裂成两个小球，这两个小球用 标号为2的绳子相连。总之，把“放进第i个球”的操作想象成把其中一个球分裂成两个用标有i-1的绳子相连的小球。联想我们前面的讨论，这些绳子的标号事 实上是一个随机的全排列，也就是说最开始绳子1的位置最后等可能地出现在每个地方。也就是说，它两边的小球个数(1,n-1)、(2,n-2)、 (3,n-3)、……、(n-1,1)这n-1种情况等可能地发生。因此，小袋子里的球数大约为n&#x2F;4个。准确地说，当n为奇数时，小袋子里的球数为 (n+1)&#x2F;4；当n为偶数时，小袋子里的球数为n^2&#x2F;(4n-4)。 No.6考虑一个n*n的棋盘，把有公共边的两个格子叫做相邻的格子。初始时，有些格子里有病毒。每一秒钟后，只要一个格子至少有两个相邻格子染上了病毒，那么他自己也会被感染。为了让所有的格子都被感染，初始时最少需要有几个带病毒的格子？给出一种方案并证明最优性。 答案至少要n个，比如一条对角线上的n个格子。n个格子也是必需的。当一个新的格子被感染后，全体被感染的格子所组成的图形的周长将减少0个、 2个或4个单位（具体减少了多少要看它周围被感染的格子有多少个）。又因为当所有格子都被感染后，图形的周长为4n，因此初始时至少要有n个被感染的格 子。 No.7在一个m*n的棋盘上，有k个格子里放有棋子。是否总能对所有棋子进行红蓝二染色，使得每行每列的红色棋子和蓝色棋子最多差一个？ 答案可以。建一个二分图G(X,Y)，其中X有m个顶点代表了棋盘的m个行，Y有n个顶点代表了棋盘的n个列。第i行第j列有棋子就在X(i) 和Y(j)之间连一条边。先找出图G里的所有环（由于是二分图，环的长度一定是偶数），把环里的边红蓝交替染色。剩下的没染色的图一定是一些树。对每棵树 递归地进行操作：去掉一个叶子节点和对应边，把剩下的树进行合法的红蓝二染色，再把刚才去掉的顶点和边加回去，给这个边适当的颜色以满足要求。 No.8任意给一个88的01矩阵，你每次只能选一个33或者4*4的子矩阵并把里面的元素全部取反。是否总有办法把矩阵里的所有数全部变为1？ 答案不能。大矩阵中有36个33的小矩阵和25个44的小矩阵，因此总共有61种可能的操作。显然，给定一个操作序列，这些操作的先后顺序 是无关紧要的；另外，在一个操作序列中使用两种或两种以上相同的操作也是无用的。因此，实质不同的操作序列只有2^61种。但8*8的01矩阵一共有 2^64种，因此不是每种情况都有办法达到目的。 No.9五个洞排成一排，其中一个洞里藏有一只狐狸。每个夜晚，狐狸都会跳到一个相邻的洞里；每个白天，你都只允许检查其中一个洞。怎样才能保证狐狸最终会被抓住？ 答案按照2, 3, 4, 2, 3, 4的顺序检查狐狸洞可以保证抓住狐狸。为了说明这个方案是可行的，用集合F表示狐狸可能出现的位置，初始时F &#x3D; {1, 2, 3, 4, 5}。如果它不在2号洞，则第二天狐狸已经跑到了F &#x3D; {2, 3, 4, 5}。如果此时它不在3号洞，则第三天狐狸一定跑到了F &#x3D; {1, 3, 4, 5}。如果此时它不在4号洞，则再过一晚后F &#x3D; {2, 4}。如果此时它不在2号洞，则再过一天F &#x3D; {3, 5}。如果此时它不在3号洞，再过一天它就一定跑到4号洞了。方案不是唯一的，下面这些方案都是可行的：2, 3, 4, 4, 3, 24, 3, 2, 2, 3, 44, 3, 2, 4, 3, 2 No.10一个经典老题是说，把一个333的立方体切成27个单位立方体，若每一刀切完后都允许重新摆放各个小块的位置，最少可以用几刀？答案仍然是6刀，因为 正中间那个单位立方体的6个面都是后来才切出来的，因此怎么也需要6刀。考虑这个问题：若把一个nnn的立方体切成一个个单位立方体，最少需要几刀？ 答案事实上，从一个更强的命题出发反而能使问题变得更简单。对于一个abc的长方体，我们需要f(a)+f(b)+f(c)刀，其中 f(x)&#x3D;⌈log(x)&#x2F;log(2)⌉。只需要注意到，在整个过程中的任何一步，切完当前最大的块所需要的刀数也就等于整个过程还需要的刀数，因为其 它小块需要的刀数都不会超过最大块所需刀数，它们都可以与最大块一道并行处理。这表明，我们的最优决策即是让当前的最大块尽可能的小，也就是说要把当前的 最大块尽可能相等地切成两半。利用数学归纳法，我们可以很快得到本段开头的结论。","tags":["智力题","algorithm"],"categories":["algorithm"]},{"title":"关于","path":"/about/index.html","content":"Believe in yourself.BlogWikiMessageResumePicSky Yanliang 🍂 🍀 个人简介 社畜一枚，坐标 SZ🌏，永远热爱，永远年轻相知🤞 爱好旅行/音乐/电影/阅读✨ 对一切新鲜的事物充满好奇🧐 目标是过简单的生活💪 对自己的要求是每天都要比昨天进步一点点👊 📆 建站历程 2021 年 7 月 30 日建站 🎨添加twikoo评论 📨添加首页置顶文章轮播功能 🎉 👨‍✈️ 版权声明 123456博客内的所有原创内容（包括但不限于文章、图像等）除特别声明外均采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议，任何人都可以自由传播，但不得用于商用且必须署名并以相同方式分享。本站部分内容转载于网络，有出处的已在文中署名作者并附加原文链接，出处已不可寻的皆已标注来源于网络，若您认为本博客有部分内容侵犯了您的版权，请电邮告知，我将认真处理。 赏 .single-reward { position: relative; width: 100%; margin: 30px auto; text-align: center; z-index: 999 } .single-reward .reward-open { position: relative; line-height: 24px; width: 25px; height: 25px; padding: 7px; color: #fff; text-align: center; display: inline-block; border-radius: 100%; background: #d34836; cursor: pointer; } .single-reward .reward-main { position: absolute; top: 48px; left: -153.5px; margin: 0; padding: 4px 0 0; width: 355px; background: 0 0; display: none; animation: main .4s } .reward-open:hover .reward-main { display: block } .single-reward .reward-row { margin: 0 auto; padding: 20px 15px 10px; background: #f5f5f5; display: inline-block; border-radius: 4px; } .single-reward .reward-row:before { content: \"\"; width: 0; height: 0; border-left: 13px solid transparent; border-right: 13px solid transparent; border-bottom: 13px solid #f5f5f5; position: absolute; top: -9px; left: -9px; right: 0; margin: 0 auto } .single-reward .reward-row li { list-style-type: none; padding: 0 12px; display: inline-block } .reward-row li img { width: 130px; max-width: 130px; border-radius: 3px; position: relative } .reward-row li::after { margin-top: 10px; display: block; font-size: 13px; color: #121212; } .alipay-code:after { content: \"支付宝\" } .wechat-code:after { content: \"微信\" } .md .single-reward ul li:before{ content: none }"},{"title":"Friends","path":"/friends/index.html","content":"友链更新通知由于近期对友链系统进行了重做，原链接失效的小伙伴请按照下方交换友链的步骤进行填写。在新的友链系统中，您随时可以对自己的信息进行修改而无需等待博主更新。 我可以交换友链吗？先友后链，在我们有一定了解了之后才可以交换友链，除此之外，您的网站还应满足以下条件： 合法的、非营利性、无商业广告 有实质性原创内容的 HTTPS 站点 如何自助添加友链？第一步：新建 Issue新建 GitHub Issue 按照模板格式填写并提交。为了提高图片加载速度，建议使用 Jsdelivr + Github 优化头像第二步：添加友链并等待管理员审核请添加本站到您的友链中，如果您也使用 issue 作为友链源，只需要告知您的友链源仓库即可。123title: Yanliangavatar: https://cdn.jsdelivr.net/gh/gaoyanliang/cdn@main/blog/img/head.pngurl: https://yanliang.cool待管理员审核通过，添加了 active 标签后，回来刷新即可生效。 如果您需要更新自己的友链，请直接修改 issue 内容，大约 3 分钟内生效，无需等待博客更新。如果无法修改，可以重新创建一个。"},{"path":"/recent/index.html","content":""},{"title":"第一章：可靠性、可伸缩性和可维护性","path":"/wiki/ddia/ch1.html","content":"互联网做得太棒了，以至于大多数人将它看作像太平洋这样的自然资源，而不是什么人工产物。上一次出现这种大规模且无差错的技术， 你还记得是什么时候吗？ —— 艾伦・凯 在接受 Dobb 博士杂志采访时说（2012 年） 现今很多应用程序都是 数据密集型（data-intensive） 的，而非 计算密集型（compute-intensive） 的。因此 CPU 很少成为这类应用的瓶颈，更大的问题通常来自数据量、数据复杂性、以及数据的变更速度。 数据密集型应用通常由标准组件构建而成，标准组件提供了很多通用的功能；例如，许多应用程序都需要： 存储数据，以便自己或其他应用程序之后能再次找到 （数据库，即 databases） 记住开销昂贵操作的结果，加快读取速度（缓存，即 caches） 允许用户按关键字搜索数据，或以各种方式对数据进行过滤（搜索索引，即 search indexes） 向其他进程发送消息，进行异步处理（流处理，即 stream processing） 定期处理累积的大批量数据（批处理，即 batch processing） 如果这些功能听上去平淡无奇，那是因为这些 数据系统（data system） 是非常成功的抽象：我们一直不假思索地使用它们并习以为常。绝大多数工程师不会幻想从零开始编写存储引擎，因为在开发应用时，数据库已经是足够完美的工具了。 但现实没有这么简单。不同的应用有着不同的需求，因而数据库系统也是百花齐放，有着各式各样的特性。实现缓存有很多种手段，创建搜索索引也有好几种方法，诸如此类。因此在开发应用前，我们依然有必要先弄清楚最适合手头工作的工具和方法。而且当单个工具解决不了你的问题时，组合使用这些工具可能还是有些难度的。 本书将是一趟关于数据系统原理、实践与应用的旅程，并讲述了设计数据密集型应用的方法。我们将探索不同工具之间的共性与特性，以及各自的实现原理。 本章将从我们所要实现的基础目标开始：可靠、可伸缩、可维护的数据系统。我们将澄清这些词语的含义，概述考量这些目标的方法。并回顾一些后续章节所需的基础知识。在接下来的章节中我们将抽丝剥茧，研究设计数据密集型应用时可能遇到的设计决策。 关于数据系统的思考我们通常认为，数据库、消息队列、缓存等工具分属于几个差异显著的类别。虽然数据库和消息队列表面上有一些相似性 —— 它们都会存储一段时间的数据 —— 但它们有迥然不同的访问模式，这意味着迥异的性能特征和实现手段。 那我们为什么要把这些东西放在 数据系统（data system） 的总称之下混为一谈呢？ 近些年来，出现了许多新的数据存储工具与数据处理工具。它们针对不同应用场景进行优化，因此不再适合生硬地归入传统类别【1】。类别之间的界限变得越来越模糊，例如：数据存储可以被当成消息队列用（Redis），消息队列则带有类似数据库的持久保证（Apache Kafka）。 其次，越来越多的应用程序有着各种严格而广泛的要求，单个工具不足以满足所有的数据处理和存储需求。取而代之的是，总体工作被拆分成一系列能被单个工具高效完成的任务，并通过应用代码将它们缝合起来。 例如，如果将缓存（应用管理的缓存层，Memcached 或同类产品）和全文搜索（全文搜索服务器，例如 Elasticsearch 或 Solr）功能从主数据库剥离出来，那么使缓存 &#x2F; 索引与主数据库保持同步通常是应用代码的责任。图 1-1 给出了这种架构可能的样子（细节将在后面的章节中详细介绍）。 图 1-1 一个可能的组合使用多个组件的数据系统架构 当你将多个工具组合在一起提供服务时，服务的接口或 应用程序编程接口（API, Application Programming Interface） 通常向客户端隐藏这些实现细节。现在，你基本上已经使用较小的通用组件创建了一个全新的、专用的数据系统。这个新的复合数据系统可能会提供特定的保证，例如：缓存在写入时会作废或更新，以便外部客户端获取一致的结果。现在你不仅是应用程序开发人员，还是数据系统设计人员了。 设计数据系统或服务时可能会遇到很多棘手的问题，例如：当系统出问题时，如何确保数据的正确性和完整性？当部分系统退化降级时，如何为客户提供始终如一的良好性能？当负载增加时，如何扩容应对？什么样的 API 才是好的 API？ 影响数据系统设计的因素很多，包括参与人员的技能和经验、历史遗留问题、系统路径依赖、交付时限、公司的风险容忍度、监管约束等，这些因素都需要具体问题具体分析。 本书着重讨论三个在大多数软件系统中都很重要的问题： 可靠性（Reliability） 系统在 困境（adversity，比如硬件故障、软件故障、人为错误）中仍可正常工作（正确完成功能，并能达到期望的性能水准）。请参阅 “可靠性”。 可伸缩性（Scalability） 有合理的办法应对系统的增长（数据量、流量、复杂性）。请参阅 “可伸缩性”。 可维护性（Maintainability） 许多不同的人（工程师、运维）在不同的生命周期，都能高效地在系统上工作（使系统保持现有行为，并适应新的应用场景）。请参阅 “可维护性”。 人们经常追求这些词汇，却没有清楚理解它们到底意味着什么。为了工程的严谨性，本章的剩余部分将探讨可靠性、可伸缩性和可维护性的含义。为实现这些目标而使用的各种技术，架构和算法将在后续的章节中研究。 可靠性人们对于一个东西是否可靠，都有一个直观的想法。人们对可靠软件的典型期望包括： 应用程序表现出用户所期望的功能。 允许用户犯错，允许用户以出乎意料的方式使用软件。 在预期的负载和数据量下，性能满足要求。 系统能防止未经授权的访问和滥用。 如果所有这些在一起意味着 “正确工作”，那么可以把可靠性粗略理解为 “即使出现问题，也能继续正确工作”。 造成错误的原因叫做 故障（fault），能预料并应对故障的系统特性可称为 容错（fault-tolerant） 或 韧性（resilient）。“容错” 一词可能会产生误导，因为它暗示着系统可以容忍所有可能的错误，但在实际中这是不可能的。比方说，如果整个地球（及其上的所有服务器）都被黑洞吞噬了，想要容忍这种错误，需要把网络托管到太空中 —— 这种预算能不能批准就祝你好运了。所以在讨论容错时，只有谈论特定类型的错误才有意义。 注意 故障（fault） 不同于 失效（failure）【2】。故障 通常定义为系统的一部分状态偏离其标准，而 失效 则是系统作为一个整体停止向用户提供服务。故障的概率不可能降到零，因此最好设计容错机制以防因 故障 而导致 失效。本书中我们将介绍几种用不可靠的部件构建可靠系统的技术。 反直觉的是，在这类容错系统中，通过故意触发来 提高 故障率是有意义的，例如：在没有警告的情况下随机地杀死单个进程。许多高危漏洞实际上是由糟糕的错误处理导致的【3】，因此我们可以通过故意引发故障来确保容错机制不断运行并接受考验，从而提高故障自然发生时系统能正确处理的信心。Netflix 公司的 Chaos Monkey【4】就是这种方法的一个例子。 尽管比起 阻止错误（prevent error），我们通常更倾向于 容忍错误。但也有 预防胜于治疗 的情况（比如不存在治疗方法时）。安全问题就属于这种情况。例如，如果攻击者破坏了系统，并获取了敏感数据，这种事是撤销不了的。但本书主要讨论的是可以恢复的故障种类，正如下面几节所述。 硬件故障当想到系统失效的原因时，硬件故障（hardware faults） 总会第一个进入脑海。硬盘崩溃、内存出错、机房断电、有人拔错网线…… 任何与大型数据中心打过交道的人都会告诉你：一旦你拥有很多机器，这些事情 总 会发生！ 据报道称，硬盘的 平均无故障时间（MTTF, mean time to failure） 约为 10 到 50 年【5】【6】。因此从数学期望上讲，在拥有 10000 个磁盘的存储集群上，平均每天会有 1 个磁盘出故障。 为了减少系统的故障率，第一反应通常都是增加单个硬件的冗余度，例如：磁盘可以组建 RAID，服务器可能有双路电源和热插拔 CPU，数据中心可能有电池和柴油发电机作为后备电源，某个组件挂掉时冗余组件可以立刻接管。这种方法虽然不能完全防止由硬件问题导致的系统失效，但它简单易懂，通常也足以让机器不间断运行很多年。 直到最近，硬件冗余对于大多数应用来说已经足够了，它使单台机器完全失效变得相当罕见。只要你能快速地把备份恢复到新机器上，故障停机时间对大多数应用而言都算不上灾难性的。只有少量高可用性至关重要的应用才会要求有多套硬件冗余。 但是随着数据量和应用计算需求的增加，越来越多的应用开始大量使用机器，这会相应地增加硬件故障率。此外，在类似亚马逊 AWS（Amazon Web Services）的一些云服务平台上，虚拟机实例不可用却没有任何警告也是很常见的【7】，因为云平台的设计就是优先考虑 灵活性（flexibility） 和 弹性（elasticity）[^i]，而不是单机可靠性。 如果在硬件冗余的基础上进一步引入软件容错机制，那么系统在容忍整个（单台）机器故障的道路上就更进一步了。这样的系统也有运维上的便利，例如：如果需要重启机器（例如应用操作系统安全补丁），单服务器系统就需要计划停机。而允许机器失效的系统则可以一次修复一个节点，无需整个系统停机。 [^i]: 在 应对负载的方法 一节定义 软件错误我们通常认为硬件故障是随机的、相互独立的：一台机器的磁盘失效并不意味着另一台机器的磁盘也会失效。虽然大量硬件组件之间可能存在微弱的相关性（例如服务器机架的温度等共同的原因），但同时发生故障也是极为罕见的。 另一类错误是内部的 系统性错误（systematic error）【8】。这类错误难以预料，而且因为是跨节点相关的，所以比起不相关的硬件故障往往可能造成更多的 系统失效【5】。例子包括： 接受特定的错误输入，便导致所有应用服务器实例崩溃的 BUG。例如 2012 年 6 月 30 日的闰秒，由于 Linux 内核中的一个错误【9】，许多应用同时挂掉了。 失控进程会用尽一些共享资源，包括 CPU 时间、内存、磁盘空间或网络带宽。 系统依赖的服务变慢，没有响应，或者开始返回错误的响应。 级联故障，一个组件中的小故障触发另一个组件中的故障，进而触发更多的故障【10】。 导致这类软件故障的 BUG 通常会潜伏很长时间，直到被异常情况触发为止。这种情况意味着软件对其环境做出了某种假设 —— 虽然这种假设通常来说是正确的，但由于某种原因最后不再成立了【11】。 虽然软件中的系统性故障没有速效药，但我们还是有很多小办法，例如：仔细考虑系统中的假设和交互；彻底的测试；进程隔离；允许进程崩溃并重启；测量、监控并分析生产环境中的系统行为。如果系统能够提供一些保证（例如在一个消息队列中，进入与发出的消息数量相等），那么系统就可以在运行时不断自检，并在出现 差异（discrepancy） 时报警【12】。 人为错误设计并构建了软件系统的工程师是人类，维持系统运行的运维也是人类。即使他们怀有最大的善意，人类也是不可靠的。举个例子，一项关于大型互联网服务的研究发现，运维配置错误是导致服务中断的首要原因，而硬件故障（服务器或网络）仅导致了 10-25% 的服务中断【13】。 尽管人类不可靠，但怎么做才能让系统变得可靠？最好的系统会组合使用以下几种办法： 以最小化犯错机会的方式设计系统。例如，精心设计的抽象、API 和管理后台使做对事情更容易，搞砸事情更困难。但如果接口限制太多，人们就会忽略它们的好处而想办法绕开。很难正确把握这种微妙的平衡。 将人们最容易犯错的地方与可能导致失效的地方 解耦（decouple）。特别是提供一个功能齐全的非生产环境 沙箱（sandbox），使人们可以在不影响真实用户的情况下，使用真实数据安全地探索和实验。 在各个层次进行彻底的测试【3】，从单元测试、全系统集成测试到手动测试。自动化测试易于理解，已经被广泛使用，特别适合用来覆盖正常情况中少见的 边缘场景（corner case）。 允许从人为错误中简单快速地恢复，以最大限度地减少失效情况带来的影响。 例如，快速回滚配置变更，分批发布新代码（以便任何意外错误只影响一小部分用户），并提供数据重算工具（以备旧的计算出错）。 配置详细和明确的监控，比如性能指标和错误率。 在其他工程学科中这指的是 遥测（telemetry）（一旦火箭离开了地面，遥测技术对于跟踪发生的事情和理解失败是至关重要的）。监控可以向我们发出预警信号，并允许我们检查是否有任何地方违反了假设和约束。当出现问题时，指标数据对于问题诊断是非常宝贵的。 良好的管理实践与充分的培训 —— 一个复杂而重要的方面，但超出了本书的范围。 可靠性有多重要？可靠性不仅仅是针对核电站和空中交通管制软件而言，我们也期望更多平凡的应用能可靠地运行。商务应用中的错误会导致生产力损失（也许数据报告不完整还会有法律风险），而电商网站的中断则可能会导致收入和声誉的巨大损失。 即使在 “非关键” 应用中，我们也对用户负有责任。试想一位家长把所有的照片和孩子的视频储存在你的照片应用里【15】。如果数据库突然损坏，他们会感觉如何？他们可能会知道如何从备份恢复吗？ 在某些情况下，我们可能会选择牺牲可靠性来降低开发成本（例如为未经证实的市场开发产品原型）或运营成本（例如利润率极低的服务），但我们偷工减料时，应该清楚意识到自己在做什么。 可伸缩性系统今天能可靠运行，并不意味未来也能可靠运行。服务 降级（degradation） 的一个常见原因是负载增加，例如：系统负载已经从一万个并发用户增长到十万个并发用户，或者从一百万增长到一千万。也许现在处理的数据量级要比过去大得多。 可伸缩性（Scalability） 是用来描述系统应对负载增长能力的术语。但是请注意，这不是贴在系统上的一维标签：说 “X 可伸缩” 或 “Y 不可伸缩” 是没有任何意义的。相反，讨论可伸缩性意味着考虑诸如 “如果系统以特定方式增长，有什么选项可以应对增长？” 和 “如何增加计算资源来处理额外的负载？” 等问题。 描述负载在讨论增长问题（如果负载加倍会发生什么？）前，首先要能简要描述系统的当前负载。负载可以用一些称为 负载参数（load parameters） 的数字来描述。参数的最佳选择取决于系统架构，它可能是每秒向 Web 服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的用户数量、缓存命中率或其他东西。除此之外，也许平均情况对你很重要，也许你的瓶颈是少数极端场景。 为了使这个概念更加具体，我们以推特在 2012 年 11 月发布的数据【16】为例。推特的两个主要业务是： 发布推文 用户可以向其粉丝发布新消息（平均 4.6k 请求 &#x2F; 秒，峰值超过 12k 请求 &#x2F; 秒）。 主页时间线 用户可以查阅他们关注的人发布的推文（300k 请求 &#x2F; 秒）。 处理每秒 12,000 次写入（发推文的速率峰值）还是很简单的。然而推特的伸缩性挑战并不是主要来自推特量，而是来自 扇出（fan-out）[^ii]—— 每个用户关注了很多人，也被很多人关注。 [^ii]: 扇出：从电子工程学中借用的术语，它描述了输入连接到另一个门输出的逻辑门数量。 输出需要提供足够的电流来驱动所有连接的输入。 在事务处理系统中，我们使用它来描述为了服务一个传入请求而需要执行其他服务的请求数量。 大体上讲，这一对操作有两种实现方式。 发布推文时，只需将新推文插入全局推文集合即可。当一个用户请求自己的主页时间线时，首先查找他关注的所有人，查询这些被关注用户发布的推文并按时间顺序合并。在如 图 1-2 所示的关系型数据库中，可以编写这样的查询： 12345SELECT tweets.*, users.* FROM tweets JOIN users ON tweets.sender_id = users.id JOIN follows ON follows.followee_id = users.id WHERE follows.follower_id = current_user 图 1-2 推特主页时间线的关系型模式简单实现 为每个用户的主页时间线维护一个缓存，就像每个用户的推文收件箱（图 1-3）。 当一个用户发布推文时，查找所有关注该用户的人，并将新的推文插入到每个主页时间线缓存中。 因此读取主页时间线的请求开销很小，因为结果已经提前计算好了。 图 1-3 用于分发推特至关注者的数据流水线，2012 年 11 月的负载参数【16】 推特的第一个版本使用了方法 1，但系统很难跟上主页时间线查询的负载。所以公司转向了方法 2，方法 2 的效果更好，因为发推频率比查询主页时间线的频率几乎低了两个数量级，所以在这种情况下，最好在写入时做更多的工作，而在读取时做更少的工作。 然而方法 2 的缺点是，发推现在需要大量的额外工作。平均来说，一条推文会发往约 75 个关注者，所以每秒 4.6k 的发推写入，变成了对主页时间线缓存每秒 345k 的写入。但这个平均值隐藏了用户粉丝数差异巨大这一现实，一些用户有超过 3000 万的粉丝，这意味着一条推文就可能会导致主页时间线缓存的 3000 万次写入！及时完成这种操作是一个巨大的挑战 —— 推特尝试在 5 秒内向粉丝发送推文。 在推特的例子中，每个用户粉丝数的分布（可能按这些用户的发推频率来加权）是探讨可伸缩性的一个关键负载参数，因为它决定了扇出负载。你的应用程序可能具有非常不同的特征，但可以采用相似的原则来考虑它的负载。 推特轶事的最终转折：现在已经稳健地实现了方法 2，推特逐步转向了两种方法的混合。大多数用户发的推文会被扇出写入其粉丝主页时间线缓存中。但是少数拥有海量粉丝的用户（即名流）会被排除在外。当用户读取主页时间线时，分别地获取出该用户所关注的每位名流的推文，再与用户的主页时间线缓存合并，如方法 1 所示。这种混合方法能始终如一地提供良好性能。在 第十二章 中我们将重新讨论这个例子，这在覆盖更多技术层面之后。 描述性能一旦系统的负载被描述好，就可以研究当负载增加会发生什么。我们可以从两种角度来看： 增加负载参数并保持系统资源（CPU、内存、网络带宽等）不变时，系统性能将受到什么影响？ 增加负载参数并希望保持性能不变时，需要增加多少系统资源？ 这两个问题都需要性能数据，所以让我们简单地看一下如何描述系统性能。 对于 Hadoop 这样的批处理系统，通常关心的是 吞吐量（throughput），即每秒可以处理的记录数量，或者在特定规模数据集上运行作业的总时间 [^iii]。对于在线系统，通常更重要的是服务的 响应时间（response time），即客户端发送请求到接收响应之间的时间。 [^iii]: 理想情况下，批量作业的运行时间是数据集的大小除以吞吐量。 在实践中由于数据倾斜（数据不是均匀分布在每个工作进程中），需要等待最慢的任务完成，所以运行时间往往更长。 延迟和响应时间 延迟（latency） 和 响应时间（response time） 经常用作同义词，但实际上它们并不一样。响应时间是客户所看到的，除了实际处理请求的时间（ 服务时间（service time） ）之外，还包括网络延迟和排队延迟。延迟是某个请求等待处理的 持续时长，在此期间它处于 休眠（latent） 状态，并等待服务【17】。 即使不断重复发送同样的请求，每次得到的响应时间也都会略有不同。现实世界的系统会处理各式各样的请求，响应时间可能会有很大差异。因此我们需要将响应时间视为一个可以测量的数值 分布（distribution），而不是单个数值。 在 图 1-4 中，每个灰条代表一次对服务的请求，其高度表示请求花费了多长时间。大多数请求是相当快的，但偶尔会出现需要更长的时间的异常值。这也许是因为缓慢的请求实质上开销更大，例如它们可能会处理更多的数据。但即使（你认为）所有请求都花费相同时间的情况下，随机的附加延迟也会导致结果变化，例如：上下文切换到后台进程，网络数据包丢失与 TCP 重传，垃圾收集暂停，强制从磁盘读取的页面错误，服务器机架中的震动【18】，还有很多其他原因。 图 1-4 展示了一个服务 100 次请求响应时间的均值与百分位数 通常报表都会展示服务的平均响应时间。 （严格来讲 “平均” 一词并不指代任何特定公式，但实际上它通常被理解为 算术平均值（arithmetic mean）：给定 n 个值，加起来除以 n ）。然而如果你想知道 “典型（typical）” 响应时间，那么平均值并不是一个非常好的指标，因为它不能告诉你有多少用户实际上经历了这个延迟。 通常使用 百分位点（percentiles） 会更好。如果将响应时间列表按最快到最慢排序，那么 中位数（median） 就在正中间：举个例子，如果你的响应时间中位数是 200 毫秒，这意味着一半请求的返回时间少于 200 毫秒，另一半比这个要长。 如果想知道典型场景下用户需要等待多长时间，那么中位数是一个好的度量标准：一半用户请求的响应时间少于响应时间的中位数，另一半服务时间比中位数长。中位数也被称为第 50 百分位点，有时缩写为 p50。注意中位数是关于单个请求的；如果用户同时发出几个请求（在一个会话过程中，或者由于一个页面中包含了多个资源），则至少一个请求比中位数慢的概率远大于 50%。 为了弄清异常值有多糟糕，可以看看更高的百分位点，例如第 95、99 和 99.9 百分位点（缩写为 p95，p99 和 p999）。它们意味着 95%、99% 或 99.9% 的请求响应时间要比该阈值快，例如：如果第 95 百分位点响应时间是 1.5 秒，则意味着 100 个请求中的 95 个响应时间快于 1.5 秒，而 100 个请求中的 5 个响应时间超过 1.5 秒。如 图 1-4 所示。 响应时间的高百分位点（也称为 尾部延迟，即 tail latencies）非常重要，因为它们直接影响用户的服务体验。例如亚马逊在描述内部服务的响应时间要求时是以 99.9 百分位点为准，即使它只影响一千个请求中的一个。这是因为请求响应最慢的客户往往也是数据最多的客户，也可以说是最有价值的客户 —— 因为他们掏钱了【19】。保证网站响应迅速对于保持客户的满意度非常重要，亚马逊观察到：响应时间增加 100 毫秒，销售量就减少 1%【20】；而另一些报告说：慢 1 秒钟会让客户满意度指标减少 16%【21，22】。 另一方面，优化第 99.99 百分位点（一万个请求中最慢的一个）被认为太昂贵了，不能为亚马逊的目标带来足够好处。减小高百分位点处的响应时间相当困难，因为它很容易受到随机事件的影响，这超出了控制范围，而且效益也很小。 百分位点通常用于 服务级别目标（SLO, service level objectives） 和 服务级别协议（SLA, service level agreements），即定义服务预期性能和可用性的合同。 SLA 可能会声明，如果服务响应时间的中位数小于 200 毫秒，且 99.9 百分位点低于 1 秒，则认为服务工作正常（如果响应时间更长，就认为服务不达标）。这些指标为客户设定了期望值，并允许客户在 SLA 未达标的情况下要求退款。 排队延迟（queueing delay） 通常占了高百分位点处响应时间的很大一部分。由于服务器只能并行处理少量的事务（如受其 CPU 核数的限制），所以只要有少量缓慢的请求就能阻碍后续请求的处理，这种效应有时被称为 头部阻塞（head-of-line blocking） 。即使后续请求在服务器上处理的非常迅速，由于需要等待先前请求完成，客户端最终看到的是缓慢的总体响应时间。因为存在这种效应，测量客户端的响应时间非常重要。 为测试系统的可伸缩性而人为产生负载时，产生负载的客户端要独立于响应时间不断发送请求。如果客户端在发送下一个请求之前等待先前的请求完成，这种行为会产生人为排队的效果，使得测试时的队列比现实情况更短，使测量结果产生偏差【23】。 实践中的百分位点 在多重调用的后端服务里，高百分位数变得特别重要。即使并行调用，最终用户请求仍然需要等待最慢的并行调用完成。如 图 1-5 所示，只需要一个缓慢的调用就可以使整个最终用户请求变慢。即使只有一小部分后端调用速度较慢，如果最终用户请求需要多个后端调用，则获得较慢调用的机会也会增加，因此较高比例的最终用户请求速度会变慢（效果称为尾部延迟放大【24】）。 如果你想将响应时间百分点添加到你的服务的监视仪表板，则需要持续有效地计算它们。例如，你可能希望在最近 10 分钟内保持请求响应时间的滚动窗口。每一分钟，你都会计算出该窗口中的中值和各种百分数，并将这些度量值绘制在图上。 简单的实现是在时间窗口内保存所有请求的响应时间列表，并且每分钟对列表进行排序。如果对你来说效率太低，那么有一些算法能够以最小的 CPU 和内存成本（如前向衰减【25】、t-digest【26】或 HdrHistogram 【27】）来计算百分位数的近似值。请注意，平均百分比（例如，减少时间分辨率或合并来自多台机器的数据）在数学上没有意义 - 聚合响应时间数据的正确方法是添加直方图【28】。 图 1-5 当一个请求需要多个后端请求时，单个后端慢请求就会拖慢整个终端用户的请求 应对负载的方法现在我们已经讨论了用于描述负载的参数和用于衡量性能的指标。可以开始认真讨论可伸缩性了：当负载参数增加时，如何保持良好的性能？ 适应某个级别负载的架构不太可能应付 10 倍于此的负载。如果你正在开发一个快速增长的服务，那么每次负载发生数量级的增长时，你可能都需要重新考虑架构 —— 或者更频繁。 人们经常讨论 纵向伸缩（scaling up，也称为垂直伸缩，即 vertical scaling，转向更强大的机器）和 横向伸缩（scaling out，也称为水平伸缩，即 horizontal scaling，将负载分布到多台小机器上）之间的对立。跨多台机器分配负载也称为 “无共享（shared-nothing）” 架构。可以在单台机器上运行的系统通常更简单，但高端机器可能非常贵，所以非常密集的负载通常无法避免地需要横向伸缩。现实世界中的优秀架构需要将这两种方法务实地结合，因为使用几台足够强大的机器可能比使用大量的小型虚拟机更简单也更便宜。 有些系统是 弹性（elastic） 的，这意味着可以在检测到负载增加时自动增加计算资源，而其他系统则是手动伸缩（人工分析容量并决定向系统添加更多的机器）。如果负载 极难预测（highly unpredictable），则弹性系统可能很有用，但手动伸缩系统更简单，并且意外操作可能会更少（请参阅 “分区再平衡”）。 跨多台机器部署 无状态服务（stateless services） 非常简单，但将带状态的数据系统从单节点变为分布式配置则可能引入许多额外复杂度。出于这个原因，常识告诉我们应该将数据库放在单个节点上（纵向伸缩），直到伸缩成本或可用性需求迫使其改为分布式。 随着分布式系统的工具和抽象越来越好，至少对于某些类型的应用而言，这种常识可能会改变。可以预见分布式数据系统将成为未来的默认设置，即使对不处理大量数据或流量的场景也如此。本书的其余部分将介绍多种分布式数据系统，不仅讨论它们在可伸缩性方面的表现，还包括易用性和可维护性。 大规模的系统架构通常是应用特定的 —— 没有一招鲜吃遍天的通用可伸缩架构（不正式的叫法：万金油（magic scaling sauce） ）。应用的问题可能是读取量、写入量、要存储的数据量、数据的复杂度、响应时间要求、访问模式或者所有问题的大杂烩。 举个例子，用于处理每秒十万个请求（每个大小为 1 kB）的系统与用于处理每分钟 3 个请求（每个大小为 2GB）的系统看上去会非常不一样，尽管两个系统有同样的数据吞吐量。 一个良好适配应用的可伸缩架构，是围绕着 假设（assumption） 建立的：哪些操作是常见的？哪些操作是罕见的？这就是所谓负载参数。如果假设最终是错误的，那么为伸缩所做的工程投入就白费了，最糟糕的是适得其反。在早期创业公司或非正式产品中，通常支持产品快速迭代的能力，要比可伸缩至未来的假想负载要重要的多。 尽管这些架构是应用程序特定的，但可伸缩的架构通常也是从通用的积木块搭建而成的，并以常见的模式排列。在本书中，我们将讨论这些构件和模式。 可维护性众所周知，软件的大部分开销并不在最初的开发阶段，而是在持续的维护阶段，包括修复漏洞、保持系统正常运行、调查失效、适配新的平台、为新的场景进行修改、偿还技术债和添加新的功能。 不幸的是，许多从事软件系统行业的人不喜欢维护所谓的 遗留（legacy） 系统，—— 也许因为涉及修复其他人的错误、和过时的平台打交道，或者系统被迫使用于一些份外工作。每一个遗留系统都以自己的方式让人不爽，所以很难给出一个通用的建议来和它们打交道。 但是我们可以，也应该以这样一种方式来设计软件：在设计之初就尽量考虑尽可能减少维护期间的痛苦，从而避免自己的软件系统变成遗留系统。为此，我们将特别关注软件系统的三个设计原则： 可操作性（Operability） 便于运维团队保持系统平稳运行。 简单性（Simplicity） 从系统中消除尽可能多的 复杂度（complexity），使新工程师也能轻松理解系统（注意这和用户接口的简单性不一样）。 可演化性（evolvability） 使工程师在未来能轻松地对系统进行更改，当需求变化时为新应用场景做适配。也称为 可扩展性（extensibility）、可修改性（modifiability） 或 可塑性（plasticity）。 和之前提到的可靠性、可伸缩性一样，实现这些目标也没有简单的解决方案。不过我们会试着想象具有可操作性，简单性和可演化性的系统会是什么样子。 可操作性：人生苦短，关爱运维有人认为，“良好的运维经常可以绕开垃圾（或不完整）软件的局限性，而再好的软件摊上垃圾运维也没法可靠运行”。尽管运维的某些方面可以，而且应该是自动化的，但在最初建立正确运作的自动化机制仍然取决于人。 运维团队对于保持软件系统顺利运行至关重要。一个优秀运维团队的典型职责如下（或者更多）【29】： 监控系统的运行状况，并在服务状态不佳时快速恢复服务。 跟踪问题的原因，例如系统故障或性能下降。 及时更新软件和平台，比如安全补丁。 了解系统间的相互作用，以便在异常变更造成损失前进行规避。 预测未来的问题，并在问题出现之前加以解决（例如，容量规划）。 建立部署、配置、管理方面的良好实践，编写相应工具。 执行复杂的维护任务，例如将应用程序从一个平台迁移到另一个平台。 当配置变更时，维持系统的安全性。 定义工作流程，使运维操作可预测，并保持生产环境稳定。 铁打的营盘流水的兵，维持组织对系统的了解。 良好的可操作性意味着更轻松的日常工作，进而运维团队能专注于高价值的事情。数据系统可以通过各种方式使日常任务更轻松： 通过良好的监控，提供对系统内部状态和运行时行为的 可见性（visibility）。 为自动化提供良好支持，将系统与标准化工具相集成。 避免依赖单台机器（在整个系统继续不间断运行的情况下允许机器停机维护）。 提供良好的文档和易于理解的操作模型（“如果做 X，会发生 Y”）。 提供良好的默认行为，但需要时也允许管理员自由覆盖默认值。 有条件时进行自我修复，但需要时也允许管理员手动控制系统状态。 行为可预测，最大限度减少意外。 简单性：管理复杂度小型软件项目可以使用简单讨喜的、富表现力的代码，但随着项目越来越大，代码往往变得非常复杂，难以理解。这种复杂度拖慢了所有系统相关人员，进一步增加了维护成本。一个陷入复杂泥潭的软件项目有时被描述为 烂泥潭（a big ball of mud） 【30】。 复杂度（complexity） 有各种可能的症状，例如：状态空间激增、模块间紧密耦合、纠结的依赖关系、不一致的命名和术语、解决性能问题的 Hack、需要绕开的特例等等，现在已经有很多关于这个话题的讨论【31,32,33】。 因为复杂度导致维护困难时，预算和时间安排通常会超支。在复杂的软件中进行变更，引入错误的风险也更大：当开发人员难以理解系统时，隐藏的假设、无意的后果和意外的交互就更容易被忽略。相反，降低复杂度能极大地提高软件的可维护性，因此简单性应该是构建系统的一个关键目标。 简化系统并不一定意味着减少功能；它也可以意味着消除 额外的（accidental） 的复杂度。 Moseley 和 Marks【32】把 额外复杂度 定义为：由具体实现中涌现，而非（从用户视角看，系统所解决的）问题本身固有的复杂度。 用于消除 额外复杂度 的最好工具之一是 抽象（abstraction）。一个好的抽象可以将大量实现细节隐藏在一个干净，简单易懂的外观下面。一个好的抽象也可以广泛用于各类不同应用。比起重复造很多轮子，重用抽象不仅更有效率，而且有助于开发高质量的软件。抽象组件的质量改进将使所有使用它的应用受益。 例如，高级编程语言是一种抽象，隐藏了机器码、CPU 寄存器和系统调用。 SQL 也是一种抽象，隐藏了复杂的磁盘 &#x2F; 内存数据结构、来自其他客户端的并发请求、崩溃后的不一致性。当然在用高级语言编程时，我们仍然用到了机器码；只不过没有 直接（directly） 使用罢了，正是因为编程语言的抽象，我们才不必去考虑这些实现细节。 抽象可以帮助我们将系统的复杂度控制在可管理的水平，不过，找到好的抽象是非常困难的。在分布式系统领域虽然有许多好的算法，但我们并不清楚它们应该打包成什么样抽象。 本书将紧盯那些允许我们将大型系统的部分提取为定义明确的、可重用的组件的优秀抽象。 可演化性：拥抱变化系统的需求永远不变，基本是不可能的。更可能的情况是，它们处于常态的变化中，例如：你了解了新的事实、出现意想不到的应用场景、业务优先级发生变化、用户要求新功能、新平台取代旧平台、法律或监管要求发生变化、系统增长迫使架构变化等。 在组织流程方面， 敏捷（agile） 工作模式为适应变化提供了一个框架。敏捷社区还开发了对在频繁变化的环境中开发软件很有帮助的技术工具和模式，如 测试驱动开发（TDD, test-driven development） 和 重构（refactoring） 。 这些敏捷技术的大部分讨论都集中在相当小的规模（同一个应用中的几个代码文件）。本书将探索在更大数据系统层面上提高敏捷性的方法，可能由几个不同的应用或服务组成。例如，为了将装配主页时间线的方法从方法 1 变为方法 2，你会如何 “重构” 推特的架构 ？ 修改数据系统并使其适应不断变化需求的容易程度，是与 简单性 和 抽象性 密切相关的：简单易懂的系统通常比复杂系统更容易修改。但由于这是一个非常重要的概念，我们将用一个不同的词来指代数据系统层面的敏捷性： 可演化性（evolvability） 【34】。 本章小结本章探讨了一些关于数据密集型应用的基本思考方式。这些原则将指导我们阅读本书的其余部分，那里将会深入技术细节。 一个应用必须满足各种需求才称得上有用。有一些 功能需求（functional requirements，即它应该做什么，比如允许以各种方式存储，检索，搜索和处理数据）以及一些 非功能性需求（nonfunctional，即通用属性，例如安全性、可靠性、合规性、可伸缩性、兼容性和可维护性）。在本章详细讨论了可靠性，可伸缩性和可维护性。 可靠性（Reliability） 意味着即使发生故障，系统也能正常工作。故障可能发生在硬件（通常是随机的和不相关的）、软件（通常是系统性的 Bug，很难处理）和人类（不可避免地时不时出错）。 容错技术 可以对终端用户隐藏某些类型的故障。 可伸缩性（Scalability） 意味着即使在负载增加的情况下也有保持性能的策略。为了讨论可伸缩性，我们首先需要定量描述负载和性能的方法。我们简要了解了推特主页时间线的例子，介绍描述负载的方法，并将响应时间百分位点作为衡量性能的一种方式。在可伸缩的系统中可以添加 处理容量（processing capacity） 以在高负载下保持可靠。 可维护性（Maintainability） 有许多方面，但实质上是关于工程师和运维团队的生活质量的。良好的抽象可以帮助降低复杂度，并使系统易于修改和适应新的应用场景。良好的可操作性意味着对系统的健康状态具有良好的可见性，并拥有有效的管理手段。 不幸的是，使应用可靠、可伸缩或可维护并不容易。但是某些模式和技术会不断重新出现在不同的应用中。在接下来的几章中，我们将看到一些数据系统的例子，并分析它们如何实现这些目标。 在本书后面的 第三部分 中，我们将看到一种模式：几个组件协同工作以构成一个完整的系统（如 图 1-1 中的例子） 参考文献 Michael Stonebraker and Uğur Çetintemel: “‘One Size Fits All’: An Idea Whose Time Has Come and Gone,” at 21st International Conference on Data Engineering (ICDE), April 2005. Walter L. Heimerdinger and Charles B. Weinstock: “A Conceptual Framework for System Fault Tolerance,” Technical Report CMU&#x2F;SEI-92-TR-033, Software Engineering Institute, Carnegie Mellon University, October 1992. Ding Yuan, Yu Luo, Xin Zhuang, et al.: “Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems,” at 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2014. Yury Izrailevsky and Ariel Tseitlin: “The Netflix Simian Army,” techblog.netflix.com, July 19, 2011. Daniel Ford, François Labelle, Florentina I. Popovici, et al.: “Availability in Globally Distributed Storage Systems,” at 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2010. Brian Beach: “Hard Drive Reliability Update – Sep 2014,” backblaze.com, September 23, 2014. Laurie Voss: “AWS: The Good, the Bad and the Ugly,” blog.awe.sm, December 18, 2012. Haryadi S. Gunawi, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: “What Bugs Live in the Cloud?,” at 5th ACM Symposium on Cloud Computing (SoCC), November 2014. doi:10.1145&#x2F;2670979.2670986 Nelson Minar: “Leap Second Crashes Half the Internet,” somebits.com, July 3, 2012. Amazon Web Services: “Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region,” aws.amazon.com, April 29, 2011. Richard I. Cook: “How Complex Systems Fail,” Cognitive Technologies Laboratory, April 2000. Jay Kreps: “Getting Real About Distributed System Reliability,” blog.empathybox.com, March 19, 2012. David Oppenheimer, Archana Ganapathi, and David A. Patterson: “Why Do Internet Services Fail, and What Can Be Done About It?,” at 4th USENIX Symposium on Internet Technologies and Systems (USITS), March 2003. Nathan Marz: “Principles of Software Engineering, Part 1,” nathanmarz.com, April 2, 2013. Michael Jurewitz:“The Human Impact of Bugs,” jury.me, March 15, 2013. Raffi Krikorian: “Timelines at Scale,” at QCon San Francisco, November 2012. Martin Fowler: Patterns of Enterprise Application Architecture. Addison Wesley, 2002. ISBN: 978-0-321-12742-6 Kelly Sommers: “After all that run around, what caused 500ms disk latency even when we replaced physical server?” twitter.com, November 13, 2014. Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: “Dynamo: Amazon’s Highly Available Key-Value Store,” at 21st ACM Symposium on Operating Systems Principles (SOSP), October 2007. Greg Linden: “Make Data Useful,” slides from presentation at Stanford University Data Mining class (CS345), December 2006. Tammy Everts: “The Real Cost of Slow Time vs Downtime,” webperformancetoday.com, November 12, 2014. Jake Brutlag:“Speed Matters for Google Web Search,” googleresearch.blogspot.co.uk, June 22, 2009. Tyler Treat: “Everything You Know About Latency Is Wrong,” bravenewgeek.com, December 12, 2015. Jeffrey Dean and Luiz André Barroso: “The Tail at Scale,” Communications of the ACM, volume 56, number 2, pages 74–80, February 2013. doi:10.1145&#x2F;2408776.2408794 Graham Cormode, Vladislav Shkapenyuk, Divesh Srivastava, and Bojian Xu: “Forward Decay: A Practical Time Decay Model for Streaming Systems,” at 25th IEEE International Conference on Data Engineering (ICDE), March 2009. Ted Dunning and Otmar Ertl: “Computing Extremely Accurate Quantiles Using t-Digests,” github.com, March 2014. Gil Tene: “HdrHistogram,” hdrhistogram.org. Baron Schwartz: “Why Percentiles Don’t Work the Way You Think,” vividcortex.com, December 7, 2015. James Hamilton: “On Designing and Deploying Internet-Scale Services,” at 21st Large Installation System Administration Conference (LISA), November 2007. Brian Foote and Joseph Yoder: “Big Ball of Mud,” at 4th Conference on Pattern Languages of Programs (PLoP), September 1997. Frederick P Brooks: “No Silver Bullet – Essence and Accident in Software Engineering,” in The Mythical Man-Month, Anniversary edition, Addison-Wesley, 1995. ISBN: 978-0-201-83595-3 Ben Moseley and Peter Marks: “Out of the Tar Pit,” at BCS Software Practice Advancement (SPA), 2006. Rich Hickey: “Simple Made Easy,” at Strange Loop, September 2011. Hongyu Pei Breivold, Ivica Crnkovic, and Peter J. Eriksson: “Analyzing Software Evolvability,” at 32nd Annual IEEE International Computer Software and Applications Conference (COMPSAC), July 2008. doi:10.1109&#x2F;COMPSAC.2008.50"},{"title":"第十章：批处理","path":"/wiki/ddia/ch10.html","content":"带有太强个人色彩的系统无法成功。当最初的设计完成并且相对稳定时，不同的人们以自己的方式进行测试，真正的考验才开始。 —— 高德纳 在本书的前两部分中，我们讨论了很多关于 请求 和 查询 以及相应的 响应 或 结果。许多现有数据系统中都采用这种数据处理方式：你发送请求指令，一段时间后（我们期望）系统会给出一个结果。数据库、缓存、搜索索引、Web 服务器以及其他一些系统都以这种方式工作。 像这样的 在线（online） 系统，无论是浏览器请求页面还是调用远程 API 的服务，我们通常认为请求是由人类用户触发的，并且正在等待响应。他们不应该等太久，所以我们非常关注系统的响应时间（请参阅 “描述性能”）。 Web 和越来越多的基于 HTTP&#x2F;REST 的 API 使交互的请求 &#x2F; 响应风格变得如此普遍，以至于很容易将其视为理所当然。但我们应该记住，这不是构建系统的唯一方式，其他方法也有其优点。我们来看看三种不同类型的系统： 服务（在线系统） 服务等待客户的请求或指令到达。每收到一个，服务会试图尽快处理它，并发回一个响应。响应时间通常是服务性能的主要衡量指标，可用性通常非常重要（如果客户端无法访问服务，用户可能会收到错误消息）。 批处理系统（离线系统） 一个批处理系统有大量的输入数据，跑一个 作业（job） 来处理它，并生成一些输出数据，这往往需要一段时间（从几分钟到几天），所以通常不会有用户等待作业完成。相反，批量作业通常会定期运行（例如，每天一次）。批处理作业的主要性能衡量标准通常是吞吐量（处理特定大小的输入所需的时间）。本章中讨论的就是批处理。 流处理系统（准实时系统） 流处理介于在线和离线（批处理）之间，所以有时候被称为 准实时（near-real-time） 或 准在线（nearline） 处理。像批处理系统一样，流处理消费输入并产生输出（并不需要响应请求）。但是，流式作业在事件发生后不久就会对事件进行操作，而批处理作业则需等待固定的一组输入数据。这种差异使流处理系统比起批处理系统具有更低的延迟。由于流处理基于批处理，我们将在 第十一章 讨论它。 正如我们将在本章中看到的那样，批处理是构建可靠、可伸缩和可维护应用程序的重要组成部分。例如，2004 年发布的批处理算法 Map-Reduce（可能被过分热情地）被称为 “造就 Google 大规模可伸缩性的算法”【2】。随后在各种开源数据系统中得到应用，包括 Hadoop、CouchDB 和 MongoDB。 与多年前为数据仓库开发的并行处理系统【3,4】相比，MapReduce 是一个相当低级别的编程模型，但它使得在商用硬件上能进行的处理规模迈上一个新的台阶。虽然 MapReduce 的重要性正在下降【5】，但它仍然值得去理解，因为它描绘了一幅关于批处理为什么有用，以及如何做到有用的清晰图景。 实际上，批处理是一种非常古老的计算方式。早在可编程数字计算机诞生之前，打孔卡制表机（例如 1890 年美国人口普查【6】中使用的霍尔里斯机）实现了半机械化的批处理形式，从大量输入中汇总计算。 Map-Reduce 与 1940 年代和 1950 年代广泛用于商业数据处理的机电 IBM 卡片分类机器有着惊人的相似之处【7】。正如我们所说，历史总是在不断重复自己。 在本章中，我们将了解 MapReduce 和其他一些批处理算法和框架，并探索它们在现代数据系统中的作用。但首先我们将看看使用标准 Unix 工具的数据处理。即使你已经熟悉了它们，Unix 的哲学也值得一读，Unix 的思想和经验教训可以迁移到大规模、异构的分布式数据系统中。 使用Unix工具的批处理我们从一个简单的例子开始。假设你有一台 Web 服务器，每次处理请求时都会在日志文件中附加一行。例如，使用 nginx 默认的访问日志格式，日志的一行可能如下所示： 123216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] &quot;GET /css/typography.css HTTP/1.1&quot;200 3377 &quot;http://martin.kleppmann.com/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36&quot; （实际上这只是一行，分成多行只是为了便于阅读。）这一行中有很多信息。为了解释它，你需要了解日志格式的定义，如下所示： 12$remote_addr - $remote_user [$time_local] &quot;$request&quot;$status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot; 日志的这一行表明在 UTC 时间的 2015 年 2 月 27 日 17 点 55 分 11 秒，服务器从客户端 IP 地址 216.58.210.78 接收到对文件 /css/typography.css 的请求。用户没有认证，所以 $remote_user 被设置为连字符（-）。响应状态是 200（即请求成功），响应的大小是 3377 字节。网页浏览器是 Chrome 40，它加载了这个文件是因为该文件在网址为 http://martin.kleppmann.com/ 的页面中被引用到了。 简单日志分析很多工具可以从这些日志文件生成关于网站流量的漂亮的报告，但为了练手，让我们使用基本的 Unix 功能创建自己的工具。 例如，假设你想在你的网站上找到五个最受欢迎的网页。 则可以在 Unix shell 中这样做：[^i] [^i]: 有些人认为 cat 这里并没有必要，因为输入文件可以直接作为 awk 的参数。 但这种写法让线性管道更为显眼。 123456cat /var/log/nginx/access.log | #1 awk &#x27;&#123;print $7&#125;&#x27; | #2 sort | #3 uniq -c | #4 sort -r -n | #5 head -n 5 #6 读取日志文件 将每一行按空格分割成不同的字段，每行只输出第七个字段，恰好是请求的 URL。在我们的例子中是 /css/typography.css。 按字母顺序排列请求的 URL 列表。如果某个 URL 被请求过 n 次，那么排序后，文件将包含连续重复出现 n 次的该 URL。 uniq 命令通过检查两个相邻的行是否相同来过滤掉输入中的重复行。 -c 则表示还要输出一个计数器：对于每个不同的 URL，它会报告输入中出现该 URL 的次数。 第二种排序按每行起始处的数字（-n）排序，这是 URL 的请求次数。然后逆序（-r）返回结果，大的数字在前。 最后，只输出前五行（-n 5），并丢弃其余的。该系列命令的输出如下所示： 123454189 /favicon.ico3631 /2013/05/24/improving-security-of-ssh-private-keys.html2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html1369 / 915 /css/typography.css 如果你不熟悉 Unix 工具，上面的命令行可能看起来有点吃力，但是它非常强大。它能在几秒钟内处理几 GB 的日志文件，并且你可以根据需要轻松修改命令。例如，如果要从报告中省略 CSS 文件，可以将 awk 参数更改为 &#39;$7 !~ /\\.css$/ &#123;print $7&#125;&#39;, 如果想统计最多的客户端 IP 地址，可以把 awk 参数改为 &#39;&#123;print $1&#125;&#39;，等等。 我们不会在这里详细探索 Unix 工具，但是它非常值得学习。令人惊讶的是，使用 awk、sed、grep、sort、uniq 和 xargs 的组合，可以在几分钟内完成许多数据分析，并且它们的性能相当的好【8】。 命令链与自定义程序除了 Unix 命令链，你还可以写一个简单的程序来做同样的事情。例如在 Ruby 中，它可能看起来像这样： 12345678910counts = Hash.new(0) # 1File.open(&#x27;/var/log/nginx/access.log&#x27;) do |file| file.each do |line| url = line.split[6] # 2 counts[url] += 1 # 3 endendtop5 = counts.map&#123;|url, count| [count, url] &#125;.sort.reverse[0...5] # 4top5.each&#123;|count, url| puts &quot;#&#123;count&#125; #&#123;url&#125;&quot; &#125; # 5 counts 是一个存储计数器的哈希表，保存了每个 URL 被浏览的次数，默认为 0。 逐行读取日志，抽取每行第七个被空格分隔的字段为 URL（这里的数组索引是 6，因为 Ruby 的数组索引从 0 开始计数） 将日志当前行中 URL 对应的计数器值加一。 按计数器值（降序）对哈希表内容进行排序，并取前五位。 打印出前五个条目。 这个程序并不像 Unix 管道那样简洁，但是它的可读性很强，喜欢哪一种属于口味的问题。但两者除了表面上的差异之外，执行流程也有很大差异，如果你在大文件上运行此分析，则会变得明显。 排序 VS 内存中的聚合Ruby 脚本在内存中保存了一个 URL 的哈希表，将每个 URL 映射到它出现的次数。 Unix 管道没有这样的哈希表，而是依赖于对 URL 列表的排序，在这个 URL 列表中，同一个 URL 的只是简单地重复出现。 哪种方法更好？这取决于你有多少个不同的 URL。对于大多数中小型网站，你可能可以为所有不同网址提供一个计数器（假设我们使用 1GB 内存）。在此例中，作业的 工作集（working set，即作业需要随机访问的内存大小）仅取决于不同 URL 的数量：如果日志中只有单个 URL，重复出现一百万次，则散列表所需的空间表就只有一个 URL 加上一个计数器的大小。当工作集足够小时，内存散列表表现良好，甚至在性能较差的笔记本电脑上也可以正常工作。 另一方面，如果作业的工作集大于可用内存，则排序方法的优点是可以高效地使用磁盘。这与我们在 “SSTables 和 LSM 树” 中讨论过的原理是一样的：数据块可以在内存中排序并作为段文件写入磁盘，然后多个排序好的段可以合并为一个更大的排序文件。 归并排序具有在磁盘上运行良好的顺序访问模式。 （请记住，针对顺序 I&#x2F;O 进行优化是 第三章 中反复出现的主题，相同的模式在此重现） GNU Coreutils（Linux）中的 sort 程序通过溢出至磁盘的方式来自动应对大于内存的数据集，并能同时使用多个 CPU 核进行并行排序【9】。这意味着我们之前看到的简单的 Unix 命令链很容易伸缩至大数据集，且不会耗尽内存。瓶颈可能是从磁盘读取输入文件的速度。 Unix哲学我们可以非常容易地使用前一个例子中的一系列命令来分析日志文件，这并非巧合：事实上，这实际上是 Unix 的关键设计思想之一，而且它直至今天也仍然令人讶异地重要。让我们更深入地研究一下，以便从 Unix 中借鉴一些想法【10】。 Unix 管道的发明者道格・麦克罗伊（Doug McIlroy）在 1964 年首先描述了这种情况【11】：“我们需要一种类似园艺胶管的方式来拼接程序 —— 当我们需要将消息从一个程序传递另一个程序时，直接接上去就行。I&#x2F;O 应该也按照这种方式进行 “。水管的类比仍然在生效，通过管道连接程序的想法成为了现在被称为 Unix 哲学 的一部分 —— 这一组设计原则在 Unix 用户与开发者之间流行起来，该哲学在 1978 年表述如下【12,13】： 让每个程序都做好一件事。要做一件新的工作，写一个新程序，而不是通过添加 “功能” 让老程序复杂化。 期待每个程序的输出成为另一个程序的输入。不要将无关信息混入输出。避免使用严格的列数据或二进制输入格式。不要坚持交互式输入。 设计和构建软件时，即使是操作系统，也让它们能够尽早地被试用，最好在几周内完成。不要犹豫，扔掉笨拙的部分，重建它们。 优先使用工具来减轻编程任务，即使必须曲线救国编写工具，且在用完后很可能要扔掉大部分。 这种方法 —— 自动化，快速原型设计，增量式迭代，对实验友好，将大型项目分解成可管理的块 —— 听起来非常像今天的敏捷开发和 DevOps 运动。奇怪的是，四十年来变化不大。 sort 工具是一个很好的例子。可以说它比大多数编程语言标准库中的实现（它们不会利用磁盘或使用多线程，即使这样做有很大好处）要更好。然而，单独使用 sort 几乎没什么用。它只能与其他 Unix 工具（如 uniq）结合使用。 像 bash 这样的 Unix shell 可以让我们轻松地将这些小程序组合成令人讶异的强大数据处理任务。尽管这些程序中有很多是由不同人群编写的，但它们可以灵活地结合在一起。 Unix 如何实现这种可组合性？ 统一的接口如果你希望一个程序的输出成为另一个程序的输入，那意味着这些程序必须使用相同的数据格式 —— 换句话说，一个兼容的接口。如果你希望能够将任何程序的输出连接到任何程序的输入，那意味着所有程序必须使用相同的 I&#x2F;O 接口。 在 Unix 中，这种接口是一个 文件（file，更准确地说，是一个文件描述符）。一个文件只是一串有序的字节序列。因为这是一个非常简单的接口，所以可以使用相同的接口来表示许多不同的东西：文件系统上的真实文件，到另一个进程（Unix 套接字，stdin，stdout）的通信通道，设备驱动程序（比如 /dev/audio 或 /dev/lp0），表示 TCP 连接的套接字，等等。很容易将这些设计视为理所当然的，但实际上能让这些差异巨大的东西共享一个统一的接口是非常厉害的，这使得它们可以很容易地连接在一起 [^ii]。 [^ii]: 统一接口的另一个例子是 URL 和 HTTP，这是 Web 的基石。 一个 URL 标识一个网站上的一个特定的东西（资源），你可以链接到任何其他网站的任何网址。 具有网络浏览器的用户因此可以通过跟随链接在网站之间无缝跳转，即使服务器可能由完全不相关的组织维护。 这个原则现在似乎非常明显，但它却是网络取能取得今天成就的关键。 之前的系统并不是那么统一：例如，在公告板系统（BBS）时代，每个系统都有自己的电话号码和波特率配置。 从一个 BBS 到另一个 BBS 的引用必须以电话号码和调制解调器设置的形式；用户将不得不挂断，拨打其他 BBS，然后手动找到他们正在寻找的信息。 直接链接到另一个 BBS 内的一些内容当时是不可能的。 按照惯例，许多（但不是全部）Unix 程序将这个字节序列视为 ASCII 文本。我们的日志分析示例使用了这个事实：awk、sort、uniq 和 head 都将它们的输入文件视为由 （换行符，ASCII 0x0A）字符分隔的记录列表。 的选择是任意的 —— 可以说，ASCII 记录分隔符 0x1E 本来就是一个更好的选择，因为它是为了这个目的而设计的【14】，但是无论如何，所有这些程序都使用相同的记录分隔符允许它们互操作。 每条记录（即一行输入）的解析则更加模糊。 Unix 工具通常通过空白或制表符将行分割成字段，但也使用 CSV（逗号分隔），管道分隔和其他编码。即使像 xargs 这样一个相当简单的工具也有六个命令行选项，用于指定如何解析输入。 ASCII 文本的统一接口大多数时候都能工作，但它不是很优雅：我们的日志分析示例使用 &#123;print $7&#125; 来提取网址，这样可读性不是很好。在理想的世界中可能是 &#123;print $request_url&#125; 或类似的东西。我们稍后会回顾这个想法。 尽管几十年后还不够完美，但统一的 Unix 接口仍然是非常出色的设计。没有多少软件能像 Unix 工具一样交互组合的这么好：你不能通过自定义分析工具轻松地将电子邮件帐户的内容和在线购物历史记录以管道传送至电子表格中，并将结果发布到社交网络或维基。今天，像 Unix 工具一样流畅地运行程序是一种例外，而不是规范。 即使是具有 相同数据模型 的数据库，将数据从一种数据库导出再导入到另一种数据库也并不容易。缺乏整合导致了数据的 巴尔干化[^译注i]。 [^译注i]: 巴尔干化（Balkanization） 是一个常带有贬义的地缘政治学术语，其定义为：一个国家或政区分裂成多个互相敌对的国家或政区的过程。 逻辑与布线相分离Unix 工具的另一个特点是使用标准输入（stdin）和标准输出（stdout）。如果你运行一个程序，而不指定任何其他的东西，标准输入来自键盘，标准输出指向屏幕。但是，你也可以从文件输入和 &#x2F; 或将输出重定向到文件。管道允许你将一个进程的标准输出附加到另一个进程的标准输入（有个小内存缓冲区，而不需要将整个中间数据流写入磁盘）。 如果需要，程序仍然可以直接读取和写入文件，但 Unix 方法在程序不关心特定的文件路径、只使用标准输入和标准输出时效果最好。这允许 shell 用户以任何他们想要的方式连接输入和输出；该程序不知道或不关心输入来自哪里以及输出到哪里。 （人们可以说这是一种 松耦合（loose coupling），晚期绑定（late binding）【15】或 控制反转（inversion of control）【16】）。将输入 &#x2F; 输出布线与程序逻辑分开，可以将小工具组合成更大的系统。 你甚至可以编写自己的程序，并将它们与操作系统提供的工具组合在一起。你的程序只需要从标准输入读取输入，并将输出写入标准输出，它就可以加入数据处理的管道中。在日志分析示例中，你可以编写一个将 Usage-Agent 字符串转换为更灵敏的浏览器标识符，或者将 IP 地址转换为国家代码的工具，并将其插入管道。sort 程序并不关心它是否与操作系统的另一部分或者你写的程序通信。 但是，使用 stdin 和 stdout 能做的事情是有限的。需要多个输入或输出的程序虽然可能，却非常棘手。你没法将程序的输出管道连接至网络连接中【17,18】[^iii] 。如果程序直接打开文件进行读取和写入，或者将另一个程序作为子进程启动，或者打开网络连接，那么 I&#x2F;O 的布线就取决于程序本身了。它仍然可以被配置（例如通过命令行选项），但在 Shell 中对输入和输出进行布线的灵活性就少了。 [^iii]: 除了使用一个单独的工具，如 netcat 或 curl。 Unix 起初试图将所有东西都表示为文件，但是 BSD 套接字 API 偏离了这个惯例【17】。研究用操作系统 Plan 9 和 Inferno 在使用文件方面更加一致：它们将 TCP 连接表示为 /net/tcp 中的文件【18】。 透明度和实验使 Unix 工具如此成功的部分原因是，它们使查看正在发生的事情变得非常容易： Unix 命令的输入文件通常被视为不可变的。这意味着你可以随意运行命令，尝试各种命令行选项，而不会损坏输入文件。 你可以在任何时候结束管道，将管道输出到 less，然后查看它是否具有预期的形式。这种检查能力对调试非常有用。 你可以将一个流水线阶段的输出写入文件，并将该文件用作下一阶段的输入。这使你可以重新启动后面的阶段，而无需重新运行整个管道。 因此，与关系数据库的查询优化器相比，即使 Unix 工具非常简单，但仍然非常有用，特别是对于实验而言。 然而，Unix 工具的最大局限在于它们只能在一台机器上运行 —— 而 Hadoop 这样的工具即应运而生。 MapReduce和分布式文件系统MapReduce 有点像 Unix 工具，但分布在数千台机器上。像 Unix 工具一样，它相当简单粗暴，但令人惊异地管用。一个 MapReduce 作业可以和一个 Unix 进程相类比：它接受一个或多个输入，并产生一个或多个输出。 和大多数 Unix 工具一样，运行 MapReduce 作业通常不会修改输入，除了生成输出外没有任何副作用。输出文件以连续的方式一次性写入（一旦写入文件，不会修改任何现有的文件部分）。 虽然 Unix 工具使用 stdin 和 stdout 作为输入和输出，但 MapReduce 作业在分布式文件系统上读写文件。在 Hadoop 的 MapReduce 实现中，该文件系统被称为 HDFS（Hadoop 分布式文件系统），一个 Google 文件系统（GFS）的开源实现【19】。 除 HDFS 外，还有各种其他分布式文件系统，如 GlusterFS 和 Quantcast File System（QFS）【20】。诸如 Amazon S3、Azure Blob 存储和 OpenStack Swift【21】等对象存储服务在很多方面都是相似的 [^iv]。在本章中，我们将主要使用 HDFS 作为示例，但是这些原则适用于任何分布式文件系统。 [^iv]: 一个不同之处在于，对于 HDFS，可以将计算任务安排在存储特定文件副本的计算机上运行，而对象存储通常将存储和计算分开。如果网络带宽是一个瓶颈，从本地磁盘读取有性能优势。但是请注意，如果使用纠删码（Erasure Coding），则会丢失局部性，因为来自多台机器的数据必须进行合并以重建原始文件【20】。 与网络连接存储（NAS）和存储区域网络（SAN）架构的共享磁盘方法相比，HDFS 基于 无共享 原则（请参阅 第二部分 的介绍）。共享磁盘存储由集中式存储设备实现，通常使用定制硬件和专用网络基础设施（如光纤通道）。而另一方面，无共享方法不需要特殊的硬件，只需要通过传统数据中心网络连接的计算机。 HDFS 在每台机器上运行了一个守护进程，它对外暴露网络服务，允许其他节点访问存储在该机器上的文件（假设数据中心中的每台通用计算机都挂载着一些磁盘）。名为 NameNode 的中央服务器会跟踪哪个文件块存储在哪台机器上。因此，HDFS 在概念上创建了一个大型文件系统，可以使用所有运行有守护进程的机器的磁盘。 为了容忍机器和磁盘故障，文件块被复制到多台机器上。复制可能意味着多个机器上的相同数据的多个副本，如 第五章 中所述，或者诸如 Reed-Solomon 码这样的纠删码方案，它能以比完全复制更低的存储开销来支持恢复丢失的数据【20,22】。这些技术与 RAID 相似，后者可以在连接到同一台机器的多个磁盘上提供冗余；区别在于在分布式文件系统中，文件访问和复制是在传统的数据中心网络上完成的，没有特殊的硬件。 HDFS 的可伸缩性已经很不错了：在撰写本书时，最大的 HDFS 部署运行在上万台机器上，总存储容量达数百 PB【23】。如此大的规模已经变得可行，因为使用商品硬件和开源软件的 HDFS 上的数据存储和访问成本远低于在专用存储设备上支持同等容量的成本【24】。 MapReduce作业执行MapReduce 是一个编程框架，你可以使用它编写代码来处理 HDFS 等分布式文件系统中的大型数据集。理解它的最简单方法是参考 “简单日志分析” 中的 Web 服务器日志分析示例。MapReduce 中的数据处理模式与此示例非常相似： 读取一组输入文件，并将其分解成 记录（records）。在 Web 服务器日志示例中，每条记录都是日志中的一行（即 是记录分隔符）。 调用 Mapper 函数，从每条输入记录中提取一对键值。在前面的例子中，Mapper 函数是 awk &#39;&#123;print $7&#125;&#39;：它提取 URL（$7）作为键，并将值留空。 按键排序所有的键值对。在日志的例子中，这由第一个 sort 命令完成。 调用 Reducer 函数遍历排序后的键值对。如果同一个键出现多次，排序使它们在列表中相邻，所以很容易组合这些值而不必在内存中保留很多状态。在前面的例子中，Reducer 是由 uniq -c 命令实现的，该命令使用相同的键来统计相邻记录的数量。 这四个步骤可以作为一个 MapReduce 作业执行。步骤 2（Map）和 4（Reduce）是你编写自定义数据处理代码的地方。步骤 1（将文件分解成记录）由输入格式解析器处理。步骤 3 中的排序步骤隐含在 MapReduce 中 —— 你不必编写它，因为 Mapper 的输出始终在送往 Reducer 之前进行排序。 要创建 MapReduce 作业，你需要实现两个回调函数，Mapper 和 Reducer，其行为如下（请参阅 “MapReduce 查询”）： Mapper Mapper 会在每条输入记录上调用一次，其工作是从输入记录中提取键值。对于每个输入，它可以生成任意数量的键值对（包括 None）。它不会保留从一个输入记录到下一个记录的任何状态，因此每个记录都是独立处理的。 Reducer MapReduce 框架拉取由 Mapper 生成的键值对，收集属于同一个键的所有值，并在这组值上迭代调用 Reducer。 Reducer 可以产生输出记录（例如相同 URL 的出现次数）。 在 Web 服务器日志的例子中，我们在第 5 步中有第二个 sort 命令，它按请求数对 URL 进行排序。在 MapReduce 中，如果你需要第二个排序阶段，则可以通过编写第二个 MapReduce 作业并将第一个作业的输出用作第二个作业的输入来实现它。这样看来，Mapper 的作用是将数据放入一个适合排序的表单中，并且 Reducer 的作用是处理已排序的数据。 分布式执行MapReduceMapReduce 与 Unix 命令管道的主要区别在于，MapReduce 可以在多台机器上并行执行计算，而无需编写代码来显式处理并行问题。Mapper 和 Reducer 一次只能处理一条记录；它们不需要知道它们的输入来自哪里，或者输出去往什么地方，所以框架可以处理在机器之间移动数据的复杂性。 在分布式计算中可以使用标准的 Unix 工具作为 Mapper 和 Reducer【25】，但更常见的是，它们被实现为传统编程语言的函数。在 Hadoop MapReduce 中，Mapper 和 Reducer 都是实现特定接口的 Java 类。在 MongoDB 和 CouchDB 中，Mapper 和 Reducer 都是 JavaScript 函数（请参阅 “MapReduce 查询”）。 图 10-1 显示了 Hadoop MapReduce 作业中的数据流。其并行化基于分区（请参阅 第六章）：作业的输入通常是 HDFS 中的一个目录，输入目录中的每个文件或文件块都被认为是一个单独的分区，可以单独处理 map 任务（图 10-1 中的 m1，m2 和 m3 标记）。 每个输入文件的大小通常是数百兆字节。 MapReduce 调度器（图中未显示）试图在其中一台存储输入文件副本的机器上运行每个 Mapper，只要该机器有足够的备用 RAM 和 CPU 资源来运行 Mapper 任务【26】。这个原则被称为 将计算放在数据附近【27】：它节省了通过网络复制输入文件的开销，减少网络负载并增加局部性。 图 10-1 具有三个 Mapper 和三个 Reducer 的 MapReduce 任务 在大多数情况下，应该在 Mapper 任务中运行的应用代码在将要运行它的机器上还不存在，所以 MapReduce 框架首先将代码（例如 Java 程序中的 JAR 文件）复制到适当的机器。然后启动 Map 任务并开始读取输入文件，一次将一条记录传入 Mapper 回调函数。Mapper 的输出由键值对组成。 计算的 Reduce 端也被分区。虽然 Map 任务的数量由输入文件块的数量决定，但 Reducer 的任务的数量是由作业作者配置的（它可以不同于 Map 任务的数量）。为了确保具有相同键的所有键值对最终落在相同的 Reducer 处，框架使用键的散列值来确定哪个 Reduce 任务应该接收到特定的键值对（请参阅 “根据键的散列分区”）。 键值对必须进行排序，但数据集可能太大，无法在单台机器上使用常规排序算法进行排序。相反，分类是分阶段进行的。首先每个 Map 任务都按照 Reducer 对输出进行分区。每个分区都被写入 Mapper 程序的本地磁盘，使用的技术与我们在 “SSTables 与 LSM 树” 中讨论的类似。 只要当 Mapper 读取完输入文件，并写完排序后的输出文件，MapReduce 调度器就会通知 Reducer 可以从该 Mapper 开始获取输出文件。Reducer 连接到每个 Mapper，并下载自己相应分区的有序键值对文件。按 Reducer 分区，排序，从 Mapper 向 Reducer 复制分区数据，这一整个过程被称为 混洗（shuffle）【26】（一个容易混淆的术语 —— 不像洗牌，在 MapReduce 中的混洗没有随机性）。 Reduce 任务从 Mapper 获取文件，并将它们合并在一起，并保留有序特性。因此，如果不同的 Mapper 生成了键相同的记录，则在 Reducer 的输入中，这些记录将会相邻。 Reducer 调用时会收到一个键，和一个迭代器作为参数，迭代器会顺序地扫过所有具有该键的记录（因为在某些情况可能无法完全放入内存中）。Reducer 可以使用任意逻辑来处理这些记录，并且可以生成任意数量的输出记录。这些输出记录会写入分布式文件系统上的文件中（通常是在跑 Reducer 的机器本地磁盘上留一份，并在其他机器上留几份副本）。 MapReduce工作流单个 MapReduce 作业可以解决的问题范围很有限。以日志分析为例，单个 MapReduce 作业可以确定每个 URL 的页面浏览次数，但无法确定最常见的 URL，因为这需要第二轮排序。 因此将 MapReduce 作业链接成为 工作流（workflow） 中是极为常见的，例如，一个作业的输出成为下一个作业的输入。Hadoop MapReduce 框架对工作流没有特殊支持，所以这个链是通过目录名隐式实现的：第一个作业必须将其输出配置为 HDFS 中的指定目录，第二个作业必须将其输入配置为从同一个目录。从 MapReduce 框架的角度来看，这是两个独立的作业。 因此，被链接的 MapReduce 作业并没有那么像 Unix 命令管道（它直接将一个进程的输出作为另一个进程的输入，仅用一个很小的内存缓冲区）。它更像是一系列命令，其中每个命令的输出写入临时文件，下一个命令从临时文件中读取。这种设计有利也有弊，我们将在 “物化中间状态” 中讨论。 只有当作业成功完成后，批处理作业的输出才会被视为有效的（MapReduce 会丢弃失败作业的部分输出）。因此，工作流中的一项作业只有在先前的作业 —— 即生产其输入的作业 —— 成功完成后才能开始。为了处理这些作业之间的依赖，有很多针对 Hadoop 的工作流调度器被开发出来，包括 Oozie、Azkaban、Luigi、Airflow 和 Pinball 【28】。 这些调度程序还具有管理功能，在维护大量批处理作业时非常有用。在构建推荐系统时，由 50 到 100 个 MapReduce 作业组成的工作流是常见的【29】。而在大型组织中，许多不同的团队可能运行不同的作业来读取彼此的输出。工具支持对于管理这样复杂的数据流而言非常重要。 Hadoop 的各种高级工具（如 Pig 【30】、Hive 【31】、Cascading 【32】、Crunch 【33】和 FlumeJava 【34】）也能自动布线组装多个 MapReduce 阶段，生成合适的工作流。 Reduce侧连接与分组我们在 第二章 中讨论了数据模型和查询语言的连接，但是我们还没有深入探讨连接是如何实现的。现在是我们再次捡起这条线索的时候了。 在许多数据集中，一条记录与另一条记录存在关联是很常见的：关系模型中的 外键，文档模型中的 文档引用 或图模型中的 边。当你需要同时访问这一关联的两侧（持有引用的记录与被引用的记录）时，连接就是必须的。正如 第二章 所讨论的，非规范化可以减少对连接的需求，但通常无法将其完全移除 [^v]。 [^v]: 我们在本书中讨论的连接通常是等值连接，即最常见的连接类型，其中记录通过与其他记录在特定字段（例如 ID）中具有 相同值 相关联。有些数据库支持更通用的连接类型，例如使用小于运算符而不是等号运算符，但是我们没有地方来讲这些东西。 在数据库中，如果执行只涉及少量记录的查询，数据库通常会使用 索引 来快速定位感兴趣的记录（请参阅 第三章）。如果查询涉及到连接，则可能涉及到查找多个索引。然而 MapReduce 没有索引的概念 —— 至少在通常意义上没有。 当 MapReduce 作业被赋予一组文件作为输入时，它读取所有这些文件的全部内容；数据库会将这种操作称为 全表扫描。如果你只想读取少量的记录，则全表扫描与索引查询相比，代价非常高昂。但是在分析查询中（请参阅 “事务处理还是分析？”），通常需要计算大量记录的聚合。在这种情况下，特别是如果能在多台机器上并行处理时，扫描整个输入可能是相当合理的事情。 当我们在批处理的语境中讨论连接时，我们指的是在数据集中解析某种关联的全量存在。 例如我们假设一个作业是同时处理所有用户的数据，而非仅仅是为某个特定用户查找数据（而这能通过索引更高效地完成）。 示例：用户活动事件分析图 10-2 给出了一个批处理作业中连接的典型例子。左侧是事件日志，描述登录用户在网站上做的事情（称为 活动事件，即 activity events，或 点击流数据，即 clickstream data），右侧是用户数据库。 你可以将此示例看作是星型模式的一部分（请参阅 “星型和雪花型：分析的模式”）：事件日志是事实表，用户数据库是其中的一个维度。 图 10-2 用户行为日志与用户档案的连接 分析任务可能需要将用户活动与用户档案信息相关联：例如，如果档案包含用户的年龄或出生日期，系统就可以确定哪些页面更受哪些年龄段的用户欢迎。然而活动事件仅包含用户 ID，而没有包含完整的用户档案信息。在每个活动事件中嵌入这些档案信息很可能会非常浪费。因此，活动事件需要与用户档案数据库相连接。 实现这一连接的最简单方法是，逐个遍历活动事件，并为每个遇到的用户 ID 查询用户数据库（在远程服务器上）。这是可能的，但是它的性能可能会非常差：处理吞吐量将受限于受数据库服务器的往返时间，本地缓存的有效性很大程度上取决于数据的分布，并行运行大量查询可能会轻易压垮数据库【35】。 为了在批处理过程中实现良好的吞吐量，计算必须（尽可能）限于单台机器上进行。为待处理的每条记录发起随机访问的网络请求实在是太慢了。而且，查询远程数据库意味着批处理作业变为 非确定的（nondeterministic），因为远程数据库中的数据可能会改变。 因此，更好的方法是获取用户数据库的副本（例如，使用 ETL 进程从数据库备份中提取数据，请参阅 “数据仓库”），并将它和用户行为日志放入同一个分布式文件系统中。然后你可以将用户数据库存储在 HDFS 中的一组文件中，而用户活动记录存储在另一组文件中，并能用 MapReduce 将所有相关记录集中到同一个地方进行高效处理。 排序合并连接回想一下，Mapper 的目的是从每个输入记录中提取一对键值。在 图 10-2 的情况下，这个键就是用户 ID：一组 Mapper 会扫过活动事件（提取用户 ID 作为键，活动事件作为值），而另一组 Mapper 将会扫过用户数据库（提取用户 ID 作为键，用户的出生日期作为值）。这个过程如 图 10-3 所示。 图 10-3 在用户 ID 上进行的 Reduce 端连接。如果输入数据集分区为多个文件，则每个分区都会被多个 Mapper 并行处理 当 MapReduce 框架通过键对 Mapper 输出进行分区，然后对键值对进行排序时，效果是具有相同 ID 的所有活动事件和用户记录在 Reducer 输入中彼此相邻。 Map-Reduce 作业甚至可以也让这些记录排序，使 Reducer 总能先看到来自用户数据库的记录，紧接着是按时间戳顺序排序的活动事件 —— 这种技术被称为 二次排序（secondary sort）【26】。 然后 Reducer 可以容易地执行实际的连接逻辑：每个用户 ID 都会被调用一次 Reducer 函数，且因为二次排序，第一个值应该是来自用户数据库的出生日期记录。 Reducer 将出生日期存储在局部变量中，然后使用相同的用户 ID 遍历活动事件，输出 已观看网址 和 观看者年龄 的结果对。随后的 Map-Reduce 作业可以计算每个 URL 的查看者年龄分布，并按年龄段进行聚集。 由于 Reducer 一次处理一个特定用户 ID 的所有记录，因此一次只需要将一条用户记录保存在内存中，而不需要通过网络发出任何请求。这个算法被称为 排序合并连接（sort-merge join），因为 Mapper 的输出是按键排序的，然后 Reducer 将来自连接两侧的有序记录列表合并在一起。 把相关数据放在一起在排序合并连接中，Mapper 和排序过程确保了所有对特定用户 ID 执行连接操作的必须数据都被放在同一个地方：单次调用 Reducer 的地方。预先排好了所有需要的数据，Reducer 可以是相当简单的单线程代码，能够以高吞吐量和与低内存开销扫过这些记录。 这种架构可以看做，Mapper 将 “消息” 发送给 Reducer。当一个 Mapper 发出一个键值对时，这个键的作用就像值应该传递到的目标地址。即使键只是一个任意的字符串（不是像 IP 地址和端口号那样的实际的网络地址），它表现的就像一个地址：所有具有相同键的键值对将被传递到相同的目标（一次 Reducer 的调用）。 使用 MapReduce 编程模型，能将计算的物理网络通信层面（从正确的机器获取数据）从应用逻辑中剥离出来（获取数据后执行处理）。这种分离与数据库的典型用法形成了鲜明对比，从数据库中获取数据的请求经常出现在应用代码内部【36】。由于 MapReduce 处理了所有的网络通信，因此它也避免了让应用代码去担心部分故障，例如另一个节点的崩溃：MapReduce 在不影响应用逻辑的情况下能透明地重试失败的任务。 分组除了连接之外，“把相关数据放在一起” 的另一种常见模式是，按某个键对记录分组（如 SQL 中的 GROUP BY 子句）。所有带有相同键的记录构成一个组，而下一步往往是在每个组内进行某种聚合操作，例如： 统计每个组中记录的数量（例如在统计 PV 的例子中，在 SQL 中表示为 COUNT(*) 聚合） 对某个特定字段求和（SQL 中的 SUM(fieldname)） 按某种分级函数取出排名前 k 条记录。 使用 MapReduce 实现这种分组操作的最简单方法是设置 Mapper，以便它们生成的键值对使用所需的分组键。然后分区和排序过程将所有具有相同分区键的记录导向同一个 Reducer。因此在 MapReduce 之上实现分组和连接看上去非常相似。 分组的另一个常见用途是整理特定用户会话的所有活动事件，以找出用户进行的一系列操作（称为 会话化（sessionization）【37】）。例如，可以使用这种分析来确定显示新版网站的用户是否比那些显示旧版本的用户更有购买欲（A&#x2F;B 测试），或者计算某个营销活动是否值得。 如果你有多个 Web 服务器处理用户请求，则特定用户的活动事件很可能分散在各个不同的服务器的日志文件中。你可以通过使用会话 cookie，用户 ID 或类似的标识符作为分组键，以将特定用户的所有活动事件放在一起来实现会话化，与此同时，不同用户的事件仍然散布在不同的分区中。 处理偏斜如果存在与单个键关联的大量数据，则 “将具有相同键的所有记录放到相同的位置” 这种模式就被破坏了。例如在社交网络中，大多数用户可能会与几百人有连接，但少数名人可能有数百万的追随者。这种不成比例的活动数据库记录被称为 关键对象（linchpin object）【38】或 热键（hot key）。 在单个 Reducer 中收集与某个名人相关的所有活动（例如他们发布内容的回复）可能导致严重的 偏斜（也称为 热点，即 hot spot）—— 也就是说，一个 Reducer 必须比其他 Reducer 处理更多的记录（请参阅 “负载偏斜与热点消除“）。由于 MapReduce 作业只有在所有 Mapper 和 Reducer 都完成时才完成，所有后续作业必须等待最慢的 Reducer 才能启动。 如果连接的输入存在热键，可以使用一些算法进行补偿。例如，Pig 中的 偏斜连接（skewed join） 方法首先运行一个抽样作业（Sampling Job）来确定哪些键是热键【39】。连接实际执行时，Mapper 会将热键的关联记录 随机（相对于传统 MapReduce 基于键散列的确定性方法）发送到几个 Reducer 之一。对于另外一侧的连接输入，与热键相关的记录需要被复制到 所有 处理该键的 Reducer 上【40】。 这种技术将处理热键的工作分散到多个 Reducer 上，这样可以使其更好地并行化，代价是需要将连接另一侧的输入记录复制到多个 Reducer 上。 Crunch 中的 分片连接（sharded join） 方法与之类似，但需要显式指定热键而不是使用抽样作业。这种技术也非常类似于我们在 “负载偏斜与热点消除” 中讨论的技术，使用随机化来缓解分区数据库中的热点。 Hive 的偏斜连接优化采取了另一种方法。它需要在表格元数据中显式指定热键，并将与这些键相关的记录单独存放，与其它文件分开。当在该表上执行连接时，对于热键，它会使用 Map 端连接（请参阅下一节）。 当按照热键进行分组并聚合时，可以将分组分两个阶段进行。第一个 MapReduce 阶段将记录发送到随机 Reducer，以便每个 Reducer 只对热键的子集执行分组，为每个键输出一个更紧凑的中间聚合结果。然后第二个 MapReduce 作业将所有来自第一阶段 Reducer 的中间聚合结果合并为每个键一个值。 Map侧连接上一节描述的连接算法在 Reducer 中执行实际的连接逻辑，因此被称为 Reduce 侧连接。Mapper 扮演着预处理输入数据的角色：从每个输入记录中提取键值，将键值对分配给 Reducer 分区，并按键排序。 Reduce 侧方法的优点是不需要对输入数据做任何假设：无论其属性和结构如何，Mapper 都可以对其预处理以备连接。然而不利的一面是，排序，复制至 Reducer，以及合并 Reducer 输入，所有这些操作可能开销巨大。当数据通过 MapReduce 阶段时，数据可能需要落盘好几次，取决于可用的内存缓冲区【37】。 另一方面，如果你 能 对输入数据作出某些假设，则通过使用所谓的 Map 侧连接来加快连接速度是可行的。这种方法使用了一个裁减掉 Reducer 与排序的 MapReduce 作业，每个 Mapper 只是简单地从分布式文件系统中读取一个输入文件块，然后将输出文件写入文件系统，仅此而已。 广播散列连接适用于执行 Map 端连接的最简单场景是大数据集与小数据集连接的情况。要点在于小数据集需要足够小，以便可以将其全部加载到每个 Mapper 的内存中。 例如，假设在 图 10-2 的情况下，用户数据库小到足以放进内存中。在这种情况下，当 Mapper 启动时，它可以首先将用户数据库从分布式文件系统读取到内存中的散列表中。完成此操作后，Mapper 可以扫描用户活动事件，并简单地在散列表中查找每个事件的用户 ID [^vi]。 [^vi]: 这个例子假定散列表中的每个键只有一个条目，这对用户数据库（用户 ID 唯一标识一个用户）可能是正确的。通常，哈希表可能需要包含具有相同键的多个条目，而连接运算符将对每个键输出所有的匹配。 参与连接的较大输入的每个文件块各有一个 Mapper（在 图 10-2 的例子中活动事件是较大的输入）。每个 Mapper 都会将较小输入整个加载到内存中。 这种简单有效的算法被称为 广播散列连接（broadcast hash join）：广播 一词反映了这样一个事实，每个连接较大输入端分区的 Mapper 都会将较小输入端数据集整个读入内存中（所以较小输入实际上 “广播” 到较大数据的所有分区上），散列 一词反映了它使用一个散列表。 Pig（名为 “复制链接（replicated join）”），Hive（“MapJoin”），Cascading 和 Crunch 支持这种连接。它也被诸如 Impala 的数据仓库查询引擎使用【41】。 除了将较小的连接输入加载到内存散列表中，另一种方法是将较小输入存储在本地磁盘上的只读索引中【42】。索引中经常使用的部分将保留在操作系统的页面缓存中，因而这种方法可以提供与内存散列表几乎一样快的随机查找性能，但实际上并不需要数据集能放入内存中。 分区散列连接如果 Map 侧连接的输入以相同的方式进行分区，则散列连接方法可以独立应用于每个分区。在 图 10-2 的情况中，你可以根据用户 ID 的最后一位十进制数字来对活动事件和用户数据库进行分区（因此连接两侧各有 10 个分区）。例如，Mapper3 首先将所有具有以 3 结尾的 ID 的用户加载到散列表中，然后扫描 ID 为 3 的每个用户的所有活动事件。 如果分区正确无误，可以确定的是，所有你可能需要连接的记录都落在同一个编号的分区中。因此每个 Mapper 只需要从输入两端各读取一个分区就足够了。好处是每个 Mapper 都可以在内存散列表中少放点数据。 这种方法只有当连接两端输入有相同的分区数，且两侧的记录都是使用相同的键与相同的哈希函数做分区时才适用。如果输入是由之前执行过这种分组的 MapReduce 作业生成的，那么这可能是一个合理的假设。 分区散列连接在 Hive 中称为 Map 侧桶连接（bucketed map joins）【37】。 Map侧合并连接如果输入数据集不仅以相同的方式进行分区，而且还基于相同的键进行 排序，则可适用另一种 Map 侧连接的变体。在这种情况下，输入是否小到能放入内存并不重要，因为这时候 Mapper 同样可以执行归并操作（通常由 Reducer 执行）的归并操作：按键递增的顺序依次读取两个输入文件，将具有相同键的记录配对。 如果能进行 Map 侧合并连接，这通常意味着前一个 MapReduce 作业可能一开始就已经把输入数据做了分区并进行了排序。原则上这个连接就可以在前一个作业的 Reduce 阶段进行。但使用独立的仅 Map 作业有时也是合适的，例如，分好区且排好序的中间数据集可能还会用于其他目的。 MapReduce工作流与Map侧连接当下游作业使用 MapReduce 连接的输出时，选择 Map 侧连接或 Reduce 侧连接会影响输出的结构。Reduce 侧连接的输出是按照 连接键 进行分区和排序的，而 Map 端连接的输出则按照与较大输入相同的方式进行分区和排序（因为无论是使用分区连接还是广播连接，连接较大输入端的每个文件块都会启动一个 Map 任务）。 如前所述，Map 侧连接也对输入数据集的大小，有序性和分区方式做出了更多假设。在优化连接策略时，了解分布式文件系统中数据集的物理布局变得非常重要：仅仅知道编码格式和数据存储目录的名称是不够的；你还必须知道数据是按哪些键做的分区和排序，以及分区的数量。 在 Hadoop 生态系统中，这种关于数据集分区的元数据通常在 HCatalog 和 Hive Metastore 中维护【37】。 批处理工作流的输出我们已经说了很多用于实现 MapReduce 工作流的算法，但却忽略了一个重要的问题：这些处理完成之后的最终结果是什么？我们最开始为什么要跑这些作业？ 在数据库查询的场景中，我们将事务处理（OLTP）与分析两种目的区分开来（请参阅 “事务处理还是分析？”）。我们看到，OLTP 查询通常根据键查找少量记录，使用索引，并将其呈现给用户（比如在网页上）。另一方面，分析查询通常会扫描大量记录，执行分组与聚合，输出通常有着报告的形式：显示某个指标随时间变化的图表，或按照某种排位取前 10 项，或将一些数字细化为子类。这种报告的消费者通常是需要做出商业决策的分析师或经理。 批处理放哪里合适？它不属于事务处理，也不是分析。它和分析比较接近，因为批处理通常会扫过输入数据集的绝大部分。然而 MapReduce 作业工作流与用于分析目的的 SQL 查询是不同的（请参阅 “Hadoop 与分布式数据库的对比”）。批处理过程的输出通常不是报表，而是一些其他类型的结构。 建立搜索索引Google 最初使用 MapReduce 是为其搜索引擎建立索引，其实现为由 5 到 10 个 MapReduce 作业组成的工作流【1】。虽然 Google 后来也不仅仅是为这个目的而使用 MapReduce 【43】，但如果从构建搜索索引的角度来看，更能帮助理解 MapReduce。 （直至今日，Hadoop MapReduce 仍然是为 Lucene&#x2F;Solr 构建索引的好方法【44】） 我们在 “全文搜索和模糊索引” 中简要地了解了 Lucene 这样的全文搜索索引是如何工作的：它是一个文件（关键词字典），你可以在其中高效地查找特定关键字，并找到包含该关键字的所有文档 ID 列表（文章列表）。这是一种非常简化的看法 —— 实际上，搜索索引需要各种额外数据，以便根据相关性对搜索结果进行排名、纠正拼写错误、解析同义词等等 —— 但这个原则是成立的。 如果需要对一组固定文档执行全文搜索，则批处理是一种构建索引的高效方法：Mapper 根据需要对文档集合进行分区，每个 Reducer 构建该分区的索引，并将索引文件写入分布式文件系统。构建这样的文档分区索引（请参阅 “分区与次级索引”）并行处理效果拔群。 由于按关键字查询搜索索引是只读操作，因而这些索引文件一旦创建就是不可变的。 如果索引的文档集合发生更改，一种选择是定期重跑整个索引工作流，并在完成后用新的索引文件批量替换以前的索引文件。如果只有少量的文档发生了变化，这种方法的计算成本可能会很高。但它的优点是索引过程很容易理解：文档进，索引出。 另一个选择是，可以增量建立索引。如 第三章 中讨论的，如果要在索引中添加，删除或更新文档，Lucene 会写新的段文件，并在后台异步合并压缩段文件。我们将在 第十一章 中看到更多这种增量处理。 键值存储作为批处理输出搜索索引只是批处理工作流可能输出的一个例子。批处理的另一个常见用途是构建机器学习系统，例如分类器（比如垃圾邮件过滤器，异常检测，图像识别）与推荐系统（例如，你可能认识的人，你可能感兴趣的产品或相关的搜索【29】）。 这些批处理作业的输出通常是某种数据库：例如，可以通过给定用户 ID 查询该用户推荐好友的数据库，或者可以通过产品 ID 查询相关产品的数据库【45】。 这些数据库需要被处理用户请求的 Web 应用所查询，而它们通常是独立于 Hadoop 基础设施的。那么批处理过程的输出如何回到 Web 应用可以查询的数据库中呢？ 最直接的选择可能是，直接在 Mapper 或 Reducer 中使用你最爱的数据库的客户端库，并从批处理作业直接写入数据库服务器，一次写入一条记录。它能工作（假设你的防火墙规则允许从你的 Hadoop 环境直接访问你的生产数据库），但这并不是一个好主意，出于以下几个原因： 正如前面在连接的上下文中讨论的那样，为每条记录发起一个网络请求，要比批处理任务的正常吞吐量慢几个数量级。即使客户端库支持批处理，性能也可能很差。 MapReduce 作业经常并行运行许多任务。如果所有 Mapper 或 Reducer 都同时写入相同的输出数据库，并以批处理的预期速率工作，那么该数据库很可能被轻易压垮，其查询性能可能变差。这可能会导致系统其他部分的运行问题【35】。 通常情况下，MapReduce 为作业输出提供了一个干净利落的 “全有或全无” 保证：如果作业成功，则结果就是每个任务恰好执行一次所产生的输出，即使某些任务失败且必须一路重试。如果整个作业失败，则不会生成输出。然而从作业内部写入外部系统，会产生外部可见的副作用，这种副作用是不能以这种方式被隐藏的。因此，你不得不去操心对其他系统可见的部分完成的作业结果，并需要理解 Hadoop 任务尝试与预测执行的复杂性。 更好的解决方案是在批处理作业 内 创建一个全新的数据库，并将其作为文件写入分布式文件系统中作业的输出目录，就像上节中的搜索索引一样。这些数据文件一旦写入就是不可变的，可以批量加载到处理只读查询的服务器中。不少键值存储都支持在 MapReduce 作业中构建数据库文件，包括 Voldemort 【46】、Terrapin 【47】、ElephantDB 【48】和 HBase 批量加载【49】。 构建这些数据库文件是 MapReduce 的一种好用法：使用 Mapper 提取出键并按该键排序，已经完成了构建索引所必需的大量工作。由于这些键值存储大多都是只读的（文件只能由批处理作业一次性写入，然后就不可变），所以数据结构非常简单。比如它们就不需要预写式日志（WAL，请参阅 “让 B 树更可靠”）。 将数据加载到 Voldemort 时，服务器将继续用旧数据文件服务请求，同时将新数据文件从分布式文件系统复制到服务器的本地磁盘。一旦复制完成，服务器会自动将查询切换到新文件。如果在这个过程中出现任何问题，它可以轻易回滚至旧文件，因为它们仍然存在而且不可变【46】。 批处理输出的哲学本章前面讨论过的 Unix 哲学（“Unix 哲学”）鼓励以显式指明数据流的方式进行实验：程序读取输入并写入输出。在这一过程中，输入保持不变，任何先前的输出都被新输出完全替换，且没有其他副作用。这意味着你可以随心所欲地重新运行一个命令，略做改动或进行调试，而不会搅乱系统的状态。 MapReduce 作业的输出处理遵循同样的原理。通过将输入视为不可变且避免副作用（如写入外部数据库），批处理作业不仅实现了良好的性能，而且更容易维护： 如果在代码中引入了一个错误，而输出错误或损坏了，则可以简单地回滚到代码的先前版本，然后重新运行该作业，输出将重新被纠正。或者，甚至更简单，你可以将旧的输出保存在不同的目录中，然后切换回原来的目录。具有读写事务的数据库没有这个属性：如果你部署了错误的代码，将错误的数据写入数据库，那么回滚代码将无法修复数据库中的数据。 （能够从错误代码中恢复的概念被称为 人类容错（human fault tolerance）【50】） 由于回滚很容易，比起在错误意味着不可挽回的伤害的环境，功能开发进展能快很多。这种 最小化不可逆性（minimizing irreversibility） 的原则有利于敏捷软件开发【51】。 如果 Map 或 Reduce 任务失败，MapReduce 框架将自动重新调度，并在同样的输入上再次运行它。如果失败是由代码中的错误造成的，那么它会不断崩溃，并最终导致作业在几次尝试之后失败。但是如果故障是由于临时问题导致的，那么故障就会被容忍。因为输入不可变，这种自动重试是安全的，而失败任务的输出会被 MapReduce 框架丢弃。 同一组文件可用作各种不同作业的输入，包括计算指标的监控作业并且评估作业的输出是否具有预期的性质（例如，将其与前一次运行的输出进行比较并测量差异） 。 与 Unix 工具类似，MapReduce 作业将逻辑与布线（配置输入和输出目录）分离，这使得关注点分离，可以重用代码：一个团队可以专注实现一个做好一件事的作业；而其他团队可以决定何时何地运行这项作业。 在这些领域，在 Unix 上表现良好的设计原则似乎也适用于 Hadoop，但 Unix 和 Hadoop 在某些方面也有所不同。例如，因为大多数 Unix 工具都假设输入输出是无类型文本文件，所以它们必须做大量的输入解析工作（本章开头的日志分析示例使用 &#123;print $7&#125; 来提取 URL）。在 Hadoop 上可以通过使用更结构化的文件格式消除一些低价值的语法转换：比如 Avro（请参阅 “Avro”）和 Parquet（请参阅 “列式存储”）经常使用，因为它们提供了基于模式的高效编码，并允许模式随时间推移而演进（见 第四章）。 Hadoop与分布式数据库的对比正如我们所看到的，Hadoop 有点像 Unix 的分布式版本，其中 HDFS 是文件系统，而 MapReduce 是 Unix 进程的怪异实现（总是在 Map 阶段和 Reduce 阶段运行 sort 工具）。我们了解了如何在这些原语的基础上实现各种连接和分组操作。 当 MapReduce 论文发表时【1】，它从某种意义上来说 —— 并不新鲜。我们在前几节中讨论的所有处理和并行连接算法已经在十多年前所谓的 大规模并行处理（MPP， massively parallel processing） 数据库中实现了【3,40】。比如 Gamma database machine、Teradata 和 Tandem NonStop SQL 就是这方面的先驱【52】。 最大的区别是，MPP 数据库专注于在一组机器上并行执行分析 SQL 查询，而 MapReduce 和分布式文件系统【19】的组合则更像是一个可以运行任意程序的通用操作系统。 存储多样性数据库要求你根据特定的模型（例如关系或文档）来构造数据，而分布式文件系统中的文件只是字节序列，可以使用任何数据模型和编码来编写。它们可能是数据库记录的集合，但同样可以是文本、图像、视频、传感器读数、稀疏矩阵、特征向量、基因组序列或任何其他类型的数据。 说白了，Hadoop 开放了将数据不加区分地转储到 HDFS 的可能性，允许后续再研究如何进一步处理【53】。相比之下，在将数据导入数据库专有存储格式之前，MPP 数据库通常需要对数据和查询模式进行仔细的前期建模。 在纯粹主义者看来，这种仔细的建模和导入似乎是可取的，因为这意味着数据库的用户有更高质量的数据来处理。然而实践经验表明，简单地使数据快速可用 —— 即使它很古怪，难以使用，使用原始格式 —— 也通常要比事先决定理想数据模型要更有价值【54】。 这个想法与数据仓库类似（请参阅 “数据仓库”）：将大型组织的各个部分的数据集中在一起是很有价值的，因为它可以跨越以前相互分离的数据集进行连接。 MPP 数据库所要求的谨慎模式设计拖慢了集中式数据收集速度；以原始形式收集数据，稍后再操心模式的设计，能使数据收集速度加快（有时被称为 “数据湖（data lake）” 或 “企业数据中心（enterprise data hub）”【55】）。 不加区分的数据转储转移了解释数据的负担：数据集的生产者不再需要强制将其转化为标准格式，数据的解释成为消费者的问题（读时模式 方法【56】；请参阅 “文档模型中的模式灵活性”）。如果生产者和消费者是不同优先级的不同团队，这可能是一种优势。甚至可能不存在一个理想的数据模型，对于不同目的有不同的合适视角。以原始形式简单地转储数据，可以允许多种这样的转换。这种方法被称为 寿司原则（sushi principle）：“原始数据更好”【57】。 因此，Hadoop 经常被用于实现 ETL 过程（请参阅 “数据仓库”）：事务处理系统中的数据以某种原始形式转储到分布式文件系统中，然后编写 MapReduce 作业来清理数据，将其转换为关系形式，并将其导入 MPP 数据仓库以进行分析。数据建模仍然在进行，但它在一个单独的步骤中进行，与数据收集相解耦。这种解耦是可行的，因为分布式文件系统支持以任何格式编码的数据。 处理模型的多样性MPP 数据库是单体的，紧密集成的软件，负责磁盘上的存储布局，查询计划，调度和执行。由于这些组件都可以针对数据库的特定需求进行调整和优化，因此整个系统可以在其设计针对的查询类型上取得非常好的性能。而且，SQL 查询语言允许以优雅的语法表达查询，而无需编写代码，可以在业务分析师使用的可视化工具（例如 Tableau）中访问到。 另一方面，并非所有类型的处理都可以合理地表达为 SQL 查询。例如，如果要构建机器学习和推荐系统，或者使用相关性排名模型的全文搜索索引，或者执行图像分析，则很可能需要更一般的数据处理模型。这些类型的处理通常是特别针对特定应用的（例如机器学习的特征工程，机器翻译的自然语言模型，欺诈预测的风险评估函数），因此它们不可避免地需要编写代码，而不仅仅是查询。 MapReduce 使工程师能够轻松地在大型数据集上运行自己的代码。如果你有 HDFS 和 MapReduce，那么你 可以 在它之上建立一个 SQL 查询执行引擎，事实上这正是 Hive 项目所做的【31】。但是，你也可以编写许多其他形式的批处理，这些批处理不必非要用 SQL 查询表示。 随后，人们发现 MapReduce 对于某些类型的处理而言局限性很大，表现很差，因此在 Hadoop 之上其他各种处理模型也被开发出来（我们将在 “MapReduce 之后” 中看到其中一些）。只有两种处理模型，SQL 和 MapReduce，还不够，需要更多不同的模型！而且由于 Hadoop 平台的开放性，实施一整套方法是可行的，而这在单体 MPP 数据库的范畴内是不可能的【58】。 至关重要的是，这些不同的处理模型都可以在共享的单个机器集群上运行，所有这些机器都可以访问分布式文件系统上的相同文件。在 Hadoop 方式中，不需要将数据导入到几个不同的专用系统中进行不同类型的处理：系统足够灵活，可以支持同一个集群内不同的工作负载。不需要移动数据，使得从数据中挖掘价值变得容易得多，也使采用新的处理模型容易的多。 Hadoop 生态系统包括随机访问的 OLTP 数据库，如 HBase（请参阅 “SSTables 和 LSM 树”）和 MPP 风格的分析型数据库，如 Impala 【41】。 HBase 与 Impala 都不使用 MapReduce，但都使用 HDFS 进行存储。它们是迥异的数据访问与处理方法，但是它们可以共存，并被集成到同一个系统中。 针对频繁故障设计当比较 MapReduce 和 MPP 数据库时，两种不同的设计思路出现了：处理故障和使用内存与磁盘的方式。与在线系统相比，批处理对故障不太敏感，因为就算失败也不会立即影响到用户，而且它们总是能再次运行。 如果一个节点在执行查询时崩溃，大多数 MPP 数据库会中止整个查询，并让用户重新提交查询或自动重新运行它【3】。由于查询通常最多运行几秒钟或几分钟，所以这种错误处理的方法是可以接受的，因为重试的代价不是太大。 MPP 数据库还倾向于在内存中保留尽可能多的数据（例如，使用散列连接）以避免从磁盘读取的开销。 另一方面，MapReduce 可以容忍单个 Map 或 Reduce 任务的失败，而不会影响作业的整体，通过以单个任务的粒度重试工作。它也会非常急切地将数据写入磁盘，一方面是为了容错，另一部分是因为假设数据集太大而不能适应内存。 MapReduce 方式更适用于较大的作业：要处理如此之多的数据并运行很长时间的作业，以至于在此过程中很可能至少遇到一个任务故障。在这种情况下，由于单个任务失败而重新运行整个作业将是非常浪费的。即使以单个任务的粒度进行恢复引入了使得无故障处理更慢的开销，但如果任务失败率足够高，这仍然是一种合理的权衡。 但是这些假设有多么现实呢？在大多数集群中，机器故障确实会发生，但是它们不是很频繁 —— 可能少到绝大多数作业都不会经历机器故障。为了容错，真的值得带来这么大的额外开销吗？ 要了解 MapReduce 节约使用内存和在任务的层次进行恢复的原因，了解最初设计 MapReduce 的环境是很有帮助的。Google 有着混用的数据中心，在线生产服务和离线批处理作业在同样机器上运行。每个任务都有一个通过容器强制执行的资源配给（CPU 核心、RAM、磁盘空间等）。每个任务也具有优先级，如果优先级较高的任务需要更多的资源，则可以终止（抢占）同一台机器上较低优先级的任务以释放资源。优先级还决定了计算资源的定价：团队必须为他们使用的资源付费，而优先级更高的进程花费更多【59】。 这种架构允许非生产（低优先级）计算资源被 过量使用（overcommitted），因为系统知道必要时它可以回收资源。与分离生产和非生产任务的系统相比，过量使用资源可以更好地利用机器并提高效率。但由于 MapReduce 作业以低优先级运行，它们随时都有被抢占的风险，因为优先级较高的进程可能需要其资源。在高优先级进程拿走所需资源后，批量作业能有效地 “捡面包屑”，利用剩下的任何计算资源。 在谷歌，运行一个小时的 MapReduce 任务有大约有 5% 的风险被终止，为了给更高优先级的进程挪地方。这一概率比硬件问题、机器重启或其他原因的概率高了一个数量级【59】。按照这种抢占率，如果一个作业有 100 个任务，每个任务运行 10 分钟，那么至少有一个任务在完成之前被终止的风险大于 50%。 这就是 MapReduce 被设计为容忍频繁意外任务终止的原因：不是因为硬件很不可靠，而是因为任意终止进程的自由有利于提高计算集群中的资源利用率。 在开源的集群调度器中，抢占的使用较少。 YARN 的 CapacityScheduler 支持抢占，以平衡不同队列的资源分配【58】，但在编写本文时，YARN，Mesos 或 Kubernetes 不支持通用的优先级抢占【60】。在任务不经常被终止的环境中，MapReduce 的这一设计决策就没有多少意义了。在下一节中，我们将研究一些与 MapReduce 设计决策相异的替代方案。 MapReduce之后虽然 MapReduce 在 2000 年代后期变得非常流行，并受到大量的炒作，但它只是分布式系统的许多可能的编程模型之一。对于不同的数据量，数据结构和处理类型，其他工具可能更适合表示计算。 不管如何，我们在这一章花了大把时间来讨论 MapReduce，因为它是一种有用的学习工具，它是分布式文件系统的一种相当简单明晰的抽象。在这里，简单 意味着我们能理解它在做什么，而不是意味着使用它很简单。恰恰相反：使用原始的 MapReduce API 来实现复杂的处理工作实际上是非常困难和费力的 —— 例如，任意一种连接算法都需要你从头开始实现【37】。 针对直接使用 MapReduce 的困难，在 MapReduce 上有很多高级编程模型（Pig、Hive、Cascading、Crunch）被创造出来，作为建立在 MapReduce 之上的抽象。如果你了解 MapReduce 的原理，那么它们学起来相当简单。而且它们的高级结构能显著简化许多常见批处理任务的实现。 但是，MapReduce 执行模型本身也存在一些问题，这些问题并没有通过增加另一个抽象层次而解决，而对于某些类型的处理，它表现得非常差劲。一方面，MapReduce 非常稳健：你可以使用它在任务会频繁终止的多租户系统上处理几乎任意大量级的数据，并且仍然可以完成工作（虽然速度很慢）。另一方面，对于某些类型的处理而言，其他工具有时会快上几个数量级。 在本章的其余部分中，我们将介绍一些批处理方法。在 第十一章 我们将转向流处理，它可以看作是加速批处理的另一种方法。 物化中间状态如前所述，每个 MapReduce 作业都独立于其他任何作业。作业与世界其他地方的主要连接点是分布式文件系统上的输入和输出目录。如果希望一个作业的输出成为第二个作业的输入，则需要将第二个作业的输入目录配置为第一个作业输出目录，且外部工作流调度程序必须在第一个作业完成后再启动第二个。 如果第一个作业的输出是要在组织内广泛发布的数据集，则这种配置是合理的。在这种情况下，你需要通过名称引用它，并将其重用为多个不同作业的输入（包括由其他团队开发的作业）。将数据发布到分布式文件系统中众所周知的位置能够带来 松耦合，这样作业就不需要知道是谁在提供输入或谁在消费输出（请参阅 “逻辑与布线相分离”）。 但在很多情况下，你知道一个作业的输出只能用作另一个作业的输入，这些作业由同一个团队维护。在这种情况下，分布式文件系统上的文件只是简单的 中间状态（intermediate state）：一种将数据从一个作业传递到下一个作业的方式。在一个用于构建推荐系统的，由 50 或 100 个 MapReduce 作业组成的复杂工作流中，存在着很多这样的中间状态【29】。 将这个中间状态写入文件的过程称为 物化（materialization）。 （在 “聚合：数据立方体和物化视图” 中已经在物化视图的背景中遇到过这个术语。它意味着对某个操作的结果立即求值并写出来，而不是在请求时按需计算） 作为对照，本章开头的日志分析示例使用 Unix 管道将一个命令的输出与另一个命令的输入连接起来。管道并没有完全物化中间状态，而是只使用一个小的内存缓冲区，将输出增量地 流（stream） 向输入。 与 Unix 管道相比，MapReduce 完全物化中间状态的方法存在不足之处： MapReduce 作业只有在前驱作业（生成其输入）中的所有任务都完成时才能启动，而由 Unix 管道连接的进程会同时启动，输出一旦生成就会被消费。不同机器上的数据偏斜或负载不均意味着一个作业往往会有一些掉队的任务，比其他任务要慢得多才能完成。必须等待至前驱作业的所有任务完成，拖慢了整个工作流程的执行。 Mapper 通常是多余的：它们仅仅是读取刚刚由 Reducer 写入的同样文件，为下一个阶段的分区和排序做准备。在许多情况下，Mapper 代码可能是前驱 Reducer 的一部分：如果 Reducer 和 Mapper 的输出有着相同的分区与排序方式，那么 Reducer 就可以直接串在一起，而不用与 Mapper 相互交织。 将中间状态存储在分布式文件系统中意味着这些文件被复制到多个节点，对这些临时数据这么搞就比较过分了。 数据流引擎为了解决 MapReduce 的这些问题，几种用于分布式批处理的新执行引擎被开发出来，其中最著名的是 Spark 【61,62】，Tez 【63,64】和 Flink 【65,66】。它们的设计方式有很多区别，但有一个共同点：把整个工作流作为单个作业来处理，而不是把它分解为独立的子作业。 由于它们将工作流显式建模为数据从几个处理阶段穿过，所以这些系统被称为 数据流引擎（dataflow engines）。像 MapReduce 一样，它们在一条线上通过反复调用用户定义的函数来一次处理一条记录，它们通过输入分区来并行化载荷，它们通过网络将一个函数的输出复制到另一个函数的输入。 与 MapReduce 不同，这些函数不需要严格扮演交织的 Map 与 Reduce 的角色，而是可以以更灵活的方式进行组合。我们称这些函数为 算子（operators），数据流引擎提供了几种不同的选项来将一个算子的输出连接到另一个算子的输入： 一种选项是对记录按键重新分区并排序，就像在 MapReduce 的混洗阶段一样（请参阅 “分布式执行 MapReduce”）。这种功能可以用于实现排序合并连接和分组，就像在 MapReduce 中一样。 另一种可能是接受多个输入，并以相同的方式进行分区，但跳过排序。当记录的分区重要但顺序无关紧要时，这省去了分区散列连接的工作，因为构建散列表还是会把顺序随机打乱。 对于广播散列连接，可以将一个算子的输出，发送到连接算子的所有分区。 这种类型的处理引擎是基于像 Dryad【67】和 Nephele【68】这样的研究系统，与 MapReduce 模型相比，它有几个优点： 排序等昂贵的工作只需要在实际需要的地方执行，而不是默认地在每个 Map 和 Reduce 阶段之间出现。 没有不必要的 Map 任务，因为 Mapper 所做的工作通常可以合并到前面的 Reduce 算子中（因为 Mapper 不会更改数据集的分区）。 由于工作流中的所有连接和数据依赖都是显式声明的，因此调度程序能够总览全局，知道哪里需要哪些数据，因而能够利用局部性进行优化。例如，它可以尝试将消费某些数据的任务放在与生成这些数据的任务相同的机器上，从而数据可以通过共享内存缓冲区传输，而不必通过网络复制。 通常，算子间的中间状态足以保存在内存中或写入本地磁盘，这比写入 HDFS 需要更少的 I&#x2F;O（必须将其复制到多台机器，并将每个副本写入磁盘）。 MapReduce 已经对 Mapper 的输出做了这种优化，但数据流引擎将这种思想推广至所有的中间状态。 算子可以在输入就绪后立即开始执行；后续阶段无需等待前驱阶段整个完成后再开始。 与 MapReduce（为每个任务启动一个新的 JVM）相比，现有 Java 虚拟机（JVM）进程可以重用来运行新算子，从而减少启动开销。 你可以使用数据流引擎执行与 MapReduce 工作流同样的计算，而且由于此处所述的优化，通常执行速度要明显快得多。既然算子是 Map 和 Reduce 的泛化，那么相同的处理代码就可以在任一执行引擎上运行：Pig，Hive 或 Cascading 中实现的工作流可以无需修改代码，可以通过修改配置，简单地从 MapReduce 切换到 Tez 或 Spark【64】。 Tez 是一个相当薄的库，它依赖于 YARN shuffle 服务来实现节点间数据的实际复制【58】，而 Spark 和 Flink 则是包含了独立网络通信层，调度器，及用户向 API 的大型框架。我们将简要讨论这些高级 API。 容错完全物化中间状态至分布式文件系统的一个优点是，它具有持久性，这使得 MapReduce 中的容错相当容易：如果一个任务失败，它可以在另一台机器上重新启动，并从文件系统重新读取相同的输入。 Spark、Flink 和 Tez 避免将中间状态写入 HDFS，因此它们采取了不同的方法来容错：如果一台机器发生故障，并且该机器上的中间状态丢失，则它会从其他仍然可用的数据重新计算（在可行的情况下是先前的中间状态，要么就只能是原始输入数据，通常在 HDFS 上）。 为了实现这种重新计算，框架必须跟踪一个给定的数据是如何计算的 —— 使用了哪些输入分区？应用了哪些算子？ Spark 使用 弹性分布式数据集（RDD，Resilient Distributed Dataset） 的抽象来跟踪数据的谱系【61】，而 Flink 对算子状态存档，允许恢复运行在执行过程中遇到错误的算子【66】。 在重新计算数据时，重要的是要知道计算是否是 确定性的：也就是说，给定相同的输入数据，算子是否始终产生相同的输出？如果一些丢失的数据已经发送给下游算子，这个问题就很重要。如果算子重新启动，重新计算的数据与原有的丢失数据不一致，下游算子很难解决新旧数据之间的矛盾。对于不确定性算子来说，解决方案通常是杀死下游算子，然后再重跑新数据。 为了避免这种级联故障，最好让算子具有确定性。但需要注意的是，非确定性行为很容易悄悄溜进来：例如，许多编程语言在迭代哈希表的元素时不能对顺序作出保证，许多概率和统计算法显式依赖于使用随机数，以及用到系统时钟或外部数据源，这些都是都不确定性的行为。为了能可靠地从故障中恢复，需要消除这种不确定性因素，例如使用固定的种子生成伪随机数。 通过重算数据来从故障中恢复并不总是正确的答案：如果中间状态数据要比源数据小得多，或者如果计算量非常大，那么将中间数据物化为文件可能要比重新计算廉价的多。 关于物化的讨论回到 Unix 的类比，我们看到，MapReduce 就像是将每个命令的输出写入临时文件，而数据流引擎看起来更像是 Unix 管道。尤其是 Flink 是基于管道执行的思想而建立的：也就是说，将算子的输出增量地传递给其他算子，不待输入完成便开始处理。 排序算子不可避免地需要消费全部的输入后才能生成任何输出，因为输入中最后一条输入记录可能具有最小的键，因此需要作为第一条记录输出。因此，任何需要排序的算子都需要至少暂时地累积状态。但是工作流的许多其他部分可以以流水线方式执行。 当作业完成时，它的输出需要持续到某个地方，以便用户可以找到并使用它 —— 很可能它会再次写入分布式文件系统。因此，在使用数据流引擎时，HDFS 上的物化数据集通常仍是作业的输入和最终输出。和 MapReduce 一样，输入是不可变的，输出被完全替换。比起 MapReduce 的改进是，你不用再自己去将中间状态写入文件系统了。 图与迭代处理在 “图数据模型” 中，我们讨论了使用图来建模数据，并使用图查询语言来遍历图中的边与点。第二章 的讨论集中在 OLTP 风格的应用场景：快速执行查询来查找少量符合特定条件的顶点。 批处理上下文中的图也很有趣，其目标是在整个图上执行某种离线处理或分析。这种需求经常出现在机器学习应用（如推荐引擎）或排序系统中。例如，最着名的图形分析算法之一是 PageRank 【69】，它试图根据链接到某个网页的其他网页来估计该网页的流行度。它作为配方的一部分，用于确定网络搜索引擎呈现结果的顺序。 像 Spark、Flink 和 Tez 这样的数据流引擎（请参阅 “物化中间状态”）通常将算子作为 有向无环图（DAG） 的一部分安排在作业中。这与图处理不一样：在数据流引擎中，从一个算子到另一个算子的数据流 被构造成一个图，而数据本身通常由关系型元组构成。在图处理中，数据本身具有图的形式。又一个不幸的命名混乱！ 许多图算法是通过一次遍历一条边来表示的，将一个顶点与近邻的顶点连接起来，以传播一些信息，并不断重复，直到满足一些条件为止 —— 例如，直到没有更多的边要跟进，或直到一些指标收敛。我们在 图 2-6 中看到一个例子，它通过重复跟进标明地点归属关系的边，生成了数据库中北美包含的所有地点列表（这种算法被称为 传递闭包，即 transitive closure）。 可以在分布式文件系统中存储图（包含顶点和边的列表的文件），但是这种 “重复至完成” 的想法不能用普通的 MapReduce 来表示，因为它只扫过一趟数据。这种算法因此经常以 迭代 的风格实现： 外部调度程序运行批处理来计算算法的一个步骤。 当批处理过程完成时，调度器检查它是否完成（基于完成条件 —— 例如，没有更多的边要跟进，或者与上次迭代相比的变化低于某个阈值）。 如果尚未完成，则调度程序返回到步骤 1 并运行另一轮批处理。 这种方法是有效的，但是用 MapReduce 实现它往往非常低效，因为 MapReduce 没有考虑算法的迭代性质：它总是读取整个输入数据集并产生一个全新的输出数据集，即使与上次迭代相比，改变的仅仅是图中的一小部分。 Pregel处理模型针对图批处理的优化 —— 批量同步并行（BSP，Bulk Synchronous Parallel） 计算模型【70】已经开始流行起来。其中，Apache Giraph 【37】，Spark 的 GraphX API 和 Flink 的 Gelly API 【71】实现了它。它也被称为 Pregel 模型，因为 Google 的 Pregel 论文推广了这种处理图的方法【72】。 回想一下在 MapReduce 中，Mapper 在概念上向 Reducer 的特定调用 “发送消息”，因为框架将所有具有相同键的 Mapper 输出集中在一起。 Pregel 背后有一个类似的想法：一个顶点可以向另一个顶点 “发送消息”，通常这些消息是沿着图的边发送的。 在每次迭代中，为每个顶点调用一个函数，将所有发送给它的消息传递给它 —— 就像调用 Reducer 一样。与 MapReduce 的不同之处在于，在 Pregel 模型中，顶点在一次迭代到下一次迭代的过程中会记住它的状态，所以这个函数只需要处理新的传入消息。如果图的某个部分没有被发送消息，那里就不需要做任何工作。 这与 Actor 模型有些相似（请参阅 “分布式的 Actor 框架”），除了顶点状态和顶点之间的消息具有容错性和持久性，且通信以固定的回合进行：在每次迭代中，框架递送上次迭代中发送的所有消息。Actor 通常没有这样的时序保证。 容错顶点只能通过消息传递进行通信（而不是直接相互查询）的事实有助于提高 Pregel 作业的性能，因为消息可以成批处理，且等待通信的次数也减少了。唯一的等待是在迭代之间：由于 Pregel 模型保证所有在一轮迭代中发送的消息都在下轮迭代中送达，所以在下一轮迭代开始前，先前的迭代必须完全完成，而所有的消息必须在网络上完成复制。 即使底层网络可能丢失、重复或任意延迟消息（请参阅 “不可靠的网络”），Pregel 的实现能保证在后续迭代中消息在其目标顶点恰好处理一次。像 MapReduce 一样，框架能从故障中透明地恢复，以简化在 Pregel 上实现算法的编程模型。 这种容错是通过在迭代结束时，定期存档所有顶点的状态来实现的，即将其全部状态写入持久化存储。如果某个节点发生故障并且其内存中的状态丢失，则最简单的解决方法是将整个图计算回滚到上一个存档点，然后重启计算。如果算法是确定性的，且消息记录在日志中，那么也可以选择性地只恢复丢失的分区（就像之前讨论过的数据流引擎）【72】。 并行执行顶点不需要知道它在哪台物理机器上执行；当它向其他顶点发送消息时，它只是简单地将消息发往某个顶点 ID。图的分区取决于框架 —— 即，确定哪个顶点运行在哪台机器上，以及如何通过网络路由消息，以便它们到达正确的地方。 由于编程模型一次仅处理一个顶点（有时称为 “像顶点一样思考”），所以框架可以以任意方式对图分区。理想情况下如果顶点需要进行大量的通信，那么它们最好能被分区到同一台机器上。然而找到这样一种优化的分区方法是很困难的 —— 在实践中，图经常按照任意分配的顶点 ID 分区，而不会尝试将相关的顶点分组在一起。 因此，图算法通常会有很多跨机器通信的额外开销，而中间状态（节点之间发送的消息）往往比原始图大。通过网络发送消息的开销会显著拖慢分布式图算法的速度。 出于这个原因，如果你的图可以放入一台计算机的内存中，那么单机（甚至可能是单线程）算法很可能会超越分布式批处理【73,74】。图比内存大也没关系，只要能放入单台计算机的磁盘，使用 GraphChi 等框架进行单机处理是就一个可行的选择【75】。如果图太大，不适合单机处理，那么像 Pregel 这样的分布式方法是不可避免的。高效的并行图算法是一个进行中的研究领域【76】。 高级API和语言自 MapReduce 开始流行的这几年以来，分布式批处理的执行引擎已经很成熟了。到目前为止，基础设施已经足够强大，能够存储和处理超过 10,000 台机器集群上的数 PB 的数据。由于在这种规模下物理执行批处理的问题已经被认为或多或少解决了，所以关注点已经转向其他领域：改进编程模型，提高处理效率，扩大这些技术可以解决的问题集。 如前所述，Hive、Pig、Cascading 和 Crunch 等高级语言和 API 变得越来越流行，因为手写 MapReduce 作业实在是个苦力活。随着 Tez 的出现，这些高级语言还有一个额外好处，可以迁移到新的数据流执行引擎，而无需重写作业代码。Spark 和 Flink 也有它们自己的高级数据流 API，通常是从 FlumeJava 中获取的灵感【34】。 这些数据流 API 通常使用关系型构建块来表达一个计算：按某个字段连接数据集；按键对元组做分组；按某些条件过滤；并通过计数求和或其他函数来聚合元组。在内部，这些操作是使用本章前面讨论过的各种连接和分组算法来实现的。 除了少写代码的明显优势之外，这些高级接口还支持交互式用法，在这种交互式使用中，你可以在 Shell 中增量式编写分析代码，频繁运行来观察它做了什么。这种开发风格在探索数据集和试验处理方法时非常有用。这也让人联想到 Unix 哲学，我们在 “Unix 哲学” 中讨论过这个问题。 此外，这些高级接口不仅提高了人类的工作效率，也提高了机器层面的作业执行效率。 向声明式查询语言的转变与硬写执行连接的代码相比，指定连接关系算子的优点是，框架可以分析连接输入的属性，并自动决定哪种上述连接算法最适合当前任务。 Hive、Spark 和 Flink 都有基于代价的查询优化器可以做到这一点，甚至可以改变连接顺序，最小化中间状态的数量【66,77,78,79】。 连接算法的选择可以对批处理作业的性能产生巨大影响，而无需理解和记住本章中讨论的各种连接算法。如果连接是以 声明式（declarative） 的方式指定的，那这就这是可行的：应用只是简单地说明哪些连接是必需的，查询优化器决定如何最好地执行连接。我们以前在 “数据查询语言” 中见过这个想法。 但 MapReduce 及其数据流后继者在其他方面，与 SQL 的完全声明式查询模型有很大区别。 MapReduce 是围绕着回调函数的概念建立的：对于每条记录或者一组记录，调用一个用户定义的函数（Mapper 或 Reducer），并且该函数可以自由地调用任意代码来决定输出什么。这种方法的优点是可以基于大量已有库的生态系统创作：解析、自然语言分析、图像分析以及运行数值或统计算法等。 自由运行任意代码，长期以来都是传统 MapReduce 批处理系统与 MPP 数据库的区别所在（请参阅 “Hadoop 与分布式数据库的对比” 一节）。虽然数据库具有编写用户定义函数的功能，但是它们通常使用起来很麻烦，而且与大多数编程语言中广泛使用的程序包管理器和依赖管理系统兼容不佳（例如 Java 的 Maven、Javascript 的 npm 以及 Ruby 的 gems）。 然而数据流引擎已经发现，支持除连接之外的更多 声明式特性 还有其他的优势。例如，如果一个回调函数只包含一个简单的过滤条件，或者只是从一条记录中选择了一些字段，那么在为每条记录调用函数时会有相当大的额外 CPU 开销。如果以声明方式表示这些简单的过滤和映射操作，那么查询优化器可以利用列式存储布局（请参阅 “列式存储”），只从磁盘读取所需的列。 Hive、Spark DataFrames 和 Impala 还使用了向量化执行（请参阅 “内存带宽和矢量化处理”）：在对 CPU 缓存友好的内部循环中迭代数据，避免函数调用。Spark 生成 JVM 字节码【79】，Impala 使用 LLVM 为这些内部循环生成本机代码【41】。 通过在高级 API 中引入声明式的部分，并使查询优化器可以在执行期间利用这些来做优化，批处理框架看起来越来越像 MPP 数据库了（并且能实现可与之媲美的性能）。同时，通过拥有运行任意代码和以任意格式读取数据的可扩展性，它们保持了灵活性的优势。 专业化的不同领域尽管能够运行任意代码的可扩展性是很有用的，但是也有很多常见的例子，不断重复着标准的处理模式。因而这些模式值得拥有自己的可重用通用构建模块实现。传统上，MPP 数据库满足了商业智能分析和业务报表的需求，但这只是许多使用批处理的领域之一。 另一个越来越重要的领域是统计和数值算法，它们是机器学习应用所需要的（例如分类器和推荐系统）。可重用的实现正在出现：例如，Mahout 在 MapReduce、Spark 和 Flink 之上实现了用于机器学习的各种算法，而 MADlib 在关系型 MPP 数据库（Apache HAWQ）中实现了类似的功能【54】。 空间算法也是有用的，例如 k 近邻搜索（k-nearest neighbors, kNN）【80】，它在一些多维空间中搜索与给定项最近的项目 —— 这是一种相似性搜索。近似搜索对于基因组分析算法也很重要，它们需要找到相似但不相同的字符串【81】。 批处理引擎正被用于分布式执行日益广泛的各领域算法。随着批处理系统获得各种内置功能以及高级声明式算子，且随着 MPP 数据库变得更加灵活和易于编程，两者开始看起来相似了：最终，它们都只是存储和处理数据的系统。 本章小结在本章中，我们探索了批处理的主题。我们首先看到了诸如 awk、grep 和 sort 之类的 Unix 工具，然后我们看到了这些工具的设计理念是如何应用到 MapReduce 和更近的数据流引擎中的。一些设计原则包括：输入是不可变的，输出是为了作为另一个（仍未知的）程序的输入，而复杂的问题是通过编写 “做好一件事” 的小工具来解决的。 在 Unix 世界中，允许程序与程序组合的统一接口是文件与管道；在 MapReduce 中，该接口是一个分布式文件系统。我们看到数据流引擎添加了自己的管道式数据传输机制，以避免将中间状态物化至分布式文件系统，但作业的初始输入和最终输出通常仍是 HDFS。 分布式批处理框架需要解决的两个主要问题是： 分区 在 MapReduce 中，Mapper 根据输入文件块进行分区。Mapper 的输出被重新分区、排序并合并到可配置数量的 Reducer 分区中。这一过程的目的是把所有的 相关 数据（例如带有相同键的所有记录）都放在同一个地方。 后 MapReduce 时代的数据流引擎若非必要会尽量避免排序，但它们也采取了大致类似的分区方法。 容错 MapReduce 经常写入磁盘，这使得从单个失败的任务恢复很轻松，无需重新启动整个作业，但在无故障的情况下减慢了执行速度。数据流引擎更多地将中间状态保存在内存中，更少地物化中间状态，这意味着如果节点发生故障，则需要重算更多的数据。确定性算子减少了需要重算的数据量。 我们讨论了几种 MapReduce 的连接算法，其中大多数也在 MPP 数据库和数据流引擎内部使用。它们也很好地演示了分区算法是如何工作的： 排序合并连接 每个参与连接的输入都通过一个提取连接键的 Mapper。通过分区、排序和合并，具有相同键的所有记录最终都会进入相同的 Reducer 调用。这个函数能输出连接好的记录。 广播散列连接 两个连接输入之一很小，所以它并没有分区，而且能被完全加载进一个哈希表中。因此，你可以为连接输入大端的每个分区启动一个 Mapper，将输入小端的散列表加载到每个 Mapper 中，然后扫描大端，一次一条记录，并为每条记录查询散列表。 分区散列连接 如果两个连接输入以相同的方式分区（使用相同的键，相同的散列函数和相同数量的分区），则可以独立地对每个分区应用散列表方法。 分布式批处理引擎有一个刻意限制的编程模型：回调函数（比如 Mapper 和 Reducer）被假定是无状态的，而且除了指定的输出外，必须没有任何外部可见的副作用。这一限制允许框架在其抽象下隐藏一些困难的分布式系统问题：当遇到崩溃和网络问题时，任务可以安全地重试，任何失败任务的输出都被丢弃。如果某个分区的多个任务成功，则其中只有一个能使其输出实际可见。 得益于这个框架，你在批处理作业中的代码无需操心实现容错机制：框架可以保证作业的最终输出与没有发生错误的情况相同，虽然实际上也许不得不重试各种任务。比起在线服务一边处理用户请求一边将写入数据库作为处理请求的副作用，批处理提供的这种可靠性语义要强得多。 批处理作业的显著特点是，它读取一些输入数据并产生一些输出数据，但不修改输入 —— 换句话说，输出是从输入衍生出的。最关键的是，输入数据是 有界的（bounded）：它有一个已知的，固定的大小（例如，它包含一些时间点的日志文件或数据库内容的快照）。因为它是有界的，一个作业知道自己什么时候完成了整个输入的读取，所以一个工作在做完后，最终总是会完成的。 在下一章中，我们将转向流处理，其中的输入是 无界的（unbounded） —— 也就是说，你还有活儿要干，然而它的输入是永无止境的数据流。在这种情况下，作业永无完成之日。因为在任何时候都可能有更多的工作涌入。我们将看到，在某些方面上，流处理和批处理是相似的。但是关于无尽数据流的假设也对我们构建系统的方式产生了很多改变。 参考文献 Jeffrey Dean and Sanjay Ghemawat: “MapReduce: Simplified Data Processing on Large Clusters,” at 6th USENIX Symposium on Operating System Design and Implementation (OSDI), December 2004. Joel Spolsky: “The Perils of JavaSchools,” joelonsoftware.com, December 25, 2005. Shivnath Babu and Herodotos Herodotou: “Massively Parallel Databases and MapReduce Systems,” Foundations and Trends in Databases, volume 5, number 1, pages 1–104, November 2013. doi:10.1561&#x2F;1900000036 David J. DeWitt and Michael Stonebraker: “MapReduce: A Major Step Backwards,” originally published at databasecolumn.vertica.com, January 17, 2008. Henry Robinson: “The Elephant Was a Trojan Horse: On the Death of Map-Reduce at Google,” the-paper-trail.org, June 25, 2014. “The Hollerith Machine,” United States Census Bureau, census.gov. “IBM 82, 83, and 84 Sorters Reference Manual,” Edition A24-1034-1, International Business Machines Corporation, July 1962. Adam Drake: “Command-Line Tools Can Be 235x Faster than Your Hadoop Cluster,” aadrake.com, January 25, 2014. “GNU Coreutils 8.23 Documentation,” Free Software Foundation, Inc., 2014. Martin Kleppmann: “Kafka, Samza, and the Unix Philosophy of Distributed Data,” martin.kleppmann.com, August 5, 2015. Doug McIlroy:Internal Bell Labs memo, October 1964. Cited in: Dennis M. Richie: “Advice from Doug McIlroy,” cm.bell-labs.com. M. D. McIlroy, E. N. Pinson, and B. A. Tague: “UNIX Time-Sharing System: Foreword,” The Bell System Technical Journal, volume 57, number 6, pages 1899–1904, July 1978. Eric S. Raymond: The Art of UNIX Programming. Addison-Wesley, 2003. ISBN: 978-0-13-142901-7 Ronald Duncan: “Text File Formats – ASCII Delimited Text – Not CSV or TAB Delimited Text,” ronaldduncan.wordpress.com, October 31, 2009. Alan Kay: “Is ‘Software Engineering’ an Oxymoron?,” tinlizzie.org. Martin Fowler: “InversionOfControl,” martinfowler.com, June 26, 2005. Daniel J. Bernstein: “Two File Descriptors for Sockets,” cr.yp.to. Rob Pike and Dennis M. Ritchie: “The Styx Architecture for Distributed Systems,” Bell Labs Technical Journal, volume 4, number 2, pages 146–152, April 1999. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: “The Google File System,” at 19th ACM Symposium on Operating Systems Principles (SOSP), October 2003. doi:10.1145&#x2F;945445.945450 Michael Ovsiannikov, Silvius Rus, Damian Reeves, et al.: “The Quantcast File System,” Proceedings of the VLDB Endowment, volume 6, number 11, pages 1092–1101, August 2013. doi:10.14778&#x2F;2536222.2536234 “OpenStack Swift 2.6.1 Developer Documentation,” OpenStack Foundation, docs.openstack.org, March 2016. Zhe Zhang, Andrew Wang, Kai Zheng, et al.: “Introduction to HDFS Erasure Coding in Apache Hadoop,” blog.cloudera.com, September 23, 2015. Peter Cnudde: “Hadoop Turns 10,” yahoohadoop.tumblr.com, February 5, 2016. Eric Baldeschwieler: “Thinking About the HDFS vs. Other Storage Technologies,” hortonworks.com, July 25, 2012. Brendan Gregg: “Manta: Unix Meets Map Reduce,” dtrace.org, June 25, 2013. Tom White: Hadoop: The Definitive Guide, 4th edition. O’Reilly Media, 2015. ISBN: 978-1-491-90163-2 Jim N. Gray: “Distributed Computing Economics,” Microsoft Research Tech Report MSR-TR-2003-24, March 2003. Márton Trencséni: “Luigi vs Airflow vs Pinball,” bytepawn.com, February 6, 2016. Roshan Sumbaly, Jay Kreps, and Sam Shah: “The ‘Big Data’ Ecosystem at LinkedIn,” at ACM International Conference on Management of Data (SIGMOD), July 2013. doi:10.1145&#x2F;2463676.2463707 Alan F. Gates, Olga Natkovich, Shubham Chopra, et al.: “Building a High-Level Dataflow System on Top of Map-Reduce: The Pig Experience,” at 35th International Conference on Very Large Data Bases (VLDB), August 2009. Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, et al.: “Hive – A Petabyte Scale Data Warehouse Using Hadoop,” at 26th IEEE International Conference on Data Engineering (ICDE), March 2010. doi:10.1109&#x2F;ICDE.2010.5447738 “Cascading 3.0 User Guide,” Concurrent, Inc., docs.cascading.org, January 2016. “Apache Crunch User Guide,” Apache Software Foundation, crunch.apache.org. Craig Chambers, Ashish Raniwala, Frances Perry, et al.: “FlumeJava: Easy, Efficient Data-Parallel Pipelines,” at 31st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), June 2010. doi:10.1145&#x2F;1806596.1806638 Jay Kreps: “Why Local State is a Fundamental Primitive in Stream Processing,” oreilly.com, July 31, 2014. Martin Kleppmann: “Rethinking Caching in Web Apps,” martin.kleppmann.com, October 1, 2012. Mark Grover, Ted Malaska, Jonathan Seidman, and Gwen Shapira: Hadoop Application Architectures. O’Reilly Media, 2015. ISBN: 978-1-491-90004-8 Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: “Challenges to Adopting Stronger Consistency at Scale,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS), May 2015. Sriranjan Manjunath: “Skewed Join,” wiki.apache.org, 2009. David J. DeWitt, Jeffrey F. Naughton, Donovan A.Schneider, and S. Seshadri: “Practical Skew Handling in Parallel Joins,” at 18th International Conference on Very Large Data Bases (VLDB), August 1992. Marcel Kornacker, Alexander Behm, Victor Bittorf, et al.: “Impala: A Modern, Open-Source SQL Engine for Hadoop,” at 7th Biennial Conference on Innovative Data Systems Research (CIDR), January 2015. Matthieu Monsch: “Open-Sourcing PalDB, a Lightweight Companion for Storing Side Data,” engineering.linkedin.com, October 26, 2015. Daniel Peng and Frank Dabek: “Large-Scale Incremental Processing Using Distributed Transactions and Notifications,” at 9th USENIX conference on Operating Systems Design and Implementation (OSDI), October 2010. ““Cloudera Search User Guide,” Cloudera, Inc., September 2015. Lili Wu, Sam Shah, Sean Choi, et al.: “The Browsemaps: Collaborative Filtering at LinkedIn,” at 6th Workshop on Recommender Systems and the Social Web (RSWeb), October 2014. Roshan Sumbaly, Jay Kreps, Lei Gao, et al.: “Serving Large-Scale Batch Computed Data with Project Voldemort,” at 10th USENIX Conference on File and Storage Technologies (FAST), February 2012. Varun Sharma: “Open-Sourcing Terrapin: A Serving System for Batch Generated Data,” engineering.pinterest.com, September 14, 2015. Nathan Marz: “ElephantDB,” slideshare.net, May 30, 2011. Jean-Daniel (JD) Cryans: “How-to: Use HBase Bulk Loading, and Why,” blog.cloudera.com, September 27, 2013. Nathan Marz: “How to Beat the CAP Theorem,” nathanmarz.com, October 13, 2011. Molly Bartlett Dishman and Martin Fowler: “Agile Architecture,” at O’Reilly Software Architecture Conference, March 2015. David J. DeWitt and Jim N. Gray: “Parallel Database Systems: The Future of High Performance Database Systems,” Communications of the ACM, volume 35, number 6, pages 85–98, June 1992. doi:10.1145&#x2F;129888.129894 Jay Kreps: “But the multi-tenancy thing is actually really really hard,” tweetstorm, twitter.com, October 31, 2014. Jeffrey Cohen, Brian Dolan, Mark Dunlap, et al.: “MAD Skills: New Analysis Practices for Big Data,” Proceedings of the VLDB Endowment, volume 2, number 2, pages 1481–1492, August 2009. doi:10.14778&#x2F;1687553.1687576 Ignacio Terrizzano, Peter Schwarz, Mary Roth, and John E. Colino: “Data Wrangling: The Challenging Journey from the Wild to the Lake,” at 7th Biennial Conference on Innovative Data Systems Research (CIDR), January 2015. Paige Roberts: “To Schema on Read or to Schema on Write, That Is the Hadoop Data Lake Question,” adaptivesystemsinc.com, July 2, 2015. Bobby Johnson and Joseph Adler: “The Sushi Principle: Raw Data Is Better,” at Strata+Hadoop World, February 2015. Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, et al.: “Apache Hadoop YARN: Yet Another Resource Negotiator,” at 4th ACM Symposium on Cloud Computing (SoCC), October 2013. doi:10.1145&#x2F;2523616.2523633 Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, et al.: “Large-Scale Cluster Management at Google with Borg,” at 10th European Conference on Computer Systems (EuroSys), April 2015. doi:10.1145&#x2F;2741948.2741964 Malte Schwarzkopf: “The Evolution of Cluster Scheduler Architectures,” firmament.io, March 9, 2016. Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, et al.: “Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,” at 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI), April 2012. Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia: Learning Spark. O’Reilly Media, 2015. ISBN: 978-1-449-35904-1 Bikas Saha and Hitesh Shah: “Apache Tez: Accelerating Hadoop Query Processing,” at Hadoop Summit, June 2014. Bikas Saha, Hitesh Shah, Siddharth Seth, et al.: “Apache Tez: A Unifying Framework for Modeling and Building Data Processing Applications,” at ACM International Conference on Management of Data (SIGMOD), June 2015. doi:10.1145&#x2F;2723372.2742790 Kostas Tzoumas: “Apache Flink: API, Runtime, and Project Roadmap,” slideshare.net, January 14, 2015. Alexander Alexandrov, Rico Bergmann, Stephan Ewen, et al.: “The Stratosphere Platform for Big Data Analytics,” The VLDB Journal, volume 23, number 6, pages 939–964, May 2014. doi:10.1007&#x2F;s00778-014-0357-y Michael Isard, Mihai Budiu, Yuan Yu, et al.: “Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks,” at European Conference on Computer Systems (EuroSys), March 2007. doi:10.1145&#x2F;1272996.1273005 Daniel Warneke and Odej Kao: “Nephele: Efficient Parallel Data Processing in the Cloud,” at 2nd Workshop on Many-Task Computing on Grids and Supercomputers (MTAGS), November 2009. doi:10.1145&#x2F;1646468.1646476 Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd: “The PageRank” Leslie G. Valiant: “A Bridging Model for Parallel Computation,” Communications of the ACM, volume 33, number 8, pages 103–111, August 1990. doi:10.1145&#x2F;79173.79181 Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, and Volker Markl: “Spinning Fast Iterative Data Flows,” Proceedings of the VLDB Endowment, volume 5, number 11, pages 1268-1279, July 2012. doi:10.14778&#x2F;2350229.2350245 Grzegorz Malewicz, Matthew H.Austern, Aart J. C. Bik, et al.: “Pregel: A System for Large-Scale Graph Processing,” at ACM International Conference on Management of Data (SIGMOD), June 2010. doi:10.1145&#x2F;1807167.1807184 Frank McSherry, Michael Isard, and Derek G. Murray: “Scalability! But at What COST?,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS), May 2015. Ionel Gog, Malte Schwarzkopf, Natacha Crooks, et al.: “Musketeer: All for One, One for All in Data Processing Systems,” at 10th European Conference on Computer Systems (EuroSys), April 2015. doi:10.1145&#x2F;2741948.2741968 Aapo Kyrola, Guy Blelloch, and Carlos Guestrin: “GraphChi: Large-Scale Graph Computation on Just a PC,” at 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2012. Andrew Lenharth, Donald Nguyen, and Keshav Pingali: “Parallel Graph Analytics,” Communications of the ACM, volume 59, number 5, pages 78–87, May doi:10.1145&#x2F;2901919 Fabian Hüske: “Peeking into Apache Flink’s Engine Room,” flink.apache.org, March 13, 2015. Mostafa Mokhtar: “Hive 0.14 Cost Based Optimizer (CBO) Technical Overview,” hortonworks.com, March 2, 2015. Michael Armbrust, Reynold S Xin, Cheng Lian, et al.: “Spark SQL: Relational Data Processing in Spark,” at ACM International Conference on Management of Data (SIGMOD), June 2015. doi:10.1145&#x2F;2723372.2742797 Daniel Blazevski: “Planting Quadtrees for Apache Flink,” insightdataengineering.com, March 25, 2016. Tom White: “Genome Analysis Toolkit: Now Using Apache Spark for Data Processing,” blog.cloudera.com, April 6, 2016."},{"title":"第十一章：流处理","path":"/wiki/ddia/ch11.html","content":"有效的复杂系统总是从简单的系统演化而来。 反之亦然：从零设计的复杂系统没一个能有效工作的。 —— 约翰・加尔，Systemantics（1975） 在 第十章 中，我们讨论了批处理技术，它读取一组文件作为输入，并生成一组新的文件作为输出。输出是 衍生数据（derived data） 的一种形式；也就是说，如果需要，可以通过再次运行批处理过程来重新创建数据集。我们看到了如何使用这个简单而强大的想法来建立搜索索引、推荐系统、做分析等等。 然而，在 第十章 中仍然有一个很大的假设：即输入是有界的，即已知和有限的大小，所以批处理知道它何时完成输入的读取。例如，MapReduce 核心的排序操作必须读取其全部输入，然后才能开始生成输出：可能发生这种情况：最后一条输入记录具有最小的键，因此需要第一个被输出，所以提早开始输出是不可行的。 实际上，很多数据是 无界限 的，因为它随着时间的推移而逐渐到达：你的用户在昨天和今天产生了数据，明天他们将继续产生更多的数据。除非你停业，否则这个过程永远都不会结束，所以数据集从来就不会以任何有意义的方式 “完成”【1】。因此，批处理程序必须将数据人为地分成固定时间段的数据块，例如，在每天结束时处理一天的数据，或者在每小时结束时处理一小时的数据。 日常批处理中的问题是，输入的变更只会在一天之后的输出中反映出来，这对于许多急躁的用户来说太慢了。为了减少延迟，我们可以更频繁地运行处理 —— 比如说，在每秒钟的末尾 —— 或者甚至更连续一些，完全抛开固定的时间切片，当事件发生时就立即进行处理，这就是 流处理（stream processing） 背后的想法。 一般来说，“流” 是指随着时间的推移逐渐可用的数据。这个概念出现在很多地方：Unix 的 stdin 和 stdout、编程语言（惰性列表）【2】、文件系统 API（如 Java 的 FileInputStream）、TCP 连接、通过互联网传送音频和视频等等。 在本章中，我们将把 事件流（event stream） 视为一种数据管理机制：无界限，增量处理，与上一章中的批量数据相对应。我们将首先讨论怎样表示、存储、通过网络传输流。在 “数据库与流” 中，我们将研究流和数据库之间的关系。最后在 “流处理” 中，我们将研究连续处理这些流的方法和工具，以及它们用于应用构建的方式。 传递事件流在批处理领域，作业的输入和输出是文件（也许在分布式文件系统上）。流处理领域中的等价物看上去是什么样子的？ 当输入是一个文件（一个字节序列），第一个处理步骤通常是将其解析为一系列记录。在流处理的上下文中，记录通常被叫做 事件（event） ，但它本质上是一样的：一个小的、自包含的、不可变的对象，包含某个时间点发生的某件事情的细节。一个事件通常包含一个来自日历时钟的时间戳，以指明事件发生的时间（请参阅 “单调钟与日历时钟”）。 例如，发生的事件可能是用户采取的行动，例如查看页面或进行购买。它也可能来源于机器，例如对温度传感器或 CPU 利用率的周期性测量。在 “使用 Unix 工具的批处理” 的示例中，Web 服务器日志的每一行都是一个事件。 事件可能被编码为文本字符串或 JSON，或者某种二进制编码，如 第四章 所述。这种编码允许你存储一个事件，例如将其追加到一个文件，将其插入关系表，或将其写入文档数据库。它还允许你通过网络将事件发送到另一个节点以进行处理。 在批处理中，文件被写入一次，然后可能被多个作业读取。类似地，在流处理术语中，一个事件由 生产者（producer） （也称为 发布者（publisher） 或 发送者（sender） ）生成一次，然后可能由多个 消费者（consumer） （ 订阅者（subscribers） 或 接收者（recipients） ）进行处理【3】。在文件系统中，文件名标识一组相关记录；在流式系统中，相关的事件通常被聚合为一个 主题（topic） 或 流（stream） 。 原则上讲，文件或数据库就足以连接生产者和消费者：生产者将其生成的每个事件写入数据存储，且每个消费者定期轮询数据存储，检查自上次运行以来新出现的事件。这实际上正是批处理在每天结束时处理当天数据时所做的事情。 但当我们想要进行低延迟的连续处理时，如果数据存储不是为这种用途专门设计的，那么轮询开销就会很大。轮询的越频繁，能返回新事件的请求比例就越低，而额外开销也就越高。相比之下，最好能在新事件出现时直接通知消费者。 数据库在传统上对这种通知机制支持的并不好，关系型数据库通常有 触发器（trigger） ，它们可以对变化（如，插入表中的一行）作出反应，但是它们的功能非常有限，并且在数据库设计中有些后顾之忧【4,5】。相应的是，已经开发了专门的工具来提供事件通知。 消息传递系统向消费者通知新事件的常用方式是使用 消息传递系统（messaging system）：生产者发送包含事件的消息，然后将消息推送给消费者。我们之前在 “消息传递中的数据流” 中谈到了这些系统，但现在我们将详细介绍这些系统。 像生产者和消费者之间的 Unix 管道或 TCP 连接这样的直接信道，是实现消息传递系统的简单方法。但是，大多数消息传递系统都在这一基本模型上进行了扩展。特别的是，Unix 管道和 TCP 将恰好一个发送者与恰好一个接收者连接，而一个消息传递系统允许多个生产者节点将消息发送到同一个主题，并允许多个消费者节点接收主题中的消息。 在这个 发布 &#x2F; 订阅 模式中，不同的系统采取各种各样的方法，并没有针对所有目的的通用答案。为了区分这些系统，问一下这两个问题会特别有帮助： 如果生产者发送消息的速度比消费者能够处理的速度快会发生什么？ 一般来说，有三种选择：系统可以丢掉消息，将消息放入缓冲队列，或使用 背压（backpressure，也称为 流量控制，即 flow control：阻塞生产者，以免其发送更多的消息）。例如 Unix 管道和 TCP 就使用了背压：它们有一个固定大小的小缓冲区，如果填满，发送者会被阻塞，直到接收者从缓冲区中取出数据（请参阅 “网络拥塞和排队”）。 如果消息被缓存在队列中，那么理解队列增长会发生什么是很重要的。当队列装不进内存时系统会崩溃吗？还是将消息写入磁盘？如果是这样，磁盘访问又会如何影响消息传递系统的性能【6】？ 如果节点崩溃或暂时脱机，会发生什么情况？ —— 是否会有消息丢失？ 与数据库一样，持久性可能需要写入磁盘和 &#x2F; 或复制的某种组合（请参阅 “复制与持久性”），这是有代价的。如果你能接受有时消息会丢失，则可能在同一硬件上获得更高的吞吐量和更低的延迟。 是否可以接受消息丢失取决于应用。例如，对于周期传输的传感器读数和指标，偶尔丢失的数据点可能并不重要，因为更新的值会在短时间内发出。但要注意，如果大量的消息被丢弃，可能无法立刻意识到指标已经不正确了【7】。如果你正在对事件计数，那么它们能够可靠送达是更重要的，因为每个丢失的消息都意味着使计数器的错误扩大。 我们在 第十章 中探讨的批处理系统的一个很好的特性是，它们提供了强大的可靠性保证：失败的任务会自动重试，失败任务的部分输出会自动丢弃。这意味着输出与没有发生故障一样，这有助于简化编程模型。在本章的后面，我们将研究如何在流处理的上下文中提供类似的保证。 直接从生产者传递给消费者许多消息传递系统使用生产者和消费者之间的直接网络通信，而不通过中间节点： UDP 组播广泛应用于金融行业，例如股票市场，其中低时延非常重要【8】。虽然 UDP 本身是不可靠的，但应用层的协议可以恢复丢失的数据包（生产者必须记住它发送的数据包，以便能按需重新发送数据包）。 无代理的消息库，如 ZeroMQ 【9】和 nanomsg 采取类似的方法，通过 TCP 或 IP 多播实现发布 &#x2F; 订阅消息传递。 StatsD 【10】和 Brubeck 【7】使用不可靠的 UDP 消息传递来收集网络中所有机器的指标并对其进行监控。 （在 StatsD 协议中，只有接收到所有消息，才认为计数器指标是正确的；使用 UDP 将使得指标处在一种最佳近似状态【11】。另请参阅 “TCP 与 UDP” 如果消费者在网络上公开了服务，生产者可以直接发送 HTTP 或 RPC 请求（请参阅 “服务中的数据流：REST 与 RPC”）将消息推送给使用者。这就是 webhooks 背后的想法【12】，一种服务的回调 URL 被注册到另一个服务中，并且每当事件发生时都会向该 URL 发出请求。 尽管这些直接消息传递系统在设计它们的环境中运行良好，但是它们通常要求应用代码意识到消息丢失的可能性。它们的容错程度极为有限：即使协议检测到并重传在网络中丢失的数据包，它们通常也只是假设生产者和消费者始终在线。 如果消费者处于脱机状态，则可能会丢失其不可达时发送的消息。一些协议允许生产者重试失败的消息传递，但当生产者崩溃时，它可能会丢失消息缓冲区及其本应发送的消息，这种方法可能就没用了。 消息代理一种广泛使用的替代方法是通过 消息代理（message broker，也称为 消息队列，即 message queue）发送消息，消息代理实质上是一种针对处理消息流而优化的数据库。它作为服务器运行，生产者和消费者作为客户端连接到服务器。生产者将消息写入代理，消费者通过从代理那里读取来接收消息。 通过将数据集中在代理上，这些系统可以更容易地容忍来来去去的客户端（连接，断开连接和崩溃），而持久性问题则转移到代理的身上。一些消息代理只将消息保存在内存中，而另一些消息代理（取决于配置）将其写入磁盘，以便在代理崩溃的情况下不会丢失。针对缓慢的消费者，它们通常会允许无上限的排队（而不是丢弃消息或背压），尽管这种选择也可能取决于配置。 排队的结果是，消费者通常是 异步（asynchronous） 的：当生产者发送消息时，通常只会等待代理确认消息已经被缓存，而不等待消息被消费者处理。向消费者递送消息将发生在未来某个未定的时间点 —— 通常在几分之一秒之内，但有时当消息堆积时会显著延迟。 消息代理与数据库的对比有些消息代理甚至可以使用 XA 或 JTA 参与两阶段提交协议（请参阅 “实践中的分布式事务”）。这个功能与数据库在本质上非常相似，尽管消息代理和数据库之间仍存在实践上很重要的差异： 数据库通常保留数据直至显式删除，而大多数消息代理在消息成功递送给消费者时会自动删除消息。这样的消息代理不适合长期的数据存储。 由于它们很快就能删除消息，大多数消息代理都认为它们的工作集相当小 —— 即队列很短。如果代理需要缓冲很多消息，比如因为消费者速度较慢（如果内存装不下消息，可能会溢出到磁盘），每个消息需要更长的处理时间，整体吞吐量可能会恶化【6】。 数据库通常支持次级索引和各种搜索数据的方式，而消息代理通常支持按照某种模式匹配主题，订阅其子集。虽然机制并不一样，但对于客户端选择想要了解的数据的一部分，都是基本的方式。 查询数据库时，结果通常基于某个时间点的数据快照；如果另一个客户端随后向数据库写入一些改变了查询结果的内容，则第一个客户端不会发现其先前结果现已过期（除非它重复查询或轮询变更）。相比之下，消息代理不支持任意查询，但是当数据发生变化时（即新消息可用时），它们会通知客户端。 这是关于消息代理的传统观点，它被封装在诸如 JMS 【14】和 AMQP 【15】的标准中，并且被诸如 RabbitMQ、ActiveMQ、HornetQ、Qpid、TIBCO 企业消息服务、IBM MQ、Azure Service Bus 和 Google Cloud Pub&#x2F;Sub 所实现 【16】。 多个消费者当多个消费者从同一主题中读取消息时，有两种主要的消息传递模式，如 图 11-1 所示： 负载均衡（load balancing） 每条消息都被传递给消费者 之一，所以处理该主题下消息的工作能被多个消费者共享。代理可以为消费者任意分配消息。当处理消息的代价高昂，希望能并行处理消息时，此模式非常有用（在 AMQP 中，可以通过让多个客户端从同一个队列中消费来实现负载均衡，而在 JMS 中则称之为 共享订阅，即 shared subscription）。 扇出（fan-out） 每条消息都被传递给 所有 消费者。扇出允许几个独立的消费者各自 “收听” 相同的消息广播，而不会相互影响 —— 这个流处理中的概念对应批处理中多个不同批处理作业读取同一份输入文件 （JMS 中的主题订阅与 AMQP 中的交叉绑定提供了这一功能）。 图 11-1 （a）负载平衡：在消费者间共享消费主题；（b）扇出：将每条消息传递给多个消费者。 两种模式可以组合使用：例如，两个独立的消费者组可以每组各订阅同一个主题，每一组都共同收到所有消息，但在每一组内部，每条消息仅由单个节点处理。 确认与重新传递消费者随时可能会崩溃，所以有一种可能的情况是：代理向消费者递送消息，但消费者没有处理，或者在消费者崩溃之前只进行了部分处理。为了确保消息不会丢失，消息代理使用 确认（acknowledgments）：客户端必须显式告知代理消息处理完毕的时间，以便代理能将消息从队列中移除。 如果与客户端的连接关闭，或者代理超出一段时间未收到确认，代理则认为消息没有被处理，因此它将消息再递送给另一个消费者。 （请注意可能发生这样的情况，消息 实际上是 处理完毕的，但 确认 在网络中丢失了。需要一种原子提交协议才能处理这种情况，正如在 “实践中的分布式事务” 中所讨论的那样） 当与负载均衡相结合时，这种重传行为对消息的顺序有种有趣的影响。在 图 11-2 中，消费者通常按照生产者发送的顺序处理消息。然而消费者 2 在处理消息 m3 时崩溃，与此同时消费者 1 正在处理消息 m4。未确认的消息 m3 随后被重新发送给消费者 1，结果消费者 1 按照 m4，m3，m5 的顺序处理消息。因此 m3 和 m4 的交付顺序与生产者 1 的发送顺序不同。 图 11-2 在处理 m3 时消费者 2 崩溃，因此稍后重传至消费者 1 即使消息代理试图保留消息的顺序（如 JMS 和 AMQP 标准所要求的），负载均衡与重传的组合也不可避免地导致消息被重新排序。为避免此问题，你可以让每个消费者使用单独的队列（即不使用负载均衡功能）。如果消息是完全独立的，则消息顺序重排并不是一个问题。但正如我们将在本章后续部分所述，如果消息之间存在因果依赖关系，这就是一个很重要的问题。 分区日志通过网络发送数据包或向网络服务发送请求通常是短暂的操作，不会留下永久的痕迹。尽管可以永久记录（通过抓包与日志），但我们通常不这么做。即使是将消息持久地写入磁盘的消息代理，在送达给消费者之后也会很快删除消息，因为它们建立在短暂消息传递的思维方式上。 数据库和文件系统采用截然相反的方法论：至少在某人显式删除前，通常写入数据库或文件的所有内容都要被永久记录下来。 这种思维方式上的差异对创建衍生数据的方式有巨大影响。如 第十章 所述，批处理过程的一个关键特性是，你可以反复运行它们，试验处理步骤，不用担心损坏输入（因为输入是只读的）。而 AMQP&#x2F;JMS 风格的消息传递并非如此：收到消息是具有破坏性的，因为确认可能导致消息从代理中被删除，因此你不能期望再次运行同一个消费者能得到相同的结果。 如果你将新的消费者添加到消息传递系统，通常只能接收到消费者注册之后开始发送的消息。先前的任何消息都随风而逝，一去不复返。作为对比，你可以随时为文件和数据库添加新的客户端，且能读取任意久远的数据（只要应用没有显式覆盖或删除这些数据）。 为什么我们不能把它俩杂交一下，既有数据库的持久存储方式，又有消息传递的低延迟通知？这就是 基于日志的消息代理（log-based message brokers） 背后的想法。 使用日志进行消息存储日志只是磁盘上简单的仅追加记录序列。我们先前在 第三章 中日志结构存储引擎和预写式日志的上下文中讨论了日志，在 第五章 复制的上下文里也讨论了它。 同样的结构可以用于实现消息代理：生产者通过将消息追加到日志末尾来发送消息，而消费者通过依次读取日志来接收消息。如果消费者读到日志末尾，则会等待新消息追加的通知。 Unix 工具 tail -f 能监视文件被追加写入的数据，基本上就是这样工作的。 为了伸缩超出单个磁盘所能提供的更高吞吐量，可以对日志进行 分区（按 第六章 的定义）。不同的分区可以托管在不同的机器上，使得每个分区都有一份能独立于其他分区进行读写的日志。一个主题可以定义为一组携带相同类型消息的分区。这种方法如 图 11-3 所示。 在每个分区内，代理为每个消息分配一个单调递增的序列号或 偏移量（offset，在 图 11-3 中，框中的数字是消息偏移量）。这种序列号是有意义的，因为分区是仅追加写入的，所以分区内的消息是完全有序的。没有跨不同分区的顺序保证。 图 11-3 生产者通过将消息追加写入主题分区文件来发送消息，消费者依次读取这些文件 Apache Kafka 【17,18】、Amazon Kinesis Streams 【19】和 Twitter 的 DistributedLog 【20,21】都是基于日志的消息代理。 Google Cloud Pub&#x2F;Sub 在架构上类似，但对外暴露的是 JMS 风格的 API，而不是日志抽象【16】。尽管这些消息代理将所有消息写入磁盘，但通过跨多台机器分区，每秒能够实现数百万条消息的吞吐量，并通过复制消息来实现容错性【22,23】。 日志与传统的消息传递相比基于日志的方法天然支持扇出式消息传递，因为多个消费者可以独立读取日志，而不会相互影响 —— 读取消息不会将其从日志中删除。为了在一组消费者之间实现负载平衡，代理可以将整个分区分配给消费者组中的节点，而不是将单条消息分配给消费者客户端。 然后每个客户端将消费被指派分区中的 所有 消息。通常情况下，当一个用户被指派了一个日志分区时，它会以简单的单线程方式顺序地读取分区中的消息。这种粗粒度的负载均衡方法有一些缺点： 共享消费主题工作的节点数，最多为该主题中的日志分区数，因为同一个分区内的所有消息被递送到同一个节点 ^i。 如果某条消息处理缓慢，则它会阻塞该分区中后续消息的处理（一种行首阻塞的形式；请参阅 “描述性能”）。 因此在消息处理代价高昂，希望逐条并行处理，以及消息的顺序并没有那么重要的情况下，JMS&#x2F;AMQP 风格的消息代理是可取的。另一方面，在消息吞吐量很高，处理迅速，顺序很重要的情况下，基于日志的方法表现得非常好。 消费者偏移量顺序消费一个分区使得判断消息是否已经被处理变得相当容易：所有偏移量小于消费者的当前偏移量的消息已经被处理，而具有更大偏移量的消息还没有被看到。因此，代理不需要跟踪确认每条消息，只需要定期记录消费者的偏移即可。这种方法减少了额外簿记开销，而且在批处理和流处理中采用这种方法有助于提高基于日志的系统的吞吐量。 实际上，这种偏移量与单领导者数据库复制中常见的日志序列号非常相似，我们在 “设置新从库” 中讨论了这种情况。在数据库复制中，日志序列号允许跟随者断开连接后，重新连接到领导者，并在不跳过任何写入的情况下恢复复制。这里原理完全相同：消息代理表现得像一个主库，而消费者就像一个从库。 如果消费者节点失效，则失效消费者的分区将指派给其他节点，并从最后记录的偏移量开始消费消息。如果消费者已经处理了后续的消息，但还没有记录它们的偏移量，那么重启后这些消息将被处理两次。我们将在本章后面讨论这个问题的处理方法。 磁盘空间使用如果只追加写入日志，则磁盘空间终究会耗尽。为了回收磁盘空间，日志实际上被分割成段，并不时地将旧段删除或移动到归档存储。 （我们将在后面讨论一种更为复杂的磁盘空间释放方式） 这就意味着如果一个慢消费者跟不上消息产生的速率而落后得太多，它的消费偏移量指向了删除的段，那么它就会错过一些消息。实际上，日志实现了一个有限大小的缓冲区，当缓冲区填满时会丢弃旧消息，它也被称为 循环缓冲区（circular buffer） 或 环形缓冲区（ring buffer）。不过由于缓冲区在磁盘上，因此缓冲区可能相当的大。 让我们做个简单计算。在撰写本文时，典型的大型硬盘容量为 6TB，顺序写入吞吐量为 150MB&#x2F;s。如果以最快的速度写消息，则需要大约 11 个小时才能填满磁盘。因而磁盘可以缓冲 11 个小时的消息，之后它将开始覆盖旧的消息。即使使用多个磁盘和机器，这个比率也是一样的。实践中的部署很少能用满磁盘的写入带宽，所以通常可以保存一个几天甚至几周的日志缓冲区。 不管保留多长时间的消息，日志的吞吐量或多或少保持不变，因为无论如何，每个消息都会被写入磁盘【18】。这种行为与默认将消息保存在内存中，仅当队列太长时才写入磁盘的消息传递系统形成鲜明对比。当队列很短时，这些系统非常快；而当这些系统开始写入磁盘时，就要慢的多，所以吞吐量取决于保留的历史数量。 当消费者跟不上生产者时在 “消息传递系统” 中，如果消费者无法跟上生产者发送信息的速度时，我们讨论了三种选择：丢弃信息，进行缓冲或施加背压。在这种分类法里，基于日志的方法是缓冲的一种形式，具有很大但大小固定的缓冲区（受可用磁盘空间的限制）。 如果消费者远远落后，而所要求的信息比保留在磁盘上的信息还要旧，那么它将不能读取这些信息，所以代理实际上丢弃了比缓冲区容量更大的旧信息。你可以监控消费者落后日志头部的距离，如果落后太多就发出报警。由于缓冲区很大，因而有足够的时间让运维人员来修复慢消费者，并在消息开始丢失之前让其赶上。 即使消费者真的落后太多开始丢失消息，也只有那个消费者受到影响；它不会中断其他消费者的服务。这是一个巨大的运维优势：你可以实验性地消费生产日志，以进行开发，测试或调试，而不必担心会中断生产服务。当消费者关闭或崩溃时，会停止消耗资源，唯一剩下的只有消费者偏移量。 这种行为也与传统的消息代理形成了鲜明对比，在那种情况下，你需要小心地删除那些消费者已经关闭的队列 —— 否则那些队列就会累积不必要的消息，从其他仍活跃的消费者那里占走内存。 重播旧消息我们之前提到，使用 AMQP 和 JMS 风格的消息代理，处理和确认消息是一个破坏性的操作，因为它会导致消息在代理上被删除。另一方面，在基于日志的消息代理中，使用消息更像是从文件中读取数据：这是只读操作，不会更改日志。 除了消费者的任何输出之外，处理的唯一副作用是消费者偏移量的前进。但偏移量是在消费者的控制之下的，所以如果需要的话可以很容易地操纵：例如你可以用昨天的偏移量跑一个消费者副本，并将输出写到不同的位置，以便重新处理最近一天的消息。你可以使用各种不同的处理代码重复任意次。 这一方面使得基于日志的消息传递更像上一章的批处理，其中衍生数据通过可重复的转换过程与输入数据显式分离。它允许进行更多的实验，更容易从错误和漏洞中恢复，使其成为在组织内集成数据流的良好工具【24】。 数据库与流我们已经在消息代理和数据库之间进行了一些比较。尽管传统上它们被视为单独的工具类别，但是我们看到基于日志的消息代理已经成功地从数据库中获取灵感并将其应用于消息传递。我们也可以反过来：从消息传递和流中获取灵感，并将它们应用于数据库。 我们之前曾经说过，事件是某个时刻发生的事情的记录。发生的事情可能是用户操作（例如键入搜索查询）或读取传感器，但也可能是 写入数据库。某些东西被写入数据库的事实是可以被捕获、存储和处理的事件。这一观察结果表明，数据库和数据流之间的联系不仅仅是磁盘日志的物理存储 —— 而是更深层的联系。 事实上，复制日志（请参阅 “复制日志的实现”）是一个由数据库写入事件组成的流，由主库在处理事务时生成。从库将写入流应用到它们自己的数据库副本，从而最终得到相同数据的精确副本。复制日志中的事件描述发生的数据更改。 我们还在 “全序广播” 中遇到了状态机复制原理，其中指出：如果每个事件代表对数据库的写入，并且每个副本按相同的顺序处理相同的事件，则副本将达到相同的最终状态 （假设事件处理是一个确定性的操作）。这是事件流的又一种场景！ 在本节中，我们将首先看看异构数据系统中出现的一个问题，然后探讨如何通过将事件流的想法带入数据库来解决这个问题。 保持系统同步正如我们在本书中所看到的，没有一个系统能够满足所有的数据存储、查询和处理需求。在实践中，大多数重要应用都需要组合使用几种不同的技术来满足所有的需求：例如，使用 OLTP 数据库来为用户请求提供服务，使用缓存来加速常见请求，使用全文索引来处理搜索查询，使用数据仓库用于分析。每一种技术都有自己的数据副本，并根据自己的目的进行存储方式的优化。 由于相同或相关的数据出现在了不同的地方，因此相互间需要保持同步：如果某个项目在数据库中被更新，它也应当在缓存、搜索索引和数据仓库中被更新。对于数据仓库，这种同步通常由 ETL 进程执行（请参阅 “数据仓库”），通常是先取得数据库的完整副本，然后执行转换，并批量加载到数据仓库中 —— 换句话说，批处理。我们在 “批处理工作流的输出” 中同样看到了如何使用批处理创建搜索索引、推荐系统和其他衍生数据系统。 如果周期性的完整数据库转储过于缓慢，有时会使用的替代方法是 双写（dual write），其中应用代码在数据变更时明确写入每个系统：例如，首先写入数据库，然后更新搜索索引，然后使缓存项失效（甚至同时执行这些写入）。 但是，双写有一些严重的问题，其中一个是竞争条件，如 图 11-4 所示。在这个例子中，两个客户端同时想要更新一个项目 X：客户端 1 想要将值设置为 A，客户端 2 想要将其设置为 B。两个客户端首先将新值写入数据库，然后将其写入到搜索索引。因为运气不好，这些请求的时序是交错的：数据库首先看到来自客户端 1 的写入将值设置为 A，然后来自客户端 2 的写入将值设置为 B，因此数据库中的最终值为 B。搜索索引首先看到来自客户端 2 的写入，然后是客户端 1 的写入，所以搜索索引中的最终值是 A。即使没发生错误，这两个系统现在也永久地不一致了。 图 11-4 在数据库中 X 首先被设置为 A，然后被设置为 B，而在搜索索引处，写入以相反的顺序到达 除非有一些额外的并发检测机制，例如我们在 “检测并发写入” 中讨论的版本向量，否则你甚至不会意识到发生了并发写入 —— 一个值将简单地以无提示方式覆盖另一个值。 双重写入的另一个问题是，其中一个写入可能会失败，而另一个成功。这是一个容错问题，而不是一个并发问题，但也会造成两个系统互相不一致的结果。确保它们要么都成功要么都失败，是原子提交问题的一个例子，解决这个问题的代价是昂贵的（请参阅 “原子提交与两阶段提交”）。 如果你只有一个单领导者复制的数据库，那么这个领导者决定了写入顺序，而状态机复制方法可以在数据库副本上工作。然而，在 图 11-4 中，没有单个主库：数据库可能有一个领导者，搜索索引也可能有一个领导者，但是两者都不追随对方，所以可能会发生冲突（请参阅 “多主复制“）。 如果实际上只有一个领导者 —— 例如，数据库 —— 而且我们能让搜索索引成为数据库的追随者，情况要好得多。但这在实践中可能吗？ 变更数据捕获大多数数据库的复制日志的问题在于，它们一直被当做数据库的内部实现细节，而不是公开的 API。客户端应该通过其数据模型和查询语言来查询数据库，而不是解析复制日志并尝试从中提取数据。 数十年来，许多数据库根本没有记录在档的获取变更日志的方式。由于这个原因，捕获数据库中所有的变更，然后将其复制到其他存储技术（搜索索引、缓存或数据仓库）中是相当困难的。 最近，人们对 变更数据捕获（change data capture, CDC） 越来越感兴趣，这是一种观察写入数据库的所有数据变更，并将其提取并转换为可以复制到其他系统中的形式的过程。 CDC 是非常有意思的，尤其是当变更能在被写入后立刻用于流时。 例如，你可以捕获数据库中的变更，并不断将相同的变更应用至搜索索引。如果变更日志以相同的顺序应用，则可以预期搜索索引中的数据与数据库中的数据是匹配的。搜索索引和任何其他衍生数据系统只是变更流的消费者，如 图 11-5 所示。 图 11-5 将数据按顺序写入一个数据库，然后按照相同的顺序将这些更改应用到其他系统 变更数据捕获的实现我们可以将日志消费者叫做 衍生数据系统，正如在 第三部分 的介绍中所讨论的：存储在搜索索引和数据仓库中的数据，只是 记录系统 数据的额外视图。变更数据捕获是一种机制，可确保对记录系统所做的所有更改都反映在衍生数据系统中，以便衍生系统具有数据的准确副本。 从本质上说，变更数据捕获使得一个数据库成为领导者（被捕获变化的数据库），并将其他组件变为追随者。基于日志的消息代理非常适合从源数据库传输变更事件，因为它保留了消息的顺序（避免了 图 11-2 的重新排序问题）。 数据库触发器可用来实现变更数据捕获（请参阅 “基于触发器的复制”），通过注册观察所有变更的触发器，并将相应的变更项写入变更日志表中。但是它们往往是脆弱的，而且有显著的性能开销。解析复制日志可能是一种更稳健的方法，但它也很有挑战，例如如何应对模式变更。 LinkedIn 的 Databus【25】，Facebook 的 Wormhole【26】和 Yahoo! 的 Sherpa【27】大规模地应用这个思路。 Bottled Water 使用解码 WAL 的 API 实现了 PostgreSQL 的 CDC【28】，Maxwell 和 Debezium 通过解析 binlog 对 MySQL 做了类似的事情【29,30,31】，Mongoriver 读取 MongoDB oplog【32,33】，而 GoldenGate 为 Oracle 提供类似的功能【34,35】。 类似于消息代理，变更数据捕获通常是异步的：记录数据库系统在提交变更之前不会等待消费者应用变更。这种设计具有的运维优势是，添加缓慢的消费者不会过度影响记录系统。不过，所有复制延迟可能有的问题在这里都可能出现（请参阅 “复制延迟问题”）。 初始快照如果你拥有 所有 对数据库进行变更的日志，则可以通过重播该日志，来重建数据库的完整状态。但是在许多情况下，永远保留所有更改会耗费太多磁盘空间，且重播过于费时，因此日志需要被截断。 例如，构建新的全文索引需要整个数据库的完整副本 —— 仅仅应用最近变更的日志是不够的，因为这样会丢失最近未曾更新的项目。因此，如果你没有完整的历史日志，则需要从一个一致的快照开始，如先前的 “设置新从库” 中所述。 数据库的快照必须与变更日志中的已知位置或偏移量相对应，以便在处理完快照后知道从哪里开始应用变更。一些 CDC 工具集成了这种快照功能，而其他工具则把它留给你手动执行。 日志压缩如果你只能保留有限的历史日志，则每次要添加新的衍生数据系统时，都需要做一次快照。但 日志压缩（log compaction） 提供了一个很好的备选方案。 我们之前在 “散列索引” 中关于日志结构存储引擎的上下文中讨论了日志压缩（请参阅 图 3-2 的示例）。原理很简单：存储引擎定期在日志中查找具有相同键的记录，丢掉所有重复的内容，并只保留每个键的最新更新。这个压缩与合并过程在后台运行。 在日志结构存储引擎中，具有特殊值 NULL（墓碑，即 tombstone）的更新表示该键被删除，并会在日志压缩过程中被移除。但只要键不被覆盖或删除，它就会永远留在日志中。这种压缩日志所需的磁盘空间仅取决于数据库的当前内容，而不取决于数据库中曾经发生的写入次数。如果相同的键经常被覆盖写入，则先前的值将最终将被垃圾回收，只有最新的值会保留下来。 在基于日志的消息代理与变更数据捕获的上下文中也适用相同的想法。如果 CDC 系统被配置为，每个变更都包含一个主键，且每个键的更新都替换了该键以前的值，那么只需要保留对键的最新写入就足够了。 现在，无论何时需要重建衍生数据系统（如搜索索引），你可以从压缩日志主题的零偏移量处启动新的消费者，然后依次扫描日志中的所有消息。日志能保证包含数据库中每个键的最新值（也可能是一些较旧的值）—— 换句话说，你可以使用它来获取数据库内容的完整副本，而无需从 CDC 源数据库取一个快照。 Apache Kafka 支持这种日志压缩功能。正如我们将在本章后面看到的，它允许消息代理被当成持久性存储使用，而不仅仅是用于临时消息。 变更流的API支持越来越多的数据库开始将变更流作为第一等的接口，而不像传统上要去做加装改造，或者费工夫逆向工程一个 CDC。例如，RethinkDB 允许查询订阅通知，当查询结果变更时获得通知【36】，Firebase 【37】和 CouchDB 【38】基于变更流进行同步，该变更流同样可用于应用。而 Meteor 使用 MongoDB oplog 订阅数据变更，并改变了用户接口【39】。 VoltDB 允许事务以流的形式连续地从数据库中导出数据【40】。数据库将关系数据模型中的输出流表示为一个表，事务可以向其中插入元组，但不能查询。已提交事务按照提交顺序写入这个特殊表，而流则由该表中的元组日志构成。外部消费者可以异步消费该日志，并使用它来更新衍生数据系统。 Kafka Connect【41】致力于将广泛的数据库系统的变更数据捕获工具与 Kafka 集成。一旦变更事件进入 Kafka 中，它就可以用于更新衍生数据系统，比如搜索索引，也可以用于本章稍后讨论的流处理系统。 事件溯源我们在这里讨论的想法和 事件溯源（Event Sourcing） 之间有一些相似之处，这是一个在 领域驱动设计（domain-driven design, DDD） 社区中折腾出来的技术。我们将简要讨论事件溯源，因为它包含了一些关于流处理系统的有用想法。 与变更数据捕获类似，事件溯源涉及到 将所有对应用状态的变更 存储为变更事件日志。最大的区别是事件溯源将这一想法应用到了一个不同的抽象层次上： 在变更数据捕获中，应用以 可变方式（mutable way） 使用数据库，可以任意更新和删除记录。变更日志是从数据库的底层提取的（例如，通过解析复制日志），从而确保从数据库中提取的写入顺序与实际写入的顺序相匹配，从而避免 图 11-4 中的竞态条件。写入数据库的应用不需要知道 CDC 的存在。 在事件溯源中，应用逻辑显式构建在写入事件日志的不可变事件之上。在这种情况下，事件存储是仅追加写入的，更新与删除是不鼓励的或禁止的。事件被设计为旨在反映应用层面发生的事情，而不是底层的状态变更。 事件溯源是一种强大的数据建模技术：从应用的角度来看，将用户的行为记录为不可变的事件更有意义，而不是在可变数据库中记录这些行为的影响。事件溯源使得应用随时间演化更为容易，通过更容易理解事情发生的原因来帮助调试的进行，并有利于防止应用 Bug（请参阅 “不可变事件的优点”）。 例如，存储 “学生取消选课” 事件以中性的方式清楚地表达了单个行为的意图，而其副作用 “从登记表中删除了一个条目，而一条取消原因的记录被添加到学生反馈表 “则嵌入了很多有关稍后对数据的使用方式的假设。如果引入一个新的应用功能，例如 “将位置留给等待列表中的下一个人” —— 事件溯源方法允许将新的副作用轻松地从现有事件中脱开。 事件溯源类似于 编年史（chronicle） 数据模型【45】，事件日志与星型模式中的事实表之间也存在相似之处（请参阅 “星型和雪花型：分析的模式”） 。 诸如 Event Store【46】这样的专业数据库已经被开发出来，供使用事件溯源的应用使用，但总的来说，这种方法独立于任何特定的工具。传统的数据库或基于日志的消息代理也可以用来构建这种风格的应用。 从事件日志中派生出当前状态事件日志本身并不是很有用，因为用户通常期望看到的是系统的当前状态，而不是变更历史。例如，在购物网站上，用户期望能看到他们购物车里的当前内容，而不是他们购物车所有变更的一个仅追加列表。 因此，使用事件溯源的应用需要拉取事件日志（表示 写入 系统的数据），并将其转换为适合向用户显示的应用状态（从系统 读取 数据的方式【47】）。这种转换可以使用任意逻辑，但它应当是确定性的，以便能再次运行，并从事件日志中衍生出相同的应用状态。 与变更数据捕获一样，重播事件日志允许让你重新构建系统的当前状态。不过，日志压缩需要采用不同的方式处理： 用于记录更新的 CDC 事件通常包含记录的 完整新版本，因此主键的当前值完全由该主键的最近事件确定，而日志压缩可以丢弃相同主键的先前事件。 另一方面，事件溯源在更高层次进行建模：事件通常表示用户操作的意图，而不是因为操作而发生的状态更新机制。在这种情况下，后面的事件通常不会覆盖先前的事件，所以你需要完整的历史事件来重新构建最终状态。这里进行同样的日志压缩是不可能的。 使用事件溯源的应用通常有一些机制，用于存储从事件日志中导出的当前状态快照，因此它们不需要重复处理完整的日志。然而这只是一种性能优化，用来加速读取，提高从崩溃中恢复的速度；真正的目的是系统能够永久存储所有原始事件，并在需要时重新处理完整的事件日志。我们将在 “不变性的局限性” 中讨论这个假设。 命令和事件事件溯源的哲学是仔细区分 事件（event） 和 命令（command）【48】。当来自用户的请求刚到达时，它一开始是一个命令：在这个时间点上它仍然可能失败，比如，因为违反了一些完整性条件。应用必须首先验证它是否可以执行该命令。如果验证成功并且命令被接受，则它变为一个持久化且不可变的事件。 例如，如果用户试图注册特定用户名，或预定飞机或剧院的座位，则应用需要检查用户名或座位是否已被占用。（先前在 “容错共识” 中讨论过这个例子）当检查成功时，应用可以生成一个事件，指示特定的用户名是由特定的用户 ID 注册的，或者座位已经预留给特定的顾客。 在事件生成的时刻，它就成为了 事实（fact）。即使客户稍后决定更改或取消预订，他们之前曾预定了某个特定座位的事实仍然成立，而更改或取消是之后添加的单独的事件。 事件流的消费者不允许拒绝事件：当消费者看到事件时，它已经成为日志中不可变的一部分，并且可能已经被其他消费者看到了。因此任何对命令的验证，都需要在它成为事件之前同步完成。例如，通过使用一个可以原子性地自动验证命令并发布事件的可串行事务。 或者，预订座位的用户请求可以拆分为两个事件：第一个是暂时预约，第二个是验证预约后的独立的确认事件（如 “使用全序广播实现线性一致的存储” 中所述） 。这种分割方式允许验证发生在一个异步的过程中。 状态、流和不变性我们在 第十章 中看到，批处理因其输入文件不变性而受益良多，你可以在现有输入文件上运行实验性处理作业，而不用担心损坏它们。这种不变性原则也是使得事件溯源与变更数据捕获如此强大的原因。 我们通常将数据库视为应用程序当前状态的存储 —— 这种表示针对读取进行了优化，而且通常对于服务查询而言是最为方便的表示。状态的本质是，它会变化，所以数据库才会支持数据的增删改。这又该如何匹配不变性呢？ 只要你的状态发生了变化，那么这个状态就是这段时间中事件修改的结果。例如，当前可用的座位列表是你已处理的预订所产生的结果，当前帐户余额是帐户中的借与贷的结果，而 Web 服务器的响应时间图，是所有已发生 Web 请求的独立响应时间的聚合结果。 无论状态如何变化，总是有一系列事件导致了这些变化。即使事情已经执行与回滚，这些事件出现是始终成立的。关键的想法是：可变的状态与不可变事件的仅追加日志相互之间并不矛盾：它们是一体两面，互为阴阳的。所有变化的日志 —— 变化日志（changelog），表示了随时间演变的状态。 如果你倾向于数学表示，那么你可能会说，应用状态是事件流对时间求积分得到的结果，而变更流是状态对时间求微分的结果，如 图 11-6 所示【49,50,51】。这个比喻有一些局限性（例如，状态的二阶导似乎没有意义），但这是考虑数据的一个实用出发点。$$state(now) &#x3D; \\int_{t&#x3D;0}^{now}{stream(t) \\ dt} \\stream(t) &#x3D; \\frac{d\\ state(t)}{dt}$$ 图 11-6 应用当前状态与事件流之间的关系 如果你持久存储了变更日志，那么重现状态就非常简单。如果你认为事件日志是你的记录系统，而所有的衍生状态都从它派生而来，那么系统中的数据流动就容易理解的多。正如帕特・赫兰（Pat Helland）所说的【52】： 事务日志记录了数据库的所有变更。高速追加是更改日志的唯一方法。从这个角度来看，数据库的内容其实是日志中记录最新值的缓存。日志才是真相，数据库是日志子集的缓存，这一缓存子集恰好来自日志中每条记录与索引值的最新值。 日志压缩（如 “日志压缩” 中所述）是连接日志与数据库状态之间的桥梁：它只保留每条记录的最新版本，并丢弃被覆盖的版本。 不可变事件的优点数据库中的不变性是一个古老的概念。例如，会计在几个世纪以来一直在财务记账中应用不变性。一笔交易发生时，它被记录在一个仅追加写入的分类帐中，实质上是描述货币、商品或服务转手的事件日志。账目，比如利润、亏损、资产负债表，是从分类账中的交易求和衍生而来【53】。 如果发生错误，会计师不会删除或更改分类帐中的错误交易 —— 而是添加另一笔交易以补偿错误，例如退还一笔不正确的费用。不正确的交易将永远保留在分类帐中，对于审计而言可能非常重要。如果从不正确的分类账衍生出的错误数字已经公布，那么下一个会计周期的数字就会包括一个更正。这个过程在会计事务中是很常见的【54】。 尽管这种可审计性只在金融系统中尤其重要，但对于不受这种严格监管的许多其他系统，也是很有帮助的。如 “批处理输出的哲学” 中所讨论的，如果你意外地部署了将错误数据写入数据库的错误代码，当代码会破坏性地覆写数据时，恢复要困难得多。使用不可变事件的仅追加日志，诊断问题与故障恢复就要容易的多。 不可变的事件也包含了比当前状态更多的信息。例如在购物网站上，顾客可以将物品添加到他们的购物车，然后再将其移除。虽然从履行订单的角度，第二个事件取消了第一个事件，但对分析目的而言，知道客户考虑过某个特定项而之后又反悔，可能是很有用的。也许他们会选择在未来购买，或者他们已经找到了替代品。这个信息被记录在事件日志中，但对于移出购物车就删除记录的数据库而言，这个信息在移出购物车时可能就丢失了【42】。 从同一事件日志中派生多个视图此外，通过从不变的事件日志中分离出可变的状态，你可以针对不同的读取方式，从相同的事件日志中衍生出几种不同的表现形式。效果就像一个流的多个消费者一样（图 11-5）：例如，分析型数据库 Druid 使用这种方式直接从 Kafka 摄取数据【55】，Pistachio 是一个分布式的键值存储，使用 Kafka 作为提交日志【56】，Kafka Connect 能将来自 Kafka 的数据导出到各种不同的数据库与索引【41】。这对于许多其他存储和索引系统（如搜索服务器）来说是很有意义的，当系统要从分布式日志中获取输入时亦然（请参阅 “保持系统同步”）。 添加从事件日志到数据库的显式转换，能够使应用更容易地随时间演进：如果你想要引入一个新功能，以新的方式表示现有数据，则可以使用事件日志来构建一个单独的、针对新功能的读取优化视图，无需修改现有系统而与之共存。并行运行新旧系统通常比在现有系统中执行复杂的模式迁移更容易。一旦不再需要旧的系统，你可以简单地关闭它并回收其资源【47,57】。 如果你不需要担心如何查询与访问数据，那么存储数据通常是非常简单的。模式设计、索引和存储引擎的许多复杂性，都是希望支持某些特定查询和访问模式的结果（请参阅 第三章）。出于这个原因，通过将数据写入的形式与读取形式相分离，并允许几个不同的读取视图，你能获得很大的灵活性。这个想法有时被称为 命令查询责任分离（command query responsibility segregation, CQRS）【42,58,59】。 数据库和模式设计的传统方法是基于这样一种谬论，数据必须以与查询相同的形式写入。如果可以将数据从针对写入优化的事件日志转换为针对读取优化的应用状态，那么有关规范化和非规范化的争论就变得无关紧要了（请参阅 “多对一和多对多的关系”）：在针对读取优化的视图中对数据进行非规范化是完全合理的，因为翻译过程提供了使其与事件日志保持一致的机制。 在 “描述负载” 中，我们讨论了推特主页时间线，它是特定用户关注的人群所发推特的缓存（类似邮箱）。这是 针对读取优化的状态 的又一个例子：主页时间线是高度非规范化的，因为你的推文与你所有粉丝的时间线都构成了重复。然而，扇出服务保持了这种重复状态与新推特以及新关注关系的同步，从而保证了重复的可管理性。 并发控制事件溯源和变更数据捕获的最大缺点是，事件日志的消费者通常是异步的，所以可能会出现这样的情况：用户会写入日志，然后从日志衍生视图中读取，结果发现他的写入还没有反映在读取视图中。我们之前在 “读己之写” 中讨论了这个问题以及可能的解决方案。 一种解决方案是将事件追加到日志时同步执行读取视图的更新。而将这些写入操作合并为一个原子单元需要 事务，所以要么将事件日志和读取视图保存在同一个存储系统中，要么就需要跨不同系统进行分布式事务。或者，你也可以使用在 “使用全序广播实现线性一致的存储” 中讨论的方法。 另一方面，从事件日志导出当前状态也简化了并发控制的某些部分。许多对于多对象事务的需求（请参阅 “单对象和多对象操作”）源于单个用户操作需要在多个不同的位置更改数据。通过事件溯源，你可以设计一个自包含的事件以表示一个用户操作。然后用户操作就只需要在一个地方进行单次写入操作 —— 即将事件附加到日志中 —— 这个还是很容易使原子化的。 如果事件日志与应用状态以相同的方式分区（例如，处理分区 3 中的客户事件只需要更新分区 3 中的应用状态），那么直接使用单线程日志消费者就不需要写入并发控制了。它从设计上一次只处理一个事件（请参阅 “真的串行执行”）。日志通过在分区中定义事件的序列顺序，消除了并发性的不确定性【24】。如果一个事件触及多个状态分区，那么需要做更多的工作，我们将在 第十二章 讨论。 不变性的局限性许多不使用事件溯源模型的系统也还是依赖不可变性：各种数据库在内部使用不可变的数据结构或多版本数据来支持时间点快照（请参阅 “索引和快照隔离” ）。 Git、Mercurial 和 Fossil 等版本控制系统也依靠不可变的数据来保存文件的版本历史记录。 永远保持所有变更的不变历史，在多大程度上是可行的？答案取决于数据集的流失率。一些工作负载主要是添加数据，很少更新或删除；它们很容易保持不变。其他工作负载在相对较小的数据集上有较高的更新 &#x2F; 删除率；在这些情况下，不可变的历史可能增至难以接受的巨大，碎片化可能成为一个问题，压缩与垃圾收集的表现对于运维的稳健性变得至关重要【60,61】。 除了性能方面的原因外，也可能有出于管理方面的原因需要删除数据的情况，尽管这些数据都是不可变的。例如，隐私条例可能要求在用户关闭帐户后删除他们的个人信息，数据保护立法可能要求删除错误的信息，或者可能需要阻止敏感信息的意外泄露。 在这种情况下，仅仅在日志中添加另一个事件来指明先前的数据应该被视为删除是不够的 —— 你实际上是想改写历史，并假装数据从一开始就没有写入。例如，Datomic 管这个特性叫 切除（excision） 【62】，而 Fossil 版本控制系统有一个类似的概念叫 避免（shunning） 【63】。 真正删除数据是非常非常困难的【64】，因为副本可能存在于很多地方：例如，存储引擎，文件系统和 SSD 通常会向一个新位置写入，而不是原地覆盖旧数据【52】，而备份通常是特意做成不可变的，防止意外删除或损坏。删除操作更多的是指 “使取回数据更困难”，而不是指 “使取回数据不可能”。无论如何，有时你必须得尝试，正如我们在 “立法与自律” 中所看到的。 流处理到目前为止，本章中我们已经讨论了流的来源（用户活动事件，传感器和写入数据库），我们讨论了流如何传输（直接通过消息传送，通过消息代理，通过事件日志）。 剩下的就是讨论一下你可以用流做什么 —— 也就是说，你可以处理它。一般来说，有三种选项： 你可以将事件中的数据写入数据库、缓存、搜索索引或类似的存储系统，然后能被其他客户端查询。如 图 11-5 所示，这是数据库与系统其他部分所发生的变更保持同步的好方法 —— 特别是当流消费者是写入数据库的唯一客户端时。如 “批处理工作流的输出” 中所讨论的，它是写入存储系统的流等价物。 你能以某种方式将事件推送给用户，例如发送报警邮件或推送通知，或将事件流式传输到可实时显示的仪表板上。在这种情况下，人是流的最终消费者。 你可以处理一个或多个输入流，并产生一个或多个输出流。流可能会经过由几个这样的处理阶段组成的流水线，最后再输出（选项 1 或 2）。 在本章的剩余部分中，我们将讨论选项 3：处理流以产生其他衍生流。处理这样的流的代码片段，被称为 算子（operator） 或 作业（job）。它与我们在 第十章 中讨论过的 Unix 进程和 MapReduce 作业密切相关，数据流的模式是相似的：一个流处理器以只读的方式使用输入流，并将其输出以仅追加的方式写入一个不同的位置。 流处理中的分区和并行化模式也非常类似于 第十章 中介绍的 MapReduce 和数据流引擎，因此我们不再重复这些主题。基本的 Map 操作（如转换和过滤记录）也是一样的。 与批量作业相比的一个关键区别是，流不会结束。这种差异会带来很多隐含的结果。正如本章开始部分所讨论的，排序对无界数据集没有意义，因此无法使用 排序合并连接（请参阅 “Reduce 侧连接与分组”）。容错机制也必须改变：对于已经运行了几分钟的批处理作业，可以简单地从头开始重启失败任务，但是对于已经运行数年的流作业，重启后从头开始跑可能并不是一个可行的选项。 流处理的应用长期以来，流处理一直用于监控目的，如果某个事件发生，组织希望能得到警报。例如： 欺诈检测系统需要确定信用卡的使用模式是否有意外地变化，如果信用卡可能已被盗刷，则锁卡。 交易系统需要检查金融市场的价格变化，并根据指定的规则进行交易。 制造系统需要监控工厂中机器的状态，如果出现故障，可以快速定位问题。 军事和情报系统需要跟踪潜在侵略者的活动，并在出现袭击征兆时发出警报。 这些类型的应用需要非常精密复杂的模式匹配与相关检测。然而随着时代的进步，流处理的其他用途也开始出现。在本节中，我们将简要比较一下这些应用。 复合事件处理复合事件处理（complex event processing, CEP） 是 20 世纪 90 年代为分析事件流而开发出的一种方法，尤其适用于需要搜索某些事件模式的应用【65,66】。与正则表达式允许你在字符串中搜索特定字符模式的方式类似，CEP 允许你指定规则以在流中搜索某些事件模式。 CEP 系统通常使用高层次的声明式查询语言，比如 SQL，或者图形用户界面，来描述应该检测到的事件模式。这些查询被提交给处理引擎，该引擎消费输入流，并在内部维护一个执行所需匹配的状态机。当发现匹配时，引擎发出一个 复合事件（即 complex event，CEP 因此得名），并附有检测到的事件模式详情【67】。 在这些系统中，查询和数据之间的关系与普通数据库相比是颠倒的。通常情况下，数据库会持久存储数据，并将查询视为临时的：当查询进入时，数据库搜索与查询匹配的数据，然后在查询完成时丢掉查询。 CEP 引擎反转了角色：查询是长期存储的，来自输入流的事件不断流过它们，搜索匹配事件模式的查询【68】。 CEP 的实现包括 Esper【69】、IBM InfoSphere Streams【70】、Apama、TIBCO StreamBase 和 SQLstream。像 Samza 这样的分布式流处理组件，支持使用 SQL 在流上进行声明式查询【71】。 流分析使用流处理的另一个领域是对流进行分析。 CEP 与流分析之间的边界是模糊的，但一般来说，分析往往对找出特定事件序列并不关心，而更关注大量事件上的聚合与统计指标 —— 例如： 测量某种类型事件的速率（每个时间间隔内发生的频率） 滚动计算一段时间窗口内某个值的平均值 将当前的统计值与先前的时间区间的值对比（例如，检测趋势，当指标与上周同比异常偏高或偏低时报警） 这些统计值通常是在固定时间区间内进行计算的，例如，你可能想知道在过去 5 分钟内服务每秒查询次数的均值，以及此时间段内响应时间的第 99 百分位点。在几分钟内取平均，能抹平秒和秒之间的无关波动，且仍然能向你展示流量模式的时间图景。聚合的时间间隔称为 窗口（window），我们将在 “时间推理” 中更详细地讨论窗口。 流分析系统有时会使用概率算法，例如 Bloom filter（我们在 “性能优化” 中遇到过）来管理成员资格，HyperLogLog【72】用于基数估计以及各种百分比估计算法（请参阅 “实践中的百分位点“）。概率算法产出近似的结果，但比起精确算法的优点是内存使用要少得多。使用近似算法有时让人们觉得流处理系统总是有损的和不精确的，但这是错误看法：流处理并没有任何内在的近似性，而概率算法只是一种优化【73】。 许多开源分布式流处理框架的设计都是针对分析设计的：例如 Apache Storm、Spark Streaming、Flink、Concord、Samza 和 Kafka Streams 【74】。托管服务包括 Google Cloud Dataflow 和 Azure Stream Analytics。 维护物化视图我们在 “数据库与流” 中看到，数据库的变更流可以用于维护衍生数据系统（如缓存、搜索索引和数据仓库），并使其与源数据库保持最新。我们可以将这些示例视作维护 物化视图（materialized view） 的一种具体场景（请参阅 “聚合：数据立方体和物化视图”）：在某个数据集上衍生出一个替代视图以便高效查询，并在底层数据变更时更新视图【50】。 同样，在事件溯源中，应用程序的状态是通过应用事件日志来维护的；这里的应用程序状态也是一种物化视图。与流分析场景不同的是，仅考虑某个时间窗口内的事件通常是不够的：构建物化视图可能需要任意时间段内的 所有 事件，除了那些可能由日志压缩丢弃的过时事件（请参阅 “日志压缩“）。实际上，你需要一个可以一直延伸到时间开端的窗口。 原则上讲，任何流处理组件都可以用于维护物化视图，尽管 “永远运行” 与一些面向分析的框架假设的 “主要在有限时间段窗口上运行” 背道而驰， Samza 和 Kafka Streams 支持这种用法，建立在 Kafka 对日志压缩的支持上【75】。 在流上搜索除了允许搜索由多个事件构成模式的 CEP 外，有时也存在基于复杂标准（例如全文搜索查询）来搜索单个事件的需求。 例如，媒体监测服务可以订阅新闻文章 Feed 与来自媒体的播客，搜索任何关于公司、产品或感兴趣的话题的新闻。这是通过预先构建一个搜索查询来完成的，然后不断地将新闻项的流与该查询进行匹配。在一些网站上也有类似的功能：例如，当市场上出现符合其搜索条件的新房产时，房地产网站的用户可以要求网站通知他们。Elasticsearch 的这种过滤器功能，是实现这种流搜索的一种选择【76】。 传统的搜索引擎首先索引文件，然后在索引上跑查询。相比之下，搜索一个数据流则反了过来：查询被存储下来，文档从查询中流过，就像在 CEP 中一样。最简单的情况就是，你可以为每个文档测试每个查询。但是如果你有大量查询，这可能会变慢。为了优化这个过程，可以像对文档一样，为查询建立索引。因而收窄可能匹配的查询集合【77】。 消息传递和RPC在 “消息传递中的数据流” 中我们讨论过，消息传递系统可以作为 RPC 的替代方案，即作为一种服务间通信的机制，比如在 Actor 模型中所使用的那样。尽管这些系统也是基于消息和事件，但我们通常不会将其视作流处理组件： Actor 框架主要是管理模块通信的并发和分布式执行的一种机制，而流处理主要是一种数据管理技术。 Actor 之间的交流往往是短暂的、一对一的；而事件日志则是持久的、多订阅者的。 Actor 可以以任意方式进行通信（包括循环的请求 &#x2F; 响应模式），但流处理通常配置在无环流水线中，其中每个流都是一个特定作业的输出，由良好定义的输入流中派生而来。 也就是说，RPC 类系统与流处理之间有一些交叉领域。例如，Apache Storm 有一个称为 分布式 RPC 的功能，它允许将用户查询分散到一系列也处理事件流的节点上；然后这些查询与来自输入流的事件交织，而结果可以被汇总并发回给用户【78】（另请参阅 “多分区数据处理”）。 也可以使用 Actor 框架来处理流。但是，很多这样的框架在崩溃时不能保证消息的传递，除非你实现了额外的重试逻辑，否则这种处理不是容错的。 时间推理流处理通常需要与时间打交道，尤其是用于分析目的时候，会频繁使用时间窗口，例如 “过去五分钟的平均值”。“过去五分钟” 的含义看上去似乎是清晰而无歧义的，但不幸的是，这个概念非常棘手。 在批处理中过程中，大量的历史事件被快速地处理。如果需要按时间来分析，批处理器需要检查每个事件中嵌入的时间戳。读取运行批处理机器的系统时钟没有任何意义，因为处理运行的时间与事件实际发生的时间无关。 批处理可以在几分钟内读取一年的历史事件；在大多数情况下，感兴趣的时间线是历史中的一年，而不是处理中的几分钟。而且使用事件中的时间戳，使得处理是 确定性 的：在相同的输入上再次运行相同的处理过程会得到相同的结果（请参阅 “容错”）。 另一方面，许多流处理框架使用处理机器上的本地系统时钟（处理时间，即 processing time）来确定 窗口（windowing）【79】。这种方法的优点是简单，如果事件创建与事件处理之间的延迟可以忽略不计，那也是合理的。然而，如果存在任何显著的处理延迟 —— 即，事件处理显著地晚于事件实际发生的时间，这种处理方式就失效了。 事件时间与处理时间很多原因都可能导致处理延迟：排队，网络故障（请参阅 “不可靠的网络”），性能问题导致消息代理 &#x2F; 消息处理器出现争用，流消费者重启，从故障中恢复时重新处理过去的事件（请参阅 “重播旧消息”），或者在修复代码 BUG 之后。 而且，消息延迟还可能导致无法预测消息顺序。例如，假设用户首先发出一个 Web 请求（由 Web 服务器 A 处理），然后发出第二个请求（由服务器 B 处理）。 A 和 B 发出描述它们所处理请求的事件，但是 B 的事件在 A 的事件发生之前到达消息代理。现在，流处理器将首先看到 B 事件，然后看到 A 事件，即使它们实际上是以相反的顺序发生的。 有一个类比也许能帮助理解，“星球大战” 电影：第四集于 1977 年发行，第五集于 1980 年，第六集于 1983 年，紧随其后的是 1999 年的第一集，2002 年的第二集，和 2005 年的第三集，以及 2015 年的第七集【80】[^ii]。如果你按照按照它们上映的顺序观看电影，你处理电影的顺序与它们叙事的顺序就是不一致的。 （集数编号就像事件时间戳，而你观看电影的日期就是处理时间）作为人类，我们能够应对这种不连续性，但是流处理算法需要专门编写，以适应这种时序与顺序的问题。 [^ii]: 感谢 Flink 社区的 Kostas Kloudas 提出这个比喻。 将事件时间和处理时间搞混会导致错误的数据。例如，假设你有一个流处理器用于测量请求速率（计算每秒请求数）。如果你重新部署流处理器，它可能会停止一分钟，并在恢复之后处理积压的事件。如果你按处理时间来衡量速率，那么在处理积压日志时，请求速率看上去就像有一个异常的突发尖峰，而实际上请求速率是稳定的（图 11-7）。 图 11-7 按处理时间分窗，会因为处理速率的变动引入人为因素 知道什么时候准备好了用事件时间来定义窗口的一个棘手的问题是，你永远也无法确定是不是已经收到了特定窗口的所有事件，还是说还有一些事件正在来的路上。 例如，假设你将事件分组为一分钟的窗口，以便统计每分钟的请求数。你已经计数了一些带有本小时内第 37 分钟时间戳的事件，时间流逝，现在进入的主要都是本小时内第 38 和第 39 分钟的事件。什么时候才能宣布你已经完成了第 37 分钟的窗口计数，并输出其计数器值？ 在一段时间没有看到任何新的事件之后，你可以超时并宣布一个窗口已经就绪，但仍然可能发生这种情况：某些事件被缓冲在另一台机器上，由于网络中断而延迟。你需要能够处理这种在窗口宣告完成之后到达的 滞留（straggler） 事件。大体上，你有两种选择【1】： 忽略这些滞留事件，因为在正常情况下它们可能只是事件中的一小部分。你可以将丢弃事件的数量作为一个监控指标，并在出现大量丢消息的情况时报警。 发布一个 更正（correction），一个包括滞留事件的更新窗口值。你可能还需要收回以前的输出。 在某些情况下，可以使用特殊的消息来指示 “从现在开始，不会有比 t 更早时间戳的消息了”，消费者可以使用它来触发窗口【81】。但是，如果不同机器上的多个生产者都在生成事件，每个生产者都有自己的最小时间戳阈值，则消费者需要分别跟踪每个生产者。在这种情况下，添加和删除生产者都是比较棘手的。 你用的是谁的时钟？当事件可能在系统内多个地方进行缓冲时，为事件分配时间戳更加困难了。例如，考虑一个移动应用向服务器上报关于用量的事件。该应用可能会在设备处于脱机状态时被使用，在这种情况下，它将在设备本地缓冲事件，并在下一次互联网连接可用时向服务器上报这些事件（可能是几小时甚至几天）。对于这个流的任意消费者而言，它们就如延迟极大的滞留事件一样。 在这种情况下，事件上的事件戳实际上应当是用户交互发生的时间，取决于移动设备的本地时钟。然而用户控制的设备上的时钟通常是不可信的，因为它可能会被无意或故意设置成错误的时间（请参阅 “时钟同步与准确性”）。服务器收到事件的时间（取决于服务器的时钟）可能是更准确的，因为服务器在你的控制之下，但在描述用户交互方面意义不大。 要校正不正确的设备时钟，一种方法是记录三个时间戳【82】： 事件发生的时间，取决于设备时钟 事件发送往服务器的时间，取决于设备时钟 事件被服务器接收的时间，取决于服务器时钟 通过从第三个时间戳中减去第二个时间戳，可以估算设备时钟和服务器时钟之间的偏移（假设网络延迟与所需的时间戳精度相比可忽略不计）。然后可以将该偏移应用于事件时间戳，从而估计事件实际发生的真实时间（假设设备时钟偏移在事件发生时与送往服务器之间没有变化）。 这并不是流处理独有的问题，批处理有着完全一样的时 间推理问题。只是在流处理的上下文中，我们更容易意识到时间的流逝。 窗口的类型当你知道如何确定一个事件的时间戳后，下一步就是如何定义时间段的窗口。然后窗口就可以用于聚合，例如事件计数，或计算窗口内值的平均值。有几种窗口很常用【79,83】： 滚动窗口（Tumbling Window） 滚动窗口有着固定的长度，每个事件都仅能属于一个窗口。例如，假设你有一个 1 分钟的滚动窗口，则所有时间戳在 10:03:00 和 10:03:59 之间的事件会被分组到一个窗口中，10:04:00 和 10:04:59 之间的事件被分组到下一个窗口，依此类推。通过将每个事件时间戳四舍五入至最近的分钟来确定它所属的窗口，可以实现 1 分钟的滚动窗口。 跳动窗口（Hopping Window） 跳动窗口也有着固定的长度，但允许窗口重叠以提供一些平滑。例如，一个带有 1 分钟跳跃步长的 5 分钟窗口将包含 10:03:00 至 10:07:59 之间的事件，而下一个窗口将覆盖 10:04:00 至 10:08:59 之间的事件，等等。通过首先计算 1 分钟的滚动窗口（tunmbling window），然后在几个相邻窗口上进行聚合，可以实现这种跳动窗口。 滑动窗口（Sliding Window） 滑动窗口包含了彼此间距在特定时长内的所有事件。例如，一个 5 分钟的滑动窗口应当覆盖 10:03:39 和 10:08:12 的事件，因为它们相距不超过 5 分钟（注意滚动窗口与步长 5 分钟的跳动窗口可能不会把这两个事件分组到同一个窗口中，因为它们使用固定的边界）。通过维护一个按时间排序的事件缓冲区，并不断从窗口中移除过期的旧事件，可以实现滑动窗口。 会话窗口（Session window） 与其他窗口类型不同，会话窗口没有固定的持续时间，而定义为：将同一用户出现时间相近的所有事件分组在一起，而当用户一段时间没有活动时（例如，如果 30 分钟内没有事件）窗口结束。会话切分是网站分析的常见需求（请参阅 “分组”）。 流连接在 第十章 中，我们讨论了批处理作业如何通过键来连接数据集，以及这种连接是如何成为数据管道的重要组成部分的。由于流处理将数据管道泛化为对无限数据集进行增量处理，因此对流进行连接的需求也是完全相同的。 然而，新事件随时可能出现在一个流中，这使得流连接要比批处理连接更具挑战性。为了更好地理解情况，让我们先来区分三种不同类型的连接：流 - 流 连接，流 - 表 连接，与 表 - 表 连接【84】。我们将在下面的章节中通过例子来说明。 流流连接（窗口连接）假设你的网站上有搜索功能，而你想要找出搜索 URL 的近期趋势。每当有人键入搜索查询时，都会记录下一个包含查询与其返回结果的事件。每当有人点击其中一个搜索结果时，就会记录另一个记录点击事件。为了计算搜索结果中每个 URL 的点击率，你需要将搜索动作与点击动作的事件连在一起，这些事件通过相同的会话 ID 进行连接。广告系统中需要类似的分析【85】。 如果用户丢弃了搜索结果，点击可能永远不会发生，即使它出现了，搜索与点击之间的时间可能是高度可变的：在很多情况下，它可能是几秒钟，但也可能长达几天或几周（如果用户执行搜索，忘掉了这个浏览器页面，过了一段时间后重新回到这个浏览器页面上，并点击了一个结果）。由于可变的网络延迟，点击事件甚至可能先于搜索事件到达。你可以选择合适的连接窗口 —— 例如，如果点击与搜索之间的时间间隔在一小时内，你可能会选择连接两者。 请注意，在点击事件中嵌入搜索详情与事件连接并不一样：这样做的话，只有当用户点击了一个搜索结果时你才能知道，而那些没有点击的搜索就无能为力了。为了衡量搜索质量，你需要准确的点击率，为此搜索事件和点击事件两者都是必要的。 为了实现这种类型的连接，流处理器需要维护 状态：例如，按会话 ID 索引最近一小时内发生的所有事件。无论何时发生搜索事件或点击事件，都会被添加到合适的索引中，而流处理器也会检查另一个索引是否有具有相同会话 ID 的事件到达。如果有匹配事件就会发出一个表示搜索结果被点击的事件；如果搜索事件直到过期都没看见有匹配的点击事件，就会发出一个表示搜索结果未被点击的事件。 流表连接（流扩充）在 “示例：用户活动事件分析”（图 10-2 ）中，我们看到了连接两个数据集的批处理作业示例：一组用户活动事件和一个用户档案数据库。将用户活动事件视为流，并在流处理器中连续执行相同的连接是很自然的想法：输入是包含用户 ID 的活动事件流，而输出还是活动事件流，但其中用户 ID 已经被扩展为用户的档案信息。这个过程有时被称为使用数据库的信息来 扩充（enriching） 活动事件。 要执行此连接，流处理器需要一次处理一个活动事件，在数据库中查找事件的用户 ID，并将档案信息添加到活动事件中。数据库查询可以通过查询远程数据库来实现。但正如在 “示例：用户活动事件分析” 一节中讨论的，此类远程查询可能会很慢，并且有可能导致数据库过载【75】。 另一种方法是将数据库副本加载到流处理器中，以便在本地进行查询而无需网络往返。这种技术与我们在 “Map 侧连接” 中讨论的散列连接非常相似：如果数据库的本地副本足够小，则可以是内存中的散列表，比较大的话也可以是本地磁盘上的索引。 与批处理作业的区别在于，批处理作业使用数据库的时间点快照作为输入，而流处理器是长时间运行的，且数据库的内容可能随时间而改变，所以流处理器数据库的本地副本需要保持更新。这个问题可以通过变更数据捕获来解决：流处理器可以订阅用户档案数据库的更新日志，如同活动事件流一样。当增添或修改档案时，流处理器会更新其本地副本。因此，我们有了两个流之间的连接：活动事件和档案更新。 流表连接实际上非常类似于流流连接；最大的区别在于对于表的变更日志流，连接使用了一个可以回溯到 “时间起点” 的窗口（概念上是无限的窗口），新版本的记录会覆盖更早的版本。对于输入的流，连接可能压根儿就没有维护任何窗口。 表表连接（维护物化视图）我们在 “描述负载” 中讨论的推特时间线例子时说过，当用户想要查看他们的主页时间线时，迭代用户所关注人群的推文并合并它们是一个开销巨大的操作。 相反，我们需要一个时间线缓存：一种每个用户的 “收件箱”，在发送推文的时候写入这些信息，因而读取时间线时只需要简单地查询即可。物化与维护这个缓存需要处理以下事件： 当用户 u 发送新的推文时，它将被添加到每个关注用户 u 的时间线上。 用户删除推文时，推文将从所有用户的时间表中删除。 当用户 $u_1$ 开始关注用户 $u_2$ 时，$u_2$ 最近的推文将被添加到 $u_1$ 的时间线上。 当用户 $u_1$ 取消关注用户 $u_2$ 时，$u_2$ 的推文将从 $u_1$ 的时间线中移除。 要在流处理器中实现这种缓存维护，你需要推文事件流（发送与删除）和关注关系事件流（关注与取消关注）。流处理需要维护一个数据库，包含每个用户的粉丝集合。以便知道当一条新推文到达时，需要更新哪些时间线【86】。 观察这个流处理过程的另一种视角是：它维护了一个连接了两个表（推文与关注）的物化视图，如下所示： 12345SELECT follows.follower_id AS timeline_id, array_agg(tweets.* ORDER BY tweets.timestamp DESC)FROM tweetsJOIN follows ON follows.followee_id = tweets.sender_idGROUP BY follows.follower_id 流连接直接对应于这个查询中的表连接。时间线实际上是这个查询结果的缓存，每当底层的表发生变化时都会更新 [^iii]。 [^iii]: 如果你将流视作表的衍生物，如 图 11-6 所示，而把一个连接看作是两个表的乘法u·v，那么会发生一些有趣的事情：物化连接的变化流遵循乘积法则：(u·v)’&#x3D; u’v + uv’。 换句话说，任何推文的变化量都与当前的关注联系在一起，任何关注的变化量都与当前的推文相连接【49,50】。 连接的时间依赖性这里描述的三种连接（流流，流表，表表）有很多共通之处：它们都需要流处理器维护连接一侧的一些状态（搜索与点击事件，用户档案，关注列表），然后当连接另一侧的消息到达时查询该状态。 用于维护状态的事件顺序是很重要的（先关注然后取消关注，或者其他类似操作）。在分区日志中，单个分区内的事件顺序是保留下来的。但典型情况下是没有跨流或跨分区的顺序保证的。 这就产生了一个问题：如果不同流中的事件发生在近似的时间范围内，则应该按照什么样的顺序进行处理？在流表连接的例子中，如果用户更新了它们的档案，哪些活动事件与旧档案连接（在档案更新前处理），哪些又与新档案连接（在档案更新之后处理）？换句话说：你需要对一些状态做连接，如果状态会随着时间推移而变化，那应当使用什么时间点来连接呢【45】？ 这种时序依赖可能出现在很多地方。例如销售东西需要对发票应用适当的税率，这取决于所处的国家 &#x2F; 州，产品类型，销售日期（因为税率时不时会变化）。当连接销售额与税率表时，你可能期望的是使用销售时的税率参与连接。如果你正在重新处理历史数据，销售时的税率可能和现在的税率有所不同。 如果跨越流的事件顺序是未定的，则连接会变为不确定性的【87】，这意味着你在同样输入上重跑相同的作业未必会得到相同的结果：当你重跑任务时，输入流上的事件可能会以不同的方式交织。 在数据仓库中，这个问题被称为 缓慢变化的维度（slowly changing dimension, SCD），通常通过对特定版本的记录使用唯一的标识符来解决：例如，每当税率改变时都会获得一个新的标识符，而发票在销售时会带有税率的标识符【88,89】。这种变化使连接变为确定性的，但也会导致日志压缩无法进行：表中所有的记录版本都需要保留。 容错在本章的最后一节中，让我们看一看流处理是如何容错的。我们在 第十章 中看到，批处理框架可以很容易地容错：如果 MapReduce 作业中的任务失败，可以简单地在另一台机器上再次启动，并且丢弃失败任务的输出。这种透明的重试是可能的，因为输入文件是不可变的，每个任务都将其输出写入到 HDFS 上的独立文件中，而输出仅当任务成功完成后可见。 特别是，批处理容错方法可确保批处理作业的输出与没有出错的情况相同，即使实际上某些任务失败了。看起来好像每条输入记录都被处理了恰好一次 —— 没有记录被跳过，而且没有记录被处理两次。尽管重启任务意味着实际上可能会多次处理记录，但输出中的可见效果看上去就像只处理过一次。这个原则被称为 恰好一次语义（exactly-once semantics），尽管 等效一次（effectively-once） 可能会是一个更写实的术语【90】。 在流处理中也出现了同样的容错问题，但是处理起来没有那么直观：等待某个任务完成之后再使其输出可见并不是一个可行选项，因为你永远无法处理完一个无限的流。 微批量与存档点一个解决方案是将流分解成小块，并像微型批处理一样处理每个块。这种方法被称为 微批次（microbatching），它被用于 Spark Streaming 【91】。批次的大小通常约为 1 秒，这是对性能妥协的结果：较小的批次会导致更大的调度与协调开销，而较大的批次意味着流处理器结果可见之前的延迟要更长。 微批次也隐式提供了一个与批次大小相等的滚动窗口（按处理时间而不是事件时间戳分窗）。任何需要更大窗口的作业都需要显式地将状态从一个微批次转移到下一个微批次。 Apache Flink 则使用不同的方法，它会定期生成状态的滚动存档点并将其写入持久存储【92,93】。如果流算子崩溃，它可以从最近的存档点重启，并丢弃从最近检查点到崩溃之间的所有输出。存档点会由消息流中的 壁障（barrier） 触发，类似于微批次之间的边界，但不会强制一个特定的窗口大小。 在流处理框架的范围内，微批次与存档点方法提供了与批处理一样的 恰好一次语义。但是，只要输出离开流处理器（例如，写入数据库，向外部消息代理发送消息，或发送电子邮件），框架就无法抛弃失败批次的输出了。在这种情况下，重启失败任务会导致外部副作用发生两次，只有微批次或存档点不足以阻止这一问题。 原子提交再现为了在出现故障时表现出恰好处理一次的样子，我们需要确保事件处理的所有输出和副作用 当且仅当 处理成功时才会生效。这些影响包括发送给下游算子或外部消息传递系统（包括电子邮件或推送通知）的任何消息，任何数据库写入，对算子状态的任何变更，以及对输入消息的任何确认（包括在基于日志的消息代理中将消费者偏移量前移）。 这些事情要么都原子地发生，要么都不发生，但是它们不应当失去同步。如果这种方法听起来很熟悉，那是因为我们在分布式事务和两阶段提交的上下文中讨论过它（请参阅 “恰好一次的消息处理”）。 在 第九章 中，我们讨论了分布式事务传统实现中的问题（如 XA）。然而在限制更为严苛的环境中，也是有可能高效实现这种原子提交机制的。 Google Cloud Dataflow【81,92】和 VoltDB 【94】中使用了这种方法，Apache Kafka 有计划加入类似的功能【95,96】。与 XA 不同，这些实现不会尝试跨异构技术提供事务，而是通过在流处理框架中同时管理状态变更与消息传递来内化事务。事务协议的开销可以通过在单个事务中处理多个输入消息来分摊。 幂等性我们的目标是丢弃任何失败任务的部分输出，以便能安全地重试，而不会生效两次。分布式事务是实现这个目标的一种方式，而另一种方式是依赖 幂等性（idempotence）【97】。 幂等操作是多次重复执行与单次执行效果相同的操作。例如，将键值存储中的某个键设置为某个特定值是幂等的（再次写入该值，只是用同样的值替代），而递增一个计数器不是幂等的（再次执行递增意味着该值递增两次）。 即使一个操作不是天生幂等的，往往可以通过一些额外的元数据做成幂等的。例如，在使用来自 Kafka 的消息时，每条消息都有一个持久的、单调递增的偏移量。将值写入外部数据库时可以将这个偏移量带上，这样你就可以判断一条更新是不是已经执行过了，因而避免重复执行。 Storm 的 Trident 基于类似的想法来处理状态【78】。依赖幂等性意味着隐含了一些假设：重启一个失败的任务必须以相同的顺序重播相同的消息（基于日志的消息代理能做这些事），处理必须是确定性的，没有其他节点能同时更新相同的值【98,99】。 当从一个处理节点故障切换到另一个节点时，可能需要进行 防护（fencing，请参阅 “领导者和锁”），以防止被假死节点干扰。尽管有这么多注意事项，幂等操作是一种实现 恰好一次语义 的有效方式，仅需很小的额外开销。 失败后重建状态任何需要状态的流处理 —— 例如，任何窗口聚合（例如计数器，平均值和直方图）以及任何用于连接的表和索引，都必须确保在失败之后能恢复其状态。 一种选择是将状态保存在远程数据存储中，并进行复制，然而正如在 “流表连接（流扩充）” 中所述，每个消息都要查询远程数据库可能会很慢。另一种方法是在流处理器本地保存状态，并定期复制。然后当流处理器从故障中恢复时，新任务可以读取状态副本，恢复处理而不丢失数据。 例如，Flink 定期捕获算子状态的快照，并将它们写入 HDFS 等持久存储中【92,93】。 Samza 和 Kafka Streams 通过将状态变更发送到具有日志压缩功能的专用 Kafka 主题来复制状态变更，这与变更数据捕获类似【84,100】。 VoltDB 通过在多个节点上对每个输入消息进行冗余处理来复制状态（请参阅 “真的串行执行”）。 在某些情况下，甚至可能都不需要复制状态，因为它可以从输入流重建。例如，如果状态是从相当短的窗口中聚合而成，则简单地重播该窗口中的输入事件可能是足够快的。如果状态是通过变更数据捕获来维护的数据库的本地副本，那么也可以从日志压缩的变更流中重建数据库（请参阅 “日志压缩”）。 然而，所有这些权衡取决于底层基础架构的性能特征：在某些系统中，网络延迟可能低于磁盘访问延迟，网络带宽也可能与磁盘带宽相当。没有针对所有情况的普适理想权衡，随着存储和网络技术的发展，本地状态与远程状态的优点也可能会互换。 本章小结在本章中，我们讨论了事件流，它们所服务的目的，以及如何处理它们。在某些方面，流处理非常类似于在 第十章 中讨论的批处理，不过是在无限的（永无止境的）流而不是固定大小的输入上持续进行。从这个角度来看，消息代理和事件日志可以视作文件系统的流式等价物。 我们花了一些时间比较两种消息代理： AMQP&#x2F;JMS 风格的消息代理 代理将单条消息分配给消费者，消费者在成功处理单条消息后确认消息。消息被确认后从代理中删除。这种方法适合作为一种异步形式的 RPC（另请参阅 “消息传递中的数据流”），例如在任务队列中，消息处理的确切顺序并不重要，而且消息在处理完之后，不需要回头重新读取旧消息。 基于日志的消息代理 代理将一个分区中的所有消息分配给同一个消费者节点，并始终以相同的顺序传递消息。并行是通过分区实现的，消费者通过存档最近处理消息的偏移量来跟踪工作进度。消息代理将消息保留在磁盘上，因此如有必要的话，可以回跳并重新读取旧消息。 基于日志的方法与数据库中的复制日志（请参阅 第五章）和日志结构存储引擎（请参阅 第三章）有相似之处。我们看到，这种方法对于消费输入流，并产生衍生状态或衍生输出数据流的系统而言特别适用。 就流的来源而言，我们讨论了几种可能性：用户活动事件，定期读数的传感器，和 Feed 数据（例如，金融中的市场数据）能够自然地表示为流。我们发现将数据库写入视作流也是很有用的：我们可以捕获变更日志 —— 即对数据库所做的所有变更的历史记录 —— 隐式地通过变更数据捕获，或显式地通过事件溯源。日志压缩允许流也能保有数据库内容的完整副本。 将数据库表示为流为系统集成带来了很多强大机遇。通过消费变更日志并将其应用至衍生系统，你能使诸如搜索索引、缓存以及分析系统这类衍生数据系统不断保持更新。你甚至能从头开始，通过读取从创世至今的所有变更日志，为现有数据创建全新的视图。 像流一样维护状态以及消息重播的基础设施，是在各种流处理框架中实现流连接和容错的基础。我们讨论了流处理的几种目的，包括搜索事件模式（复杂事件处理），计算分窗聚合（流分析），以及保证衍生数据系统处于最新状态（物化视图）。 然后我们讨论了在流处理中对时间进行推理的困难，包括处理时间与事件时间戳之间的区别，以及当你认为窗口已经完事之后，如何处理到达的掉队事件的问题。 我们区分了流处理中可能出现的三种连接类型： 流流连接 两个输入流都由活动事件组成，而连接算子在某个时间窗口内搜索相关的事件。例如，它可能会将同一个用户 30 分钟内进行的两个活动联系在一起。如果你想要找出一个流内的相关事件，连接的两侧输入可能实际上都是同一个流（自连接，即 self-join）。 流表连接 一个输入流由活动事件组成，另一个输入流是数据库变更日志。变更日志保证了数据库的本地副本是最新的。对于每个活动事件，连接算子将查询数据库，并输出一个扩展的活动事件。 表表连接 两个输入流都是数据库变更日志。在这种情况下，一侧的每一个变化都与另一侧的最新状态相连接。结果是两表连接所得物化视图的变更流。 最后，我们讨论了在流处理中实现容错和恰好一次语义的技术。与批处理一样，我们需要放弃任何失败任务的部分输出。然而由于流处理长时间运行并持续产生输出，所以不能简单地丢弃所有的输出。相反，可以使用更细粒度的恢复机制，基于微批次、存档点、事务或幂等写入。 参考文献 Tyler Akidau, Robert Bradshaw, Craig Chambers, et al.: “The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing,” Proceedings of the VLDB Endowment, volume 8, number 12, pages 1792–1803, August 2015. doi:10.14778&#x2F;2824032.2824076 Harold Abelson, Gerald Jay Sussman, and Julie Sussman: Structure and Interpretation of Computer Programs, 2nd edition. MIT Press, 1996. ISBN: 978-0-262-51087-5, available online at mitpress.mit.edu Patrick Th. Eugster, Pascal A. Felber, Rachid Guerraoui, and Anne-Marie Kermarrec: “The Many Faces of Publish&#x2F;Subscribe,” ACM Computing Surveys, volume 35, number 2, pages 114–131, June 2003. doi:10.1145&#x2F;857076.857078 Joseph M. Hellerstein and Michael Stonebraker: Readings in Database Systems, 4th edition. MIT Press, 2005. ISBN: 978-0-262-69314-1, available online at redbook.cs.berkeley.edu Don Carney, Uğur Çetintemel, Mitch Cherniack, et al.: “Monitoring Streams – A New Class of Data Management Applications,” at 28th International Conference on Very Large Data Bases (VLDB), August 2002. Matthew Sackman: “Pushing Back,” lshift.net, May 5, 2016. Vicent Martí: “Brubeck, a statsd-Compatible Metrics Aggregator,” githubengineering.com, June 15, 2015. Seth Lowenberger: “MoldUDP64 Protocol Specification V 1.00,” nasdaqtrader.com, July 2009. Pieter Hintjens: ZeroMQ – The Guide. O’Reilly Media, 2013. ISBN: 978-1-449-33404-8 Ian Malpass: “Measure Anything, Measure Everything,” codeascraft.com, February 15, 2011. Dieter Plaetinck: “25 Graphite, Grafana and statsd Gotchas,” blog.raintank.io, March 3, 2016. Jeff Lindsay: “Web Hooks to Revolutionize the Web,” progrium.com, May 3, 2007. Jim N. Gray: “Queues Are Databases,” Microsoft Research Technical Report MSR-TR-95-56, December 1995. Mark Hapner, Rich Burridge, Rahul Sharma, et al.: “JSR-343 Java Message Service (JMS) 2.0 Specification,” jms-spec.java.net, March 2013. Sanjay Aiyagari, Matthew Arrott, Mark Atwell, et al.: “AMQP: Advanced Message Queuing Protocol Specification,” Version 0-9-1, November 2008. “Google Cloud Pub&#x2F;Sub: A Google-Scale Messaging Service,” cloud.google.com, 2016. “Apache Kafka 0.9 Documentation,” kafka.apache.org, November 2015. Jay Kreps, Neha Narkhede, and Jun Rao: “Kafka: A Distributed Messaging System for Log Processing,” at 6th International Workshop on Networking Meets Databases (NetDB), June 2011. “Amazon Kinesis Streams Developer Guide,” docs.aws.amazon.com, April 2016. Leigh Stewart and Sijie Guo: “Building DistributedLog: Twitter’s High-Performance Replicated Log Service,” blog.twitter.com, September 16, 2015. “DistributedLog Documentation,” Twitter, Inc., distributedlog.io, May 2016. Jay Kreps: “Benchmarking Apache Kafka: 2 Million Writes Per Second (On Three Cheap Machines),” engineering.linkedin.com, April 27, 2014. Kartik Paramasivam: “How We’re Improving and Advancing Kafka at LinkedIn,” engineering.linkedin.com, September 2, 2015. Jay Kreps: “The Log: What Every Software Engineer Should Know About Real-Time Data’s Unifying Abstraction,” engineering.linkedin.com, December 16, 2013. Shirshanka Das, Chavdar Botev, Kapil Surlaker, et al.: “All Aboard the Databus!,” at 3rd ACM Symposium on Cloud Computing (SoCC), October 2012. Yogeshwer Sharma, Philippe Ajoux, Petchean Ang, et al.: “Wormhole: Reliable Pub-Sub to Support Geo-Replicated Internet Services,” at 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI), May 2015. P. P. S. Narayan: “Sherpa Update,” developer.yahoo.com, June 8, . Martin Kleppmann: “Bottled Water: Real-Time Integration of PostgreSQL and Kafka,” martin.kleppmann.com, April 23, 2015. Ben Osheroff: “Introducing Maxwell, a mysql-to-kafka Binlog Processor,” developer.zendesk.com, August 20, 2015. Randall Hauch: “Debezium 0.2.1 Released,” debezium.io, June 10, 2016. Prem Santosh Udaya Shankar: “Streaming MySQL Tables in Real-Time to Kafka,” engineeringblog.yelp.com, August 1, 2016. “Mongoriver,” Stripe, Inc., github.com, September 2014. Dan Harvey: “Change Data Capture with Mongo + Kafka,” at Hadoop Users Group UK, August 2015. “Oracle GoldenGate 12c: Real-Time Access to Real-Time Information,” Oracle White Paper, March 2015. “Oracle GoldenGate Fundamentals: How Oracle GoldenGate Works,” Oracle Corporation, youtube.com, November 2012. Slava Akhmechet: “Advancing the Realtime Web,” rethinkdb.com, January 27, 2015. “Firebase Realtime Database Documentation,” Google, Inc., firebase.google.com, May 2016. “Apache CouchDB 1.6 Documentation,” docs.couchdb.org, 2014. Matt DeBergalis: “Meteor 0.7.0: Scalable Database Queries Using MongoDB Oplog Instead of Poll-and-Diff,” info.meteor.com, December 17, 2013. “Chapter 15. Importing and Exporting Live Data,” VoltDB 6.4 User Manual, docs.voltdb.com, June 2016. Neha Narkhede: “Announcing Kafka Connect: Building Large-Scale Low-Latency Data Pipelines,” confluent.io, February 18, 2016. Greg Young: “CQRS and Event Sourcing,” at Code on the Beach, August 2014. Martin Fowler: “Event Sourcing,” martinfowler.com, December 12, 2005. Vaughn Vernon: Implementing Domain-Driven Design. Addison-Wesley Professional, 2013. ISBN: 978-0-321-83457-7 H. V. Jagadish, Inderpal Singh Mumick, and Abraham Silberschatz: “View Maintenance Issues for the Chronicle Data Model,” at 14th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS), May 1995. doi:10.1145&#x2F;212433.220201 “Event Store 3.5.0 Documentation,” Event Store LLP, docs.geteventstore.com, February 2016. Martin Kleppmann: Making Sense of Stream Processing. Report, O’Reilly Media, May 2016. Sander Mak: “Event-Sourced Architectures with Akka,” at JavaOne, September 2014. Julian Hyde: personal communication, June 2016. Ashish Gupta and Inderpal Singh Mumick: Materialized Views: Techniques, Implementations, and Applications. MIT Press, 1999. ISBN: 978-0-262-57122-7 Timothy Griffin and Leonid Libkin: “Incremental Maintenance of Views with Duplicates,” at ACM International Conference on Management of Data (SIGMOD), May 1995. doi:10.1145&#x2F;223784.223849 Pat Helland: “Immutability Changes Everything,” at 7th Biennial Conference on Innovative Data Systems Research (CIDR), January 2015. Martin Kleppmann: “Accounting for Computer Scientists,” martin.kleppmann.com, March 7, 2011. Pat Helland: “Accountants Don’t Use Erasers,” blogs.msdn.com, June 14, 2007. Fangjin Yang: “Dogfooding with Druid, Samza, and Kafka: Metametrics at Metamarkets,” metamarkets.com, June 3, 2015. Gavin Li, Jianqiu Lv, and Hang Qi: “Pistachio: Co-Locate the Data and Compute for Fastest Cloud Compute,” yahoohadoop.tumblr.com, April 13, 2015. Kartik Paramasivam: “Stream Processing Hard Problems – Part 1: Killing Lambda,” engineering.linkedin.com, June 27, 2016. Martin Fowler: “CQRS,” martinfowler.com, July 14, 2011. Greg Young: “CQRS Documents,” cqrs.files.wordpress.com, November 2010. Baron Schwartz: “Immutability, MVCC, and Garbage Collection,” xaprb.com, December 28, 2013. Daniel Eloff, Slava Akhmechet, Jay Kreps, et al.: “Re: Turning the Database Inside-out with Apache Samza,” Hacker News discussion, news.ycombinator.com, March 4, 2015. “Datomic Development Resources: Excision,” Cognitect, Inc., docs.datomic.com. “Fossil Documentation: Deleting Content from Fossil,” fossil-scm.org, 2016. Jay Kreps: “The irony of distributed systems is that data loss is really easy but deleting data is surprisingly hard,” twitter.com, March 30, 2015. David C. Luckham: “What’s the Difference Between ESP and CEP?,” complexevents.com, August 1, 2006. Srinath Perera: “How Is Stream Processing and Complex Event Processing (CEP) Different?,” quora.com, December 3, 2015. Arvind Arasu, Shivnath Babu, and Jennifer Widom: “The CQL Continuous Query Language: Semantic Foundations and Query Execution,” The VLDB Journal, volume 15, number 2, pages 121–142, June 2006. doi:10.1007&#x2F;s00778-004-0147-z Julian Hyde: “Data in Flight: How Streaming SQL Technology Can Help Solve the Web 2.0 Data Crunch,” ACM Queue, volume 7, number 11, December 2009. doi:10.1145&#x2F;1661785.1667562 “Esper Reference, Version 5.4.0,” EsperTech, Inc., espertech.com, April 2016. Zubair Nabi, Eric Bouillet, Andrew Bainbridge, and Chris Thomas: “Of Streams and Storms,” IBM technical report, developer.ibm.com, April 2014. Milinda Pathirage, Julian Hyde, Yi Pan, and Beth Plale: “SamzaSQL: Scalable Fast Data Management with Streaming SQL,” at IEEE International Workshop on High-Performance Big Data Computing (HPBDC), May 2016. doi:10.1109&#x2F;IPDPSW.2016.141 Philippe Flajolet, Éric Fusy, Olivier Gandouet, and Frédéric Meunier: “HyperLo&amp;#x2060;g&amp;#x200b;Log: The Analysis of a Near-Optimal Cardinality Estimation Algorithm,” at Conference on Analysis of Algorithms (AofA), June 2007. Jay Kreps: “Questioning the Lambda Architecture,” oreilly.com, July 2, 2014. Ian Hellström: “An Overview of Apache Streaming Technologies,” databaseline.wordpress.com, March 12, 2016. Jay Kreps: “Why Local State Is a Fundamental Primitive in Stream Processing,” oreilly.com, July 31, 2014. Shay Banon: “Percolator,” elastic.co, February 8, 2011. Alan Woodward and Martin Kleppmann: “Real-Time Full-Text Search with Luwak and Samza,” martin.kleppmann.com, April 13, 2015. “Apache Storm 1.0.1 Documentation,” storm.apache.org, May 2016. Tyler Akidau: “The World Beyond Batch: Streaming 102,” oreilly.com, January 20, 2016. Stephan Ewen: “Streaming Analytics with Apache Flink,” at Kafka Summit, April 2016. Tyler Akidau, Alex Balikov, Kaya Bekiroğlu, et al.: “MillWheel: Fault-Tolerant Stream Processing at Internet Scale,” at 39th International Conference on Very Large Data Bases (VLDB), August 2013. Alex Dean: “Improving Snowplow’s Understanding of Time,” snowplowanalytics.com, September 15, 2015. “Windowing (Azure Stream Analytics),” Microsoft Azure Reference, msdn.microsoft.com, April 2016. “State Management,” Apache Samza 0.10 Documentation, samza.apache.org, December 2015. Rajagopal Ananthanarayanan, Venkatesh Basker, Sumit Das, et al.: “Photon: Fault-Tolerant and Scalable Joining of Continuous Data Streams,” at ACM International Conference on Management of Data (SIGMOD), June 2013. doi:10.1145&#x2F;2463676.2465272 Martin Kleppmann: “Samza Newsfeed Demo,” github.com, September 2014. Ben Kirwin: “Doing the Impossible: Exactly-Once Messaging Patterns in Kafka,” ben.kirw.in, November 28, 2014. Pat Helland: “Data on the Outside Versus Data on the Inside,” at 2nd Biennial Conference on Innovative Data Systems Research (CIDR), January 2005. Ralph Kimball and Margy Ross: The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling, 3rd edition. John Wiley &amp; Sons, 2013. ISBN: 978-1-118-53080-1 Viktor Klang: “I’m coining the phrase ‘effectively-once’ for message processing with at-least-once + idempotent operations,” twitter.com, October 20, 2016. Matei Zaharia, Tathagata Das, Haoyuan Li, et al.: “Discretized Streams: An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters,” at 4th USENIX Conference in Hot Topics in Cloud Computing (HotCloud), June 2012. Kostas Tzoumas, Stephan Ewen, and Robert Metzger: “High-Throughput, Low-Latency, and Exactly-Once Stream Processing with Apache Flink,” data-artisans.com, August 5, 2015. Paris Carbone, Gyula Fóra, Stephan Ewen, et al.: “Lightweight Asynchronous Snapshots for Distributed Dataflows,” arXiv:1506.08603 &amp;#91;cs.DC&amp;#93;, June 29, 2015. Ryan Betts and John Hugg: Fast Data: Smart and at Scale. Report, O’Reilly Media, October 2015. Flavio Junqueira: “Making Sense of Exactly-Once Semantics,” at Strata+Hadoop World London, June 2016. Jason Gustafson, Flavio Junqueira, Apurva Mehta, Sriram Subramanian, and Guozhang Wang: “KIP-98 – Exactly Once Delivery and Transactional Messaging,” cwiki.apache.org, November 2016. Pat Helland: “Idempotence Is Not a Medical Condition,” Communications of the ACM, volume 55, number 5, page 56, May 2012. doi:10.1145&#x2F;2160718.2160734 Jay Kreps: “Re: Trying to Achieve Deterministic Behavior on Recovery&#x2F;Rewind,” email to samza-dev mailing list, September 9, 2014. E. N. (Mootaz) Elnozahy, Lorenzo Alvisi, Yi-Min Wang, and David B. Johnson: “A Survey of Rollback-Recovery Protocols in Message-Passing Systems,” ACM Computing Surveys, volume 34, number 3, pages 375–408, September 2002. doi:10.1145&#x2F;568522.568525 Adam Warski: “Kafka Streams – How Does It Fit the Stream Processing Landscape?,” softwaremill.com, June 1, 2016."},{"title":"第十二章：数据系统的未来","path":"/wiki/ddia/ch12.html","content":"如果船长的终极目标是保护船只，他应该永远待在港口。 —— 圣托马斯・阿奎那《神学大全》（1265-1274） 到目前为止，本书主要描述的是 现状。在这最后一章中，我们将放眼 未来，讨论应该是怎么样的：我将提出一些想法与方法，我相信它们能从根本上改进我们设计与构建应用的方式。 对未来的看法与推测当然具有很大的主观性。所以在撰写本章时，当提及我个人的观点时会使用第一人称。你完全可以不同意这些观点并提出自己的看法，但我希望本章中的概念，至少能成为富有成效的讨论出发点，并澄清一些经常被混淆的概念。 第一章 概述了本书的目标：探索如何创建 可靠、可伸缩 和 可维护 的应用与系统。这一主题贯穿了所有的章节：例如，我们讨论了许多有助于提高可靠性的容错算法，有助于提高可伸缩性的分区，以及有助于提高可维护性的演化与抽象机制。在本章中，我们将把所有这些想法结合在一起，并在它们的基础上展望未来。我们的目标是，发现如何设计出比现有应用更好的应用 —— 健壮、正确、可演化、且最终对人类有益。 数据集成本书中反复出现的主题是，对于任何给定的问题都会有好几种解决方案，所有这些解决方案都有不同的优缺点与利弊权衡。例如在 第三章 讨论存储引擎时，我们看到了日志结构存储、B 树以及列式存储。在 第五章 讨论复制时，我们看到了单领导者、多领导者和无领导者的方法。 如果你有一个类似于 “我想存储一些数据并稍后再查询” 的问题，那么并没有一种正确的解决方案。但对于不同的具体环境，总会有不同的合适方法。软件实现通常必须选择一种特定的方法。使单条代码路径能做到稳定健壮且表现良好已经是一件非常困难的事情了 —— 尝试在单个软件中完成所有事情，几乎可以保证，实现效果会很差。 因此软件工具的最佳选择也取决于情况。每一种软件，甚至所谓的 “通用” 数据库，都是针对特定的使用模式设计的。 面对让人眼花缭乱的诸多替代品，第一个挑战就是弄清软件与其适用环境的映射关系。供应商不愿告诉你他们软件不适用的工作负载，这是可以理解的。但是希望先前的章节能给你提供一些问题，让你读出字里行间的言外之意，并更好地理解这些权衡。 但是，即使你已经完全理解各种工具与其适用环境间的关系，还有一个挑战：在复杂的应用中，数据的用法通常花样百出。不太可能存在适用于 所有 不同数据应用场景的软件，因此你不可避免地需要拼凑几个不同的软件来以提供应用所需的功能。 组合使用衍生数据的工具例如，为了处理任意关键词的搜索查询，将 OLTP 数据库与全文搜索索引集成在一起是很常见的需求。尽管一些数据库（例如 PostgreSQL）包含了全文索引功能，对于简单的应用完全够了【1】，但更复杂的搜索能力就需要专业的信息检索工具了。相反的是，搜索索引通常不适合作为持久的记录系统，因此许多应用需要组合这两种不同的工具以满足所有需求。 我们在 “保持系统同步” 中接触过集成数据系统的问题。随着数据不同表示形式的增加，集成问题变得越来越困难。除了数据库和搜索索引之外，也许你需要在分析系统（数据仓库，或批处理和流处理系统）中维护数据副本；维护从原始数据中衍生的缓存，或反规范化的数据版本；将数据灌入机器学习、分类、排名或推荐系统中；或者基于数据变更发送通知。 令人惊讶的是，我经常看到软件工程师做出这样的陈述：“根据我的经验，99% 的人只需要 X” 或者 “…… 不需要 X”（对于各种各样的 X）。我认为这种陈述更像是发言人自己的经验，而不是技术实际上的实用性。可能对数据执行的操作，其范围极其宽广。某人认为鸡肋而毫无意义的功能可能是别人的核心需求。当你拉高视角，并考虑跨越整个组织范围的数据流时，数据集成的需求往往就会变得明显起来。 理解数据流当需要在多个存储系统中维护相同数据的副本以满足不同的访问模式时，你要对输入和输出了如指掌：哪些数据先写入，哪些数据表示衍生自哪些来源？如何以正确的格式，将所有数据导入正确的地方？ 例如，你可能会首先将数据写入 记录系统 数据库，捕获对该数据库所做的变更（请参阅 “变更数据捕获”），然后将变更以相同的顺序应用于搜索索引。如果变更数据捕获（CDC）是更新索引的唯一方式，则可以确定该索引完全派生自记录系统，因此与其保持一致（除软件错误外）。写入数据库是向该系统提供新输入的唯一方式。 允许应用程序直接写入搜索索引和数据库引入了如 图 11-4 所示的问题，其中两个客户端同时发送冲突的写入，且两个存储系统按不同顺序处理它们。在这种情况下，既不是数据库说了算，也不是搜索索引说了算，所以它们做出了相反的决定，进入彼此间持久性的不一致状态。 如果你可以通过单个系统来提供所有用户输入，从而决定所有写入的排序，则通过按相同顺序处理写入，可以更容易地衍生出其他数据表示。 这是状态机复制方法的一个应用，我们在 “全序广播” 中看到。无论你使用变更数据捕获还是事件溯源日志，都不如简单的基于全序的决策原则更重要。 基于事件日志来更新衍生数据的系统，通常可以做到 确定性 与 幂等性（请参阅 “幂等性”），使得从故障中恢复相当容易。 衍生数据与分布式事务保持不同数据系统彼此一致的经典方法涉及分布式事务，如 “原子提交与两阶段提交” 中所述。与分布式事务相比，使用衍生数据系统的方法如何？ 在抽象层面，它们通过不同的方式达到类似的目标。分布式事务通过 锁 进行互斥来决定写入的顺序（请参阅 “两阶段锁定”），而 CDC 和事件溯源使用日志进行排序。分布式事务使用原子提交来确保变更只生效一次，而基于日志的系统通常基于 确定性重试 和 幂等性。 最大的不同之处在于事务系统通常提供 线性一致性，这包含着有用的保证，例如 读己之写。另一方面，衍生数据系统通常是异步更新的，因此它们默认不会提供相同的时序保证。 在愿意为分布式事务付出代价的有限场景中，它们已被成功应用。但是，我认为 XA 的容错能力和性能很差劲（请参阅 “实践中的分布式事务”），这严重限制了它的实用性。我相信为分布式事务设计一种更好的协议是可行的。但使这样一种协议被现有工具广泛接受是很有挑战的，且不是立竿见影的事。 在没有广泛支持的良好分布式事务协议的情况下，我认为基于日志的衍生数据是集成不同数据系统的最有前途的方法。然而，诸如读己之写的保证是有用的，我认为告诉所有人 “最终一致性是不可避免的 —— 忍一忍并学会和它打交道” 是没有什么建设性的（至少在缺乏 如何 应对的良好指导时）。 在 “将事情做正确” 中，我们将讨论一些在异步衍生系统之上实现更强保障的方法，并迈向分布式事务和基于日志的异步系统之间的中间地带。 全序的限制对于足够小的系统，构建一个完全有序的事件日志是完全可行的（正如单主复制数据库的流行所证明的那样，它正好建立了这样一种日志）。但是，随着系统向更大更复杂的工作负载伸缩，限制开始出现： 在大多数情况下，构建完全有序的日志，需要所有事件汇集于决定顺序的 单个领导者节点。如果事件吞吐量大于单台计算机的处理能力，则需要将其分区到多台计算机上（请参阅 “分区日志”）。然后两个不同分区中的事件顺序关系就不明确了。 如果服务器分布在多个 地理位置分散 的数据中心上，例如为了容忍整个数据中心掉线，你通常在每个数据中心都有单独的主库，因为网络延迟会导致同步的跨数据中心协调效率低下（请参阅 “多主复制“）。这意味着源自两个不同数据中心的事件顺序未定义。 将应用程序部署为微服务时（请参阅 “服务中的数据流：REST 与 RPC”），常见的设计选择是将每个服务及其持久状态作为独立单元进行部署，服务之间不共享持久状态。当两个事件来自不同的服务时，这些事件间的顺序未定义。 某些应用程序在客户端保存状态，该状态在用户输入时立即更新（无需等待服务器确认），甚至可以继续脱机工作（请参阅 “需要离线操作的客户端”）。对于这样的应用程序，客户端和服务器很可能以不同的顺序看到事件。 在形式上，决定事件的全局顺序称为 全序广播，相当于 共识（请参阅 “共识算法和全序广播”）。大多数共识算法都是针对单个节点的吞吐量足以处理整个事件流的情况而设计的，并且这些算法不提供多个节点共享事件排序工作的机制。设计可以伸缩至单个节点的吞吐量之上，且在地理位置分散的环境中仍然工作良好的的共识算法仍然是一个开放的研究问题。 排序事件以捕获因果关系在事件之间不存在因果关系的情况下，全序的缺乏并不是一个大问题，因为并发事件可以任意排序。其他一些情况很容易处理：例如，当同一对象有多个更新时，它们可以通过将特定对象 ID 的所有更新路由到相同的日志分区来完全排序。然而，因果关系有时会以更微妙的方式出现（请参阅 “顺序与因果关系”）。 例如，考虑一个社交网络服务，以及一对曾处于恋爱关系但刚分手的用户。其中一个用户将另一个用户从好友中移除，然后向剩余的好友发送消息，抱怨他们的前任。用户的心思是他们的前任不应该看到这些粗鲁的消息，因为消息是在好友状态解除后发送的。 但是如果好友关系状态与消息存储在不同的地方，在这样一个系统中，可能会出现 解除好友 事件与 发送消息 事件之间的因果依赖丢失的情况。如果因果依赖关系没有被捕捉到，则发送有关新消息的通知的服务可能会在 解除好友 事件之前处理 发送消息 事件，从而错误地向前任发送通知。 在本例中，通知实际上是消息和好友列表之间的连接，使得它与我们先前讨论的连接的时序问题有关（请参阅 “连接的时间依赖性”）。不幸的是，这个问题似乎并没有一个简单的答案【2,3】。起点包括： 逻辑时间戳可以提供无需协调的全局顺序（请参阅 “序列号顺序”），因此它们可能有助于全序广播不可行的情况。但是，他们仍然要求收件人处理不按顺序发送的事件，并且需要传递其他元数据。 如果你可以记录一个事件来记录用户在做出决定之前所看到的系统状态，并给该事件一个唯一的标识符，那么后面的任何事件都可以引用该事件标识符来记录因果关系【4】。我们将在 “读也是事件” 中回到这个想法。 冲突解决算法（请参阅 “自动冲突解决”）有助于处理以意外顺序传递的事件。它们对于维护状态很有用，但如果行为有外部副作用（例如，给用户发送通知），就没什么帮助了。 也许，随着时间的推移，应用开发模式将出现，使得能够有效地捕获因果依赖关系，并且保持正确的衍生状态，而不会迫使所有事件经历全序广播的瓶颈）。 批处理与流处理我会说数据集成的目标是，确保数据最终能在所有正确的地方表现出正确的形式。这样做需要消费输入、转换、连接、过滤、聚合、训练模型、评估、以及最终写出适当的输出。批处理和流处理是实现这一目标的工具。 批处理和流处理的输出是衍生数据集，例如搜索索引、物化视图、向用户显示的建议、聚合指标等（请参阅 “批处理工作流的输出” 和 “流处理的应用”）。 正如我们在 第十章 和 第十一章 中看到的，批处理和流处理有许多共同的原则，主要的根本区别在于流处理器在无限数据集上运行，而批处理输入是已知的有限大小。处理引擎的实现方式也有很多细节上的差异，但是这些区别已经开始模糊。 Spark 在批处理引擎上执行流处理，将流分解为 微批次（microbatches），而 Apache Flink 则在流处理引擎上执行批处理【5】。原则上，一种类型的处理可以用另一种类型来模拟，但是性能特征会有所不同：例如，在跳跃或滑动窗口上，微批次可能表现不佳【6】。 维护衍生状态批处理有着很强的函数式风格（即使其代码不是用函数式语言编写的）：它鼓励确定性的纯函数，其输出仅依赖于输入，除了显式输出外没有副作用，将输入视作不可变的，且输出是仅追加的。流处理与之类似，但它扩展了算子以允许受管理的、容错的状态（请参阅 “失败后重建状态”）。 具有良好定义的输入和输出的确定性函数的原理不仅有利于容错（请参阅 “幂等性”），也简化了有关组织中数据流的推理【7】。无论衍生数据是搜索索引、统计模型还是缓存，采用这种观点思考都是很有帮助的：将其视为从一个东西衍生出另一个的数据管道，通过函数式应用代码推送一个系统的状态变更，并将其效果应用至衍生系统中。 原则上，衍生数据系统可以同步地维护，就像关系数据库在与索引表写入操作相同的事务中同步更新次级索引一样。然而，异步是使基于事件日志的系统稳健的原因：它允许系统的一部分故障被抑制在本地。而如果任何一个参与者失败，分布式事务将中止，因此它们倾向于通过将故障传播到系统的其余部分来放大故障（请参阅 “分布式事务的限制”）。 我们在 “分区与次级索引” 中看到，次级索引经常跨越分区边界。具有次级索引的分区系统需要将写入发送到多个分区（如果索引按关键词分区的话）或将读取发送到所有分区（如果索引是按文档分区的话）。如果索引是异步维护的，这种跨分区通信也是最可靠和最可伸缩的【8】（另请参阅 “多分区数据处理”）。 应用演化后重新处理数据在维护衍生数据时，批处理和流处理都是有用的。流处理允许将输入中的变化以低延迟反映在衍生视图中，而批处理允许重新处理大量累积的历史数据以便将新视图导出到现有数据集上。 特别是，重新处理现有数据为维护系统、演化并支持新功能和需求变更提供了一个良好的机制（请参阅 第四章）。没有重新进行处理，模式演化将仅限于简单的变化，例如向记录中添加新的可选字段或添加新类型的记录。无论是在写时模式还是在读时模式中都是如此（请参阅 “文档模型中的模式灵活性”）。另一方面，通过重新处理，可以将数据集重组为一个完全不同的模型，以便更好地满足新的要求。 铁路上的模式迁移 大规模的 “模式迁移” 也发生在非计算机系统中。例如，在 19 世纪英国铁路建设初期，轨距（两轨之间的距离）就有了各种各样的竞争标准。为一种轨距而建的列车不能在另一种轨距的轨道上运行，这限制了火车网络中可能的相互连接【9】。 在 1846 年最终确定了一个标准轨距之后，其他轨距的轨道必须转换 —— 但是如何在不停运火车线路的情况下进行数月甚至数年的迁移？解决的办法是首先通过添加第三条轨道将轨道转换为 双轨距（dual guage） 或 混合轨距。这种转换可以逐渐完成，当完成时，两种轨距的列车都可以在线路上跑，使用三条轨道中的两条。事实上，一旦所有的列车都转换成标准轨距，那么可以移除提供非标准轨距的轨道。 以这种方式 “再加工” 现有的轨道，让新旧版本并存，可以在几年的时间内逐渐改变轨距。然而，这是一项昂贵的事业，这就是今天非标准轨距仍然存在的原因。例如，旧金山湾区的 BART 系统使用了与美国大部分地区不同的轨距。 衍生视图允许 渐进演化（gradual evolution）。如果你想重新构建数据集，不需要执行突然切换式的迁移。取而代之的是，你可以将旧架构和新架构并排维护为相同基础数据上的两个独立衍生视图。然后可以开始将少量用户转移到新视图，以测试其性能并发现任何错误，而大多数用户仍然会被路由到旧视图。你可以逐渐地增加访问新视图的用户比例，最终可以删除旧视图【10】。 这种逐渐迁移的美妙之处在于，如果出现问题，每个阶段的过程都很容易逆转：你始终有一个可以回滚的可用系统。通过降低不可逆损害的风险，你能对继续前进更有信心，从而更快地改善系统【11】。 Lambda架构如果批处理用于重新处理历史数据，而流处理用于处理最近的更新，那么如何将这两者结合起来？Lambda 架构【12】是这方面的一个建议，引起了很多关注。 Lambda 架构的核心思想是通过将不可变事件附加到不断增长的数据集来记录传入数据，这类似于事件溯源（请参阅 “事件溯源”）。为了从这些事件中衍生出读取优化的视图，Lambda 架构建议并行运行两个不同的系统：批处理系统（如 Hadoop MapReduce）和独立的流处理系统（如 Storm）。 在 Lambda 方法中，流处理器消耗事件并快速生成对视图的近似更新；批处理器稍后将使用同一组事件并生成衍生视图的更正版本。这个设计背后的原因是批处理更简单，因此不易出错，而流处理器被认为是不太可靠和难以容错的（请参阅 “容错”）。而且，流处理可以使用快速近似算法，而批处理使用较慢的精确算法。 Lambda 架构是一种有影响力的想法，它将数据系统的设计变得更好，尤其是通过推广这样的原则：在不可变事件流上建立衍生视图，并在需要时重新处理事件。但是我也认为它有一些实际问题： 在批处理和流处理框架中维护相同的逻辑是很显著的额外工作。虽然像 Summingbird【13】这样的库提供了一种可以在批处理和流处理的上下文中运行的计算抽象。调试、调整和维护两个不同系统的操作复杂性依然存在【14】。 由于流管道和批处理管道产生独立的输出，因此需要合并它们以响应用户请求。如果计算是基于滚动窗口的简单聚合，则合并相当容易，但如果视图基于更复杂的操作（例如连接和会话化）而导出，或者输出不是时间序列，则会变得非常困难。 尽管有能力重新处理整个历史数据集是很好的，但在大型数据集上这样做经常会开销巨大。因此，批处理流水线通常需要设置为处理增量批处理（例如，在每小时结束时处理一小时的数据），而不是重新处理所有内容。这引发了 “时间推理” 中讨论的问题，例如处理滞留事件和处理跨批次边界的窗口。增量化批处理计算会增加复杂性，使其更类似于流式传输层，这与保持批处理层尽可能简单的目标背道而驰。 统一批处理和流处理最近的工作使得 Lambda 架构的优点在没有其缺点的情况下得以实现，允许批处理计算（重新处理历史数据）和流计算（在事件到达时即处理）在同一个系统中实现【15】。 在一个系统中统一批处理和流处理需要以下功能，这些功能也正在越来越广泛地被提供： 通过处理最近事件流的相同处理引擎来重播历史事件的能力。例如，基于日志的消息代理可以重播消息（请参阅 “重播旧消息”），某些流处理器可以从 HDFS 等分布式文件系统读取输入。 对于流处理器来说，恰好一次语义 —— 即确保输出与未发生故障的输出相同，即使事实上发生故障（请参阅 “容错”）。与批处理一样，这需要丢弃任何失败任务的部分输出。 按事件时间进行窗口化的工具，而不是按处理时间进行窗口化，因为处理历史事件时，处理时间毫无意义（请参阅 “时间推理”）。例如，Apache Beam 提供了用于表达这种计算的 API，可以在 Apache Flink 或 Google Cloud Dataflow 使用。 分拆数据库在最抽象的层面上，数据库，Hadoop 和操作系统都发挥相同的功能：它们存储一些数据，并允许你处理和查询这些数据【16】。数据库将数据存储为特定数据模型的记录（表中的行、文档、图中的顶点等），而操作系统的文件系统则将数据存储在文件中 —— 但其核心都是 “信息管理” 系统【17】。正如我们在 第十章 中看到的，Hadoop 生态系统有点像 Unix 的分布式版本。 当然，有很多实际的差异。例如，许多文件系统都不能很好地处理包含 1000 万个小文件的目录，而包含 1000 万个小记录的数据库完全是寻常而不起眼的。无论如何，操作系统和数据库之间的相似之处和差异值得探讨。 Unix 和关系数据库以非常不同的哲学来处理信息管理问题。Unix 认为它的目的是为程序员提供一种相当低层次的硬件的逻辑抽象，而关系数据库则希望为应用程序员提供一种高层次的抽象，以隐藏磁盘上数据结构的复杂性、并发性、崩溃恢复等等。Unix 发展出的管道和文件只是字节序列，而数据库则发展出了 SQL 和事务。 哪种方法更好？当然这取决于你想要的是什么。 Unix 是 “简单的”，因为它是对硬件资源相当薄的包装；关系数据库是 “更简单” 的，因为一个简短的声明性查询可以利用很多强大的基础设施（查询优化、索引、连接方法、并发控制、复制等），而不需要查询的作者理解其实现细节。 这些哲学之间的矛盾已经持续了几十年（Unix 和关系模型都出现在 70 年代初），仍然没有解决。例如，我将 NoSQL 运动解释为，希望将类 Unix 的低级别抽象方法应用于分布式 OLTP 数据存储的领域。 在这一部分我将试图调和这两个哲学，希望我们能各取其美。 组合使用数据存储技术在本书的过程中，我们讨论了数据库提供的各种功能及其工作原理，其中包括： 次级索引，使你可以根据字段的值有效地搜索记录（请参阅 “其他索引结构”） 物化视图，这是一种预计算的查询结果缓存（请参阅 “聚合：数据立方体和物化视图”） 复制日志，保持其他节点上数据的副本最新（请参阅 “复制日志的实现”） 全文搜索索引，允许在文本中进行关键字搜索（请参阅 “全文搜索和模糊索引”），也内置于某些关系数据库【1】 在 第十章 和 第十一章 中，出现了类似的主题。我们讨论了如何构建全文搜索索引（请参阅 “批处理工作流的输出”），了解了如何维护物化视图（请参阅 “维护物化视图”）以及如何将变更从数据库复制到衍生数据系统（请参阅 “变更数据捕获”）。 数据库中内置的功能与人们用批处理和流处理器构建的衍生数据系统似乎有相似之处。 创建索引想想当你运行 CREATE INDEX 在关系数据库中创建一个新的索引时会发生什么。数据库必须扫描表的一致性快照，挑选出所有被索引的字段值，对它们进行排序，然后写出索引。然后它必须处理自一致快照以来所做的写入操作（假设表在创建索引时未被锁定，所以写操作可能会继续）。一旦完成，只要事务写入表中，数据库就必须继续保持索引最新。 此过程非常类似于设置新的从库副本（请参阅 “设置新从库”），也非常类似于流处理系统中的 引导（bootstrap） 变更数据捕获（请参阅 “初始快照”）。 无论何时运行 CREATE INDEX，数据库都会重新处理现有数据集（如 “应用演化后重新处理数据” 中所述），并将该索引作为新视图导出到现有数据上。现有数据可能是状态的快照，而不是所有发生变化的日志，但两者密切相关（请参阅 “状态、流和不变性”）。 一切的元数据库有鉴于此，我认为整个组织的数据流开始像一个巨大的数据库【7】。每当批处理、流或 ETL 过程将数据从一个地方传输到另一个地方并组装时，它表现地就像数据库子系统一样，使索引或物化视图保持最新。 从这种角度来看，批处理和流处理器就像精心实现的触发器、存储过程和物化视图维护例程。它们维护的衍生数据系统就像不同的索引类型。例如，关系数据库可能支持 B 树索引、散列索引、空间索引（请参阅 “多列索引”）以及其他类型的索引。在新兴的衍生数据系统架构中，不是将这些设施作为单个集成数据库产品的功能实现，而是由各种不同的软件提供，运行在不同的机器上，由不同的团队管理。 这些发展在未来将会把我们带到哪里？如果我们从没有适合所有访问模式的单一数据模型或存储格式的前提出发，我推测有两种途径可以将不同的存储和处理工具组合成一个有凝聚力的系统： 联合数据库：统一读取 可以为各种各样的底层存储引擎和处理方法提供一个统一的查询接口 —— 一种称为 联合数据库（federated database） 或 多态存储（polystore） 的方法【18,19】。例如，PostgreSQL 的 外部数据包装器（foreign data wrapper） 功能符合这种模式【20】。需要专用数据模型或查询接口的应用程序仍然可以直接访问底层存储引擎，而想要组合来自不同位置的数据的用户可以通过联合接口轻松完成操作。 联合查询接口遵循着单一集成系统的关系型传统，带有高级查询语言和优雅的语义，但实现起来非常复杂。 分拆数据库：统一写入 虽然联合能解决跨多个不同系统的只读查询问题，但它并没有很好的解决跨系统 同步 写入的问题。我们说过，在单个数据库中，创建一致的索引是一项内置功能。当我们构建多个存储系统时，我们同样需要确保所有数据变更都会在所有正确的位置结束，即使在出现故障时也是如此。想要更容易地将存储系统可靠地插接在一起（例如，通过变更数据捕获和事件日志），就像将数据库的索引维护功能以可以跨不同技术同步写入的方式分开【7,21】。 分拆方法遵循 Unix 传统的小型工具，它可以很好地完成一件事【22】，通过统一的低层级 API（管道）进行通信，并且可以使用更高层级的语言进行组合（shell）【16】 。 开展分拆工作联合和分拆是一个硬币的两面：用不同的组件构成可靠、 可伸缩和可维护的系统。联合只读查询需要将一个数据模型映射到另一个数据模型，这需要一些思考，但最终还是一个可解决的问题。而我认为同步写入到几个存储系统是更困难的工程问题，所以我将重点关注它。 传统的同步写入方法需要跨异构存储系统的分布式事务【18】，我认为这是错误的解决方案（请参阅 “衍生数据与分布式事务”）。单个存储或流处理系统内的事务是可行的，但是当数据跨越不同技术之间的边界时，我认为具有幂等写入的异步事件日志是一种更加健壮和实用的方法。 例如，分布式事务在某些流处理组件内部使用，以匹配 恰好一次（exactly-once） 语义（请参阅 “原子提交再现”），这可以很好地工作。然而，当事务需要涉及由不同人群编写的系统时（例如，当数据从流处理组件写入分布式键值存储或搜索索引时），缺乏标准化的事务协议会使集成更难。有幂等消费者的有序事件日志（请参阅 “幂等性”）是一种更简单的抽象，因此在异构系统中实现更加可行【7】。 基于日志的集成的一大优势是各个组件之间的 松散耦合（loose coupling），这体现在两个方面： 在系统级别，异步事件流使整个系统在个别组件的中断或性能下降时更加稳健。如果消费者运行缓慢或失败，那么事件日志可以缓冲消息（请参阅 “磁盘空间使用”），以便生产者和任何其他消费者可以继续不受影响地运行。有问题的消费者可以在问题修复后赶上，因此不会错过任何数据，并且包含故障。相比之下，分布式事务的同步交互往往会将本地故障升级为大规模故障（请参阅 “分布式事务的限制”）。 在人力方面，分拆数据系统允许不同的团队独立开发，改进和维护不同的软件组件和服务。专业化使得每个团队都可以专注于做好一件事，并与其他团队的系统以明确的接口交互。事件日志提供了一个足够强大的接口，以捕获相当强的一致性属性（由于持久性和事件的顺序），但也足够普适于几乎任何类型的数据。 分拆系统vs集成系统如果分拆确实成为未来的方式，它也不会取代目前形式的数据库 —— 它们仍然会像以往一样被需要。为了维护流处理组件中的状态，数据库仍然是需要的，并且为批处理和流处理器的输出提供查询服务（请参阅 “批处理工作流的输出” 与 “流处理”）。专用查询引擎对于特定的工作负载仍然非常重要：例如，MPP 数据仓库中的查询引擎针对探索性分析查询进行了优化，并且能够很好地处理这种类型的工作负载（请参阅 “Hadoop 与分布式数据库的对比” 。 运行几种不同基础设施的复杂性可能是一个问题：每种软件都有一个学习曲线，配置问题和操作怪癖，因此部署尽可能少的移动部件是很有必要的。比起使用应用代码拼接多个工具而成的系统，单一集成软件产品也可以在其设计应对的工作负载类型上实现更好、更可预测的性能【23】。正如在前言中所说的那样，为了不需要的规模而构建系统是白费精力，而且可能会将你锁死在一个不灵活的设计中。实际上，这是一种过早优化的形式。 分拆的目标不是要针对个别数据库与特定工作负载的性能进行竞争；我们的目标是允许你结合多个不同的数据库，以便在比单个软件可能实现的更广泛的工作负载范围内实现更好的性能。这是关于广度，而不是深度 —— 与我们在 “Hadoop 与分布式数据库的对比” 中讨论的存储和处理模型的多样性一样。 因此，如果有一项技术可以满足你的所有需求，那么最好使用该产品，而不是试图用更低层级的组件重新实现它。只有当没有单一软件满足你的所有需求时，才会出现拆分和联合的优势。 少了什么？用于组成数据系统的工具正在变得越来越好，但我认为还缺少一个主要的东西：我们还没有与 Unix shell 类似的分拆数据库等价物（即，一种声明式的、简单的、用于组装存储和处理系统的高级语言）。 例如，如果我们可以简单地声明 mysql | elasticsearch，类似于 Unix 管道【22】，成为 CREATE INDEX 的分拆等价物：它将读取 MySQL 数据库中的所有文档并将其索引到 Elasticsearch 集群中。然后它会不断捕获对数据库所做的所有变更，并自动将它们应用于搜索索引，而无需编写自定义应用代码。这种集成应当支持几乎任何类型的存储或索引系统。 同样，能够更容易地预先计算和更新缓存将是一件好事。回想一下，物化视图本质上是一个预先计算的缓存，所以你可以通过为复杂查询声明指定物化视图来创建缓存，包括图上的递归查询（请参阅 “图数据模型”）和应用逻辑。在这方面有一些有趣的早期研究，如 差分数据流（differential dataflow）【24,25】，我希望这些想法能够在生产系统中找到自己的方法。 围绕数据流设计应用使用应用代码组合专用存储与处理系统来分拆数据库的方法，也被称为 “数据库由内而外（database inside-out）” 方法【26】，该名称来源于我在 2014 年的一次会议演讲标题【27】。然而称它为 “新架构” 过于夸大，我仅将其看作是一种设计模式，一个讨论的起点，我们只是简单地给它起一个名字，以便我们能更好地讨论它。 这些想法不是我的；它们是很多人的思想的融合，这些思想非常值得我们学习。尤其是，以 Oz【28】和 Juttle【29】为代表的数据流语言，以 Elm【30,31】为代表的 函数式响应式编程（functional reactive programming, FRP），以 Bloom【32】为代表的逻辑编程语言。在这一语境中的术语 分拆（unbundling） 是由 Jay Kreps 提出的【7】。 即使是 电子表格 也在数据流编程能力上甩开大多数主流编程语言几条街【33】。在电子表格中，可以将公式放入一个单元格中（例如，对另一列中的单元格求和），并且只要公式的任何输入发生变更，公式的结果都会自动重新计算。这正是我们在数据系统层次所需要的：当数据库中的记录发生变更时，我们希望自动更新该记录的任何索引，并且自动刷新依赖于记录的任何缓存视图或聚合。你不必担心这种刷新如何发生的技术细节，但能够简单地相信它可以正常工作。 因此，我认为绝大多数数据系统仍然可以从 VisiCalc 在 1979 年已经具备的功能中学习【34】。与电子表格的不同之处在于，今天的数据系统需要具有容错性，可伸缩性以及持久存储数据。它们还需要能够整合不同人群编写的不同技术，并重用现有的库和服务：期望使用某一种特定的语言、框架或工具来开发所有软件是不切实际的。 在本节中，我将详细介绍这些想法，并探讨一些围绕分拆数据库和数据流的想法构建应用的方法。 应用代码作为衍生函数当一个数据集衍生自另一个数据集时，它会经历某种转换函数。例如： 次级索引是由一种直白的转换函数生成的衍生数据集：对于基础表中的每行或每个文档，它挑选被索引的列或字段中的值，并按这些值排序（假设使用 B 树或 SSTable 索引，按键排序，如 第三章 所述）。 全文搜索索引是通过应用各种自然语言处理函数而创建的，诸如语言检测、分词、词干或词汇化、拼写纠正和同义词识别，然后构建用于高效查找的数据结构（例如倒排索引）。 在机器学习系统中，我们可以将模型视作从训练数据通过应用各种特征提取、统计分析函数衍生的数据，当模型应用于新的输入数据时，模型的输出是从输入和模型（因此间接地从训练数据）中衍生的。 缓存通常包含将以用户界面（UI）显示的形式的数据聚合。因此填充缓存需要知道 UI 中引用的字段；UI 中的变更可能需要更新缓存填充方式的定义，并重建缓存。 用于次级索引的衍生函数是如此常用的需求，以致于它作为核心功能被内建至许多数据库中，你可以简单地通过 CREATE INDEX 来调用它。对于全文索引，常见语言的基本语言特征可能内置到数据库中，但更复杂的特征通常需要领域特定的调整。在机器学习中，特征工程是众所周知的特定于应用的特征，通常需要包含很多关于用户交互与应用部署的详细知识【35】。 当创建衍生数据集的函数不是像创建次级索引那样的标准搬砖函数时，需要自定义代码来处理特定于应用的东西。而这个自定义代码是让许多数据库挣扎的地方，虽然关系数据库通常支持触发器、存储过程和用户定义的函数，可以用它们来在数据库中执行应用代码，但它们有点像数据库设计里的事后反思。（请参阅 “传递事件流”）。 应用代码和状态的分离理论上，数据库可以是任意应用代码的部署环境，就如同操作系统一样。然而实践中它们对这一目标适配的很差。它们不满足现代应用开发的要求，例如依赖和软件包管理、版本控制、滚动升级、可演化性、监控、指标、对网络服务的调用以及与外部系统的集成。 另一方面，Mesos、YARN、Docker、Kubernetes 等部署和集群管理工具专为运行应用代码而设计。通过专注于做好一件事情，他们能够做得比将数据库作为其众多功能之一执行用户定义的功能要好得多。 我认为让系统的某些部分专门用于持久数据存储并让其他部分专门运行应用程序代码是有意义的。这两者可以在保持独立的同时互动。 现在大多数 Web 应用程序都是作为无状态服务部署的，其中任何用户请求都可以路由到任何应用程序服务器，并且服务器在发送响应后会忘记所有请求。这种部署方式很方便，因为可以随意添加或删除服务器，但状态必须到某个地方：通常是数据库。趋势是将无状态应用程序逻辑与状态管理（数据库）分开：不将应用程序逻辑放入数据库中，也不将持久状态置于应用程序中【36】。正如函数式编程社区喜欢开玩笑说的那样，“我们相信 教会（Church） 与 国家（state） 的分离”【37】 [^i] [^i]: 解释笑话很少会让人感觉更好，但我不想让任何人感到被遗漏。 在这里，Church 指代的是数学家的阿隆佐・邱奇，他创立了 lambda 演算，这是计算的早期形式，是大多数函数式编程语言的基础。 lambda 演算不具有可变状态（即没有变量可以被覆盖），所以可以说可变状态与 Church 的工作是分离的。 在这个典型的 Web 应用模型中，数据库充当一种可以通过网络同步访问的可变共享变量。应用程序可以读取和更新变量，而数据库负责维持它的持久性，提供一些诸如并发控制和容错的功能。 但是，在大多数编程语言中，你无法订阅可变变量中的变更 —— 你只能定期读取它。与电子表格不同，如果变量的值发生变化，变量的读者不会收到通知（你可以在自己的代码中实现这样的通知 —— 这被称为 观察者模式 —— 但大多数语言没有将这种模式作为内置功能）。 数据库继承了这种可变数据的被动方法：如果你想知道数据库的内容是否发生了变化，通常你唯一的选择就是轮询（即定期重复你的查询）。 订阅变更只是刚刚开始出现的功能（请参阅 “变更流的 API 支持”）。 数据流：应用代码与状态变化的交互从数据流的角度思考应用程序，意味着重新协调应用代码和状态管理之间的关系。我们不再将数据库视作被应用操纵的被动变量，取而代之的是更多地考虑状态，状态变更和处理它们的代码之间的相互作用与协同关系。应用代码通过在另一个地方触发状态变更来响应状态变更。 我们在 “数据库与流” 中看到了这一思路，我们讨论了将数据库的变更日志视为一种我们可以订阅的事件流。诸如 Actor 的消息传递系统（请参阅 “消息传递中的数据流”）也具有响应事件的概念。早在 20 世纪 80 年代，元组空间（tuple space） 模型就已经探索了表达分布式计算的方式：观察状态变更并作出反应的过程【38,39】。 如前所述，当触发器由于数据变更而被触发时，或次级索引更新以反映索引表中的变更时，数据库内部也发生着类似的情况。分拆数据库意味着将这个想法应用于在主数据库之外，用于创建衍生数据集：缓存、全文搜索索引、机器学习或分析系统。我们可以为此使用流处理和消息传递系统。 需要记住的重要一点是，维护衍生数据不同于执行异步任务。传统的消息传递系统通常是为执行异步任务设计的（请参阅 “日志与传统的消息传递相比”）： 在维护衍生数据时，状态变更的顺序通常很重要（如果多个视图是从事件日志衍生的，则需要按照相同的顺序处理事件，以便它们之间保持一致）。如 “确认与重新传递” 中所述，许多消息代理在重传未确认消息时没有此属性，双写也被排除在外（请参阅 “保持系统同步”）。 容错是衍生数据的关键：仅仅丢失单个消息就会导致衍生数据集永远与其数据源失去同步。消息传递和衍生状态更新都必须可靠。例如，许多 Actor 系统默认在内存中维护 Actor 的状态和消息，所以如果运行 Actor 的机器崩溃，状态和消息就会丢失。 稳定的消息排序和容错消息处理是相当严格的要求，但与分布式事务相比，它们开销更小，运行更稳定。现代流处理组件可以提供这些排序和可靠性保证，并允许应用代码以流算子的形式运行。 这些应用代码可以执行任意处理，包括数据库内置衍生函数通常不提供的功能。就像通过管道链接的 Unix 工具一样，流算子可以围绕着数据流构建大型系统。每个算子接受状态变更的流作为输入，并产生其他状态变化的流作为输出。 流处理器和服务当今流行的应用开发风格涉及将功能分解为一组通过同步网络请求（如 REST API）进行通信的 服务（service，请参阅 “服务中的数据流：REST 与 RPC”）。这种面向服务的架构优于单一庞大应用的优势主要在于：通过松散耦合来提供组织上的可伸缩性：不同的团队可以专职于不同的服务上，从而减少团队之间的协调工作（因为服务可以独立部署和更新）。 在数据流中组装流算子与微服务方法有很多相似之处【40】。但底层通信机制是有很大区别：数据流采用单向异步消息流，而不是同步的请求 &#x2F; 响应式交互。 除了在 “消息传递中的数据流” 中列出的优点（如更好的容错性），数据流系统还能实现更好的性能。例如，假设客户正在购买以一种货币定价，但以另一种货币支付的商品。为了执行货币换算，你需要知道当前的汇率。这个操作可以通过两种方式实现【40,41】： 在微服务方法中，处理购买的代码可能会查询汇率服务或数据库，以获取特定货币的当前汇率。 在数据流方法中，处理订单的代码会提前订阅汇率变更流，并在汇率发生变动时将当前汇率存储在本地数据库中。处理订单时只需查询本地数据库即可。 第二种方法能将对另一服务的同步网络请求替换为对本地数据库的查询（可能在同一台机器甚至同一个进程中）[^ii]。数据流方法不仅更快，而且当其他服务失效时也更稳健。最快且最可靠的网络请求就是压根没有网络请求！我们现在不再使用 RPC，而是在购买事件和汇率更新事件之间建立流联接（请参阅 “流表连接（流扩充）”）。 [^ii]: 在微服务方法中，你也可以通过在处理购买的服务中本地缓存汇率来避免同步网络请求。 但是为了保证缓存的新鲜度，你需要定期轮询汇率以获取其更新，或订阅变更流 —— 这恰好是数据流方法中发生的事情。 连接是时间相关的：如果购买事件在稍后的时间点被重新处理，汇率可能已经改变。如果要重建原始输出，则需要获取原始购买时的历史汇率。无论是查询服务还是订阅汇率更新流，你都需要处理这种时间相关性（请参阅 “连接的时间依赖性”）。 订阅变更流，而不是在需要时查询当前状态，使我们更接近类似电子表格的计算模型：当某些数据发生变更时，依赖于此的所有衍生数据都可以快速更新。还有很多未解决的问题，例如关于时间相关连接等问题，但我认为围绕数据流构建应用的想法是一个非常有希望的方向。 观察衍生数据状态在抽象层面，上一节讨论的数据流系统提供了创建衍生数据集（例如搜索索引、物化视图和预测模型）并使其保持更新的过程。我们将这个过程称为 写路径（write path）：只要某些信息被写入系统，它可能会经历批处理与流处理的多个阶段，而最终每个衍生数据集都会被更新，以适配写入的数据。图 12-1 显示了一个更新搜索索引的例子。 图 12-1 在搜索索引中，写（文档更新）遇上读（查询） 但你为什么一开始就要创建衍生数据集？很可能是因为你想在以后再次查询它。这就是 读路径（read path）：当服务用户请求时，你需要从衍生数据集中读取，也许还要对结果进行一些额外处理，然后构建给用户的响应。 总而言之，写路径和读路径涵盖了数据的整个旅程，从收集数据开始，到使用数据结束（可能是由另一个人）。写路径是预计算过程的一部分 —— 即，一旦数据进入，即刻完成，无论是否有人需要看它。读路径是这个过程中只有当有人请求时才会发生的部分。如果你熟悉函数式编程语言，则可能会注意到写路径类似于立即求值，读路径类似于惰性求值。 如 图 12-1 所示，衍生数据集是写路径和读路径相遇的地方。它代表了在写入时需要完成的工作量与在读取时需要完成的工作量之间的权衡。 物化视图和缓存全文搜索索引就是一个很好的例子：写路径更新索引，读路径在索引中搜索关键字。读写都需要做一些工作。写入需要更新文档中出现的所有关键词的索引条目。读取需要搜索查询中的每个单词，并应用布尔逻辑来查找包含查询中所有单词（AND 运算符）的文档，或者每个单词（OR 运算符）的任何同义词。 如果没有索引，搜索查询将不得不扫描所有文档（如 grep），如果有着大量文档，这样做的开销巨大。没有索引意味着写入路径上的工作量较少（没有要更新的索引），但是在读取路径上需要更多工作。 另一方面，可以想象为所有可能的查询预先计算搜索结果。在这种情况下，读路径上的工作量会减少：不需要布尔逻辑，只需查找查询结果并返回即可。但写路径会更加昂贵：可能的搜索查询集合是无限大的，因此预先计算所有可能的搜索结果将需要无限的时间和存储空间。那肯定没戏 ^iii。 另一种选择是预先计算一组固定的最常见查询的搜索结果，以便可以快速提供它们而无需转到索引。不常见的查询仍然可以通过索引来提供服务。这通常被称为常见查询的 缓存（cache），尽管我们也可以称之为 物化视图（materialized view），因为当新文档出现，且需要被包含在这些常见查询的搜索结果之中时，这些索引就需要更新。 从这个例子中我们可以看到，索引不是写路径和读路径之间唯一可能的边界；缓存常见搜索结果也是可行的；而在少量文档上使用没有索引的类 grep 扫描也是可行的。由此来看，缓存，索引和物化视图的作用很简单：它们改变了读路径与写路径之间的边界。通过预先计算结果，从而允许我们在写路径上做更多的工作，以节省读路径上的工作量。 在写路径上完成的工作和读路径之间的界限，实际上是本书开始处在 “描述负载” 中推特例子里谈到的主题。在该例中，我们还看到了与普通用户相比，名人的写路径和读路径可能有所不同。在 500 页之后，我们已经绕回了起点！ 有状态、可离线的客户端我发现写路径和读路径之间的边界很有趣，因为我们可以试着改变这个边界，并探讨这种改变的实际意义。我们来看看不同上下文中的这一想法。 过去二十年来，Web 应用的火热让我们对应用开发作出了一些很容易视作理所当然的假设。具体来说就是，客户端 &#x2F; 服务器模型 —— 客户端大多是无状态的，而服务器拥有数据的权威 —— 已经普遍到我们几乎忘掉了还有其他任何模型的存在。但是技术在不断地发展，我认为不时地质疑现状非常重要。 传统上，网络浏览器是无状态的客户端，只有当连接到互联网时才能做一些有用的事情（能离线执行的唯一事情基本上就是上下滚动之前在线时加载好的页面）。然而，最近的 “单页面” JavaScript Web 应用已经获得了很多有状态的功能，包括客户端用户界面交互，以及 Web 浏览器中的持久化本地存储。移动应用可以类似地在设备上存储大量状态，而且大多数用户交互都不需要与服务器往返交互。 这些不断变化的功能重新引发了对 离线优先（offline-first） 应用的兴趣，这些应用尽可能地在同一设备上使用本地数据库，无需连接互联网，并在后台网络连接可用时与远程服务器同步【42】。由于移动设备通常具有缓慢且不可靠的蜂窝网络连接，因此，如果用户的用户界面不必等待同步网络请求，且应用主要是离线工作的，则这是一个巨大优势（请参阅 “需要离线操作的客户端”）。 当我们摆脱无状态客户端与中央数据库交互的假设，并转向在终端用户设备上维护状态时，这就开启了新世界的大门。特别是，我们可以将设备上的状态视为 服务器状态的缓存。屏幕上的像素是客户端应用中模型对象的物化视图；模型对象是远程数据中心的本地状态副本【27】。 将状态变更推送给客户端在典型的网页中，如果你在 Web 浏览器中加载页面，并且随后服务器上的数据发生变更，则浏览器在重新加载页面之前对此一无所知。浏览器只能在一个时间点读取数据，假设它是静态的 —— 它不会订阅来自服务器的更新。因此设备上的状态是陈旧的缓存，除非你显式轮询变更否则不会更新。（像 RSS 这样基于 HTTP 的 Feed 订阅协议实际上只是一种基本的轮询形式） 最近的协议已经超越了 HTTP 的基本请求 &#x2F; 响应模式：服务端发送的事件（EventSource API）和 WebSockets 提供了通信信道，通过这些信道，Web 浏览器可以与服务器保持打开的 TCP 连接，只要浏览器仍然连接着，服务器就能主动向浏览器推送信息。这为服务器提供了主动通知终端用户客户端的机会，服务器能告知客户端其本地存储状态的任何变化，从而减少客户端状态的陈旧程度。 用我们的写路径与读路径模型来讲，主动将状态变更推至到客户端设备，意味着将写路径一直延伸到终端用户。当客户端首次初始化时，它仍然需要使用读路径来获取其初始状态，但此后它就能够依赖服务器发送的状态变更流了。我们在流处理和消息传递部分讨论的想法并不局限于数据中心中：我们可以进一步采纳这些想法，并将它们一直延伸到终端用户设备【43】。 这些设备有时会离线，并在此期间无法收到服务器状态变更的任何通知。但是我们已经解决了这个问题：在 “消费者偏移量” 中，我们讨论了基于日志的消息代理的消费者能在失败或断开连接后重连，并确保它不会错过掉线期间任何到达的消息。同样的技术适用于单个用户，每个设备都是一个小事件流的小小订阅者。 端到端的事件流最近用于开发有状态的客户端与用户界面的工具，例如如 Elm 语言【30】和 Facebook 的 React、Flux 和 Redux 工具链，已经通过订阅表示用户输入或服务器响应的事件流来管理客户端的内部状态，其结构与事件溯源相似（请参阅 “事件溯源”）。 将这种编程模型扩展为：允许服务器将状态变更事件推送到客户端的事件管道中，是非常自然的。因此，状态变化可以通过 端到端（end-to-end） 的写路径流动：从一个设备上的交互触发状态变更开始，经由事件日志，并穿过几个衍生数据系统与流处理器，一直到另一台设备上的用户界面，而有人正在观察用户界面上的状态变化。这些状态变化能以相当低的延迟传播 —— 比如说，在一秒内从一端到另一端。 一些应用（如即时消息传递与在线游戏）已经具有这种 “实时” 架构（在低延迟交互的意义上，不是在 “响应时间保证” 中的意义上）。但我们为什么不用这种方式构建所有的应用？ 挑战在于，关于无状态客户端和请求 &#x2F; 响应交互的假设已经根深蒂固地植入在在我们的数据库、库、框架以及协议之中。许多数据存储支持读取与写入操作，为请求返回一个响应，但只有极少数提供订阅变更的能力 —— 请求返回一个随时间推移的响应流（请参阅 “变更流的 API 支持” ）。 为了将写路径延伸至终端用户，我们需要从根本上重新思考我们构建这些系统的方式：从请求 &#x2F; 响应交互转向发布 &#x2F; 订阅数据流【27】。更具响应性的用户界面与更好的离线支持，我认为这些优势值得我们付出努力。如果你正在设计数据系统，我希望你对订阅变更的选项留有印象，而不只是查询当前状态。 读也是事件我们讨论过，当流处理器将衍生数据写入存储（数据库，缓存或索引）时，以及当用户请求查询该存储时，存储将充当写路径和读路径之间的边界。该存储应当允许对数据进行随机访问的读取查询，否则这些查询将需要扫描整个事件日志。 在很多情况下，数据存储与流处理系统是分开的。但回想一下，流处理器还是需要维护状态以执行聚合和连接的（请参阅 “流连接”）。这种状态通常隐藏在流处理器内部，但一些框架也允许这些状态被外部客户端查询【45】，将流处理器本身变成一种简单的数据库。 我愿意进一步思考这个想法。正如到目前为止所讨论的那样，对存储的写入是通过事件日志进行的，而读取是临时的网络请求，直接流向存储着待查数据的节点。这是一个合理的设计，但不是唯一可行的设计。也可以将读取请求表示为事件流，并同时将读事件与写事件送往流处理器；流处理器通过将读取结果发送到输出流来响应读取事件【46】。 当写入和读取都被表示为事件，并且被路由到同一个流算子以便处理时，我们实际上是在读取查询流和数据库之间执行流表连接。读取事件需要被送往保存数据的数据库分区（请参阅 “请求路由”），就像批处理和流处理器在连接时需要在同一个键上对输入分区一样（请参阅 “Reduce 侧连接与分组“）。 服务请求与执行连接之间的这种相似之处是非常关键的【47】。一次性读取请求只是将请求传过连接算子，然后请求马上就被忘掉了；而一个订阅请求，则是与连接另一侧过去与未来事件的持久化连接。 记录读取事件的日志可能对于追踪整个系统中的因果关系与数据来源也有好处：它可以让你重现出当用户做出特定决策之前看见了什么。例如在网商中，向客户显示的预测送达日期与库存状态，可能会影响他们是否选择购买一件商品【4】。要分析这种联系，则需要记录用户查询运输与库存状态的结果。 将读取事件写入持久存储可以更好地跟踪因果关系（请参阅 “排序事件以捕获因果关系”），但会产生额外的存储与 I&#x2F;O 成本。优化这些系统以减少开销仍然是一个开放的研究问题【2】。但如果你已经出于运维目的留下了读取请求日志，将其作为请求处理的副作用，那么将这份日志作为请求事件源并不是什么特别大的变更。 多分区数据处理对于只涉及单个分区的查询，通过流来发送查询与收集响应可能是杀鸡用牛刀了。然而，这个想法开启了分布式执行复杂查询的可能性，这需要合并来自多个分区的数据，利用了流处理器已经提供的消息路由、分区和连接的基础设施。 Storm 的分布式 RPC 功能支持这种使用模式（请参阅 “消息传递和 RPC”）。例如，它已经被用来计算浏览过某个推特 URL 的人数 —— 即，发推包含该 URL 的所有人的粉丝集合的并集【48】。由于推特的用户是分区的，因此这种计算需要合并来自多个分区的结果。 这种模式的另一个例子是欺诈预防：为了评估特定购买事件是否具有欺诈风险，你可以检查该用户 IP 地址，电子邮件地址，帐单地址，送货地址的信用分。这些信用数据库中的每一个都是有分区的，因此为特定购买事件采集分数需要连接一系列不同的分区数据集【49】。 MPP 数据库的内部查询执行图有着类似的特征（请参阅 “Hadoop 与分布式数据库的对比”）。如果需要执行这种多分区连接，则直接使用提供此功能的数据库，可能要比使用流处理器实现它要更简单。然而将查询视为流提供了一种选项，可以用于实现超出传统现成解决方案的大规模应用。 将事情做正确对于只读取数据的无状态服务，出问题也没什么大不了的：你可以修复该错误并重启服务，而一切都恢复正常。像数据库这样的有状态系统就没那么简单了：它们被设计为永远记住事物（或多或少），所以如果出现问题，这种（错误的）效果也将潜在地永远持续下去，这意味着它们需要更仔细的思考【50】。 我们希望构建可靠且 正确 的应用（即使面对各种故障，程序的语义也能被很好地定义与理解）。约四十年来，原子性、隔离性和持久性（第七章）等事务特性一直是构建正确应用的首选工具。然而这些地基没有看上去那么牢固：例如弱隔离级别带来的困惑可以佐证（请参阅 “弱隔离级别”）。 事务在某些领域被完全抛弃，并被提供更好性能与可伸缩性的模型取代，但后者有更复杂的语义（例如，请参阅 “无主复制”）。一致性（Consistency） 经常被谈起，但其定义并不明确（请参阅 “一致性” 和 第九章）。有些人断言我们应当为了高可用而 “拥抱弱一致性”，但却对这些概念实际上意味着什么缺乏清晰的认识。 对于如此重要的话题，我们的理解，以及我们的工程方法却是惊人地薄弱。例如，确定在特定事务隔离等级或复制配置下运行特定应用是否安全是非常困难的【51,52】。通常简单的解决方案似乎在低并发性的情况下工作正常，并且没有错误，但在要求更高的情况下却会出现许多微妙的错误。 例如，Kyle Kingsbury 的 Jepsen 实验【53】标出了一些产品声称的安全保证与其在网络问题与崩溃时的实际行为之间的明显差异。即使像数据库这样的基础设施产品没有问题，应用代码仍然需要正确使用它们提供的功能才行，如果配置很难理解，这是很容易出错的（在这种情况下指的是弱隔离级别，法定人数配置等）。 如果你的应用可以容忍偶尔的崩溃，以及以不可预料的方式损坏或丢失数据，那生活就要简单得多，而你可能只要双手合十念阿弥陀佛，期望佛祖能保佑最好的结果。另一方面，如果你需要更强的正确性保证，那么可串行化与原子提交就是久经考验的方法，但它们是有代价的：它们通常只在单个数据中心中工作（这就排除了地理位置分散的架构），并限制了系统能够实现的规模与容错特性。 虽然传统的事务方法并没有走远，但我也相信在使应用正确而灵活地处理错误方面上，事务也不是最后一个可以谈的。在本节中，我将提出一些在数据流架构中考量正确性的方式。 数据库的端到端原则仅仅因为一个应用程序使用了具有相对较强安全属性的数据系统（例如可串行化的事务），并不意味着就可以保证没有数据丢失或损坏。例如，如果某个应用有个 Bug，导致它写入不正确的数据，或者从数据库中删除数据，那么可串行化的事务也救不了你。 这个例子可能看起来很无聊，但值得认真对待：应用会出 Bug，而人也会犯错误。我在 “状态、流和不变性” 中使用了这个例子来支持不可变和仅追加的数据，阉割掉错误代码摧毁良好数据的能力，能让从错误中恢复更为容易。 虽然不变性很有用，但它本身并非万灵药。让我们来看一个可能发生的、非常微妙的数据损坏案例。 正好执行一次操作在 “容错” 中，我们见到了 恰好一次（或 等效一次）语义的概念。如果在处理消息时出现问题，你可以选择放弃（丢弃消息 —— 导致数据丢失）或重试。如果重试，就会有这种风险：第一次实际上成功了，只不过你没有发现。结果这个消息就被处理了两次。 处理两次是数据损坏的一种形式：为同样的服务向客户收费两次（收费太多）或增长计数器两次（夸大指标）都不是我们想要的。在这种情况下，恰好一次意味着安排计算，使得最终效果与没有发生错误的情况一样，即使操作实际上因为某种错误而重试。我们先前讨论过实现这一目标的几种方法。 最有效的方法之一是使操作 幂等（idempotent，请参阅 “幂等性”）；即确保它无论是执行一次还是执行多次都具有相同的效果。但是，将不是天生幂等的操作变为幂等的操作需要一些额外的努力与关注：你可能需要维护一些额外的元数据（例如更新了值的操作 ID 集合），并在从一个节点故障切换至另一个节点时做好防护（请参阅 “领导者和锁”）。 抑制重复除了流处理之外，其他许多地方也需要抑制重复的模式。例如，TCP 使用了数据包上的序列号，以便接收方可以将它们正确排序，并确定网络上是否有数据包丢失或重复。在将数据交付应用前，TCP 协议栈会重新传输任何丢失的数据包，也会移除任何重复的数据包。 但是，这种重复抑制仅适用于单条 TCP 连接的场景中。假设 TCP 连接是一个客户端与数据库的连接，并且它正在执行 例 12-1 中的事务。在许多数据库中，事务是绑定在客户端连接上的（如果客户端发送了多个查询，数据库就知道它们属于同一个事务，因为它们是在同一个 TCP 连接上发送的）。如果客户端在发送 COMMIT 之后并在从数据库服务器收到响应之前遇到网络中断与连接超时，客户端是不知道事务是否已经被提交的（图 8-1）。 例 12-1 资金从一个账户到另一个账户的非幂等转移 1234BEGIN TRANSACTION; UPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234; UPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;COMMIT; 客户端可以重连到数据库并重试事务，但现在已经处于 TCP 重复抑制的范围之外了。因为 例 12-1 中的事务不是幂等的，可能会发生转了 $22 而不是期望的 $11。因此，尽管 例 12-1 是一个事务原子性的标准样例，但它实际上并不正确，而真正的银行并不会这样办事【3】。 两阶段提交（请参阅 “原子提交与两阶段提交”）协议会破坏 TCP 连接与事务之间的 1:1 映射，因为它们必须在故障后允许事务协调器重连到数据库，告诉数据库将存疑事务提交还是中止。这足以确保事务只被恰好执行一次吗？不幸的是，并不能。 即使我们可以抑制数据库客户端与服务器之间的重复事务，我们仍然需要担心终端用户设备与应用服务器之间的网络。例如，如果终端用户的客户端是 Web 浏览器，则它可能会使用 HTTP POST 请求向服务器提交指令。也许用户正处于一个信号微弱的蜂窝数据网络连接中，它们成功地发送了 POST，但却在能够从服务器接收响应之前没了信号。 在这种情况下，可能会向用户显示错误消息，而他们可能会手动重试。 Web 浏览器警告说，“你确定要再次提交这个表单吗？” —— 用户选 “是”，因为他们希望操作发生（Post&#x2F;Redirect&#x2F;Get 模式【54】可以避免在正常操作中出现此警告消息，但 POST 请求超时就没办法了）。从 Web 服务器的角度来看，重试是一个独立的请求；从数据库的角度来看，这是一个独立的事务。通常的除重机制无济于事。 操作标识符要在通过几跳的网络通信上使操作具有幂等性，仅仅依赖数据库提供的事务机制是不够的 —— 你需要考虑 端到端（end-to-end） 的请求流。例如，你可以为操作生成一个唯一的标识符（例如 UUID），并将其作为隐藏表单字段包含在客户端应用中，或通过计算所有表单相关字段的散列来生成操作 ID 【3】。如果 Web 浏览器提交了两次 POST 请求，这两个请求将具有相同的操作 ID。然后，你可以将该操作 ID 一路传递到数据库，并检查你是否曾经使用给定的 ID 执行过一个操作，如 例 12-2 中所示。 例 12-2 使用唯一 ID 来抑制重复请求 123456789ALTER TABLE requests ADD UNIQUE (request_id);BEGIN TRANSACTION; INSERT INTO requests (request_id, from_account, to_account, amount) VALUES(&#x27;0286FDB8-D7E1-423F-B40B-792B3608036C&#x27;, 4321, 1234, 11.00); UPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234; UPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;COMMIT; 例 12-2 依赖于 request_id 列上的唯一约束。如果一个事务尝试插入一个已经存在的 ID，那么 INSERT 失败，事务被中止，使其无法生效两次。即使在较弱的隔离级别下，关系数据库也能正确地维护唯一性约束（而在 “写入偏差与幻读” 中讨论过，应用级别的 检查 - 然后 - 插入 可能会在不可串行化的隔离下失败）。 除了抑制重复的请求之外，例 12-2 中的请求表表现得就像一种事件日志，暗示着事件溯源的想法（请参阅 “事件溯源”）。更新账户余额事实上不必与插入事件发生在同一个事务中，因为它们是冗余的，而能由下游消费者从请求事件中衍生出来 —— 只要该事件被恰好处理一次，这又一次可以使用请求 ID 来强制执行。 端到端原则抑制重复事务的这种情况只是一个更普遍的原则的一个例子，这个原则被称为 端到端原则（end-to-end argument），它在 1984 年由 Saltzer、Reed 和 Clark 阐述【55】： 只有在通信系统两端应用的知识与帮助下，所讨论的功能才能完全地正确地实现。因而将这种被质疑的功能作为通信系统本身的功能是不可能的（有时，通信系统可以提供这种功能的不完备版本，可能有助于提高性能）。 在我们的例子中 所讨论的功能 是重复抑制。我们看到 TCP 在 TCP 连接层次抑制了重复的数据包，一些流处理器在消息处理层次提供了所谓的恰好一次语义，但这些都无法阻止当一个请求超时时，用户亲自提交重复的请求。TCP，数据库事务，以及流处理器本身并不能完全排除这些重复。解决这个问题需要一个端到端的解决方案：从终端用户的客户端一路传递到数据库的事务标识符。 端到端原则也适用于检查数据的完整性：以太网，TCP 和 TLS 中内置的校验和可以检测网络中数据包的损坏情况，但是它们无法检测到由连接两端发送 &#x2F; 接收软件中 Bug 导致的损坏。或数据存储所在磁盘上的损坏。如果你想捕获数据所有可能的损坏来源，你也需要端到端的校验和。 类似的原则也适用于加密【55】：家庭 WiFi 网络上的密码可以防止人们窃听你的 WiFi 流量，但无法阻止互联网上其他地方攻击者的窥探；客户端与服务器之间的 TLS&#x2F;SSL 可以阻挡网络攻击者，但无法阻止恶意服务器。只有端到端的加密和认证可以防止所有这些事情。 尽管低层级的功能（TCP 重复抑制、以太网校验和、WiFi 加密）无法单独提供所需的端到端功能，但它们仍然很有用，因为它们能降低较高层级出现问题的可能性。例如，如果我们没有 TCP 来将数据包排成正确的顺序，那么 HTTP 请求通常就会被搅烂。我们只需要记住，低级别的可靠性功能本身并不足以确保端到端的正确性。 在数据系统中应用端到端思考这将我带回最初的论点：仅仅因为应用使用了提供相对较强安全属性的数据系统，例如可串行化的事务，并不意味着应用的数据就不会丢失或损坏了。应用本身也需要采取端到端的措施，例如除重。 这实在是一个遗憾，因为容错机制很难弄好。低层级的可靠机制（比如 TCP 中的那些）运行的相当好，因而剩下的高层级错误基本很少出现。如果能将这些剩下的高层级容错机制打包成抽象，而应用不需要再去操心，那该多好呀 —— 但恐怕我们还没有找到这一正确的抽象。 长期以来，事务被认为是一个很好的抽象，我相信它们确实是很有用的。正如 第七章 导言中所讨论的，它们将各种可能的问题（并发写入、违背约束、崩溃、网络中断、磁盘故障）合并为两种可能结果：提交或中止。这是对编程模型而言是一种巨大的简化，但恐怕这还不够。 事务是代价高昂的，当涉及异构存储技术时尤为甚（请参阅 “实践中的分布式事务”）。我们拒绝使用分布式事务是因为它开销太大，结果我们最后不得不在应用代码中重新实现容错机制。正如本书中大量的例子所示，对并发性与部分失败的推理是困难且违反直觉的，所以我怀疑大多数应用级别的机制都不能正确工作，最终结果是数据丢失或损坏。 出于这些原因，我认为探索对容错的抽象是很有价值的。它使提供应用特定的端到端的正确性属性变得更简单，而且还能在大规模分布式环境中提供良好的性能与运维特性。 强制约束让我们思考一下在 分拆数据库 上下文中的 正确性（correctness）。我们看到端到端的除重可以通过从客户端一路透传到数据库的请求 ID 实现。那么其他类型的约束呢？ 我们先来特别关注一下 唯一性约束 —— 例如我们在 例 12-2 中所依赖的约束。在 “约束和唯一性保证” 中，我们看到了几个其他需要强制实施唯一性的应用功能例子：用户名或电子邮件地址必须唯一标识用户，文件存储服务不能包含多个重名文件，两个人不能在航班或剧院预订同一个座位。 其他类型的约束也非常类似：例如，确保帐户余额永远不会变为负数，确保不会超卖库存，或者会议室没有重复的预订。执行唯一性约束的技术通常也可以用于这些约束。 唯一性约束需要达成共识在 第九章 中我们看到，在分布式环境中，强制执行唯一性约束需要共识：如果存在多个具有相同值的并发请求，则系统需要决定冲突操作中的哪一个被接受，并拒绝其他违背约束的操作。 达成这一共识的最常见方式是使单个节点作为领导，并使其负责所有决策。只要你不介意所有请求都挤过单个节点（即使客户端位于世界的另一端），只要该节点没有失效，系统就能正常工作。如果你需要容忍领导者失效，那么就又回到了共识问题（请参阅 “单主复制与共识”）。 唯一性检查可以通过对唯一性字段分区做横向伸缩。例如，如果需要通过请求 ID 确保唯一性（如 例 12-2 所示），你可以确保所有具有相同请求 ID 的请求都被路由到同一分区（请参阅 第六章）。如果你需要让用户名是唯一的，则可以按用户名的散列值做分区。 但异步多主复制排除在外，因为可能会发生不同主库同时接受冲突写操作的情况，因而这些值不再是唯一的（请参阅 “实现线性一致的系统”）。如果你想立刻拒绝任何违背约束的写入，同步协调是无法避免的【56】。 基于日志消息传递中的唯一性日志确保所有消费者以相同的顺序看见消息 —— 这种保证在形式上被称为 全序广播（total order boardcast） 并且等价于共识（请参阅 “全序广播”）。在使用基于日志的消息传递的分拆数据库方法中，我们可以使用非常类似的方法来执行唯一性约束。 流处理器在单个线程上依次消费单个日志分区中的所有消息（请参阅 “日志与传统的消息传递相比”）。因此，如果日志是按需要确保唯一的值做的分区，则流处理器可以无歧义地、确定性地决定几个冲突操作中的哪一个先到达。例如，在多个用户尝试宣告相同用户名的情况下【57】： 每个对用户名的请求都被编码为一条消息，并追加到按用户名散列值确定的分区。 流处理器依序读取日志中的请求，并使用本地数据库来追踪哪些用户名已经被占用了。对于所有申请可用用户名的请求，它都会记录该用户名，并向输出流发送一条成功消息。对于所有申请已占用用户名的请求，它都会向输出流发送一条拒绝消息。 请求用户名的客户端监视输出流，等待与其请求相对应的成功或拒绝消息。 该算法基本上与 “使用全序广播实现线性一致的存储” 中的算法相同。它可以简单地通过增加分区数伸缩至较大的请求吞吐量，因为每个分区都可以被独立处理。 该方法不仅适用于唯一性约束，而且适用于许多其他类型的约束。其基本原理是，任何可能冲突的写入都会路由到相同的分区并按顺序处理。正如 “什么是冲突？” 与 “写入偏差与幻读” 中所述，冲突的定义可能取决于应用，但流处理器可以使用任意逻辑来验证请求。这个想法与 Bayou 在 90 年代开创的方法类似【58】。 多分区请求处理当涉及多个分区时，确保操作以原子方式执行且同时满足约束就变得很有趣了。在 例 12-2 中，可能有三个分区：一个包含请求 ID，一个包含收款人账户，另一个包含付款人账户。没有理由把这三种东西放入同一个分区，因为它们都是相互独立的。 在数据库的传统方法中，执行此事务需要跨全部三个分区进行原子提交，就这些分区上的所有其他事务而言，这实质上是将该事务嵌入一个全序。而这样就要求跨分区协调，不同的分区无法再独立地进行处理，因此吞吐量很可能会受到影响。 但事实证明，使用分区日志可以达到等价的正确性而无需原子提交： 从账户 A 向账户 B 转账的请求由客户端提供一个唯一的请求 ID，并按请求 ID 追加写入相应日志分区。 流处理器读取请求日志。对于每个请求消息，它向输出流发出两条消息：付款人账户 A 的借记指令（按 A 分区），收款人 B 的贷记指令（按 B 分区）。被发出的消息中会带有原始的请求 ID。 后续处理器消费借记 &#x2F; 贷记指令流，按照请求 ID 除重，并将变更应用至账户余额。 步骤 1 和步骤 2 是必要的，因为如果客户直接发送贷记与借记指令，则需要在这两个分区之间进行原子提交，以确保两者要么都发生或都不发生。为了避免对分布式事务的需要，我们首先将请求持久化记录为单条消息，然后从这第一条消息中衍生出贷记指令与借记指令。几乎在所有数据系统中，单对象写入都是原子性的（请参阅 “单对象写入），因此请求要么出现在日志中，要么就不出现，无需多分区原子提交。 如果流处理器在步骤 2 中崩溃，则它会从上一个存档点恢复处理。这样做时，它不会跳过任何请求消息，但可能会多次处理请求并产生重复的贷记与借记指令。但由于它是确定性的，因此它只是再次生成相同的指令，而步骤 3 中的处理器可以使用端到端请求 ID 轻松地对其除重。 如果你想确保付款人的帐户不会因此次转账而透支，则可以使用一个额外的流处理器来维护账户余额并校验事务（按付款人账户分区），只有有效的事务会被记录在步骤 1 中的请求日志中。 通过将多分区事务分解为两个不同分区方式的阶段，并使用端到端的请求 ID，我们实现了同样的正确性属性（每个请求对付款人与收款人都恰好生效一次），即使在出现故障，且没有使用原子提交协议的情况下依然如此。使用多个不同分区阶段的想法与我们在 “多分区数据处理” 中讨论的想法类似（也请参阅 “并发控制”）。 及时性与完整性事务的一个便利属性是，它们通常是线性一致的（请参阅 “线性一致性”），也就是说，写入者会等到事务提交，而之后其写入立刻对所有读取者可见。 当我们把一个操作拆分为跨越多个阶段的流处理器时，却并非如此：日志的消费者在设计上就是异步的，因此发送者不会等其消息被消费者处理完。但是，客户端等待输出流中的特定消息是可能的。这正是我们在 “基于日志消息传递中的唯一性” 一节中检查唯一性约束时所做的事情。 在这个例子中，唯一性检查的正确性不取决于消息发送者是否等待结果。等待的目的仅仅是同步通知发送者唯一性检查是否成功。但该通知可以与消息处理的结果相解耦。 更一般地来讲，我认为术语 一致性（consistency） 这个术语混淆了两个值得分别考虑的需求： 及时性（Timeliness） 及时性意味着确保用户观察到系统的最新状态。我们之前看到，如果用户从陈旧的数据副本中读取数据，它们可能会观察到系统处于不一致的状态（请参阅 “复制延迟问题”）。但这种不一致是暂时的，而最终会通过等待与重试简单地得到解决。 CAP 定理（请参阅 “线性一致性的代价”）使用 线性一致性（linearizability） 意义上的一致性，这是实现及时性的强有力方法。像 写后读 这样及时性更弱的一致性也很有用（请参阅 “读己之写”）。 完整性（Integrity） 完整性意味着没有损坏；即没有数据丢失，并且没有矛盾或错误的数据。尤其是如果某些衍生数据集是作为底层数据之上的视图而维护的（请参阅 “从事件日志中派生出当前状态”），这种衍生必须是正确的。例如，数据库索引必须正确地反映数据库的内容 —— 缺失某些记录的索引并不是很有用。 如果完整性被违背，这种不一致是永久的：在大多数情况下，等待与重试并不能修复数据库损坏。相反的是，需要显式地检查与修复。在 ACID 事务的上下文中（请参阅 “ACID 的含义”），一致性通常被理解为某种特定于应用的完整性概念。原子性和持久性是保持完整性的重要工具。 口号形式：违反及时性，“最终一致性”；违反完整性，“永无一致性”。 我断言在大多数应用中，完整性比及时性重要得多。违反及时性可能令人困惑与讨厌，但违反完整性的结果可能是灾难性的。 例如在你的信用卡对账单上，如果某一笔过去 24 小时内完成的交易尚未出现并不令人奇怪 —— 这些系统有一定的滞后是正常的。我们知道银行是异步核算与敲定交易的，这里的及时性并不是非常重要【3】。但果当期对账单余额与上期对账单余额加交易总额对不上（求和错误），或者出现一笔向你收费但未向商家付款的交易（消失的钱），那实在是太糟糕了。这样的问题就违背了系统的完整性。 数据流系统的正确性ACID 事务通常既提供及时性（例如线性一致性）也提供完整性保证（例如原子提交）。因此如果你从 ACID 事务的角度来看待应用的正确性，那么及时性与完整性的区别是无关紧要的。 另一方面，对于在本章中讨论的基于事件的数据流系统而言，它们的一个有趣特性就是将及时性与完整性分开。在异步处理事件流时不能保证及时性，除非你显式构建一个在返回之前明确等待特定消息到达的消费者。但完整性实际上才是流处理系统的核心。 恰好一次 或 等效一次 语义（请参阅 “容错”）是一种保持完整性的机制。如果事件丢失或者生效两次，就有可能违背数据系统的完整性。因此在出现故障时，容错消息传递与重复抑制（例如，幂等操作）对于维护数据系统的完整性是很重要的。 正如我们在上一节看到的那样，可靠的流处理系统可以在无需分布式事务与原子提交协议的情况下保持完整性，这意味着它们有潜力达到与后者相当的正确性，同时还具备好得多的性能与运维稳健性。为了达成这种正确性，我们组合使用了多种机制： 将写入操作的内容表示为单条消息，从而可以轻松地被原子写入 —— 与事件溯源搭配效果拔群（请参阅 “事件溯源”）。 使用与存储过程类似的确定性衍生函数，从这一消息中衍生出所有其他的状态变更（请参阅 “真的串行执行” 和 “应用代码作为衍生函数”） 将客户端生成的请求 ID 传递通过所有的处理层次，从而允许端到端的除重，带来幂等性。 使消息不可变，并允许衍生数据能随时被重新处理，这使从错误中恢复更加容易（请参阅 “不可变事件的优点”） 这种机制组合在我看来，是未来构建容错应用的一个非常有前景的方向。 宽松地解释约束如前所述，执行唯一性约束需要共识，通常通过在单个节点中汇集特定分区中的所有事件来实现。如果我们想要传统的唯一性约束形式，这种限制是不可避免的，流处理也不例外。 然而另一个需要了解的事实是，许多真实世界的应用实际上可以摆脱这种形式，接受弱得多的唯一性： 如果两个人同时注册了相同的用户名或预订了相同的座位，你可以给其中一个人发消息道歉，并要求他们换一个不同的用户名或座位。这种纠正错误的变化被称为 补偿性事务（compensating transaction）【59,60】。 如果客户订购的物品多于仓库中的物品，你可以下单补仓，并为延误向客户道歉，向他们提供折扣。实际上，这么说吧，如果叉车在仓库中轧过了你的货物，剩下的货物比你想象的要少，那么你也是得这么做【61】。因此，既然道歉工作流无论如何已经成为你商业过程中的一部分了，那么对库存物品数目添加线性一致的约束可能就没必要了。 与之类似，许多航空公司都会超卖机票，打着一些旅客可能会错过航班的算盘；许多旅馆也会超卖客房，抱着部分客人可能会取消预订的期望。在这些情况下，出于商业原因而故意违反了 “一人一座” 的约束；当需求超过供给的情况出现时，就会进入补偿流程（退款、升级舱位 &#x2F; 房型、提供隔壁酒店的免费的房间）。即使没有超卖，为了应对由恶劣天气或员工罢工导致的航班取消，你还是需要道歉与补偿流程 —— 从这些问题中恢复仅仅是商业活动的正常组成部分。 如果有人从账户超额取款，银行可以向他们收取透支费用，并要求他们偿还欠款。通过限制每天的提款总额，银行的风险是有限的。 在许多商业场景中，临时违背约束并稍后通过道歉来修复，实际上是可以接受的。道歉的成本各不相同，但通常很低（以金钱或名声来算）：你无法撤回已发送的电子邮件，但可以发送一封后续电子邮件进行更正。如果你不小心向信用卡收取了两次费用，则可以将其中一项收费退款，而代价仅仅是手续费，也许还有客户的投诉。尽管一旦 ATM 吐了钱，你无法直接取回，但原则上如果账户透支而客户拒不支付，你可以派催收员收回欠款。 道歉的成本是否能接受是一个商业决策。如果可以接受的话，在写入数据之前检查所有约束的传统模型反而会带来不必要的限制，而线性一致性的约束也不是必须的。乐观写入，事后检查可能是一种合理的选择。你仍然可以在做一些挽回成本高昂的事情前确保有相关的验证，但这并不意味着写入数据之前必须先进行验证。 这些应用 确实 需要完整性：你不会希望丢失预订信息，或者由于借方贷方不匹配导致资金消失。但是它们在执行约束时 并不需要 及时性：如果你销售的货物多于仓库中的库存，可以在事后道歉后并弥补问题。这种做法与我们在 “处理写入冲突” 中讨论的冲突解决方法类似。 无协调数据系统我们现在已经做了两个有趣的观察： 数据流系统可以维持衍生数据的完整性保证，而无需原子提交、线性一致性或者同步的跨分区协调。 虽然严格的唯一性约束要求及时性和协调，但许多应用实际上可以接受宽松的约束：只要整个过程保持完整性，这些约束可能会被临时违反并在稍后被修复。 总之这些观察意味着，数据流系统可以为许多应用提供无需协调的数据管理服务，且仍能给出很强的完整性保证。这种 无协调（coordination-avoiding） 的数据系统有着很大的吸引力：比起需要执行同步协调的系统，它们能达到更好的性能与更强的容错能力【56】。 例如，这种系统可以使用多领导者配置运维，跨越多个数据中心，在区域间异步复制。任何一个数据中心都可以持续独立运行，因为不需要同步的跨区域协调。这样的系统的及时性保证会很弱 —— 如果不引入协调它是不可能是线性一致的 —— 但它仍然可以提供有力的完整性保证。 在这种情况下，可串行化事务作为维护衍生状态的一部分仍然是有用的，但它们只能在小范围内运行，在那里它们工作得很好【8】。异构分布式事务（如 XA 事务，请参阅 “实践中的分布式事务”）不是必需的。同步协调仍然可以在需要的地方引入（例如在无法恢复的操作之前强制执行严格的约束），但是如果只是应用的一小部分地方需要它，没必要让所有操作都付出协调的代价。【43】。 另一种审视协调与约束的角度是：它们减少了由于不一致而必须做出的道歉数量，但也可能会降低系统的性能和可用性，从而可能增加由于宕机中断而需要做出的道歉数量。你不可能将道歉数量减少到零，但可以根据自己的需求寻找最佳平衡点 —— 既不存在太多不一致性，又不存在太多可用性问题。 信任但验证我们所有关于正确性，完整性和容错的讨论都基于一些假设，假设某些事情可能会出错，但其他事情不会。我们将这些假设称为我们的 系统模型（system model，请参阅 “将系统模型映射到现实世界”）：例如，我们应该假设进程可能会崩溃，机器可能突然断电，网络可能会任意延迟或丢弃消息。但是我们也可能假设写入磁盘的数据在执行 fsync 后不会丢失，内存中的数据没有损坏，而 CPU 的乘法指令总是能返回正确的结果。 这些假设是相当合理的，因为大多数时候它们都是成立的，如果我们不得不经常担心计算机出错，那么基本上寸步难行。在传统上，系统模型采用二元方法处理故障：我们假设有些事情可能会发生，而其他事情 永远 不会发生。实际上，这更像是一个概率问题：有些事情更有可能，其他事情不太可能。问题在于违反我们假设的情况是否经常发生，以至于我们可能在实践中遇到它们。 我们已经看到，数据可能会在尚未落盘时损坏（请参阅 “复制与持久性”），而网络上的数据损坏有时可能规避了 TCP 校验和（请参阅 “弱谎言形式” ）。也许我们应当更关注这些事情？ 我过去所从事的一个应用收集了来自客户端的崩溃报告，我们收到的一些报告，只有在这些设备内存中出现了随机位翻转才解释的通。这看起来不太可能，但是如果有足够多的设备运行你的软件，那么即使再不可能发生的事也确实会发生。除了由于硬件故障或辐射导致的随机存储器损坏之外，一些病态的存储器访问模式甚至可以在没有故障的存储器中翻转位【62】 —— 一种可用于破坏操作系统安全机制的效应【63】（这种技术被称为 Rowhammer）。一旦你仔细观察，硬件并不是看上去那样完美的抽象。 要澄清的是，随机位翻转在现代硬件上仍是非常罕见的【64】。我只想指出，它们并没有超越可能性的范畴，所以值得一些关注。 维护完整性，尽管软件有Bug除了这些硬件问题之外，总是存在软件 Bug 的风险，这些错误不会被较低层次的网络、内存或文件系统校验和所捕获。即使广泛使用的数据库软件也有 Bug：即使像 MySQL 与 PostgreSQL 这样稳健、口碑良好、多年来被许多人充分测试过的软件，就我个人所见也有 Bug，比如 MySQL 未能正确维护唯一约束【65】，以及 PostgreSQL 的可串行化隔离等级存在特定的写入偏差异常【66】。对于不那么成熟的软件来说，情况可能要糟糕得多。 尽管在仔细设计，测试，以及审查上做出很多努力，但 Bug 仍然会在不知不觉中产生。尽管它们很少，而且最终会被发现并被修复，但总会有那么一段时间，这些 Bug 可能会损坏数据。 而对于应用代码，我们不得不假设会有更多的错误，因为绝大多数应用的代码经受的评审与测试远远无法与数据库的代码相比。许多应用甚至没有正确使用数据库提供的用于维持完整性的功能，例如外键或唯一性约束【36】。 ACID 意义下的一致性（请参阅 “一致性”）基于这样一种想法：数据库以一致的状态启动，而事务将其从一个一致状态转换至另一个一致的状态。因此，我们期望数据库始终处于一致状态。然而，只有当你假设事务没有 Bug 时，这种想法才有意义。如果应用以某种错误的方式使用数据库，例如，不安全地使用弱隔离等级，数据库的完整性就无法得到保证。 不要盲目信任承诺由于硬件和软件并不总是符合我们的理想，所以数据损坏似乎早晚不可避免。因此，我们至少应该有办法查明数据是否已经损坏，以便我们能够修复它，并尝试追查错误的来源。检查数据完整性称为 审计（auditing）。 如 “不可变事件的优点” 一节中所述，审计不仅仅适用于财务应用程序。不过，可审计性在财务中是非常非常重要的，因为每个人都知道错误总会发生，我们也都认为能够检测和解决问题是合理的需求。 成熟的系统同样倾向于考虑不太可能的事情出错的可能性，并管理这种风险。例如，HDFS 和 Amazon S3 等大规模存储系统并不完全信任磁盘：它们运行后台进程持续回读文件，并将其与其他副本进行比较，并将文件从一个磁盘移动到另一个，以便降低静默损坏的风险【67】。 如果你想确保你的数据仍然存在，你必须真正读取它并进行检查。大多数时候它们仍然会在那里，但如果不是这样，你一定想尽早知道答案，而不是更晚。按照同样的原则，不时地尝试从备份中恢复是非常重要的 —— 否则当你发现备份损坏时，你可能已经遇到了数据丢失，那时候就真的太晚了。不要盲目地相信它们全都管用。 验证的文化像 HDFS 和 S3 这样的系统仍然需要假设磁盘大部分时间都能正常工作 —— 这是一个合理的假设，但与它们 始终 能正常工作的假设并不相同。然而目前还没有多少系统采用这种 “信任但是验证” 的方式来持续审计自己。许多人认为正确性保证是绝对的，并且没有为罕见的数据损坏的可能性做过准备。我希望未来能看到更多的 自我验证（self-validating） 或 自我审计（self-auditing） 系统，不断检查自己的完整性，而不是依赖盲目的信任【68】。 我担心 ACID 数据库的文化导致我们在盲目信任技术（如事务机制）的基础上开发应用，而忽视了这种过程中的任何可审计性。由于我们所信任的技术在大多数情况下工作得很好，通常会认为审计机制并不值得投资。 但随之而来的是，数据库的格局发生了变化：在 NoSQL 的旗帜下，更弱的一致性保证成为常态，更不成熟的存储技术越来越被广泛使用。但是由于审计机制还没有被开发出来，尽管这种方式越来越危险，我们仍不断在盲目信任的基础上构建应用。让我们想一想如何针对可审计性而设计吧。 为可审计性而设计如果一个事务在一个数据库中改变了多个对象，在这一事实发生后，很难说清这个事务到底意味着什么。即使你捕获了事务日志（请参阅 “变更数据捕获”），各种表中的插入、更新和删除操作并不一定能清楚地表明 为什么 要执行这些变更。决定这些变更的是应用逻辑中的调用，而这一应用逻辑稍纵即逝，无法重现。 相比之下，基于事件的系统可以提供更好的可审计性。在事件溯源方法中，系统的用户输入被表示为一个单一不可变事件，而任何其导致的状态变更都衍生自该事件。衍生可以实现为具有确定性与可重复性，因而相同的事件日志通过相同版本的衍生代码时，会导致相同的状态变更。 显式处理数据流（请参阅 “批处理输出的哲学”）可以使数据的 来龙去脉（provenance） 更加清晰，从而使完整性检查更具可行性。对于事件日志，我们可以使用散列来检查事件存储没有被破坏。对于任何衍生状态，我们可以重新运行从事件日志中衍生它的批处理器与流处理器，以检查是否获得相同的结果，或者，甚至并行运行冗余的衍生流程。 具有确定性且定义良好的数据流，也使调试与跟踪系统的执行变得容易，以便确定它 为什么 做了某些事情【4,69】。如果出现意想之外的事情，那么重现导致意外事件的确切事故现场的诊断能力 —— 一种时间旅行调试功能是非常有价值的。 端到端原则重现如果我们不能完全相信系统的每个组件都不会损坏 —— 每一个硬件都没缺陷，每一个软件都没有 Bug —— 那我们至少必须定期检查数据的完整性。如果我们不检查，我们就不能发现损坏，直到无可挽回地导致对下游的破坏时，那时候再去追踪问题就要难得多，且代价也要高的多。 检查数据系统的完整性，最好是以端到端的方式进行（请参阅 “数据库的端到端原则”）：我们能在完整性检查中涵盖的系统越多，某些处理阶中出现不被察觉损坏的几率就越小。如果我们能检查整个衍生数据管道端到端的正确性，那么沿着这一路径的任何磁盘、网络、服务以及算法的正确性检查都隐含在其中了。 持续的端到端完整性检查可以不断提高你对系统正确性的信心，从而使你能更快地进步【70】。与自动化测试一样，审计提高了快速发现错误的可能性，从而降低了系统变更或新存储技术可能导致损失的风险。如果你不害怕进行变更，就可以更好地充分演化一个应用，使其满足不断变化的需求。 用于可审计数据系统的工具目前，将可审计性作为顶层关注点的数据系统并不多。一些应用实现了自己的审计机制，例如将所有变更记录到单独的审计表中，但是确保审计日志与数据库状态的完整性仍然是很困难的。可以定期使用硬件安全模块对事务日志进行签名来防止篡改，但这无法保证正确的事务一开始就能进入到日志中。 使用密码学工具来证明系统的完整性是十分有趣的，这种方式对于宽泛的硬件与软件问题，甚至是潜在的恶意行为都很稳健有效。加密货币、区块链、以及诸如比特币、以太坊、Ripple、Stellar 的分布式账本技术已经迅速出现在这一领域【71,72,73】。 我没有资格评论这些技术用于货币，或者合同商定机制的价值。但从数据系统的角度来看，它们包含了一些有趣的想法。实质上，它们是分布式数据库，具有数据模型与事务机制，而不同副本可以由互不信任的组织托管。副本不断检查其他副本的完整性，并使用共识协议对应当执行的事务达成一致。 我对这些技术的拜占庭容错方面有些怀疑（请参阅 “拜占庭故障”），而且我发现 工作证明（proof of work） 技术非常浪费（比如，比特币挖矿）。比特币的交易吞吐量相当低，尽管更多是出于政治与经济原因而非技术上的原因。不过，完整性检查的方面是很有趣的。 密码学审计与完整性检查通常依赖 默克尔树（Merkle tree）【74】，这是一颗散列值的树，能够用于高效地证明一条记录出现在一个数据集中（以及其他一些特性）。除了炒作的沸沸扬扬的加密货币之外，证书透明性（certificate transparency） 也是一种依赖 Merkle 树的安全技术，用来检查 TLS&#x2F;SSL 证书的有效性【75,76】。 我可以想象，那些在证书透明度与分布式账本中使用的完整性检查和审计算法，将会在通用数据系统中得到越来越广泛的应用。要使得这些算法对于没有密码学审计的系统同样可伸缩，并尽可能降低性能损失还需要一些工作。 但我认为这是一个值得关注的有趣领域。 做正确的事情在本书的最后部分，我想退后一步。在本书中，我们考察了各种不同的数据系统架构，评价了它们的优点与缺点，并探讨了构建可靠，可伸缩，可维护应用的技术。但是，我们忽略了讨论中一个重要而基础的部分，现在我想补充一下。 每个系统都服务于一个目的；我们采取的每个举措都会同时产生期望的后果与意外的后果。这个目的可能只是简单地赚钱，但其对世界的影响，可能会远远超出最初的目的。我们，建立这些系统的工程师，有责任去仔细考虑这些后果，并有意识地决定，我们希望生活在怎样的世界中。 我们将数据当成一种抽象的东西来讨论，但请记住，许多数据集都是关于人的：他们的行为，他们的兴趣，他们的身份。对待这些数据，我们必须怀着人性与尊重。用户也是人类，人类的尊严是至关重要的。 软件开发越来越多地涉及重要的道德抉择。有一些指导原则可以帮助软件工程师解决这些问题，例如 ACM 的软件工程道德规范与专业实践【77】，但实践中很少会讨论这些，更不用说应用与强制执行了。因此，工程师和产品经理有时会对隐私与产品潜在的负面后果抱有非常傲慢的态度【78,79,80】。 技术本身并无好坏之分 —— 关键在于它被如何使用，以及它如何影响人们。这对枪械这样的武器是成立的，而搜索引擎这样的软件系统与之类似。我认为，软件工程师仅仅专注于技术而忽视其后果是不够的：道德责任也是我们的责任。对道德推理很困难，但它太重要了，我们无法忽视。 预测性分析举个例子，预测性分析是 “大数据” 炒作的主要内容之一。使用数据分析预测天气或疾病传播是一码事【81】；而预测一个罪犯是否可能再犯，一个贷款申请人是否有可能违约，或者一个保险客户是否可能进行昂贵的索赔，则是另外一码事。后者会直接影响到个人的生活。 当然，支付网络希望防止欺诈交易，银行希望避免不良贷款，航空公司希望避免劫机，公司希望避免雇佣效率低下或不值得信任的人。从它们的角度来看，失去商机的成本很低，而不良贷款或问题员工的成本则要高得多，因而组织希望保持谨慎也是自然而然的事情。所以如果存疑，它们通常会 Say No。 然而，随着算法决策变得越来越普遍，被某种算法（准确地或错误地）标记为有风险的某人可能会遭受大量这种 “No” 的决定。系统性地被排除在工作，航旅，保险，租赁，金融服务，以及其他社会关键领域之外。这是一种对个体自由的极大约束，因此被称为 “算法监狱”【82】。在尊重人权的国家，刑事司法系统会做无罪推定（默认清白，直到被证明有罪）。另一方面，自动化系统可以系统地，任意地将一个人排除在社会参与之外，不需要任何有罪的证明，而且几乎没有申诉的机会。 偏见与歧视算法做出的决定不一定比人类更好或更差。每个人都可能有偏见，即使他们主动抗拒这一点；而歧视性做法也可能已经在文化上被制度化了。人们希望根据数据做出决定，而不是通过人的主观评价与直觉，希望这样能更加公平，并给予传统体制中经常被忽视的人更好的机会【83】。 当我们开发预测性分析系统时，不是仅仅用软件通过一系列 IF ELSE 规则将人类的决策过程自动化，那些规则本身甚至都是从数据中推断出来的。但这些系统学到的模式是个黑盒：即使数据中存在一些相关性，我们可能也压根不知道为什么。如果算法的输入中存在系统性的偏见，则系统很有可能会在输出中学习并放大这种偏见【84】。 在许多国家，反歧视法律禁止按种族、年龄、性别、性取向、残疾或信仰等受保护的特征区分对待不同的人。其他的个人特征可能是允许用于分析的，但是如果这些特征与受保护的特征存在关联，又会发生什么？例如在种族隔离地区中，一个人的邮政编码，甚至是他们的 IP 地址，都是很强的种族指示物。这样的话，相信一种算法可以以某种方式将有偏见的数据作为输入，并产生公平和公正的输出【85】似乎是很荒谬的。然而这种观点似乎常常潜伏在数据驱动型决策的支持者中，这种态度被讽刺为 “在处理偏差上，机器学习与洗钱类似”（machine learning is like money laundering for bias）【86】。 预测性分析系统只是基于过去进行推断；如果过去是歧视性的，它们就会将这种歧视归纳为规律。如果我们希望未来比过去更好，那么就需要道德想象力，而这是只有人类才能提供的东西【87】。数据与模型应该是我们的工具，而不是我们的主人。 责任与问责自动决策引发了关于责任与问责的问题【87】。如果一个人犯了错误，他可以被追责，受决定影响的人可以申诉。算法也会犯错误，但是如果它们出错，谁来负责【88】？当一辆自动驾驶汽车引发事故时，谁来负责？如果自动信用评分算法系统性地歧视特定种族或宗教的人，这些人是否有任何追索权？如果机器学习系统的决定要受到司法审查，你能向法官解释算法是如何做出决定的吗？ 收集关于人的数据并进行决策，信用评级机构是一个很经典的例子。不良的信用评分会使生活变得更艰难，但至少信用分通常是基于个人 实际的 借款历史记录，而记录中的任何错误都能被纠正（尽管机构通常会设置门槛）。然而，基于机器学习的评分算法通常会使用更宽泛的输入，并且更不透明；因而很难理解特定决策是怎样作出的，以及是否有人被不公正地，歧视性地对待【89】。 信用分总结了 “你过去的表现如何？”，而预测性分析通常是基于 “谁与你类似，以及与你类似的人过去表现的如何？”。与他人的行为画上等号意味着刻板印象，例如，根据他们居住的地方（与种族和阶级关系密切的特征）。那么那些放错位置的人怎么办？而且，如果是因为错误数据导致的错误决定，追索几乎是不可能的【87】。 很多数据本质上是统计性的，这意味着即使概率分布在总体上是正确的，对于个例也可能是错误的。例如，如果贵国的平均寿命是 80 岁，这并不意味着你在 80 岁生日时就会死掉。很难从平均值与概率分布中对某个特定个体的寿命作出什么判断，同样，预测系统的输出是概率性的，对于个例可能是错误的。 盲目相信数据决策至高无上，这不仅仅是一种妄想，而是有切实危险的。随着数据驱动的决策变得越来越普遍，我们需要弄清楚，如何使算法更负责任且更加透明，如何避免加强现有的偏见，以及如何在它们不可避免地出错时加以修复。 我们还需要想清楚，如何避免数据被用于害人，如何认识数据的积极潜力。例如，分析可以揭示人们生活的财务特点与社会特点。一方面，这种权力可以用来将援助与支持集中在帮助那些最需要援助的人身上。另一方面，它有时会被掠夺性企业用于识别弱势群体，并向其兜售高风险产品，比如高利贷和没有价值的大学文凭【87,90】。 反馈循环即使是那些对人直接影响比较小的预测性应用，比如推荐系统，也有一些必须正视的难题。当服务变得善于预测用户想要看到什么内容时，它最终可能只会向人们展示他们已经同意的观点，将人们带入滋生刻板印象，误导信息，与极端思想的 回音室。我们已经看到过社交媒体回音室对竞选的影响了【91】。 当预测性分析影响人们的生活时，自我强化的反馈循环会导致非常有害的问题。例如，考虑雇主使用信用分来评估候选人的例子。你可能是一个信用分不错的好员工，但因不可抗力的意外而陷入财务困境。由于不能按期付账单，你的信用分会受到影响，进而导致找到工作更为困难。失业使你陷入贫困，这进一步恶化了你的分数，使你更难找到工作【87】。在数据与数学严谨性的伪装背后，隐藏的是由恶毒假设导致的恶性循环。 我们无法预测这种反馈循环何时发生。然而通过对整个系统（不仅仅是计算机化的部分，而且还有与之互动的人）进行整体思考，许多后果是可以够预测的 —— 一种称为 系统思维（systems thinking） 的方法【92】。我们可以尝试理解数据分析系统如何响应不同的行为，结构或特性。该系统是否加强和增大了人们之间现有的差异（例如，损不足以奉有余，富者愈富，贫者愈贫），还是试图与不公作斗争？而且即使有着最好的动机，我们也必须当心意想不到的后果。 隐私和追踪除了预测性分析 —— 使用数据来做出关于人的自动决策 —— 数据收集本身也存在道德问题。收集数据的组织，与被收集数据的人之间，到底属于什么关系？ 当系统只存储用户明确输入的数据时，是因为用户希望系统以特定方式存储和处理这些数据，系统是在为用户提供服务：用户就是客户。但是，当用户的活动被跟踪并记录，作为他们正在做的其他事情的副作用时，这种关系就没有那么清晰了。该服务不再仅仅完成用户想要它要做的事情，而是服务于它自己的利益，而这可能与用户的利益相冲突。 追踪用户行为数据对于许多面向用户的在线服务而言，变得越来越重要：追踪用户点击了哪些搜索结果有助于改善搜索结果的排名；推荐 “喜欢 X 的人也喜欢 Y”，可以帮助用户发现实用有趣的东西； A&#x2F;B 测试和用户流量分析有助于改善用户界面。这些功能需要一定量的用户行为跟踪，而用户也可以从中受益。 但不同公司有着不同的商业模式，追踪并未止步于此。如果服务是通过广告盈利的，那么广告主才是真正的客户，而用户的利益则屈居其次。跟踪的数据会变得更详细，分析变得更深入，数据会保留很长时间，以便为每个人建立详细画像，用于营销。 现在，公司与被收集数据的用户之间的关系，看上去就不太一样了。公司会免费服务用户，并引诱用户尽可能多地使用服务。对用户的追踪，主要不是服务于该用户个体，而是服务于掏钱资助该服务的广告商。我认为这种关系可以用一个更具罪犯内涵的词来恰当地描述：监视（surveilance）。 监视让我们做一个思想实验，尝试用 监视（surveillance） 一词替换 数据（data），再看看常见的短语是不是听起来还那么漂亮【93】。比如：“在我们的监视驱动的组织中，我们收集实时监视流并将它们存储在我们的监视仓库中。我们的监视科学家使用高级分析和监视处理来获得新的见解。” 对于本书《设计监控密集型应用》而言，这个思想实验是罕见的争议性内容，但我认为需要激烈的言辞来强调这一点。在我们尝试制造软件 “吞噬世界” 的过程中【94】，我们已经建立了世界上迄今为止所见过的最伟大的大规模监视基础设施。我们正朝着万物互联迈进，我们正在迅速走近这样一个世界：每个有人居住的空间至少包含一个带互联网连接的麦克风，以智能手机、智能电视、语音控制助理设备、婴儿监视器甚至儿童玩具的形式存在，并使用基于云的语音识别。这些设备中的很多都有着可怕的安全记录【95】。 即使是最为极权与专制的政权，可能也只会想着在每个房间装一个麦克风，并强迫每个人始终携带能够追踪其位置与动向的设备。然而，我们显然是自愿地，甚至热情地投身于这个全域监视的世界。不同之处在于，数据是由公司，而不是由政府机构收集的【96】。 并不是所有的数据收集都称得上监视，但检视这一点有助于理解我们与数据收集者之间的关系。为什么我们似乎很乐意接受企业的监视呢？也许你觉得自己没有什么好隐瞒的 —— 换句话说，你与当权阶级穿一条裤子，你不是被边缘化的少数派，也不必害怕受到迫害【97】。不是每个人都如此幸运。或者，也许这是因为目的似乎是温和的 —— 这不是公然胁迫，也不是强制性的，而只是更好的推荐与更个性化的营销。但是，结合上一节中对预测性分析的讨论，这种区别似乎并不是很清晰。 我们已经看到与汽车追踪设备挂钩的汽车保险费，以及取决于需要人佩戴健身追踪设备来确定的健康保险范围。当监视被用于决定生活的重要方面时，例如保险或就业，它就开始变得不那么温和了。此外，数据分析可以揭示出令人惊讶的私密事物：例如，智能手表或健身追踪器中的运动传感器能以相当好的精度计算出你正在输入的内容（比如密码）【98】。而分析算法只会变得越来越精确。 同意与选择的自由我们可能会断言用户是自愿选择使用了会跟踪其活动的服务，而且他们已经同意了服务条款与隐私政策，因此他们同意数据收集。我们甚至可以声称，用户在用所提供的数据来 换取 有价值的服务，并且为了提供服务，追踪是必要的。毫无疑问，社交网络、搜索引擎以及各种其他免费的在线服务对于用户来说都是有价值的，但是这个说法却存在问题。 用户几乎不知道他们提供给我们的是什么数据，哪些数据被放进了数据库，数据又是怎样被保留与处理的 —— 大多数隐私政策都是模棱两可的，忽悠用户而不敢打开天窗说亮话。如果用户不了解他们的数据会发生什么，就无法给出任何有意义的同意。有时来自一个用户的数据还会提到一些关于其他人的事，而其他那些人既不是该服务的用户，也没有同意任何条款。我们在本书这一部分中讨论的衍生数据集 —— 来自整个用户群的数据，加上行为追踪与外部数据源 —— 就恰好是用户无法（在真正意义上）理解的数据类型。 而且从用户身上挖掘数据是一个单向过程，而不是真正的互惠关系，也不是公平的价值交换。用户对能用多少数据换来什么样的服务，既没有没有发言权也没有选择权：服务与用户之间的关系是非常不对称与单边的。这些条款是由服务提出的，而不是由用户提出的【99】。 对于不同意监视的用户，唯一真正管用的备选项，就是简单地不使用服务。但这个选择也不是真正自由的：如果一项服务如此受欢迎，以至于 “被大多数人认为是基本社会参与的必要条件”【99】，那么指望人们选择退出这项服务是不合理的 —— 使用它 事实上（de facto） 是强制性的。例如，在大多数西方社会群体中，携带智能手机，使用 Facebook 进行社交，以及使用 Google 查找信息已成为常态。特别是当一项服务具有网络效应时，人们选择 不 使用会产生社会成本。 因为一个服务会跟踪用户而拒绝使用它，这只是少数人才拥有的权力，他们有足够的时间与知识来了解隐私政策，并承受得起代价：错过社会参与，以及使用服务可能带来的专业机会。对于那些处境不太好的人而言，并没有真正意义上的选择：监控是不可避免的。 隐私与数据使用有时候，人们声称 “隐私已死”，理由是有些用户愿意把各种关于他们生活的事情发布到社交媒体上，有时是平凡俗套，但有时是高度私密的。但这种说法是错误的，而且是对 隐私（privacy） 一词的误解。 拥有隐私并不意味着保密一切东西；它意味着拥有选择向谁展示哪些东西的自由，要公开什么，以及要保密什么。隐私权是一项决定权：在从保密到透明的光谱上，隐私使得每个人都能决定自己想要在什么地方位于光谱上的哪个位置【99】。这是一个人自由与自主的重要方面。 当通过监控基础设施从人身上提取数据时，隐私权不一定受到损害，而是转移到了数据收集者手中。获取数据的公司实际上是说 “相信我们会用你的数据做正确的事情”，这意味着，决定要透露什么和保密什么的权利从个体手中转移到了公司手中。 这些公司反过来选择保密这些监视结果，因为揭露这些会令人毛骨悚然，并损害它们的商业模式（比其他公司更了解人）。用户的私密信息只会间接地披露，例如针对特定人群定向投放广告的工具（比如那些患有特定疾病的人群）。 即使特定用户无法从特定广告定向的人群中以个体的形式区分出来，但他们已经失去了披露一些私密信息的能动性，例如他们是否患有某种疾病。决定向谁透露什么并不是由个体按照自己的喜好决定的，而是由 公司，以利润最大化为目标来行使隐私权的。 许多公司都有一个目标，不要让人 感觉到 毛骨悚然 —— 先不说它们收集数据实际上是多么具有侵犯性，让我们先关注对用户感受的管理。这些用户感受经常被管理得很糟糕：例如，在事实上可能正确的一些东西，如果会触发痛苦的回忆，用户可能并不希望被提醒【100】。对于任何类型的数据，我们都应当考虑它出错、不可取、不合时宜的可能性，并且需要建立处理这些失效的机制。无论是 “不可取” 还是 “不合时宜”，当然都是由人的判断决定的；除非我们明确地将算法编码设计为尊重人类的需求，否则算法会无视这些概念。作为这些系统的工程师，我们必须保持谦卑，充分规划，接受这些失效。 允许在线服务的用户控制其隐私设置，例如控制其他用户可以看到哪些东西，是将一些控制交还给用户的第一步。但无论怎么设置，服务本身仍然可以不受限制地访问数据，并能以隐私策略允许的任何方式自由使用它。即使服务承诺不会将数据出售给第三方，它通常会授予自己不受限制的权利，以便在内部处理与分析数据，而且往往比用户公开可见的部分要深入的多。 这种从个体到公司的大规模隐私权转移在历史上是史无前例的【99】。监控一直存在，但它过去是昂贵的、手动的，不是可伸缩的、自动化的。信任关系一直存在，例如患者与其医生之间，或被告与其律师之间 —— 但在这些情况下，数据的使用严格受到道德，法律和监管限制的约束。互联网服务使得在未经有意义的同意下收集大量敏感信息变得容易得多，而且无需用户理解他们的私人数据到底发生了什么。 数据资产与权力由于行为数据是用户与服务交互的副产品，因此有时被称为 “数据废气” —— 暗示数据是毫无价值的废料。从这个角度来看，行为和预测性分析可以被看作是一种从数据中提取价值的回收形式，否则这些数据就会被浪费。 更准确的看法恰恰相反：从经济的角度来看，如果定向广告是服务的金主，那么关于人的行为数据就是服务的核心资产。在这种情况下，用户与之交互的应用仅仅是一种诱骗用户将更多的个人信息提供给监控基础设施的手段【99】。在线服务中经常表现出的令人愉悦的人类创造力与社会关系，十分讽刺地被数据提取机器所滥用。 个人数据是珍贵资产的说法因为数据中介的存在得到支持，这是阴影中的秘密行业，购买、聚合、分析、推断以及转售私密个人数据，主要用于市场营销【90】。初创公司按照它们的用户数量，“眼球数”，—— 即它们的监视能力来估值。 因为数据很有价值，所以很多人都想要它。当然，公司也想要它 —— 这就是为什么它们一开始就收集数据的原因。但政府也想获得它：通过秘密交易、胁迫、法律强制或者只是窃取【101】。当公司破产时，收集到的个人数据就是被出售的资产之一。而且数据安全很难保护，因此经常发生令人难堪的泄漏事件【102】。 这些观察已经导致批评者声称，数据不仅仅是一种资产，而且是一种 “有毒资产”【101】，或者至少是 “有害物质”【103】。即使我们认为自己有能力阻止数据滥用，但每当我们收集数据时，我们都需要平衡收益以及这些数据落入恶人手中的风险：计算机系统可能会被犯罪分子或敌国特务渗透，数据可能会被内鬼泄露，公司可能会落入不择手段的管理层手中，而这些管理者有着迥然不同的价值观，或者国家可能被能毫无愧色迫使我们交出数据的政权所接管。 俗话说，“知识就是力量”。更进一步，“在避免自己被审视的同时审视他人，是权力最重要的形式之一”【105】。这就是极权政府想要监控的原因：这让它们有能力控制全体居民。尽管今天的科技公司并没有公开地寻求政治权力，但是它们积累的数据与知识却给它们带来了很多权力，其中大部分是在公共监督之外偷偷进行的【106】。 回顾工业革命数据是信息时代的决定性特征。互联网，数据存储，处理和软件驱动的自动化正在对全球经济和人类社会产生重大影响。我们的日常生活与社会组织在过去十年中发生了变化，而且在未来的十年中可能会继续发生根本性的变化，所以我们会想到与工业革命对比【87,96】。 工业革命是通过重大的技术与农业进步实现的，它带来了持续的经济增长，长期的生活水平显著提高。然而它也带来了一些严重的问题：空气污染（由于烟雾和化学过程）和水污染（工业垃圾和人类垃圾）是可怖的。工厂老板生活在纷奢之中，而城市工人经常居住在非常糟糕的住房中，并且在恶劣的条件下长时间工作。童工很常见，甚至包括矿井中危险而低薪的工作。 制定保护措施花费了很长的时间，例如环境保护条例、工作场所安全条例、宣布使用童工非法以及食品卫生检查。毫无疑问，生产成本增加了，因为工厂再也不能把废物倒入河流、销售污染的食物或者剥削工人。但是整个社会都从中受益良多，我们中很少会有人想回到这些管制条例之前的日子【87】。 就像工业革命有着黑暗面需要应对一样，我们转向信息时代的过程中，也有需要应对与解决的重大问题。我相信数据的收集与使用就是其中一个问题。用 Bruce Schneier 的话来说【96】： 数据是信息时代的污染问题，保护隐私是环境挑战。几乎所有的电脑都能生产信息。它堆积在周围，开始溃烂。我们如何处理它 —— 我们如何控制它，以及如何摆脱它 —— 是信息经济健康发展的核心议题。正如我们今天回顾工业时代的早期年代，并想知道我们的祖先在忙于建设工业世界的过程时怎么能忽略污染问题；我们的孙辈在回望信息时代的早期年代时，将会就我们如何应对数据收集和滥用的挑战来评断我们。 我们应该设法让他们感到骄傲。 立法与自律数据保护法可能有助于维护个人的权利。例如，1995 年的 “欧洲数据保护指示” 规定，个人数据必须 “为特定的、明确的和合法的目的收集，而不是以与这些目的不相符的方式进一步处理”，并且数据必须 “就收集的目的而言适当、相关、不过分。“【107】。 但是，这个立法在今天的互联网环境下是否有效还是有疑问的【108】。这些规则直接否定了大数据的哲学，即最大限度地收集数据，将其与其他数据集结合起来进行试验和探索，以便产生新的洞察。探索意味着将数据用于未曾预期的目的，这与用户同意的 “特定和明确” 目的相反（如果我们可以有意义地表示同意的话）【109】。更新的规章正在制定中【89】。 那些收集了大量有关人的数据的公司反对监管，认为这是创新的负担与阻碍。在某种程度上，这种反对是有道理的。例如，分享医疗数据时，存在明显的隐私风险，但也有潜在的机遇：如果数据分析能够帮助我们实现更好的诊断或找到更好的治疗方法，能够阻止多少人的死亡【110】？过度监管可能会阻止这种突破。在这种潜在机会与风险之间找出平衡是很困难的【105】。 从根本上说，我认为我们需要科技行业在个人数据方面的文化转变。我们应该停止将用户视作待优化的指标数据，并记住他们是值得尊重、有尊严和能动性的人。我们应当在数据收集和实际处理中自我约束，以建立和维持依赖我们软件的人们的信任【111】。我们应当将教育终端用户视为己任，告诉他们我们是如何使用他们的数据的，而不是将他们蒙在鼓里。 我们应该允许每个人保留自己的隐私 —— 即，对自己数据的控制 —— 而不是通过监视来窃取这种控制权。我们控制自己数据的个体权利就像是国家公园的自然环境：如果我们不去明确地保护它、关心它，它就会被破坏。这将是公地的悲剧，我们都会因此而变得更糟。无所不在的监视并非不可避免的 —— 我们现在仍然能阻止它。 我们究竟能做到哪一步，是一个开放的问题。首先，我们不应该永久保留数据，而是一旦不再需要就立即清除数据【111,112】。清除数据与不变性的想法背道而驰（请参阅 “不变性的局限性”），但这是可以解决的问题。我所看到的一种很有前景的方法是通过加密协议来实施访问控制，而不仅仅是通过策略【113,114】。总的来说，文化与态度的改变是必要的。 本章小结在本章中，我们讨论了设计数据系统的新方式，而且也包括了我的个人观点，以及对未来的猜测。我们从这样一种观察开始：没有单种工具能高效服务所有可能的用例，因此应用必须组合使用几种不同的软件才能实现其目标。我们讨论了如何使用批处理与事件流来解决这一 数据集成（data integration） 问题，以便让数据变更在不同系统之间流动。 在这种方法中，某些系统被指定为记录系统，而其他数据则通过转换衍生自记录系统。通过这种方式，我们可以维护索引、物化视图、机器学习模型、统计摘要等等。通过使这些衍生和转换操作异步且松散耦合，能够防止一个区域中的问题扩散到系统中不相关部分，从而增加整个系统的稳健性与容错性。 将数据流表示为从一个数据集到另一个数据集的转换也有助于演化应用程序：如果你想变更其中一个处理步骤，例如变更索引或缓存的结构，则可以在整个输入数据集上重新运行新的转换代码，以便重新衍生输出。同样，出现问题时，你也可以修复代码并重新处理数据以便恢复。 这些过程与数据库内部已经完成的过程非常类似，因此我们将数据流应用的概念重新改写为，分拆（unbundling） 数据库组件，并通过组合这些松散耦合的组件来构建应用程序。 衍生状态可以通过观察底层数据的变更来更新。此外，衍生状态本身可以进一步被下游消费者观察。我们甚至可以将这种数据流一路传送至显示数据的终端用户设备，从而构建可动态更新以反映数据变更，并在离线时能继续工作的用户界面。 接下来，我们讨论了如何确保所有这些处理在出现故障时保持正确。我们看到可伸缩的强完整性保证可以通过异步事件处理来实现，通过使用端到端操作标识符使操作幂等，以及通过异步检查约束。客户端可以等到检查通过，或者不等待继续前进，但是可能会冒有违反约束需要道歉的风险。这种方法比使用分布式事务的传统方法更具可伸缩性与可靠性，并且在实践中适用于很多业务流程。 通过围绕数据流构建应用，并异步检查约束，我们可以避免绝大多数的协调工作，创建保证完整性且性能仍然表现良好的系统，即使在地理散布的情况下与出现故障时亦然。然后，我们对使用审计来验证数据完整性，以及损坏检测进行了一些讨论。 最后，我们退后一步，审视了构建数据密集型应用的一些道德问题。我们看到，虽然数据可以用来做好事，但它也可能造成很大伤害：作出严重影响人们生活的决定却难以申诉，导致歧视与剥削、监视常态化、曝光私密信息。我们也冒着数据被泄露的风险，并且可能会发现，即使是善意地使用数据也可能会导致意想不到的后果。 由于软件和数据对世界产生了如此巨大的影响，我们工程师们必须牢记，我们有责任为我们想要的那种世界而努力：一个尊重人们，尊重人性的世界。我希望我们能够一起为实现这一目标而努力。 参考文献 Rachid Belaid: “Postgres Full-Text Search is Good Enough!,” rachbelaid.com, July 13, 2015. Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: “Challenges to Adopting Stronger Consistency at Scale,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS), May 2015. Pat Helland and Dave Campbell: “Building on Quicksand,” at 4th Biennial Conference on Innovative Data Systems Research (CIDR), January 2009. Jessica Kerr: “Provenance and Causality in Distributed Systems,” blog.jessitron.com, September 25, 2016. Kostas Tzoumas: “Batch Is a Special Case of Streaming,” data-artisans.com, September 15, 2015. Shinji Kim and Robert Blafford: “Stream Windowing Performance Analysis: Concord and Spark Streaming,” concord.io, July 6, 2016. Jay Kreps: “The Log: What Every Software Engineer Should Know About Real-Time Data’s Unifying Abstraction,” engineering.linkedin.com, December 16, 2013. Pat Helland: “Life Beyond Distributed Transactions: An Apostate’s Opinion,” at 3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 2007. “Great Western Railway (1835–1948),” Network Rail Virtual Archive, networkrail.co.uk. Jacqueline Xu: “Online Migrations at Scale,” stripe.com, February 2, 2017. Molly Bartlett Dishman and Martin Fowler: “Agile Architecture,” at O’Reilly Software Architecture Conference, March 2015. Nathan Marz and James Warren: Big Data: Principles and Best Practices of Scalable Real-Time Data Systems. Manning, 2015. ISBN: 978-1-617-29034-3 Oscar Boykin, Sam Ritchie, Ian O’Connell, and Jimmy Lin: “Summingbird: A Framework for Integrating Batch and Online MapReduce Computations,” at 40th International Conference on Very Large Data Bases (VLDB), September 2014. Jay Kreps: “Questioning the Lambda Architecture,” oreilly.com, July 2, 2014. Raul Castro Fernandez, Peter Pietzuch, Jay Kreps, et al.: “Liquid: Unifying Nearline and Offline Big Data Integration,” at 7th Biennial Conference on Innovative Data Systems Research (CIDR), January 2015. Dennis M. Ritchie and Ken Thompson: “The UNIX Time-Sharing System,” Communications of the ACM, volume 17, number 7, pages 365–375, July 1974. doi:10.1145&#x2F;361011.361061 Eric A. Brewer and Joseph M. Hellerstein: “CS262a: Advanced Topics in Computer Systems,” lecture notes, University of California, Berkeley, cs.berkeley.edu, August 2011. Michael Stonebraker: “The Case for Polystores,” wp.sigmod.org, July 13, 2015. Jennie Duggan, Aaron J. Elmore, Michael Stonebraker, et al.: “The BigDAWG Polystore System,” ACM SIGMOD Record, volume 44, number 2, pages 11–16, June 2015. doi:10.1145&#x2F;2814710.2814713 Patrycja Dybka: “Foreign Data Wrappers for PostgreSQL,” vertabelo.com, March 24, 2015. David B. Lomet, Alan Fekete, Gerhard Weikum, and Mike Zwilling: “Unbundling Transaction Services in the Cloud,” at 4th Biennial Conference on Innovative Data Systems Research (CIDR), January 2009. Martin Kleppmann and Jay Kreps: “Kafka, Samza and the Unix Philosophy of Distributed Data,” IEEE Data Engineering Bulletin, volume 38, number 4, pages 4–14, December 2015. John Hugg: “Winning Now and in the Future: Where VoltDB Shines,” voltdb.com, March 23, 2016. Frank McSherry, Derek G. Murray, Rebecca Isaacs, and Michael Isard: “Differential Dataflow,” at 6th Biennial Conference on Innovative Data Systems Research (CIDR), January 2013. Derek G Murray, Frank McSherry, Rebecca Isaacs, et al.: “Naiad: A Timely Dataflow System,” at 24th ACM Symposium on Operating Systems Principles (SOSP), pages 439–455, November 2013. doi:10.1145&#x2F;2517349.2522738 Gwen Shapira: “We have a bunch of customers who are implementing ‘database inside-out’ concept and they all ask ‘is anyone else doing it? are we crazy?’” twitter.com, July 28, 2016. Martin Kleppmann: “Turning the Database Inside-out with Apache Samza,” at Strange Loop, September 2014. Peter Van Roy and Seif Haridi: Concepts, Techniques, and Models of Computer Programming. MIT Press, 2004. ISBN: 978-0-262-22069-9 “Juttle Documentation,” juttle.github.io, 2016. Evan Czaplicki and Stephen Chong: “Asynchronous Functional Reactive Programming for GUIs,” at 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), June 2013. doi:10.1145&#x2F;2491956.2462161 Engineer Bainomugisha, Andoni Lombide Carreton, Tom van Cutsem, Stijn Mostinckx, and Wolfgang de Meuter: “A Survey on Reactive Programming,” ACM Computing Surveys, volume 45, number 4, pages 1–34, August 2013. doi:10.1145&#x2F;2501654.2501666 Peter Alvaro, Neil Conway, Joseph M. Hellerstein, and William R. Marczak: “Consistency Analysis in Bloom: A CALM and Collected Approach,” at 5th Biennial Conference on Innovative Data Systems Research (CIDR), January 2011. Felienne Hermans: “Spreadsheets Are Code,” at Code Mesh, November 2015. Dan Bricklin and Bob Frankston: “VisiCalc: Information from Its Creators,” danbricklin.com. D. Sculley, Gary Holt, Daniel Golovin, et al.: “Machine Learning: The High-Interest Credit Card of Technical Debt,” at NIPS Workshop on Software Engineering for Machine Learning (SE4ML), December 2014. Peter Bailis, Alan Fekete, Michael J Franklin, et al.: “Feral Concurrency Control: An Empirical Investigation of Modern Application Integrity,” at ACM International Conference on Management of Data (SIGMOD), June 2015. doi:10.1145&#x2F;2723372.2737784 Guy Steele: “Re: Need for Macros (Was Re: Icon),” email to ll1-discuss mailing list, people.csail.mit.edu, December 24, 2001. David Gelernter: “Generative Communication in Linda,” ACM Transactions on Programming Languages and Systems (TOPLAS), volume 7, number 1, pages 80–112, January 1985. doi:10.1145&#x2F;2363.2433 Patrick Th. Eugster, Pascal A. Felber, Rachid Guerraoui, and Anne-Marie Kermarrec: “The Many Faces of Publish&#x2F;Subscribe,” ACM Computing Surveys, volume 35, number 2, pages 114–131, June 2003. doi:10.1145&#x2F;857076.857078 Ben Stopford: “Microservices in a Streaming World,” at QCon London, March 2016. Christian Posta: “Why Microservices Should Be Event Driven: Autonomy vs Authority,” blog.christianposta.com, May 27, 2016. Alex Feyerke: “Say Hello to Offline First,” hood.ie, November 5, 2013. Sebastian Burckhardt, Daan Leijen, Jonathan Protzenko, and Manuel Fähndrich: “Global Sequence Protocol: A Robust Abstraction for Replicated Shared State,” at 29th European Conference on Object-Oriented Programming (ECOOP), July 2015. doi:10.4230&#x2F;LIPIcs.ECOOP.2015.568 Mark Soper: “Clearing Up React Data Management Confusion with Flux, Redux, and Relay,” medium.com, December 3, 2015. Eno Thereska, Damian Guy, Michael Noll, and Neha Narkhede: “Unifying Stream Processing and Interactive Queries in Apache Kafka,” confluent.io, October 26, 2016. Frank McSherry: “Dataflow as Database,” github.com, July 17, 2016. Peter Alvaro: “I See What You Mean,” at Strange Loop, September 2015. Nathan Marz: “Trident: A High-Level Abstraction for Realtime Computation,” blog.twitter.com, August 2, 2012. Edi Bice: “Low Latency Web Scale Fraud Prevention with Apache Samza, Kafka and Friends,” at Merchant Risk Council MRC Vegas Conference, March 2016. Charity Majors: “The Accidental DBA,” charity.wtf, October 2, 2016. Arthur J. Bernstein, Philip M. Lewis, and Shiyong Lu: “Semantic Conditions for Correctness at Different Isolation Levels,” at 16th International Conference on Data Engineering (ICDE), February 2000. doi:10.1109&#x2F;ICDE.2000.839387 Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan: “Automating the Detection of Snapshot Isolation Anomalies,” at 33rd International Conference on Very Large Data Bases (VLDB), September 2007. Kyle Kingsbury: Jepsen blog post series, aphyr.com, 2013–2016. Michael Jouravlev: “Redirect After Post,” theserverside.com, August 1, 2004. Jerome H. Saltzer, David P. Reed, and David D. Clark: “End-to-End Arguments in System Design,” ACM Transactions on Computer Systems, volume 2, number 4, pages 277–288, November 1984. doi:10.1145&#x2F;357401.357402 Peter Bailis, Alan Fekete, Michael J. Franklin, et al.: “Coordination-Avoiding Database Systems,” Proceedings of the VLDB Endowment, volume 8, number 3, pages 185–196, November 2014. Alex Yarmula: “Strong Consistency in Manhattan,” blog.twitter.com, March 17, 2016. Douglas B Terry, Marvin M Theimer, Karin Petersen, et al.: “Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System,” at 15th ACM Symposium on Operating Systems Principles (SOSP), pages 172–182, December 1995. doi:10.1145&#x2F;224056.224070 Jim Gray: “The Transaction Concept: Virtues and Limitations,” at 7th International Conference on Very Large Data Bases (VLDB), September 1981. Hector Garcia-Molina and Kenneth Salem: “Sagas,” at ACM International Conference on Management of Data (SIGMOD), May 1987. doi:10.1145&#x2F;38713.38742 Pat Helland: “Memories, Guesses, and Apologies,” blogs.msdn.com, May 15, 2007. Yoongu Kim, Ross Daly, Jeremie Kim, et al.: “Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors,” at 41st Annual International Symposium on Computer Architecture (ISCA), June 2014. doi:10.1145&#x2F;2678373.2665726 Mark Seaborn and Thomas Dullien: “Exploiting the DRAM Rowhammer Bug to Gain Kernel Privileges,” googleprojectzero.blogspot.co.uk, March 9, 2015. Jim N. Gray and Catharine van Ingen: “Empirical Measurements of Disk Failure Rates and Error Rates,” Microsoft Research, MSR-TR-2005-166, December 2005. Annamalai Gurusami and Daniel Price: “Bug #73170: Duplicates in Unique Secondary Index Because of Fix of Bug#68021,” bugs.mysql.com, July 2014. Gary Fredericks: “Postgres Serializability Bug,” github.com, September 2015. Xiao Chen: “HDFS DataNode Scanners and Disk Checker Explained,” blog.cloudera.com, December 20, 2016. Jay Kreps: “Getting Real About Distributed System Reliability,” blog.empathybox.com, March 19, 2012. Martin Fowler: “The LMAX Architecture,” martinfowler.com, July 12, 2011. Sam Stokes: “Move Fast with Confidence,” blog.samstokes.co.uk, July 11, 2016. “Sawtooth Lake Documentation,” Intel Corporation, intelledger.github.io, 2016. Richard Gendal Brown: “Introducing R3 Corda™: A Distributed Ledger Designed for Financial Services,” gendal.me, April 5, 2016. Trent McConaghy, Rodolphe Marques, Andreas Müller, et al.: “BigchainDB: A Scalable Blockchain Database,” bigchaindb.com, June 8, 2016. Ralph C. Merkle: “A Digital Signature Based on a Conventional Encryption Function,” at CRYPTO ‘87, August 1987. doi:10.1007&#x2F;3-540-48184-2_32 Ben Laurie: “Certificate Transparency,” ACM Queue, volume 12, number 8, pages 10-19, August 2014. doi:10.1145&#x2F;2668152.2668154 Mark D. Ryan: “Enhanced Certificate Transparency and End-to-End Encrypted Mail,” at Network and Distributed System Security Symposium (NDSS), February 2014. doi:10.14722&#x2F;ndss.2014.23379 “Software Engineering Code of Ethics and Professional Practice,” Association for Computing Machinery, acm.org, 1999. François Chollet: “Software development is starting to involve important ethical choices,” twitter.com, October 30, 2016. Igor Perisic: “Making Hard Choices: The Quest for Ethics in Machine Learning,” engineering.linkedin.com, November 2016. John Naughton: “Algorithm Writers Need a Code of Conduct,” theguardian.com, December 6, 2015. Logan Kugler: “What Happens When Big Data Blunders?,” Communications of the ACM, volume 59, number 6, pages 15–16, June 2016. doi:10.1145&#x2F;2911975 Bill Davidow: “Welcome to Algorithmic Prison,” theatlantic.com, February 20, 2014. Don Peck: “They’re Watching You at Work,” theatlantic.com, December 2013. Leigh Alexander: “Is an Algorithm Any Less Racist Than a Human?” theguardian.com, August 3, 2016. Jesse Emspak: “How a Machine Learns Prejudice,” scientificamerican.com, December 29, 2016. Maciej Cegłowski: “The Moral Economy of Tech,” idlewords.com, June 2016. Cathy O’Neil: Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown Publishing, 2016. ISBN: 978-0-553-41881-1 Julia Angwin: “Make Algorithms Accountable,” nytimes.com, August 1, 2016. Bryce Goodman and Seth Flaxman: “European Union Regulations on Algorithmic Decision-Making and a ‘Right to Explanation’,” arXiv:1606.08813, August 31, 2016. “A Review of the Data Broker Industry: Collection, Use, and Sale of Consumer Data for Marketing Purposes,” Staff Report, United States Senate Committee on Commerce, Science, and Transportation, commerce.senate.gov, December 2013. Olivia Solon: “Facebook’s Failure: Did Fake News and Polarized Politics Get Trump Elected?” theguardian.com, November 10, 2016. Donella H. Meadows and Diana Wright: Thinking in Systems: A Primer. Chelsea Green Publishing, 2008. ISBN: 978-1-603-58055-7 Daniel J. Bernstein: “Listening to a ‘big data’&#x2F;‘data science’ talk,” twitter.com, May 12, 2015. Marc Andreessen: “Why Software Is Eating the World,” The Wall Street Journal, 20 August 2011. J. M. Porup: “‘Internet of Things’ Security Is Hilariously Broken and Getting Worse,” arstechnica.com, January 23, 2016. Bruce Schneier: Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World. W. W. Norton, 2015. ISBN: 978-0-393-35217-7 The Grugq: “Nothing to Hide,” grugq.tumblr.com, April 15, 2016. Tony Beltramelli: “Deep-Spying: Spying Using Smartwatch and Deep Learning,” Masters Thesis, IT University of Copenhagen, December 2015. Available at arxiv.org&#x2F;abs&#x2F;1512.05616 Shoshana Zuboff: “Big Other: Surveillance Capitalism and the Prospects of an Information Civilization,” Journal of Information Technology, volume 30, number 1, pages 75–89, April 2015.doi:10.1057&#x2F;jit.2015.5 Carina C. Zona: “Consequences of an Insightful Algorithm,” at GOTO Berlin, November 2016. Bruce Schneier: “Data Is a Toxic Asset, So Why Not Throw It Out?,” schneier.com, March 1, 2016. John E. Dunn: “The UK’s 15 Most Infamous Data Breaches,” techworld.com, November 18, 2016. Cory Scott: “Data is not toxic - which implies no benefit - but rather hazardous material, where we must balance need vs. want,” twitter.com, March 6, 2016. Bruce Schneier: “Mission Creep: When Everything Is Terrorism,” schneier.com, July 16, 2013. Lena Ulbricht and Maximilian von Grafenstein: “Big Data: Big Power Shifts?,” Internet Policy Review, volume 5, number 1, March 2016. doi:10.14763&#x2F;2016.1.406 Ellen P. Goodman and Julia Powles: “Facebook and Google: Most Powerful and Secretive Empires We’ve Ever Known,” theguardian.com, September 28, 2016. Directive 95&#x2F;46&#x2F;EC on the protection of individuals with regard to the processing of personal data and on the free movement of such data, Official Journal of the European Communities No. L 281&#x2F;31, eur-lex.europa.eu, November 1995. Brendan Van Alsenoy: “Regulating Data Protection: The Allocation of Responsibility and Risk Among Actors Involved in Personal Data Processing,” Thesis, KU Leuven Centre for IT and IP Law, August 2016. Michiel Rhoen: “Beyond Consent: Improving Data Protection Through Consumer Protection Law,” Internet Policy Review, volume 5, number 1, March 2016. doi:10.14763&#x2F;2016.1.404 Jessica Leber: “Your Data Footprint Is Affecting Your Life in Ways You Can’t Even Imagine,” fastcoexist.com, March 15, 2016. Maciej Cegłowski: “Haunted by Data,” idlewords.com, October 2015. Sam Thielman: “You Are Not What You Read: Librarians Purge User Data to Protect Privacy,” theguardian.com, January 13, 2016. Conor Friedersdorf: “Edward Snowden’s Other Motive for Leaking,” theatlantic.com, May 13, 2014. Phillip Rogaway: “The Moral Character of Cryptographic Work,” Cryptology ePrint 2015&#x2F;1162, December 2015."},{"title":"第二章：数据模型与查询语言","path":"/wiki/ddia/ch2.html","content":"语言的边界就是思想的边界。 —— 路德维奇・维特根斯坦，《逻辑哲学》（1922） 数据模型可能是软件开发中最重要的部分了，因为它们的影响如此深远：不仅仅影响着软件的编写方式，而且影响着我们的 解题思路。 多数应用使用层层叠加的数据模型构建。对于每层数据模型的关键问题是：它是如何用低一层数据模型来 表示 的？例如： 作为一名应用开发人员，你观察现实世界（里面有人员、组织、货物、行为、资金流向、传感器等），并采用对象或数据结构，以及操控那些数据结构的 API 来进行建模。那些结构通常是特定于应用程序的。 当要存储那些数据结构时，你可以利用通用数据模型来表示它们，如 JSON 或 XML 文档、关系数据库中的表或图模型。 数据库软件的工程师选定如何以内存、磁盘或网络上的字节来表示 JSON &#x2F; XML&#x2F; 关系 &#x2F; 图数据。这类表示形式使数据有可能以各种方式来查询，搜索，操纵和处理。 在更低的层次上，硬件工程师已经想出了使用电流、光脉冲、磁场或者其他东西来表示字节的方法。 一个复杂的应用程序可能会有更多的中间层次，比如基于 API 的 API，不过基本思想仍然是一样的：每个层都通过提供一个明确的数据模型来隐藏更低层次中的复杂性。这些抽象允许不同的人群有效地协作（例如数据库厂商的工程师和使用数据库的应用程序开发人员）。 数据模型种类繁多，每个数据模型都带有如何使用的设想。有些用法很容易，有些则不支持如此；有些操作运行很快，有些则表现很差；有些数据转换非常自然，有些则很麻烦。 掌握一个数据模型需要花费很多精力（想想关系数据建模有多少本书）。即便只使用一个数据模型，不用操心其内部工作机制，构建软件也是非常困难的。然而，因为数据模型对上层软件的功能（能做什么，不能做什么）有着至深的影响，所以选择一个适合的数据模型是非常重要的。 在本章中，我们将研究一系列用于数据存储和查询的通用数据模型（前面列表中的第 2 点）。特别地，我们将比较关系模型，文档模型和少量基于图形的数据模型。我们还将查看各种查询语言并比较它们的用例。在 第三章 中，我们将讨论存储引擎是如何工作的。也就是说，这些数据模型实际上是如何实现的（列表中的第 3 点）。 关系模型与文档模型现在最著名的数据模型可能是 SQL。它基于 Edgar Codd 在 1970 年提出的关系模型【1】：数据被组织成 关系（SQL 中称作 表），其中每个关系是 元组（SQL 中称作 行) 的无序集合。 关系模型曾是一个理论性的提议，当时很多人都怀疑是否能够有效实现它。然而到了 20 世纪 80 年代中期，关系数据库管理系统（RDBMSes）和 SQL 已成为大多数人们存储和查询某些常规结构的数据的首选工具。关系数据库已经持续称霸了大约 25~30 年 —— 这对计算机史来说是极其漫长的时间。 关系数据库起源于商业数据处理，在 20 世纪 60 年代和 70 年代用大型计算机来执行。从今天的角度来看，那些用例显得很平常：典型的 事务处理（将销售或银行交易，航空公司预订，库存管理信息记录在库）和 批处理（客户发票，工资单，报告）。 当时的其他数据库迫使应用程序开发人员必须考虑数据库内部的数据表示形式。关系模型致力于将上述实现细节隐藏在更简洁的接口之后。 多年来，在数据存储和查询方面存在着许多相互竞争的方法。在 20 世纪 70 年代和 80 年代初，网状模型（network model）和层次模型（hierarchical model）曾是主要的选择，但关系模型（relational model）随后占据了主导地位。对象数据库在 20 世纪 80 年代末和 90 年代初来了又去。XML 数据库在二十一世纪初出现，但只有小众采用过。关系模型的每个竞争者都在其时代产生了大量的炒作，但从来没有持续【2】。 随着电脑越来越强大和互联，它们开始用于日益多样化的目的。关系数据库非常成功地被推广到业务数据处理的原始范围之外更为广泛的用例上。你今天在网上看到的大部分内容依旧是由关系数据库来提供支持，无论是在线发布、讨论、社交网络、电子商务、游戏、软件即服务生产力应用程序等内容。 NoSQL 的诞生现在 - 2010 年代，NoSQL 开始了最新一轮尝试，试图推翻关系模型的统治地位。“NoSQL” 这个名字让人遗憾，因为实际上它并没有涉及到任何特定的技术。最初它只是作为一个醒目的 Twitter 标签，用在 2009 年一个关于分布式，非关系数据库上的开源聚会上。无论如何，这个术语触动了某些神经，并迅速在网络创业社区内外传播开来。好些有趣的数据库系统现在都与 #NoSQL 标签相关联，并且 NoSQL 被追溯性地重新解释为 不仅是 SQL（Not Only SQL） 【4】。 采用 NoSQL 数据库的背后有几个驱动因素，其中包括： 需要比关系数据库更好的可伸缩性，包括非常大的数据集或非常高的写入吞吐量 相比商业数据库产品，免费和开源软件更受偏爱 关系模型不能很好地支持一些特殊的查询操作 受挫于关系模型的限制性，渴望一种更具多动态性与表现力的数据模型【5】 不同的应用程序有不同的需求，一个用例的最佳技术选择可能不同于另一个用例的最佳技术选择。因此，在可预见的未来，关系数据库似乎可能会继续与各种非关系数据库一起使用 - 这种想法有时也被称为 混合持久化（polyglot persistence）。 对象关系不匹配目前大多数应用程序开发都使用面向对象的编程语言来开发，这导致了对 SQL 数据模型的普遍批评：如果数据存储在关系表中，那么需要一个笨拙的转换层，处于应用程序代码中的对象和表，行，列的数据库模型之间。模型之间的不连贯有时被称为 阻抗不匹配（impedance mismatch）^i。 像 ActiveRecord 和 Hibernate 这样的 对象关系映射（ORM object-relational mapping） 框架可以减少这个转换层所需的样板代码的数量，但是它们不能完全隐藏这两个模型之间的差异。 图 2-1 使用关系型模式来表示领英简介 例如，图 2-1 展示了如何在关系模式中表示简历（一个 LinkedIn 简介）。整个简介可以通过一个唯一的标识符 user_id 来标识。像 first_name 和 last_name 这样的字段每个用户只出现一次，所以可以在 User 表上将其建模为列。但是，大多数人在职业生涯中拥有多于一份的工作，人们可能有不同样的教育阶段和任意数量的联系信息。从用户到这些项目之间存在一对多的关系，可以用多种方式来表示： 传统 SQL 模型（SQL：1999 之前）中，最常见的规范化表示形式是将职位，教育和联系信息放在单独的表中，对 User 表提供外键引用，如 图 2-1 所示。 后续的 SQL 标准增加了对结构化数据类型和 XML 数据的支持；这允许将多值数据存储在单行内，并支持在这些文档内查询和索引。这些功能在 Oracle，IBM DB2，MS SQL Server 和 PostgreSQL 中都有不同程度的支持【6,7】。JSON 数据类型也得到多个数据库的支持，包括 IBM DB2，MySQL 和 PostgreSQL 【8】。 第三种选择是将职业，教育和联系信息编码为 JSON 或 XML 文档，将其存储在数据库的文本列中，并让应用程序解析其结构和内容。这种配置下，通常不能使用数据库来查询该编码列中的值。 对于一个像简历这样自包含文档的数据结构而言，JSON 表示是非常合适的：请参阅 例 2-1。JSON 比 XML 更简单。面向文档的数据库（如 MongoDB 【9】，RethinkDB 【10】，CouchDB 【11】和 Espresso【12】）支持这种数据模型。 例 2-1. 用 JSON 文档表示一个 LinkedIn 简介 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;user_id&quot;: 251, &quot;first_name&quot;: &quot;Bill&quot;, &quot;last_name&quot;: &quot;Gates&quot;, &quot;summary&quot;: &quot;Co-chair of the Bill &amp; Melinda Gates... Active blogger.&quot;, &quot;region_id&quot;: &quot;us:91&quot;, &quot;industry_id&quot;: 131, &quot;photo_url&quot;: &quot;/p/7/000/253/05b/308dd6e.jpg&quot;, &quot;positions&quot;: [ &#123; &quot;job_title&quot;: &quot;Co-chair&quot;, &quot;organization&quot;: &quot;Bill &amp; Melinda Gates Foundation&quot; &#125;, &#123; &quot;job_title&quot;: &quot;Co-founder, Chairman&quot;, &quot;organization&quot;: &quot;Microsoft&quot; &#125; ], &quot;education&quot;: [ &#123; &quot;school_name&quot;: &quot;Harvard University&quot;, &quot;start&quot;: 1973, &quot;end&quot;: 1975 &#125;, &#123; &quot;school_name&quot;: &quot;Lakeside School, Seattle&quot;, &quot;start&quot;: null, &quot;end&quot;: null &#125; ], &quot;contact_info&quot;: &#123; &quot;blog&quot;: &quot;http://thegatesnotes.com&quot;, &quot;twitter&quot;: &quot;http://twitter.com/BillGates&quot; &#125;&#125; 有一些开发人员认为 JSON 模型减少了应用程序代码和存储层之间的阻抗不匹配。不过，正如我们将在 第四章 中看到的那样，JSON 作为数据编码格式也存在问题。缺乏一个模式往往被认为是一个优势；我们将在 “文档模型中的模式灵活性” 中讨论这个问题。 JSON 表示比 图 2-1 中的多表模式具有更好的 局部性（locality）。如果在前面的关系型示例中获取简介，那需要执行多个查询（通过 user_id 查询每个表），或者在 User 表与其下属表之间混乱地执行多路连接。而在 JSON 表示中，所有相关信息都在同一个地方，一个查询就足够了。 从用户简介文件到用户职位，教育历史和联系信息，这种一对多关系隐含了数据中的一个树状结构，而 JSON 表示使得这个树状结构变得明确（见 图 2-2 ）。 图 2-2 一对多关系构建了一个树结构 多对一和多对多的关系在上一节的 例 2-1 中，region_id 和 industry_id 是以 ID，而不是纯字符串 “Greater Seattle Area” 和 “Philanthropy” 的形式给出的。为什么？ 如果用户界面用一个自由文本字段来输入区域和行业，那么将他们存储为纯文本字符串是合理的。另一方式是给出地理区域和行业的标准化的列表，并让用户从下拉列表或自动填充器中进行选择，其优势如下： 各个简介之间样式和拼写统一 避免歧义（例如，如果有几个同名的城市） 易于更新 —— 名称只存储在一个地方，如果需要更改（例如，由于政治事件而改变城市名称），很容易进行全面更新。 本地化支持 —— 当网站翻译成其他语言时，标准化的列表可以被本地化，使得地区和行业可以使用用户的语言来显示 更好的搜索 —— 例如，搜索华盛顿州的慈善家就会匹配这份简介，因为地区列表可以编码记录西雅图在华盛顿这一事实（从 “Greater Seattle Area” 这个字符串中看不出来） 存储 ID 还是文本字符串，这是个 副本（duplication） 问题。当使用 ID 时，对人类有意义的信息（比如单词：Philanthropy）只存储在一处，所有引用它的地方使用 ID（ID 只在数据库中有意义）。当直接存储文本时，对人类有意义的信息会复制在每处使用记录中。 使用 ID 的好处是，ID 对人类没有任何意义，因而永远不需要改变：ID 可以保持不变，即使它标识的信息发生变化。任何对人类有意义的东西都可能需要在将来某个时候改变 —— 如果这些信息被复制，所有的冗余副本都需要更新。这会导致写入开销，也存在不一致的风险（一些副本被更新了，还有些副本没有被更新）。去除此类重复是数据库 规范化（normalization） 的关键思想。[^ii] [^ii]: 关于关系模型的文献区分了几种不同的规范形式，但这些区别几乎没有实际意义。一个经验法则是，如果重复存储了可以存储在一个地方的值，则模式就不是 规范化（normalized） 的。 数据库管理员和开发人员喜欢争论规范化和非规范化，让我们暂时保留判断吧。在本书的 第三部分，我们将回到这个话题，探讨系统的方法用以处理缓存，非规范化和衍生数据。 不幸的是，对这些数据进行规范化需要多对一的关系（许多人生活在一个特定的地区，许多人在一个特定的行业工作），这与文档模型不太吻合。在关系数据库中，通过 ID 来引用其他表中的行是正常的，因为连接很容易。在文档数据库中，一对多树结构没有必要用连接，对连接的支持通常很弱 [^iii]。 [^iii]: 在撰写本文时，RethinkDB 支持连接，MongoDB 不支持连接，而 CouchDB 只支持预先声明的视图。 如果数据库本身不支持连接，则必须在应用程序代码中通过对数据库进行多个查询来模拟连接。（在这种情况中，地区和行业的列表可能很小，改动很少，应用程序可以简单地将其保存在内存中。不过，执行连接的工作从数据库被转移到应用程序代码上。） 此外，即便应用程序的最初版本适合无连接的文档模型，随着功能添加到应用程序中，数据会变得更加互联。例如，考虑一下对简历例子进行的一些修改： 组织和学校作为实体 在前面的描述中，organization（用户工作的公司）和 school_name（他们学习的地方）只是字符串。也许他们应该是对实体的引用呢？然后，每个组织、学校或大学都可以拥有自己的网页（标识、新闻提要等）。每个简历可以链接到它所提到的组织和学校，并且包括他们的图标和其他信息（请参阅 图 2-3 ，来自 LinkedIn 的一个例子）。 推荐 假设你想添加一个新的功能：一个用户可以为另一个用户写一个推荐。在用户的简历上显示推荐，并附上推荐用户的姓名和照片。如果推荐人更新他们的照片，那他们写的任何推荐都需要显示新的照片。因此，推荐应该拥有作者个人简介的引用。 图 2-3 公司名不仅是字符串，还是一个指向公司实体的链接（LinkedIn 截图） 图 2-4 阐明了这些新功能需要如何使用多对多关系。每个虚线矩形内的数据可以分组成一个文档，但是对单位，学校和其他用户的引用需要表示成引用，并且在查询时需要连接。 图 2-4 使用多对多关系扩展简历 文档数据库是否在重蹈覆辙？在多对多的关系和连接已常规用在关系数据库时，文档数据库和 NoSQL 重启了辩论：如何以最佳方式在数据库中表示多对多关系。那场辩论可比 NoSQL 古老得多，事实上，最早可以追溯到计算机化数据库系统。 20 世纪 70 年代最受欢迎的业务数据处理数据库是 IBM 的信息管理系统（IMS），最初是为了阿波罗太空计划的库存管理而开发的，并于 1968 年有了首次商业发布【13】。目前它仍在使用和维护，运行在 IBM 大型机的 OS&#x2F;390 上【14】。 IMS 的设计中使用了一个相当简单的数据模型，称为 层次模型（hierarchical model），它与文档数据库使用的 JSON 模型有一些惊人的相似之处【2】。它将所有数据表示为嵌套在记录中的记录树，这很像 图 2-2 的 JSON 结构。 同文档数据库一样，IMS 能良好处理一对多的关系，但是很难应对多对多的关系，并且不支持连接。开发人员必须决定是否复制（非规范化）数据或手动解决从一个记录到另一个记录的引用。这些二十世纪六七十年代的问题与现在开发人员遇到的文档数据库问题非常相似【15】。 那时人们提出了各种不同的解决方案来解决层次模型的局限性。其中最突出的两个是 关系模型（relational model，它变成了 SQL，并统治了世界）和 网状模型（network model，最初很受关注，但最终变得冷门）。这两个阵营之间的 “大辩论” 在 70 年代持续了很久时间【2】。 那两个模式解决的问题与当前的问题相关，因此值得简要回顾一下那场辩论。 网状模型网状模型由一个称为数据系统语言会议（CODASYL）的委员会进行了标准化，并被数个不同的数据库厂商实现；它也被称为 CODASYL 模型【16】。 CODASYL 模型是层次模型的推广。在层次模型的树结构中，每条记录只有一个父节点；在网络模式中，每条记录可能有多个父节点。例如，“Greater Seattle Area” 地区可能是一条记录，每个居住在该地区的用户都可以与之相关联。这允许对多对一和多对多的关系进行建模。 网状模型中记录之间的链接不是外键，而更像编程语言中的指针（同时仍然存储在磁盘上）。访问记录的唯一方法是跟随从根记录起沿这些链路所形成的路径。这被称为 访问路径（access path）。 最简单的情况下，访问路径类似遍历链表：从列表头开始，每次查看一条记录，直到找到所需的记录。但在多对多关系的情况中，数条不同的路径可以到达相同的记录，网状模型的程序员必须跟踪这些不同的访问路径。 CODASYL 中的查询是通过利用遍历记录列和跟随访问路径表在数据库中移动游标来执行的。如果记录有多个父结点（即多个来自其他记录的传入指针），则应用程序代码必须跟踪所有的各种关系。甚至 CODASYL 委员会成员也承认，这就像在 n 维数据空间中进行导航【17】。 尽管手动选择访问路径能够最有效地利用 20 世纪 70 年代非常有限的硬件功能（如磁带驱动器，其搜索速度非常慢），但这使得查询和更新数据库的代码变得复杂不灵活。无论是分层还是网状模型，如果你没有所需数据的路径，就会陷入困境。你可以改变访问路径，但是必须浏览大量手写数据库查询代码，并重写来处理新的访问路径。更改应用程序的数据模型是很难的。 关系模型相比之下，关系模型做的就是将所有的数据放在光天化日之下：一个 关系（表） 只是一个 元组（行） 的集合，仅此而已。如果你想读取数据，它没有迷宫似的嵌套结构，也没有复杂的访问路径。你可以选中符合任意条件的行，读取表中的任何或所有行。你可以通过指定某些列作为匹配关键字来读取特定行。你可以在任何表中插入一个新的行，而不必担心与其他表的外键关系 [^iv]。 [^iv]: 外键约束允许对修改进行限制，但对于关系模型这并不是必选项。即使有约束，外键连接在查询时执行，而在 CODASYL 中，连接在插入时高效完成。 在关系数据库中，查询优化器自动决定查询的哪些部分以哪个顺序执行，以及使用哪些索引。这些选择实际上是 “访问路径”，但最大的区别在于它们是由查询优化器自动生成的，而不是由程序员生成，所以我们很少需要考虑它们。 如果想按新的方式查询数据，你可以声明一个新的索引，查询会自动使用最合适的那些索引。无需更改查询来利用新的索引（请参阅 “数据查询语言”）。关系模型因此使添加应用程序新功能变得更加容易。 关系数据库的查询优化器是复杂的，已耗费了多年的研究和开发精力【18】。关系模型的一个关键洞察是：只需构建一次查询优化器，随后使用该数据库的所有应用程序都可以从中受益。如果你没有查询优化器的话，那么为特定查询手动编写访问路径比编写通用优化器更容易 —— 不过从长期看通用解决方案更好。 与文档数据库相比在一个方面，文档数据库还原为层次模型：在其父记录中存储嵌套记录（图 2-1 中的一对多关系，如 positions，education 和 contact_info），而不是在单独的表中。 但是，在表示多对一和多对多的关系时，关系数据库和文档数据库并没有根本的不同：在这两种情况下，相关项目都被一个唯一的标识符引用，这个标识符在关系模型中被称为 外键，在文档模型中称为 文档引用【9】。该标识符在读取时通过连接或后续查询来解析。迄今为止，文档数据库没有走 CODASYL 的老路。 关系型数据库与文档数据库在今日的对比将关系数据库与文档数据库进行比较时，可以考虑许多方面的差异，包括它们的容错属性（请参阅 第五章）和处理并发性（请参阅 第七章）。本章将只关注数据模型中的差异。 支持文档数据模型的主要论据是架构灵活性，因局部性而拥有更好的性能，以及对于某些应用程序而言更接近于应用程序使用的数据结构。关系模型通过为连接提供更好的支持以及支持多对一和多对多的关系来反击。 哪种数据模型更有助于简化应用代码？如果应用程序中的数据具有类似文档的结构（即，一对多关系树，通常一次性加载整个树），那么使用文档模型可能是一个好主意。将类似文档的结构分解成多个表（如 图 2-1 中的 positions、education 和 contact_info）的关系技术可能导致繁琐的模式和不必要的复杂的应用程序代码。 文档模型有一定的局限性：例如，不能直接引用文档中的嵌套的项目，而是需要说 “用户 251 的位置列表中的第二项”（很像层次模型中的访问路径）。但是，只要文件嵌套不太深，这通常不是问题。 文档数据库对连接的糟糕支持可能是个问题，也可能不是问题，这取决于应用程序。例如，如果某分析型应用程序使用一个文档数据库来记录何时何地发生了何事，那么多对多关系可能永远也用不上。【19】。 但如果你的应用程序确实会用到多对多关系，那么文档模型就没有那么诱人了。尽管可以通过反规范化来消除对连接的需求，但这需要应用程序代码来做额外的工作以确保数据一致性。尽管应用程序代码可以通过向数据库发出多个请求的方式来模拟连接，但这也将复杂性转移到应用程序中，而且通常也会比由数据库内的专用代码更慢。在这种情况下，使用文档模型可能会导致更复杂的应用代码与更差的性能【15】。 我们没有办法说哪种数据模型更有助于简化应用代码，因为它取决于数据项之间的关系种类。对高度关联的数据而言，文档模型是极其糟糕的，关系模型是可以接受的，而选用图形模型（请参阅 “图数据模型”）是最自然的。 文档模型中的模式灵活性大多数文档数据库以及关系数据库中的 JSON 支持都不会强制文档中的数据采用何种模式。关系数据库的 XML 支持通常带有可选的模式验证。没有模式意味着可以将任意的键和值添加到文档中，并且当读取时，客户端无法保证文档可能包含的字段。 文档数据库有时称为 无模式（schemaless），但这具有误导性，因为读取数据的代码通常假定某种结构 —— 即存在隐式模式，但不由数据库强制执行【20】。一个更精确的术语是 读时模式（即 schema-on-read，数据的结构是隐含的，只有在数据被读取时才被解释），相应的是 写时模式（即 schema-on-write，传统的关系数据库方法中，模式明确，且数据库确保所有的数据都符合其模式）【21】。 读时模式类似于编程语言中的动态（运行时）类型检查，而写时模式类似于静态（编译时）类型检查。就像静态和动态类型检查的相对优点具有很大的争议性一样【22】，数据库中模式的强制性是一个具有争议的话题，一般来说没有正确或错误的答案。 在应用程序想要改变其数据格式的情况下，这些方法之间的区别尤其明显。例如，假设你把每个用户的全名存储在一个字段中，而现在想分别存储名字和姓氏【23】。在文档数据库中，只需开始写入具有新字段的新文档，并在应用程序中使用代码来处理读取旧文档的情况。例如： 1234if (user &amp;&amp; user.name &amp;&amp; !user.first_name) &#123; // Documents written before Dec 8, 2013 don&#x27;t have first_name user.first_name = user.name.split(&quot; &quot;)[0];&#125; 另一方面，在 “静态类型” 数据库模式中，通常会执行以下 迁移（migration） 操作： 123ALTER TABLE users ADD COLUMN first_name text;UPDATE users SET first_name = split_part(name, &#x27; &#x27;, 1); -- PostgreSQLUPDATE users SET first_name = substring_index(name, &#x27; &#x27;, 1); -- MySQL 模式变更的速度很慢，而且要求停运。它的这种坏名誉并不是完全应得的：大多数关系数据库系统可在几毫秒内执行 ALTER TABLE 语句。MySQL 是一个值得注意的例外，它执行 ALTER TABLE 时会复制整个表，这可能意味着在更改一个大型表时会花费几分钟甚至几个小时的停机时间，尽管存在各种工具来解决这个限制【24,25,26】。 大型表上运行 UPDATE 语句在任何数据库上都可能会很慢，因为每一行都需要重写。要是不可接受的话，应用程序可以将 first_name 设置为默认值 NULL，并在读取时再填充，就像使用文档数据库一样。 当由于某种原因（例如，数据是异构的）集合中的项目并不都具有相同的结构时，读时模式更具优势。例如，如果： 存在许多不同类型的对象，将每种类型的对象放在自己的表中是不现实的。 数据的结构由外部系统决定。你无法控制外部系统且它随时可能变化。 在上述情况下，模式的坏处远大于它的帮助，无模式文档可能是一个更加自然的数据模型。但是，要是所有记录都具有相同的结构，那么模式是记录并强制这种结构的有效机制。第四章将更详细地讨论模式和模式演化。 查询的数据局部性文档通常以单个连续字符串形式进行存储，编码为 JSON、XML 或其二进制变体（如 MongoDB 的 BSON）。如果应用程序经常需要访问整个文档（例如，将其渲染至网页），那么存储局部性会带来性能优势。如果将数据分割到多个表中（如 图 2-1 所示），则需要进行多次索引查找才能将其全部检索出来，这可能需要更多的磁盘查找并花费更多的时间。 局部性仅仅适用于同时需要文档绝大部分内容的情况。数据库通常需要加载整个文档，即使只访问其中的一小部分，这对于大型文档来说是很浪费的。更新文档时，通常需要整个重写。只有不改变文档大小的修改才可以容易地原地执行。因此，通常建议保持相对小的文档，并避免增加文档大小的写入【9】。这些性能限制大大减少了文档数据库的实用场景。 值得指出的是，为了局部性而分组集合相关数据的想法并不局限于文档模型。例如，Google 的 Spanner 数据库在关系数据模型中提供了同样的局部性属性，允许模式声明一个表的行应该交错（嵌套）在父表内【27】。Oracle 类似地允许使用一个称为 多表索引集群表（multi-table index cluster tables） 的类似特性【28】。Bigtable 数据模型（用于 Cassandra 和 HBase）中的 列族（column-family） 概念与管理局部性的目的类似【29】。 在 第三章 将还会看到更多关于局部性的内容。 文档和关系数据库的融合自 2000 年代中期以来，大多数关系数据库系统（MySQL 除外）都已支持 XML。这包括对 XML 文档进行本地修改的功能，以及在 XML 文档中进行索引和查询的功能。这允许应用程序使用那种与文档数据库应当使用的非常类似的数据模型。 从 9.3 版本开始的 PostgreSQL 【8】，从 5.7 版本开始的 MySQL 以及从版本 10.5 开始的 IBM DB2【30】也对 JSON 文档提供了类似的支持级别。鉴于用在 Web APIs 的 JSON 流行趋势，其他关系数据库很可能会跟随他们的脚步并添加 JSON 支持。 在文档数据库中，RethinkDB 在其查询语言中支持类似关系的连接，一些 MongoDB 驱动程序可以自动解析数据库引用（有效地执行客户端连接，尽管这可能比在数据库中执行的连接慢，需要额外的网络往返，并且优化更少）。 随着时间的推移，关系数据库和文档数据库似乎变得越来越相似，这是一件好事：数据模型相互补充 [^v]，如果一个数据库能够处理类似文档的数据，并能够对其执行关系查询，那么应用程序就可以使用最符合其需求的功能组合。 关系模型和文档模型的混合是未来数据库一条很好的路线。 [^v]: Codd 对关系模型【1】的原始描述实际上允许在关系模式中与 JSON 文档非常相似。他称之为 非简单域（nonsimple domains）。这个想法是，一行中的值不一定是一个像数字或字符串一样的原始数据类型，也可以是一个嵌套的关系（表），因此可以把一个任意嵌套的树结构作为一个值，这很像 30 年后添加到 SQL 中的 JSON 或 XML 支持。 数据查询语言当引入关系模型时，关系模型包含了一种查询数据的新方法：SQL 是一种 声明式 查询语言，而 IMS 和 CODASYL 使用 命令式 代码来查询数据库。那是什么意思？ 许多常用的编程语言是命令式的。例如，给定一个动物物种的列表，返回列表中的鲨鱼可以这样写： 123456789function getSharks() &#123; var sharks = []; for (var i = 0; i &lt; animals.length; i++) &#123; if (animals[i].family === &quot;Sharks&quot;) &#123; sharks.push(animals[i]); &#125; &#125; return sharks;&#125; 而在关系代数中，你可以这样写： $$sharks &#x3D; \\sigma_{family &#x3D; “sharks”}(animals)$$ 其中 $\\sigma$（希腊字母西格玛）是选择操作符，只返回符合 family=&quot;shark&quot; 条件的动物。 定义 SQL 时，它紧密地遵循关系代数的结构： 1SELECT * FROM animals WHERE family =&#x27;Sharks&#x27;; 命令式语言告诉计算机以特定顺序执行某些操作。可以想象一下，逐行地遍历代码，评估条件，更新变量，并决定是否再循环一遍。 在声明式查询语言（如 SQL 或关系代数）中，你只需指定所需数据的模式 - 结果必须符合哪些条件，以及如何将数据转换（例如，排序，分组和集合） - 但不是如何实现这一目标。数据库系统的查询优化器决定使用哪些索引和哪些连接方法，以及以何种顺序执行查询的各个部分。 声明式查询语言是迷人的，因为它通常比命令式 API 更加简洁和容易。但更重要的是，它还隐藏了数据库引擎的实现细节，这使得数据库系统可以在无需对查询做任何更改的情况下进行性能提升。 例如，在本节开头所示的命令代码中，动物列表以特定顺序出现。如果数据库想要在后台回收未使用的磁盘空间，则可能需要移动记录，这会改变动物出现的顺序。数据库能否安全地执行，而不会中断查询？ SQL 示例不确保任何特定的顺序，因此不在意顺序是否改变。但是如果查询用命令式的代码来写的话，那么数据库就永远不可能确定代码是否依赖于排序。SQL 相当有限的功能性为数据库提供了更多自动优化的空间。 最后，声明式语言往往适合并行执行。现在，CPU 的速度通过核心（core）的增加变得更快，而不是以比以前更高的时钟速度运行【31】。命令代码很难在多个核心和多个机器之间并行化，因为它指定了指令必须以特定顺序执行。声明式语言更具有并行执行的潜力，因为它们仅指定结果的模式，而不指定用于确定结果的算法。在适当情况下，数据库可以自由使用查询语言的并行实现【32】。 Web 上的声明式查询声明式查询语言的优势不仅限于数据库。为了说明这一点，让我们在一个完全不同的环境中比较声明式和命令式方法：一个 Web 浏览器。 假设你有一个关于海洋动物的网站。用户当前正在查看鲨鱼页面，因此你将当前所选的导航项目 “鲨鱼” 标记为当前选中项目。 1234567891011121314151617&lt;ul&gt; &lt;li class=&quot;selected&quot;&gt; &lt;p&gt;Sharks&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Great White Shark&lt;/li&gt; &lt;li&gt;Tiger Shark&lt;/li&gt; &lt;li&gt;Hammerhead Shark&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;Whales&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blue Whale&lt;/li&gt; &lt;li&gt;Humpback Whale&lt;/li&gt; &lt;li&gt;Fin Whale&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt;&lt;/ul&gt; 现在想让当前所选页面的标题具有一个蓝色的背景，以便在视觉上突出显示。使用 CSS 实现起来非常简单： 123li.selected &gt; p &#123; background-color: blue;&#125; 这里的 CSS 选择器 li.selected &gt; p 声明了我们想要应用蓝色样式的元素的模式：即其直接父元素是具有 CSS 类 selected 的 &lt;li&gt; 元素的所有 &lt;p&gt; 元素。示例中的元素 &lt;p&gt;Sharks&lt;/p&gt; 匹配此模式，但 &lt;p&gt;Whales&lt;/p&gt; 不匹配，因为其 &lt;li&gt; 父元素缺少 class=&quot;selected&quot;。 如果使用 XSL 而不是 CSS，你可以做类似的事情： 12345&lt;xsl:template match=&quot;li[@class=&#x27;selected&#x27;]/p&quot;&gt; &lt;fo:block background-color=&quot;blue&quot;&gt; &lt;xsl:apply-templates/&gt; &lt;/fo:block&gt;&lt;/xsl:template&gt; 这里的 XPath 表达式 li[@class=&#39;selected&#39;]/p 相当于上例中的 CSS 选择器 li.selected &gt; p。CSS 和 XSL 的共同之处在于，它们都是用于指定文档样式的声明式语言。 想象一下，必须使用命令式方法的情况会是如何。在 Javascript 中，使用 文档对象模型（DOM） API，其结果可能如下所示： 123456789101112var liElements = document.getElementsByTagName(&quot;li&quot;);for (var i = 0; i &lt; liElements.length; i++) &#123; if (liElements[i].className === &quot;selected&quot;) &#123; var children = liElements[i].childNodes; for (var j = 0; j &lt; children.length; j++) &#123; var child = children[j]; if (child.nodeType === Node.ELEMENT_NODE &amp;&amp; child.tagName === &quot;P&quot;) &#123; child.setAttribute(&quot;style&quot;, &quot;background-color: blue&quot;); &#125; &#125; &#125;&#125; 这段 JavaScript 代码命令式地将元素设置为蓝色背景，但是代码看起来很糟糕。不仅比 CSS 和 XSL 等价物更长，更难理解，而且还有一些严重的问题： 如果选定的类被移除（例如，因为用户点击了不同的页面），即使代码重新运行，蓝色背景也不会被移除 - 因此该项目将保持突出显示，直到整个页面被重新加载。使用 CSS，浏览器会自动检测 li.selected &gt; p 规则何时不再适用，并在选定的类被移除后立即移除蓝色背景。 如果你想要利用新的 API（例如 document.getElementsByClassName(&quot;selected&quot;) 甚至 document.evaluate()）来提高性能，则必须重写代码。另一方面，浏览器供应商可以在不破坏兼容性的情况下提高 CSS 和 XPath 的性能。 在 Web 浏览器中，使用声明式 CSS 样式比使用 JavaScript 命令式地操作样式要好得多。类似地，在数据库中，使用像 SQL 这样的声明式查询语言比使用命令式查询 API 要好得多 [^vi]。 [^vi]: IMS 和 CODASYL 都使用命令式 API。应用程序通常使用 COBOL 代码遍历数据库中的记录，一次一条记录【2,16】。 MapReduce查询MapReduce 是一个由 Google 推广的编程模型，用于在多台机器上批量处理大规模的数据【33】。一些 NoSQL 数据存储（包括 MongoDB 和 CouchDB）支持有限形式的 MapReduce，作为在多个文档中执行只读查询的机制。 关于 MapReduce 更详细的介绍在 第十章。现在我们只简要讨论一下 MongoDB 使用的模型。 MapReduce 既不是一个声明式的查询语言，也不是一个完全命令式的查询 API，而是处于两者之间：查询的逻辑用代码片段来表示，这些代码片段会被处理框架重复性调用。它基于 map（也称为 collect）和 reduce（也称为 fold 或 inject）函数，两个函数存在于许多函数式编程语言中。 最好举例来解释 MapReduce 模型。假设你是一名海洋生物学家，每当你看到海洋中的动物时，你都会在数据库中添加一条观察记录。现在你想生成一个报告，说明你每月看到多少鲨鱼。 在 PostgreSQL 中，你可以像这样表述这个查询： 123456SELECT date_trunc(&#x27;month&#x27;, observation_timestamp) AS observation_month, sum(num_animals) AS total_animalsFROM observationsWHERE family = &#x27;Sharks&#x27;GROUP BY observation_month; date_trunc(&#39;month&#39;，timestamp) 函数用于确定包含 timestamp 的日历月份，并返回代表该月份开始的另一个时间戳。换句话说，它将时间戳舍入成最近的月份。 这个查询首先过滤观察记录，以只显示鲨鱼家族的物种，然后根据它们发生的日历月份对观察记录果进行分组，最后将在该月的所有观察记录中看到的动物数目加起来。 同样的查询用 MongoDB 的 MapReduce 功能可以按如下来表述： 1234567891011121314db.observations.mapReduce(function map() &#123; var year = this.observationTimestamp.getFullYear(); var month = this.observationTimestamp.getMonth() + 1; emit(year + &quot;-&quot; + month, this.numAnimals); &#125;, function reduce(key, values) &#123; return Array.sum(values); &#125;, &#123; query: &#123; family: &quot;Sharks&quot; &#125;, out: &quot;monthlySharkReport&quot; &#125;); 可以声明式地指定一个只考虑鲨鱼种类的过滤器（这是 MongoDB 特定的 MapReduce 扩展）。 每个匹配查询的文档都会调用一次 JavaScript 函数 map，将 this 设置为文档对象。 map 函数发出一个键（包括年份和月份的字符串，如 &quot;2013-12&quot; 或 &quot;2014-1&quot;）和一个值（该观察记录中的动物数量）。 map 发出的键值对按键来分组。对于具有相同键（即，相同的月份和年份）的所有键值对，调用一次 reduce 函数。 reduce 函数将特定月份内所有观测记录中的动物数量相加。 将最终的输出写入到 monthlySharkReport 集合中。 例如，假设 observations 集合包含这两个文档： 123456789101112&#123; observationTimestamp: Date.parse( &quot;Mon, 25 Dec 1995 12:34:56 GMT&quot;), family: &quot;Sharks&quot;, species: &quot;Carcharodon carcharias&quot;, numAnimals: 3&#125;&#123; observationTimestamp: Date.parse(&quot;Tue, 12 Dec 1995 16:17:18 GMT&quot;), family: &quot;Sharks&quot;, species: &quot;Carcharias taurus&quot;, numAnimals: 4&#125; 对每个文档都会调用一次 map 函数，结果将是 emit(&quot;1995-12&quot;,3) 和 emit(&quot;1995-12&quot;,4)。随后，以 reduce(&quot;1995-12&quot;,[3,4]) 调用 reduce 函数，将返回 7。 map 和 reduce 函数在功能上有所限制：它们必须是 纯 函数，这意味着它们只使用传递给它们的数据作为输入，它们不能执行额外的数据库查询，也不能有任何副作用。这些限制允许数据库以任何顺序运行任何功能，并在失败时重新运行它们。然而，map 和 reduce 函数仍然是强大的：它们可以解析字符串、调用库函数、执行计算等等。 MapReduce 是一个相当底层的编程模型，用于计算机集群上的分布式执行。像 SQL 这样的更高级的查询语言可以用一系列的 MapReduce 操作来实现（见 第十章），但是也有很多不使用 MapReduce 的分布式 SQL 实现。请注意，SQL 中没有任何内容限制它在单个机器上运行，而 MapReduce 在分布式查询执行上没有垄断权。 能够在查询中使用 JavaScript 代码是高级查询的一个重要特性，但这不限于 MapReduce，一些 SQL 数据库也可以用 JavaScript 函数进行扩展【34】。 MapReduce 的一个可用性问题是，必须编写两个密切合作的 JavaScript 函数，这通常比编写单个查询更困难。此外，声明式查询语言为查询优化器提供了更多机会来提高查询的性能。基于这些原因，MongoDB 2.2 添加了一种叫做 聚合管道 的声明式查询语言的支持【9】。用这种语言表述鲨鱼计数查询如下所示： 123456789db.observations.aggregate([ &#123; $match: &#123; family: &quot;Sharks&quot; &#125; &#125;, &#123; $group: &#123; _id: &#123; year: &#123; $year: &quot;$observationTimestamp&quot; &#125;, month: &#123; $month: &quot;$observationTimestamp&quot; &#125; &#125;, totalAnimals: &#123; $sum: &quot;$numAnimals&quot; &#125; &#125;&#125;]); 聚合管道语言的表现力与（前述 PostgreSQL 例子的）SQL 子集相当，但是它使用基于 JSON 的语法而不是 SQL 那种接近英文句式的语法；这种差异也许只是口味问题。这个故事的寓意是：NoSQL 系统可能会意外发现自己只是重新发明了一套经过乔装改扮的 SQL。 图数据模型如我们之前所见，多对多关系是不同数据模型之间具有区别性的重要特征。如果你的应用程序大多数的关系是一对多关系（树状结构化数据），或者大多数记录之间不存在关系，那么使用文档模型是合适的。 但是，要是多对多关系在你的数据中很常见呢？关系模型可以处理多对多关系的简单情况，但是随着数据之间的连接变得更加复杂，将数据建模为图形显得更加自然。 一个图由两种对象组成：顶点（vertices，也称为 节点，即 nodes，或 实体，即 entities），和 边（edges，也称为 关系，即 relationships，或 弧，即 arcs）。多种数据可以被建模为一个图形。典型的例子包括： 社交图谱 顶点是人，边指示哪些人彼此认识。 网络图谱 顶点是网页，边缘表示指向其他页面的 HTML 链接。 公路或铁路网络 顶点是交叉路口，边线代表它们之间的道路或铁路线。 可以将那些众所周知的算法运用到这些图上：例如，汽车导航系统搜索道路网络中两点之间的最短路径，PageRank 可以用在网络图上来确定网页的流行程度，从而确定该网页在搜索结果中的排名。 在刚刚给出的例子中，图中的所有顶点代表了相同类型的事物（人、网页或交叉路口）。不过，图并不局限于这样的同类数据：同样强大地是，图提供了一种一致的方式，用来在单个数据存储中存储完全不同类型的对象。例如，Facebook 维护一个包含许多不同类型的顶点和边的单个图：顶点表示人、地点、事件、签到和用户的评论；边表示哪些人是好友、签到发生在哪里、谁评论了什么帖子、谁参与了什么事件等等【35】。 在本节中，我们将使用 图 2-5 所示的示例。它可以从社交网络或系谱数据库中获得：它显示了两个人，来自爱达荷州的 Lucy 和来自法国 Beaune 的 Alain。他们已婚，住在伦敦。 图 2-5 图数据结构示例（框代表顶点，箭头代表边） 有几种不同但相关的方法用来构建和查询图表中的数据。在本节中，我们将讨论属性图模型（由 Neo4j，Titan 和 InfiniteGraph 实现）和三元组存储（triple-store）模型（由 Datomic、AllegroGraph 等实现）。我们将查看图的三种声明式查询语言：Cypher，SPARQL 和 Datalog。除此之外，还有像 Gremlin 【36】这样的图形查询语言和像 Pregel 这样的图形处理框架（见 第十章）。 属性图在属性图模型中，每个顶点（vertex）包括： 唯一的标识符 一组出边（outgoing edges） 一组入边（ingoing edges） 一组属性（键值对） 每条边（edge）包括： 唯一标识符 边的起点（尾部顶点，即 tail vertex） 边的终点（头部顶点，即 head vertex） 描述两个顶点之间关系类型的标签 一组属性（键值对） 可以将图存储看作由两个关系表组成：一个存储顶点，另一个存储边，如 例 2-2 所示（该模式使用 PostgreSQL JSON 数据类型来存储每个顶点或每条边的属性）。头部和尾部顶点用来存储每条边；如果你想要一组顶点的输入或输出边，你可以分别通过 head_vertex 或 tail_vertex 来查询 edges 表。 例 2-2 使用关系模式来表示属性图 123456789101112131415CREATE TABLE vertices ( vertex_id INTEGER PRIMARY KEY, properties JSON);CREATE TABLE edges ( edge_id INTEGER PRIMARY KEY, tail_vertex INTEGER REFERENCES vertices (vertex_id), head_vertex INTEGER REFERENCES vertices (vertex_id), label TEXT, properties JSON);CREATE INDEX edges_tails ON edges (tail_vertex);CREATE INDEX edges_heads ON edges (head_vertex); 关于这个模型的一些重要方面是： 任何顶点都可以有一条边连接到任何其他顶点。没有模式限制哪种事物可不可以关联。 给定任何顶点，可以高效地找到它的入边和出边，从而遍历图，即沿着一系列顶点的路径前后移动（这就是为什么 例 2-2 在 tail_vertex 和 head_vertex 列上都有索引的原因）。 通过对不同类型的关系使用不同的标签，可以在一个图中存储几种不同的信息，同时仍然保持一个清晰的数据模型。 这些特性为数据建模提供了很大的灵活性，如 图 2-5 所示。图中显示了一些传统关系模式难以表达的事情，例如不同国家的不同地区结构（法国有省和大区，美国有县和州），国中国的怪事（先忽略主权国家和民族错综复杂的烂摊子），不同的数据粒度（Lucy 现在的住所记录具体到城市，而她的出生地点只是在一个州的级别）。 你可以想象该图还能延伸出许多关于 Lucy 和 Alain 的事实，或其他人的其他更多的事实。例如，你可以用它来表示食物过敏（为每个过敏源增加一个顶点，并增加人与过敏源之间的一条边来指示一种过敏情况），并链接到过敏源，每个过敏源具有一组顶点用来显示哪些食物含有哪些物质。然后，你可以写一个查询，找出每个人吃什么是安全的。图在可演化性方面是富有优势的：当你向应用程序添加功能时，可以轻松扩展图以适应程序数据结构的变化。 Cypher 查询语言Cypher 是属性图的声明式查询语言，为 Neo4j 图形数据库而发明【37】（它是以电影 “黑客帝国” 中的一个角色来命名的，而与密码学中的加密算法无关【38】）。 例 2-3 显示了将 图 2-5 的左边部分插入图形数据库的 Cypher 查询。可以类似地添加图的其余部分，为了便于阅读而省略。每个顶点都有一个像 USA 或 Idaho 这样的符号名称，查询的其他部分可以使用这些名称在顶点之间创建边，使用箭头符号：（Idaho） - [：WITHIN] -&gt;（USA） 创建一条标记为 WITHIN 的边，Idaho 为尾节点，USA 为头节点。 例 2-3 将图 2-5 中的数据子集表示为 Cypher 查询 1234567CREATE (NAmerica:Location &#123;name:&#x27;North America&#x27;, type:&#x27;continent&#x27;&#125;), (USA:Location &#123;name:&#x27;United States&#x27;, type:&#x27;country&#x27; &#125;), (Idaho:Location &#123;name:&#x27;Idaho&#x27;, type:&#x27;state&#x27; &#125;), (Lucy:Person &#123;name:&#x27;Lucy&#x27; &#125;), (Idaho) -[:WITHIN]-&gt; (USA) -[:WITHIN]-&gt; (NAmerica), (Lucy) -[:BORN_IN]-&gt; (Idaho) 当 图 2-5 的所有顶点和边被添加到数据库后，让我们提些有趣的问题：例如，找到所有从美国移民到欧洲的人的名字。更确切地说，这里我们想要找到符合下面条件的所有顶点，并且返回这些顶点的 name 属性：该顶点拥有一条连到美国任一位置的 BORN_IN 边，和一条连到欧洲的任一位置的 LIVING_IN 边。 例 2-4 展示了如何在 Cypher 中表达这个查询。在 MATCH 子句中使用相同的箭头符号来查找图中的模式：(person) -[:BORN_IN]-&gt; () 可以匹配 BORN_IN 边的任意两个顶点。该边的尾节点被绑定了变量 person，头节点则未被绑定。 例 2-4 查找所有从美国移民到欧洲的人的 Cypher 查询： 1234MATCH (person) -[:BORN_IN]-&gt; () -[:WITHIN*0..]-&gt; (us:Location &#123;name:&#x27;United States&#x27;&#125;), (person) -[:LIVES_IN]-&gt; () -[:WITHIN*0..]-&gt; (eu:Location &#123;name:&#x27;Europe&#x27;&#125;)RETURN person.name 查询按如下来解读： 找到满足以下两个条件的所有顶点（称之为 person 顶点）： person 顶点拥有一条到某个顶点的 BORN_IN 出边。从那个顶点开始，沿着一系列 WITHIN 出边最终到达一个类型为 Location，name 属性为 United States 的顶点。 person 顶点还拥有一条 LIVES_IN 出边。沿着这条边，可以通过一系列 WITHIN 出边最终到达一个类型为 Location，name 属性为 Europe 的顶点。 对于这样的 Person 顶点，返回其 name 属性。 执行这条查询可能会有几种可行的查询路径。这里给出的描述建议首先扫描数据库中的所有人，检查每个人的出生地和居住地，然后只返回符合条件的那些人。 等价地，也可以从两个 Location 顶点开始反向地查找。假如 name 属性上有索引，则可以高效地找到代表美国和欧洲的两个顶点。然后，沿着所有 WITHIN 入边，可以继续查找出所有在美国和欧洲的位置（州、地区、城市等）。最后，查找出那些可以由 BORN_IN 或 LIVES_IN 入边到那些位置顶点的人。 通常对于声明式查询语言来说，在编写查询语句时，不需要指定执行细节：查询优化程序会自动选择预测效率最高的策略，因此你可以专注于编写应用程序的其他部分。 SQL 中的图查询例 2-2 指出，可以在关系数据库中表示图数据。但是，如果图数据已经以关系结构存储，我们是否也可以使用 SQL 查询它？ 答案是肯定的，但有些困难。在关系数据库中，你通常会事先知道在查询中需要哪些连接。在图查询中，你可能需要在找到待查找的顶点之前，遍历可变数量的边。也就是说，连接的数量事先并不确定。 在我们的例子中，这发生在 Cypher 查询中的 () -[:WITHIN*0..]-&gt; () 规则中。一个人的 LIVES_IN 边可以指向任何类型的位置：街道、城市、地区、国家等。一个城市可以在（WITHIN）一个地区内，一个地区可以在（WITHIN）在一个州内，一个州可以在（WITHIN）一个国家内，等等。LIVES_IN 边可以直接指向正在查找的位置，或者一个在位置层次结构中隔了数层的位置。 在 Cypher 中，用 WITHIN*0.. 非常简洁地表述了上述事实：“沿着 WITHIN 边，零次或多次”。它很像正则表达式中的 * 运算符。 自 SQL:1999，查询可变长度遍历路径的思想可以使用称为 递归公用表表达式（WITH RECURSIVE 语法）的东西来表示。例 2-5 显示了同样的查询 - 查找从美国移民到欧洲的人的姓名 - 在 SQL 使用这种技术（PostgreSQL、IBM DB2、Oracle 和 SQL Server 均支持）来表述。但是，与 Cypher 相比，其语法非常笨拙。 例 2-5 与示例 2-4 同样的查询，在 SQL 中使用递归公用表表达式表示 123456789101112131415161718192021222324252627282930313233WITH RECURSIVE -- in_usa 包含所有的美国境内的位置 ID in_usa(vertex_id) AS ( SELECT vertex_id FROM vertices WHERE properties -&gt;&gt; &#x27;name&#x27; = &#x27;United States&#x27; UNION SELECT edges.tail_vertex FROM edges JOIN in_usa ON edges.head_vertex = in_usa.vertex_id WHERE edges.label = &#x27;within&#x27; ), -- in_europe 包含所有的欧洲境内的位置 ID in_europe(vertex_id) AS ( SELECT vertex_id FROM vertices WHERE properties -&gt;&gt; &#x27;name&#x27; = &#x27;Europe&#x27; UNION SELECT edges.tail_vertex FROM edges JOIN in_europe ON edges.head_vertex = in_europe.vertex_id WHERE edges.label = &#x27;within&#x27; ), -- born_in_usa 包含了所有类型为 Person，且出生在美国的顶点 born_in_usa(vertex_id) AS ( SELECT edges.tail_vertex FROM edges JOIN in_usa ON edges.head_vertex = in_usa.vertex_id WHERE edges.label = &#x27;born_in&#x27; ), -- lives_in_europe 包含了所有类型为 Person，且居住在欧洲的顶点。 lives_in_europe(vertex_id) AS ( SELECT edges.tail_vertex FROM edges JOIN in_europe ON edges.head_vertex = in_europe.vertex_id WHERE edges.label = &#x27;lives_in&#x27;) SELECT vertices.properties -&gt;&gt; &#x27;name&#x27; FROM vertices JOIN born_in_usa ON vertices.vertex_id = born_in_usa.vertex_id JOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id; 首先，查找 name 属性为 United States 的顶点，将其作为 in_usa 顶点的集合的第一个元素。 从 in_usa 集合的顶点出发，沿着所有的 with_in 入边，将其尾顶点加入同一集合，不断递归直到所有 with_in 入边都被访问完毕。 同理，从 name 属性为 Europe 的顶点出发，建立 in_europe 顶点的集合。 对于 in_usa 集合中的每个顶点，根据 born_in 入边来查找出生在美国某个地方的人。 同样，对于 in_europe 集合中的每个顶点，根据 lives_in 入边来查找居住在欧洲的人。 最后，把在美国出生的人的集合与在欧洲居住的人的集合相交。 同一个查询，用某一个查询语言可以写成 4 行，而用另一个查询语言需要 29 行，这恰恰说明了不同的数据模型是为不同的应用场景而设计的。选择适合应用程序的数据模型非常重要。 三元组存储和 SPARQL三元组存储模式大体上与属性图模型相同，用不同的词来描述相同的想法。不过仍然值得讨论，因为三元组存储有很多现成的工具和语言，这些工具和语言对于构建应用程序的工具箱可能是宝贵的补充。 在三元组存储中，所有信息都以非常简单的三部分表示形式存储（主语，谓语，宾语）。例如，三元组 (吉姆, 喜欢, 香蕉) 中，吉姆 是主语，喜欢 是谓语（动词），香蕉 是对象。 三元组的主语相当于图中的一个顶点。而宾语是下面两者之一： 原始数据类型中的值，例如字符串或数字。在这种情况下，三元组的谓语和宾语相当于主语顶点上的属性的键和值。例如，(lucy, age, 33) 就像属性 &#123;“age”：33&#125; 的顶点 lucy。 图中的另一个顶点。在这种情况下，谓语是图中的一条边，主语是其尾部顶点，而宾语是其头部顶点。例如，在 (lucy, marriedTo, alain) 中主语和宾语 lucy 和 alain 都是顶点，并且谓语 marriedTo 是连接他们的边的标签。 例 2-6 展示了与 例 2-3 相同的数据，以称为 Turtle 的格式（Notation3（N3）【39】的一个子集）写成三元组。 例 2-6 图 2-5 中的数据子集，表示为 Turtle 三元组 123456789101112131415@prefix : &lt;urn:example:&gt;._:lucy a :Person._:lucy :name &quot;Lucy&quot;._:lucy :bornIn _:idaho._:idaho a :Location._:idaho :name &quot;Idaho&quot;._:idaho :type &quot;state&quot;._:idaho :within _:usa._:usa a :Location_:usa :name &quot;United States&quot;_:usa :type &quot;country&quot;._:usa :within _:namerica._:namerica a :Location_:namerica :name &quot;North America&quot;_:namerica :type :&quot;continent&quot; 在这个例子中，图的顶点被写为：_：someName。这个名字并不意味着这个文件以外的任何东西。它的存在只是帮助我们明确哪些三元组引用了同一顶点。当谓语表示边时，该宾语是一个顶点，如 _:idaho :within _:usa.。当谓语是一个属性时，该宾语是一个字符串，如 _:usa :name&quot;United States&quot; 一遍又一遍地重复相同的主语看起来相当重复，但幸运的是，可以使用分号来说明关于同一主语的多个事情。这使得 Turtle 格式相当不错，可读性强：请参阅 例 2-7。 例 2-7 一种相对例 2-6 写入数据的更为简洁的方法。 12345@prefix : &lt;urn:example:&gt;._:lucy a :Person; :name &quot;Lucy&quot;; :bornIn _:idaho._:idaho a :Location; :name &quot;Idaho&quot;; :type &quot;state&quot;; :within _:usa_:usa a :Loaction; :name &quot;United States&quot;; :type &quot;country&quot;; :within _:namerica._:namerica a :Location; :name &quot;North America&quot;; :type &quot;continent&quot;. 语义网如果你深入了解关于三元组存储的信息，可能会陷入关于语义网的讨论漩涡中。三元组存储模型其实是完全独立于语义网存在的，例如，Datomic【40】作为一种三元组存储数据库 [^vii]，从未被用于语义网中。但是，由于在很多人眼中这两者紧密相连，我们应该简要地讨论一下。 [^vii]: 从技术上讲，Datomic 使用的是五元组而不是三元组，两个额外的字段是用于版本控制的元数据 从本质上讲，语义网是一个简单且合理的想法：网站已经将信息发布为文字和图片供人类阅读，为什么不将信息作为机器可读的数据也发布给计算机呢？（基于三元组模型的）资源描述框架（RDF）【41】，被用作不同网站以统一的格式发布数据的一种机制，允许来自不同网站的数据自动合并成 一个数据网络 —— 成为一种互联网范围内的 “通用语义网数据库”。 不幸的是，语义网在二十一世纪初被过度炒作，但到目前为止没有任何迹象表明已在实践中应用，这使得许多人嗤之以鼻。它还饱受眼花缭乱的缩略词、过于复杂的标准提案和狂妄自大的困扰。 然而，如果从过去的失败中汲取教训，语义网项目还是拥有很多优秀的成果。即使你没有兴趣在语义网上发布 RDF 数据，三元组这种模型也是一种好的应用程序内部数据模型。 RDF 数据模型例 2-7 中使用的 Turtle 语言是一种用于 RDF 数据的人类可读格式。有时候，RDF 也可以以 XML 格式编写，不过完成同样的事情会相对啰嗦，请参阅 例 2-8。Turtle&#x2F;N3 是更可取的，因为它更容易阅读，像 Apache Jena 【42】这样的工具可以根据需要在不同的 RDF 格式之间进行自动转换。 例 2-8 用 RDF&#x2F;XML 语法表示例 2-7 的数据 1234567891011121314151617181920212223&lt;rdf:RDF xmlns=&quot;urn:example:&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;&gt; &lt;Location rdf:nodeID=&quot;idaho&quot;&gt; &lt;name&gt;Idaho&lt;/name&gt; &lt;type&gt;state&lt;/type&gt; &lt;within&gt; &lt;Location rdf:nodeID=&quot;usa&quot;&gt; &lt;name&gt;United States&lt;/name&gt; &lt;type&gt;country&lt;/type&gt; &lt;within&gt; &lt;Location rdf:nodeID=&quot;namerica&quot;&gt; &lt;name&gt;North America&lt;/name&gt; &lt;type&gt;continent&lt;/type&gt; &lt;/Location&gt; &lt;/within&gt; &lt;/Location&gt; &lt;/within&gt; &lt;/Location&gt; &lt;Person rdf:nodeID=&quot;lucy&quot;&gt; &lt;name&gt;Lucy&lt;/name&gt; &lt;bornIn rdf:nodeID=&quot;idaho&quot;/&gt; &lt;/Person&gt;&lt;/rdf:RDF&gt; RDF 有一些奇怪之处，因为它是为了在互联网上交换数据而设计的。三元组的主语，谓语和宾语通常是 URI。例如，谓语可能是一个 URI，如 &lt;http://my-company.com/namespace#within&gt; 或 &lt;http://my-company.com/namespace#lives_in&gt;，而不仅仅是 WITHIN 或 LIVES_IN。这个设计背后的原因为了让你能够把你的数据和其他人的数据结合起来，如果他们赋予单词 within 或者 lives_in 不同的含义，两者也不会冲突，因为它们的谓语实际上是 &lt;http://other.org/foo#within&gt; 和 &lt;http://other.org/foo#lives_in&gt;。 从 RDF 的角度来看，URL &lt;http://my-company.com/namespace&gt; 不一定需要能解析成什么东西，它只是一个命名空间。为避免与 http://URL 混淆，本节中的示例使用不可解析的 URI，如 urn：example：within。幸运的是，你只需在文件顶部对这个前缀做一次声明，后续就不用再管了。 SPARQL 查询语言SPARQL 是一种用于三元组存储的面向 RDF 数据模型的查询语言【43】（它是 SPARQL 协议和 RDF 查询语言的缩写，发音为 “sparkle”）。SPARQL 早于 Cypher，并且由于 Cypher 的模式匹配借鉴于 SPARQL，这使得它们看起来非常相似【37】。 与之前相同的查询 —— 查找从美国移民到欧洲的人 —— 使用 SPARQL 比使用 Cypher 甚至更为简洁（请参阅 例 2-9）。 例 2-9 与示例 2-4 相同的查询，用 SPARQL 表示 123456PREFIX : &lt;urn:example:&gt;SELECT ?personName WHERE &#123; ?person :name ?personName. ?person :bornIn / :within* / :name &quot;United States&quot;. ?person :livesIn / :within* / :name &quot;Europe&quot;.&#125; 结构非常相似。以下两个表达式是等价的（SPARQL 中的变量以问号开头）： 12(person) -[:BORN_IN]-&gt; () -[:WITHIN*0..]-&gt; (location) # Cypher?person :bornIn / :within* ?location. # SPARQL 因为 RDF 不区分属性和边，而只是将它们作为谓语，所以可以使用相同的语法来匹配属性。在下面的表达式中，变量 usa 被绑定到任意 name 属性为字符串值 &quot;United States&quot; 的顶点： 12(usa &#123;name:&#x27;United States&#x27;&#125;) # Cypher?usa :name &quot;United States&quot;. # SPARQL SPARQL 是一种很好的查询语言 —— 尽管它构想的语义网从未实现，但它仍然是一种可用于应用程序内部的强大工具。 图形数据库与网状模型相比较在 “文档数据库是否在重蹈覆辙？” 中，我们讨论了 CODASYL 和关系模型如何竞相解决 IMS 中的多对多关系问题。乍一看，CODASYL 的网状模型看起来与图模型相似。CODASYL 是否是图形数据库的第二个变种？ 不，他们在几个重要方面有所不同： 在 CODASYL 中，数据库有一个模式，用于指定哪种记录类型可以嵌套在其他记录类型中。在图形数据库中，不存在这样的限制：任何顶点都可以具有到其他任何顶点的边。这为应用程序适应不断变化的需求提供了更大的灵活性。 在 CODASYL 中，达到特定记录的唯一方法是遍历其中的一个访问路径。在图形数据库中，可以通过其唯一 ID 直接引用任何顶点，也可以使用索引来查找具有特定值的顶点。 在 CODASYL 中，记录的子项目是一个有序集合，所以数据库必须去管理它们的次序（这会影响存储布局），并且应用程序在插入新记录到数据库时必须关注新记录在这些集合中的位置。在图形数据库中，顶点和边是无序的（只能在查询时对结果进行排序）。 在 CODASYL 中，所有查询都是命令式的，难以编写，并且很容易因架构变化而受到破坏。在图形数据库中，你可以在命令式代码中手写遍历过程，但大多数图形数据库都支持高级声明式查询，如 Cypher 或 SPARQL。 基础：DatalogDatalog 是比 SPARQL、Cypher 更古老的语言，在 20 世纪 80 年代被学者广泛研究【44,45,46】。它在软件工程师中不太知名，但是它是重要的，因为它为以后的查询语言提供了基础。 实践中，Datalog 在有限的几个数据系统中使用：例如，它是 Datomic 【40】的查询语言，Cascalog 【47】是一种用于查询 Hadoop 大数据集的 Datalog 实现 [^viii]。 [^viii]: Datomic 和 Cascalog 使用 Datalog 的 Clojure S 表达式语法。在下面的例子中使用了一个更容易阅读的 Prolog 语法，但两者没有任何功能差异。 Datalog 的数据模型类似于三元组模式，但进行了一点泛化。把三元组写成 谓语（主语，宾语），而不是写三元语（主语，谓语，宾语）。例 2-10 显示了如何用 Datalog 写入我们的例子中的数据。 例 2-10 用 Datalog 来表示图 2-5 中的数据子集 12345678910111213name(namerica, &#x27;North America&#x27;).type(namerica, continent).name(usa, &#x27;United States&#x27;).type(usa, country).within(usa, namerica).name(idaho, &#x27;Idaho&#x27;).type(idaho, state).within(idaho, usa).name(lucy, &#x27;Lucy&#x27;).born_in(lucy, idaho). 既然已经定义了数据，我们可以像之前一样编写相同的查询，如 例 2-11 所示。它看起来与 Cypher 或 SPARQL 的语法差异较大，但请不要抗拒它。Datalog 是 Prolog 的一个子集，如果你是计算机科学专业的学生，可能已经见过 Prolog。 例 2-11 与示例 2-4 相同的查询，用 Datalog 表示 123456789101112within_recursive(Location, Name) :- name(Location, Name). /* Rule 1 */within_recursive(Location, Name) :- within(Location, Via), /* Rule 2 */ within_recursive(Via, Name).migrated(Name, BornIn, LivingIn) :- name(Person, Name), /* Rule 3 */ born_in(Person, BornLoc), within_recursive(BornLoc, BornIn), lives_in(Person, LivingLoc), within_recursive(LivingLoc, LivingIn).?- migrated(Who, &#x27;United States&#x27;, &#x27;Europe&#x27;). /* Who = &#x27;Lucy&#x27;. */ Cypher 和 SPARQL 使用 SELECT 立即跳转，但是 Datalog 一次只进行一小步。我们定义 规则，以将新谓语告诉数据库：在这里，我们定义了两个新的谓语，within_recursive 和 migrated。这些谓语不是存储在数据库中的三元组中，而是从数据或其他规则派生而来的。规则可以引用其他规则，就像函数可以调用其他函数或者递归地调用自己一样。像这样，复杂的查询可以借由小的砖瓦构建起来。 在规则中，以大写字母开头的单词是变量，谓语则用 Cypher 和 SPARQL 的方式一样来匹配。例如，name(Location, Name) 通过变量绑定 Location = namerica 和 Name =&#39;North America&#39; 可以匹配三元组 name(namerica, &#39;North America&#39;)。 要是系统可以在 :- 操作符的右侧找到与所有谓语的一个匹配，就运用该规则。当规则运用时，就好像通过 :- 的左侧将其添加到数据库（将变量替换成它们匹配的值）。 因此，一种可能的应用规则的方式是： 数据库存在 name (namerica, &#39;North America&#39;)，故运用规则 1。它生成 within_recursive (namerica, &#39;North America&#39;)。 数据库存在 within (usa, namerica)，在上一步骤中生成 within_recursive (namerica, &#39;North America&#39;)，故运用规则 2。它会产生 within_recursive (usa, &#39;North America&#39;)。 数据库存在 within (idaho, usa)，在上一步生成 within_recursive (usa, &#39;North America&#39;)，故运用规则 2。它产生 within_recursive (idaho, &#39;North America&#39;)。 通过重复应用规则 1 和 2，within_recursive 谓语可以告诉我们在数据库中包含北美（或任何其他位置名称）的所有位置。这个过程如 图 2-6 所示。 图 2-6 使用示例 2-11 中的 Datalog 规则来确定爱达荷州在北美。 现在规则 3 可以找到出生在某个地方 BornIn 的人，并住在某个地方 LivingIn。通过查询 BornIn =&#39;United States&#39; 和 LivingIn =&#39;Europe&#39;，并将此人作为变量 Who，让 Datalog 系统找出变量 Who 会出现哪些值。因此，最后得到了与早先的 Cypher 和 SPARQL 查询相同的答案。 相对于本章讨论的其他查询语言，我们需要采取不同的思维方式来思考 Datalog 方法，但这是一种非常强大的方法，因为规则可以在不同的查询中进行组合和重用。虽然对于简单的一次性查询，显得不太方便，但是它可以更好地处理数据很复杂的情况。 本章小结数据模型是一个巨大的课题，在本章中，我们快速浏览了各种不同的模型。我们没有足够的篇幅来详述每个模型的细节，但是希望这个概述足以激起你的兴趣，以更多地了解最适合你的应用需求的模型。 在历史上，数据最开始被表示为一棵大树（层次数据模型），但是这不利于表示多对多的关系，所以发明了关系模型来解决这个问题。最近，开发人员发现一些应用程序也不适合采用关系模型。新的非关系型 “NoSQL” 数据存储分化为两个主要方向： 文档数据库 主要关注自我包含的数据文档，而且文档之间的关系非常稀少。 图形数据库 用于相反的场景：任意事物之间都可能存在潜在的关联。 这三种模型（文档，关系和图形）在今天都被广泛使用，并且在各自的领域都发挥很好。一个模型可以用另一个模型来模拟 —— 例如，图数据可以在关系数据库中表示 —— 但结果往往是糟糕的。这就是为什么我们有着针对不同目的的不同系统，而不是一个单一的万能解决方案。 文档数据库和图数据库有一个共同点，那就是它们通常不会将存储的数据强制约束为特定模式，这可以使应用程序更容易适应不断变化的需求。但是应用程序很可能仍会假定数据具有一定的结构；区别仅在于模式是明确的（写入时强制）还是隐含的（读取时处理）。 每个数据模型都具有各自的查询语言或框架，我们讨论了几个例子：SQL，MapReduce，MongoDB 的聚合管道，Cypher，SPARQL 和 Datalog。我们也谈到了 CSS 和 XSL&#x2F;XPath，它们不是数据库查询语言，而包含有趣的相似之处。 虽然我们已经覆盖了很多层面，但仍然有许多数据模型没有提到。举几个简单的例子： 使用基因组数据的研究人员通常需要执行 序列相似性搜索，这意味着需要一个很长的字符串（代表一个 DNA 序列），并在一个拥有类似但不完全相同的字符串的大型数据库中寻找匹配。这里所描述的数据库都不能处理这种用法，这就是为什么研究人员编写了像 GenBank 这样的专门的基因组数据库软件的原因【48】。 粒子物理学家数十年来一直在进行大数据类型的大规模数据分析，像大型强子对撞机（LHC）这样的项目现在会处理数百 PB 的数据！在这样的规模下，需要定制解决方案来阻止硬件成本的失控【49】。 全文搜索 可以说是一种经常与数据库一起使用的数据模型。信息检索是一个很大的专业课题，我们不会在本书中详细介绍，但是我们将在第三章和第三部分中介绍搜索索引。 让我们暂时将其放在一边。在 下一章 中，我们将讨论在 实现 本章描述的数据模型时会遇到的一些权衡。 参考文献 Edgar F. Codd: “A Relational Model of Data for Large Shared Data Banks,” Communications of the ACM, volume 13, number 6, pages 377–387, June 1970. doi:10.1145&#x2F;362384.362685 Michael Stonebraker and Joseph M. Hellerstein: “What Goes Around Comes Around,” in Readings in Database Systems, 4th edition, MIT Press, pages 2–41, 2005. ISBN: 978-0-262-69314-1 Pramod J. Sadalage and Martin Fowler: NoSQL Distilled. Addison-Wesley, August 2012. ISBN: 978-0-321-82662-6 Eric Evans: “NoSQL: What’s in a Name?,” blog.sym-link.com, October 30, 2009. James Phillips: “Surprises in Our NoSQL Adoption Survey,” blog.couchbase.com, February 8, 2012. Michael Wagner: SQL&#x2F;XML:2006 – Evaluierung der Standardkonformität ausgewählter Datenbanksysteme. Diplomica Verlag, Hamburg, 2010. ISBN: 978-3-836-64609-3 “XML Data in SQL Server,” SQL Server 2012 documentation, technet.microsoft.com, 2013. “PostgreSQL 9.3.1 Documentation,” The PostgreSQL Global Development Group, 2013. “The MongoDB 2.4 Manual,” MongoDB, Inc., 2013. “RethinkDB 1.11 Documentation,” rethinkdb.com, 2013. “Apache CouchDB 1.6 Documentation,” docs.couchdb.org, 2014. Lin Qiao, Kapil Surlaker, Shirshanka Das, et al.: “On Brewing Fresh Espresso: LinkedIn’s Distributed Data Serving Platform,” at ACM International Conference on Management of Data (SIGMOD), June 2013. Rick Long, Mark Harrington, Robert Hain, and Geoff Nicholls: IMS Primer. IBM Redbook SG24-5352-00, IBM International Technical Support Organization, January 2000. Stephen D. Bartlett: “IBM’s IMS—Myths, Realities, and Opportunities,” The Clipper Group Navigator, TCG2013015LI, July 2013. Sarah Mei: “Why You Should Never Use MongoDB,” sarahmei.com, November 11, 2013. J. S. Knowles and D. M. R. Bell: “The CODASYL Model,” in Databases—Role and Structure: An Advanced Course, edited by P. M. Stocker, P. M. D. Gray, and M. P. Atkinson, pages 19–56, Cambridge University Press, 1984. ISBN: 978-0-521-25430-4 Charles W. Bachman: “The Programmer as Navigator,” Communications of the ACM, volume 16, number 11, pages 653–658, November 1973. doi:10.1145&#x2F;355611.362534 Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton: “Architecture of a Database System,” Foundations and Trends in Databases, volume 1, number 2, pages 141–259, November 2007. doi:10.1561&#x2F;1900000002 Sandeep Parikh and Kelly Stirman: “Schema Design for Time Series Data in MongoDB,” blog.mongodb.org, October 30, 2013. Martin Fowler: “Schemaless Data Structures,” martinfowler.com, January 7, 2013. Amr Awadallah: “Schema-on-Read vs. Schema-on-Write,” at Berkeley EECS RAD Lab Retreat, Santa Cruz, CA, May 2009. Martin Odersky: “The Trouble with Types,” at Strange Loop, September 2013. Conrad Irwin: “MongoDB—Confessions of a PostgreSQL Lover,” at HTML5DevConf, October 2013. “Percona Toolkit Documentation: pt-online-schema-change,” Percona Ireland Ltd., 2013. Rany Keddo, Tobias Bielohlawek, and Tobias Schmidt: “Large Hadron Migrator,” SoundCloud, 2013. Shlomi Noach: “gh-ost: GitHub’s Online Schema Migration Tool for MySQL,” githubengineering.com, August 1, 2016. James C. Corbett, Jeffrey Dean, Michael Epstein, et al.: “Spanner: Google’s Globally-Distributed Database,” at 10th USENIX Symposium on Operating System Design and Implementation (OSDI), October 2012. Donald K. Burleson: “Reduce I&#x2F;O with Oracle Cluster Tables,” dba-oracle.com. Fay Chang, Jeffrey Dean, Sanjay Ghemawat, et al.: “Bigtable: A Distributed Storage System for Structured Data,” at 7th USENIX Symposium on Operating System Design and Implementation (OSDI), November 2006. Bobbie J. Cochrane and Kathy A. McKnight: “DB2 JSON Capabilities, Part 1: Introduction to DB2 JSON,” IBM developerWorks, June 20, 2013. Herb Sutter: “The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software,” Dr. Dobb’s Journal, volume 30, number 3, pages 202-210, March 2005. Joseph M. Hellerstein: “The Declarative Imperative: Experiences and Conjectures in Distributed Logic,” Electrical Engineering and Computer Sciences, University of California at Berkeley, Tech report UCB&#x2F;EECS-2010-90, June 2010. Jeffrey Dean and Sanjay Ghemawat: “MapReduce: Simplified Data Processing on Large Clusters,” at 6th USENIX Symposium on Operating System Design and Implementation (OSDI), December 2004. Craig Kerstiens: “JavaScript in Your Postgres,” blog.heroku.com, June 5, 2013. Nathan Bronson, Zach Amsden, George Cabrera, et al.: “TAO: Facebook’s Distributed Data Store for the Social Graph,” at USENIX Annual Technical Conference (USENIX ATC), June 2013. “Apache TinkerPop3.2.3 Documentation,” tinkerpop.apache.org, October 2016. “The Neo4j Manual v2.0.0,” Neo Technology, 2013. Emil Eifrem: Twitter correspondence, January 3, 2014. David Beckett and Tim Berners-Lee: “Turtle – Terse RDF Triple Language,” W3C Team Submission, March 28, 2011. “Datomic Development Resources,” Metadata Partners, LLC, 2013. W3C RDF Working Group: “Resource Description Framework (RDF),” w3.org, 10 February 2004. “Apache Jena,” Apache Software Foundation. Steve Harris, Andy Seaborne, and Eric Prud’hommeaux: “SPARQL 1.1 Query Language,” W3C Recommendation, March 2013. Todd J. Green, Shan Shan Huang, Boon Thau Loo, and Wenchao Zhou: “Datalog and Recursive Query Processing,” Foundations and Trends in Databases, volume 5, number 2, pages 105–195, November 2013. doi:10.1561&#x2F;1900000017 Stefano Ceri, Georg Gottlob, and Letizia Tanca: “What You Always Wanted to Know About Datalog (And Never Dared to Ask),” IEEE Transactions on Knowledge and Data Engineering, volume 1, number 1, pages 146–166, March 1989. doi:10.1109&#x2F;69.43410 Serge Abiteboul, Richard Hull, and Victor Vianu: Foundations of Databases. Addison-Wesley, 1995. ISBN: 978-0-201-53771-0, available online at webdam.inria.fr&#x2F;Alice Nathan Marz: “Cascalog,” cascalog.org. Dennis A. Benson, Ilene Karsch-Mizrachi, David J. Lipman, et al.: “GenBank,” Nucleic Acids Research, volume 36, Database issue, pages D25–D30, December 2007. doi:10.1093&#x2F;nar&#x2F;gkm929 Fons Rademakers: “ROOT for Big Data Analysis,” at Workshop on the Future of Big Data Management, London, UK, June 2013."},{"title":"第三章：存储与检索","path":"/wiki/ddia/ch3.html","content":"建立秩序，省却搜索 —— 德国谚语 一个数据库在最基础的层次上需要完成两件事情：当你把数据交给数据库时，它应当把数据存储起来；而后当你向数据库要数据时，它应当把数据返回给你。 在 第二章 中，我们讨论了数据模型和查询语言，即程序员将数据录入数据库的格式，以及再次要回数据的机制。在本章中我们会从数据库的视角来讨论同样的问题：数据库如何存储我们提供的数据，以及如何在我们需要时重新找到数据。 作为程序员，为什么要关心数据库内部存储与检索的机理？你可能不会去从头开始实现自己的存储引擎，但是你 确实 需要从许多可用的存储引擎中选择一个合适的。而且为了让存储引擎能在你的工作负载类型上运行良好，你也需要大致了解存储引擎在底层究竟做了什么。 特别需要注意，针对 事务性 负载优化的和针对 分析性 负载优化的存储引擎之间存在巨大差异。稍后我们将在 “事务处理还是分析？” 一节中探讨这一区别，并在 “列式存储” 中讨论一系列针对分析性负载而优化的存储引擎。 但首先，我们将从你可能已经很熟悉的两大类数据库（传统的关系型数据库和很多所谓的 “NoSQL” 数据库）中使用的 存储引擎 来开始本章的内容。我们将研究两大类存储引擎：日志结构（log-structured） 的存储引擎，以及 面向页面（page-oriented） 的存储引擎（例如 B 树）。 驱动数据库的数据结构世界上最简单的数据库可以用两个 Bash 函数实现： 12345678#!/bin/bashdb_set () &#123; echo &quot;$1,$2&quot; &gt;&gt; database&#125;db_get () &#123; grep &quot;^$1,&quot; database | sed -e &quot;s/^$1,//&quot; | tail -n 1&#125; 这两个函数实现了键值存储的功能。执行 db_set key value 会将 键（key） 和 值（value） 存储在数据库中。键和值（几乎）可以是你喜欢的任何东西，例如，值可以是 JSON 文档。然后调用 db_get key 会查找与该键关联的最新值并将其返回。 麻雀虽小，五脏俱全： 123456$ db_set 123456 &#x27;&#123;&quot;name&quot;:&quot;London&quot;,&quot;attractions&quot;:[&quot;Big Ben&quot;,&quot;London Eye&quot;]&#125;&#x27;$ db_set 42 &#x27;&#123;&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Golden Gate Bridge&quot;]&#125;&#x27;$ db_get 42&#123;&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Golden Gate Bridge&quot;]&#125; 底层的存储格式非常简单：一个文本文件，每行包含一条逗号分隔的键值对（忽略转义问题的话，大致与 CSV 文件类似）。每次对 db_set 的调用都会向文件末尾追加记录，所以更新键的时候旧版本的值不会被覆盖 —— 因而查找最新值的时候，需要找到文件中键最后一次出现的位置（因此 db_get 中使用了 tail -n 1 )。 123456789$ db_set 42 &#x27;&#123;&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Exploratorium&quot;]&#125;&#x27;$ db_get 42&#123;&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Exploratorium&quot;]&#125;$ cat database123456,&#123;&quot;name&quot;:&quot;London&quot;,&quot;attractions&quot;:[&quot;Big Ben&quot;,&quot;London Eye&quot;]&#125;42,&#123;&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Golden Gate Bridge&quot;]&#125;42,&#123;&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Exploratorium&quot;]&#125; db_set 函数对于极其简单的场景其实有非常好的性能，因为在文件尾部追加写入通常是非常高效的。与 db_set 做的事情类似，许多数据库在内部使用了 日志（log），也就是一个 仅追加（append-only） 的数据文件。真正的数据库有更多的问题需要处理（如并发控制，回收硬盘空间以避免日志无限增长，处理错误与部分写入的记录），但基本原理是一样的。日志极其有用，我们还将在本书的其它部分重复见到它好几次。 日志（log） 这个词通常指应用日志：即应用程序输出的描述正在发生的事情的文本。本书在更普遍的意义下使用 日志 这一词：一个仅追加的记录序列。它可能压根就不是给人类看的，它可以使用二进制格式，并仅能由其他程序读取。 另一方面，如果这个数据库中有着大量记录，则这个 db_get 函数的性能会非常糟糕。每次你想查找一个键时，db_get 必须从头到尾扫描整个数据库文件来查找键的出现。用算法的语言来说，查找的开销是 O(n) ：如果数据库记录数量 n 翻了一倍，查找时间也要翻一倍。这就不好了。 为了高效查找数据库中特定键的值，我们需要一个数据结构：索引（index）。本章将介绍一系列的索引结构，并在它们之间进行比较。索引背后的大致思想是通过保存一些额外的元数据作为路标来帮助你找到想要的数据。如果你想以几种不同的方式搜索同一份数据，那么你也许需要在数据的不同部分上建立多个索引。 索引是从主数据衍生的 额外的（additional） 结构。许多数据库允许添加与删除索引，这不会影响数据的内容，而只会影响查询的性能。维护额外的结构会产生开销，特别是在写入时。写入性能很难超过简单地追加写入文件，因为追加写入是最简单的写入操作。任何类型的索引通常都会减慢写入速度，因为每次写入数据时都需要更新索引。 这是存储系统中一个重要的权衡：精心选择的索引加快了读查询的速度，但是每个索引都会拖慢写入速度。因为这个原因，数据库默认并不会索引所有的内容，而需要你，也就是程序员或数据库管理员（DBA），基于对应用的典型查询模式的了解来手动选择索引。你可以选择那些能为应用带来最大收益而且又不会引入超出必要开销的索引。 散列索引让我们从 键值数据（key-value Data） 的索引开始。这不是你可以索引的唯一数据类型，但键值数据是很常见的。在引入更复杂的索引之前，它是重要的第一步。 键值存储与在大多数编程语言中可以找到的 字典（dictionary） 类型非常相似，通常字典都是用 散列映射（hash map） 或 散列表（hash table） 实现的。散列映射在许多算法教科书中都有描述【1,2】，所以这里我们不会讨论它的工作细节。既然我们已经可以用散列映射来表示 内存中 的数据结构，为什么不使用它来索引 硬盘上 的数据呢？ 假设我们的数据存储只是一个追加写入的文件，就像前面的例子一样，那么最简单的索引策略就是：保留一个内存中的散列映射，其中每个键都映射到数据文件中的一个字节偏移量，指明了可以找到对应值的位置，如 图 3-1 所示。当你将新的键值对追加写入文件中时，还要更新散列映射，以反映刚刚写入的数据的偏移量（这同时适用于插入新键与更新现有键）。当你想查找一个值时，使用散列映射来查找数据文件中的偏移量，寻找（seek） 该位置并读取该值即可。 图 3-1 以类 CSV 格式存储键值对的日志，并使用内存散列映射进行索引。 听上去简单，但这是一个可行的方法。现实中，Bitcask 实际上就是这么做的（Riak 中默认的存储引擎）【3】。 Bitcask 提供高性能的读取和写入操作，但要求所有的键必须能放入可用内存中，因为散列映射完全保留在内存中。而数据值可以使用比可用内存更多的空间，因为可以在硬盘上通过一次硬盘查找操作来加载所需部分，如果数据文件的那部分已经在文件系统缓存中，则读取根本不需要任何硬盘 I&#x2F;O。 像 Bitcask 这样的存储引擎非常适合每个键的值经常更新的情况。例如，键可能是某个猫咪视频的网址（URL），而值可能是该视频被播放的次数（每次有人点击播放按钮时递增）。在这种类型的工作负载中，有很多写操作，但是没有太多不同的键 —— 每个键有很多的写操作，但是将所有键保存在内存中是可行的。 到目前为止，我们只是在追加写入一个文件 —— 所以如何避免最终用完硬盘空间？一种好的解决方案是，将日志分为特定大小的 段（segment），当日志增长到特定尺寸时关闭当前段文件，并开始写入一个新的段文件。然后，我们就可以对这些段进行 压缩（compaction），如 图 3-2 所示。这里的压缩意味着在日志中丢弃重复的键，只保留每个键的最近更新。 图 3-2 键值更新日志（统计猫咪视频的播放次数）的压缩，只保留每个键的最近值 而且，由于压缩经常会使得段变得很小（假设在一个段内键被平均重写了好几次），我们也可以在执行压缩的同时将多个段合并在一起，如 图 3-3 所示。段被写入后永远不会被修改，所以合并的段被写入一个新的文件。冻结段的合并和压缩可以在后台线程中完成，这个过程进行的同时，我们仍然可以继续使用旧的段文件来正常提供读写请求。合并过程完成后，我们将读取请求转换为使用新合并的段而不是旧的段 —— 然后旧的段文件就可以简单地删除掉了。 图 3-3 同时执行压缩和分段合并 每个段现在都有自己的内存散列表，将键映射到文件偏移量。为了找到一个键的值，我们首先检查最近的段的散列映射；如果键不存在，我们就检查第二个最近的段，依此类推。合并过程将保持段的数量足够小，所以查找过程不需要检查太多的散列映射。 要让这个简单的想法在实际中能工作会涉及到大量的细节。简单来说，下面几点都是实现过程中需要认真考虑的问题： 文件格式 CSV 不是日志的最佳格式。使用二进制格式更快，更简单：首先以字节为单位对字符串的长度进行编码，然后是原始的字符串（不需要转义）。 删除记录 如果要删除一个键及其关联的值，则必须在数据文件中追加一个特殊的删除记录（逻辑删除，有时被称为墓碑，即 tombstone）。当日志段被合并时，合并过程会通过这个墓碑知道要将被删除键的所有历史值都丢弃掉。 崩溃恢复 如果数据库重新启动，则内存散列映射将丢失。原则上，你可以通过从头到尾读取整个段文件并记录下来每个键的最近值来恢复每个段的散列映射。但是，如果段文件很大，可能需要很长时间，这会使服务的重启比较痛苦。 Bitcask 通过将每个段的散列映射的快照存储在硬盘上来加速恢复，可以使散列映射更快地加载到内存中。 部分写入记录 数据库随时可能崩溃，包括在将记录追加到日志的过程中。 Bitcask 文件包含校验和，允许检测和忽略日志中的这些损坏部分。 并发控制 由于写操作是以严格的顺序追加到日志中的，所以常见的实现是只有一个写入线程。也因为数据文件段是仅追加的或者说是不可变的，所以它们可以被多个线程同时读取。 乍一看，仅追加日志似乎很浪费：为什么不直接在文件里更新，用新值覆盖旧值？仅追加的设计之所以是个好的设计，有如下几个原因： 追加和分段合并都是顺序写入操作，通常比随机写入快得多，尤其是在磁性机械硬盘上。在某种程度上，顺序写入在基于闪存的 固态硬盘（SSD） 上也是好的选择【4】。我们将在“比较 B 树和 LSM 树”中进一步讨论这个问题。 如果段文件是仅追加的或不可变的，并发和崩溃恢复就简单多了。例如，当一个数据值被更新的时候发生崩溃，你不用担心文件里将会同时包含旧值和新值各自的一部分。 合并旧段的处理也可以避免数据文件随着时间的推移而碎片化的问题。 但是，散列表索引也有其局限性： 散列表必须能放进内存。如果你有非常多的键，那真是倒霉。原则上可以在硬盘上维护一个散列映射，不幸的是硬盘散列映射很难表现优秀。它需要大量的随机访问 I&#x2F;O，而后者耗尽时想要再扩充是很昂贵的，并且需要很烦琐的逻辑去解决散列冲突【5】。 范围查询效率不高。例如，你无法轻松扫描 kitty00000 和 kitty99999 之间的所有键 —— 你必须在散列映射中单独查找每个键。 在下一节中，我们将看到一个没有这些限制的索引结构。 SSTables和LSM树在 图 3-3 中，每个日志结构存储段都是一系列键值对。这些键值对按照它们写入的顺序排列，日志中稍后的值优先于日志中较早的相同键的值。除此之外，文件中键值对的顺序并不重要。 现在我们可以对段文件的格式做一个简单的改变：要求键值对的序列按键排序。乍一看，这个要求似乎打破了我们使用顺序写入的能力，我们将稍后再回到这个问题。 我们把这个格式称为 排序字符串表（Sorted String Table），简称 SSTable。我们还要求每个键只在每个合并的段文件中出现一次（压缩过程已经保证）。与使用散列索引的日志段相比，SSTable 有几个大的优势： 即使文件大于可用内存，合并段的操作仍然是简单而高效的。这种方法就像归并排序算法中使用的方法一样，如 图 3-4 所示：你开始并排读取多个输入文件，查看每个文件中的第一个键，复制最低的键（根据排序顺序）到输出文件，不断重复此步骤，将产生一个新的合并段文件，而且它也是也按键排序的。 图 3-4 合并几个 SSTable 段，只保留每个键的最新值 如果在几个输入段中出现相同的键，该怎么办？请记住，每个段都包含在一段时间内写入数据库的所有值。这意味着一个输入段中的所有值一定比另一个段中的所有值都更近（假设我们总是合并相邻的段）。当多个段包含相同的键时，我们可以保留最近段的值，并丢弃旧段中的值。 为了在文件中找到一个特定的键，你不再需要在内存中保存所有键的索引。以 图 3-5 为例：假设你正在内存中寻找键 handiwork，但是你不知道这个键在段文件中的确切偏移量。然而，你知道 handbag 和 handsome 的偏移，而且由于排序特性，你知道 handiwork 必须出现在这两者之间。这意味着你可以跳到 handbag 的偏移位置并从那里扫描，直到你找到 handiwork（或没找到，如果该文件中没有该键）。 图 3-5 具有内存索引的 SSTable 你仍然需要一个内存中的索引来告诉你一些键的偏移量，但它可以是稀疏的：每几千字节的段文件有一个键就足够了，因为几千字节可以很快地被扫描完 ^i。 由于读取请求无论如何都需要扫描所请求范围内的多个键值对，因此可以将这些记录分组为块（block），并在将其写入硬盘之前对其进行压缩（如 图 3-5 中的阴影区域所示）[^ 译注 i] 。稀疏内存索引中的每个条目都指向压缩块的开始处。除了节省硬盘空间之外，压缩还可以减少对 I&#x2F;O 带宽的使用。 [^译注i]: 这里的压缩是 compression，不是前文的 compaction，请注意区分。 构建和维护SSTables到目前为止还不错，但是如何让你的数据能够预先排好序呢？毕竟我们接收到的写入请求可能以任何顺序发生。 虽然在硬盘上维护有序结构也是可能的（请参阅 “B 树”），但在内存保存则要容易得多。有许多可以使用的众所周知的树形数据结构，例如红黑树或 AVL 树【2】。使用这些数据结构，你可以按任何顺序插入键，并按排序顺序读取它们。 现在我们可以让我们的存储引擎以如下方式工作： 有新写入时，将其添加到内存中的平衡树数据结构（例如红黑树）。这个内存树有时被称为 内存表（memtable）。 当 内存表 大于某个阈值（通常为几兆字节）时，将其作为 SSTable 文件写入硬盘。这可以高效地完成，因为树已经维护了按键排序的键值对。新的 SSTable 文件将成为数据库中最新的段。当该 SSTable 被写入硬盘时，新的写入可以在一个新的内存表实例上继续进行。 收到读取请求时，首先尝试在内存表中找到对应的键，如果没有就在最近的硬盘段中寻找，如果还没有就在下一个较旧的段中继续寻找，以此类推。 时不时地，在后台运行一个合并和压缩过程，以合并段文件并将已覆盖或已删除的值丢弃掉。 这个方案效果很好。它只会遇到一个问题：如果数据库崩溃，则最近的写入（在内存表中，但尚未写入硬盘）将丢失。为了避免这个问题，我们可以在硬盘上保存一个单独的日志，每个写入都会立即被追加到这个日志上，就像在前面的章节中所描述的那样。这个日志没有按排序顺序，但这并不重要，因为它的唯一目的是在崩溃后恢复内存表。每当内存表写出到 SSTable 时，相应的日志都可以被丢弃。 用SSTables制作LSM树这里描述的算法本质上是 LevelDB【6】和 RocksDB【7】这些键值存储引擎库所使用的技术，这些存储引擎被设计嵌入到其他应用程序中。除此之外，LevelDB 可以在 Riak 中用作 Bitcask 的替代品。在 Cassandra 和 HBase 中也使用了类似的存储引擎【8】，而且他们都受到了 Google 的 Bigtable 论文【9】（引入了术语 SSTable 和 memtable ）的启发。 这种索引结构最早由 Patrick O’Neil 等人发明，且被命名为日志结构合并树（或 LSM 树）【10】，它是基于更早之前的日志结构文件系统【11】来构建的。基于这种合并和压缩排序文件原理的存储引擎通常被称为 LSM 存储引擎。 Lucene，是一种全文搜索的索引引擎，在 Elasticsearch 和 Solr 被使用，它使用类似的方法来存储它的关键词词典【12,13】。全文索引比键值索引复杂得多，但是基于类似的想法：在搜索查询中，由一个给定的单词，找到提及单词的所有文档（网页、产品描述等）。这也是通过键值结构实现的：其中键是 单词（term），值是所有包含该单词的文档的 ID 列表（postings list）。在 Lucene 中，从词语到记录列表的这种映射保存在类似于 SSTable 的有序文件中，并根据需要在后台执行合并【14】。 性能优化与往常一样，要让存储引擎在实践中表现良好涉及到大量设计细节。例如，当查找数据库中不存在的键时，LSM 树算法可能会很慢：你必须先检查内存表，然后查看从最近的到最旧的所有的段（可能还必须从硬盘读取每一个段文件），然后才能确定这个键不存在。为了优化这种访问，存储引擎通常使用额外的布隆过滤器（Bloom filters）【15】。 （布隆过滤器是一种节省内存的数据结构，用于近似表达集合的内容，它可以告诉你数据库中是否存在某个键，从而为不存在的键节省掉许多不必要的硬盘读取操作。) 还有一些不同的策略来确定 SSTables 被压缩和合并的顺序和时间。最常见的选择是 size-tiered 和 leveled compaction。LevelDB 和 RocksDB 使用 leveled compaction（LevelDB 因此得名），HBase 使用 size-tiered，Cassandra 同时支持这两种【16】。对于 sized-tiered，较新和较小的 SSTables 相继被合并到较旧的和较大的 SSTable 中。对于 leveled compaction，key （按照分布范围）被拆分到较小的 SSTables，而较旧的数据被移动到单独的层级（level），这使得压缩（compaction）能够更加增量地进行，并且使用较少的硬盘空间。 即使有许多微妙的东西，LSM 树的基本思想 —— 保存一系列在后台合并的 SSTables —— 简单而有效。即使数据集比可用内存大得多，它仍能继续正常工作。由于数据按排序顺序存储，你可以高效地执行范围查询（扫描所有从某个最小值到某个最大值之间的所有键），并且因为硬盘写入是连续的，所以 LSM 树可以支持非常高的写入吞吐量。 B树前面讨论的日志结构索引看起来已经相当可用了，但它们却不是最常见的索引类型。使用最广泛的索引结构和日志结构索引相当不同，它就是我们接下来要讨论的 B 树。 从 1970 年被引入【17】，仅不到 10 年后就变得 “无处不在”【18】，B 树很好地经受了时间的考验。在几乎所有的关系数据库中，它们仍然是标准的索引实现，许多非关系数据库也会使用到 B 树。 像 SSTables 一样，B 树保持按键排序的键值对，这允许高效的键值查找和范围查询。但这也就是仅有的相似之处了：B 树有着非常不同的设计理念。 我们前面看到的日志结构索引将数据库分解为可变大小的段，通常是几兆字节或更大的大小，并且总是按顺序写入段。相比之下，B 树将数据库分解成固定大小的 块（block） 或 分页（page），传统上大小为 4KB（有时会更大），并且一次只能读取或写入一个页面。这种设计更接近于底层硬件，因为硬盘空间也是按固定大小的块来组织的。 每个页面都可以使用地址或位置来标识，这允许一个页面引用另一个页面 —— 类似于指针，但在硬盘而不是在内存中。我们可以使用这些页面引用来构建一个页面树，如 图 3-6 所示。 图 3-6 使用 B 树索引查找一个键 一个页面会被指定为 B 树的根；在索引中查找一个键时，就从这里开始。该页面包含几个键和对子页面的引用。每个子页面负责一段连续范围的键，根页面上每两个引用之间的键，表示相邻子页面管理的键的范围（边界）。 在 图 3-6 的例子中，我们正在寻找键 251 ，所以我们知道我们需要跟踪边界 200 和 300 之间的页面引用。这将我们带到一个类似的页面，进一步将 200 到 300 的范围拆分到子范围。 最终，我们将到达某个包含单个键的页面（叶子页面，leaf page），该页面或者直接包含每个键的值，或者包含了对可以找到值的页面的引用。 在 B 树的一个页面中对子页面的引用的数量称为 分支因子（branching factor）。例如，在 图 3-6 中，分支因子是 6。在实践中，分支因子的大小取决于存储页面引用和范围边界所需的空间，但这个值通常是几百。 如果要更新 B 树中现有键的值，需要搜索包含该键的叶子页面，更改该页面中的值，并将该页面写回到硬盘（对该页面的任何引用都将保持有效）。如果你想添加一个新的键，你需要找到其范围能包含新键的页面，并将其添加到该页面。如果页面中没有足够的可用空间容纳新键，则将其分成两个半满页面，并更新父页面以反映新的键范围分区，如 图 3-7 所示 [^ii]。 图 3-7 通过分割页面来生长 B 树 [^ii]: 向 B 树中插入一个新的键是相当符合直觉的，但删除一个键（同时保持树平衡）就会牵扯很多其他东西了【2】。 这个算法可以确保树保持平衡：具有 n 个键的 B 树总是具有 $O (log n)$ 的深度。大多数数据库可以放入一个三到四层的 B 树，所以你不需要追踪多个页面引用来找到你正在查找的页面（分支因子为 500 的 4KB 页面的四层树可以存储多达 256TB 的数据）。 让B树更可靠B 树的基本底层写操作是用新数据覆写硬盘上的页面，并假定覆写不改变页面的位置：即，当页面被覆写时，对该页面的所有引用保持完整。这与日志结构索引（如 LSM 树）形成鲜明对比，后者只追加到文件（并最终删除过时的文件），但从不修改文件中已有的内容。 你可以把覆写硬盘上的页面对应为实际的硬件操作。在磁性硬盘驱动器上，这意味着将磁头移动到正确的位置，等待旋转盘上的正确位置出现，然后用新的数据覆写适当的扇区。在固态硬盘上，由于 SSD 必须一次擦除和重写相当大的存储芯片块，所以会发生更复杂的事情【19】。 而且，一些操作需要覆写几个不同的页面。例如，如果因为插入导致页面过满而拆分页面，则需要写入新拆分的两个页面，并覆写其父页面以更新对两个子页面的引用。这是一个危险的操作，因为如果数据库在系列操作进行到一半时崩溃，那么最终将导致一个损坏的索引（例如，可能有一个孤儿页面没有被任何页面引用） 。 为了使数据库能处理异常崩溃的场景，B 树实现通常会带有一个额外的硬盘数据结构：预写式日志（WAL，即 write-ahead log，也称为 重做日志，即 redo log）。这是一个仅追加的文件，每个 B 树的修改在其能被应用到树本身的页面之前都必须先写入到该文件。当数据库在崩溃后恢复时，这个日志将被用来使 B 树恢复到一致的状态【5,20】。 另外还有一个更新页面的复杂情况是，如果多个线程要同时访问 B 树，则需要仔细的并发控制 —— 否则线程可能会看到树处于不一致的状态。这通常是通过使用 锁存器（latches，轻量级锁）保护树的数据结构来完成。日志结构化的方法在这方面更简单，因为它们在后台进行所有的合并，而不会干扰新接收到的查询，并且能够时不时地将段文件切换为新的（该切换是原子操作）。 B树的优化由于 B 树已经存在了很久，所以并不奇怪这么多年下来有很多优化的设计被开发出来，仅举几例： 不同于覆写页面并维护 WAL 以支持崩溃恢复，一些数据库（如 LMDB）使用写时复制方案【21】。经过修改的页面被写入到不同的位置，并且还在树中创建了父页面的新版本，以指向新的位置。这种方法对于并发控制也很有用，我们将在 “快照隔离和可重复读” 中看到。 我们可以通过不存储整个键，而是缩短其大小，来节省页面空间。特别是在树内部的页面上，键只需要提供足够的信息来充当键范围之间的边界。在页面中包含更多的键允许树具有更高的分支因子，因此也就允许更少的层级 [^iii]。 通常，页面可以放置在硬盘上的任何位置；没有什么要求相邻键范围的页面也放在硬盘上相邻的区域。如果某个查询需要按照排序顺序扫描大部分的键范围，那么这种按页面存储的布局可能会效率低下，因为每个页面的读取都需要执行一次硬盘查找。因此，许多 B 树的实现在布局树时会尽量使叶子页面按顺序出现在硬盘上。但是，随着树的增长，要维持这个顺序是很困难的。相比之下，由于 LSM 树在合并过程中一次性重写一大段存储，所以它们更容易使顺序键在硬盘上连续存储。 额外的指针被添加到树中。例如，每个叶子页面可以引用其左边和右边的兄弟页面，使得不用跳回父页面就能按顺序对键进行扫描。 B 树的变体如 分形树（fractal trees）【22】借用了一些日志结构的思想来减少硬盘查找（而且它们与分形无关）。 [^iii]: 这个变种有时被称为 B+ 树，但因为这个优化已被广泛使用，所以经常无法区分于其它的 B 树变种。 比较B树和LSM树尽管 B 树实现通常比 LSM 树实现更成熟，但 LSM 树由于性能特征也非常有趣。根据经验，通常 LSM 树的写入速度更快，而 B 树的读取速度更快【23】。 LSM 树上的读取通常比较慢，因为它们必须检查几种不同的数据结构和不同压缩（Compaction）层级的 SSTables。 然而，基准测试的结果通常和工作负载的细节相关。你需要用你特有的工作负载来测试系统，以便进行有效的比较。在本节中，我们将简要讨论一些在衡量存储引擎性能时值得考虑的事情。 LSM树的优点B 树索引中的每块数据都必须至少写入两次：一次写入预先写入日志（WAL），一次写入树页面本身（如果有分页还需要再写入一次）。即使在该页面中只有几个字节发生了变化，也需要接受写入整个页面的开销。有些存储引擎甚至会覆写同一个页面两次，以免在电源故障的情况下页面未完整更新【24,25】。 由于反复压缩和合并 SSTables，日志结构索引也会多次重写数据。这种影响 —— 在数据库的生命周期中每笔数据导致对硬盘的多次写入 —— 被称为 写入放大（write amplification）。使用固态硬盘的机器需要额外关注这点，固态硬盘的闪存寿命在覆写有限次数后就会耗尽。 在写入繁重的应用程序中，性能瓶颈可能是数据库可以写入硬盘的速度。在这种情况下，写放大会导致直接的性能代价：存储引擎写入硬盘的次数越多，可用硬盘带宽内它能处理的每秒写入次数就越少。 进而，LSM 树通常能够比 B 树支持更高的写入吞吐量，部分原因是它们有时具有较低的写放大（尽管这取决于存储引擎的配置和工作负载），部分是因为它们顺序地写入紧凑的 SSTable 文件而不是必须覆写树中的几个页面【26】。这种差异在机械硬盘上尤其重要，其顺序写入比随机写入要快得多。 LSM 树可以被压缩得更好，因此通常能比 B 树在硬盘上产生更小的文件。B 树存储引擎会由于碎片化（fragmentation）而留下一些未使用的硬盘空间：当页面被拆分或某行不能放入现有页面时，页面中的某些空间仍未被使用。由于 LSM 树不是面向页面的，并且会通过定期重写 SSTables 以去除碎片，所以它们具有较低的存储开销，特别是当使用分层压缩（leveled compaction）时【27】。 在许多固态硬盘上，固件内部使用了日志结构化算法，以将随机写入转变为顺序写入底层存储芯片，因此存储引擎写入模式的影响不太明显【19】。但是，较低的写入放大率和减少的碎片仍然对固态硬盘更有利：更紧凑地表示数据允许在可用的 I&#x2F;O 带宽内处理更多的读取和写入请求。 LSM树的缺点日志结构存储的缺点是压缩过程有时会干扰正在进行的读写操作。尽管存储引擎尝试增量地执行压缩以尽量不影响并发访问，但是硬盘资源有限，所以很容易发生某个请求需要等待硬盘先完成昂贵的压缩操作。对吞吐量和平均响应时间的影响通常很小，但是日志结构化存储引擎在更高百分位的响应时间（请参阅 “描述性能”）有时会相当长，而 B 树的行为则相对更具有可预测性【28】。 压缩的另一个问题出现在高写入吞吐量时：硬盘的有限写入带宽需要在初始写入（记录日志和刷新内存表到硬盘）和在后台运行的压缩线程之间共享。写入空数据库时，可以使用全硬盘带宽进行初始写入，但数据库越大，压缩所需的硬盘带宽就越多。 如果写入吞吐量很高，并且压缩没有仔细配置好，有可能导致压缩跟不上写入速率。在这种情况下，硬盘上未合并段的数量不断增加，直到硬盘空间用完，读取速度也会减慢，因为它们需要检查更多的段文件。通常情况下，即使压缩无法跟上，基于 SSTable 的存储引擎也不会限制传入写入的速率，所以你需要进行明确的监控来检测这种情况【29,30】。 B 树的一个优点是每个键只存在于索引中的一个位置，而日志结构化的存储引擎可能在不同的段中有相同键的多个副本。这个方面使得 B 树在想要提供强大的事务语义的数据库中很有吸引力：在许多关系数据库中，事务隔离是通过在键范围上使用锁来实现的，在 B 树索引中，这些锁可以直接附加到树上【5】。在 第七章 中，我们将更详细地讨论这一点。 B 树在数据库架构中是非常根深蒂固的，为许多工作负载都提供了始终如一的良好性能，所以它们不可能在短期内消失。在新的数据库中，日志结构化索引变得越来越流行。没有简单易行的办法来判断哪种类型的存储引擎对你的使用场景更好，所以需要通过一些测试来得到相关经验。 其他索引结构到目前为止，我们只讨论了键值索引，它们就像关系模型中的 主键（primary key） 索引。主键唯一标识关系表中的一行，或文档数据库中的一个文档或图形数据库中的一个顶点。数据库中的其他记录可以通过其主键（或 ID）引用该行 &#x2F; 文档 &#x2F; 顶点，索引就被用于解析这样的引用。 次级索引（secondary indexes）也很常见。在关系数据库中，你可以使用 CREATE INDEX 命令在同一个表上创建多个次级索引，而且这些索引通常对于有效地执行联接（join）而言至关重要。例如，在 第二章 中的 图 2-1 中，很可能在 user_id 列上有一个次级索引，以便你可以在每个表中找到属于同一用户的所有行。 次级索引可以很容易地从键值索引构建。次级索引主要的不同是键不是唯一的，即可能有许多行（文档，顶点）具有相同的键。这可以通过两种方式来解决：将匹配行标识符的列表作为索引里的值（就像全文索引中的记录列表），或者通过向每个键添加行标识符来使键唯一。无论哪种方式，B 树和日志结构索引都可以用作次级索引。 将值存储在索引中索引中的键是查询要搜索的内容，而其值可以是以下两种情况之一：它可以是实际的行（文档，顶点），也可以是对存储在别处的行的引用。在后一种情况下，行被存储的地方被称为 堆文件（heap file），并且存储的数据没有特定的顺序（它可以是仅追加的，或者它可以跟踪被删除的行以便后续可以用新的数据进行覆盖）。堆文件方法很常见，因为它避免了在存在多个次级索引时对数据的复制：每个索引只引用堆文件中的一个位置，实际的数据都保存在一个地方。 在不更改键的情况下更新值时，堆文件方法可以非常高效：只要新值的字节数不大于旧值，就可以覆盖该记录。如果新值更大，情况会更复杂，因为它可能需要移到堆中有足够空间的新位置。在这种情况下，要么所有的索引都需要更新，以指向记录的新堆位置，或者在旧堆位置留下一个转发指针【5】。 在某些情况下，从索引到堆文件的额外跳跃对读取来说性能损失太大，因此可能希望将被索引的行直接存储在索引中。这被称为聚集索引（clustered index）。例如，在 MySQL 的 InnoDB 存储引擎中，表的主键总是一个聚集索引，次级索引则引用主键（而不是堆文件中的位置）【31】。在 SQL Server 中，可以为每个表指定一个聚集索引【32】。 在 聚集索引（在索引中存储所有的行数据）和 非聚集索引（仅在索引中存储对数据的引用）之间的折衷被称为 覆盖索引（covering index） 或 包含列的索引（index with included columns），其在索引内存储表的一部分列【33】。这允许通过单独使用索引来处理一些查询（这种情况下，可以说索引 覆盖（cover） 了查询）【32】。 与任何类型的数据重复一样，聚集索引和覆盖索引可以加快读取速度，但是它们需要额外的存储空间，并且会增加写入开销。数据库还需要额外的努力来执行事务保证，因为应用程序不应看到任何因为使用副本而导致的不一致。 多列索引至今讨论的索引只是将一个键映射到一个值。如果我们需要同时查询一个表中的多个列（或文档中的多个字段），这显然是不够的。 最常见的多列索引被称为 连接索引（concatenated index） ，它通过将一列的值追加到另一列后面，简单地将多个字段组合成一个键（索引定义中指定了字段的连接顺序）。这就像一个老式的纸质电话簿，它提供了一个从（姓氏，名字）到电话号码的索引。由于排序顺序，索引可以用来查找所有具有特定姓氏的人，或所有具有特定姓氏 - 名字组合的人。但如果你想找到所有具有特定名字的人，这个索引是没有用的。 多维索引（multi-dimensional index） 是一种查询多个列的更一般的方法，这对于地理空间数据尤为重要。例如，餐厅搜索网站可能有一个数据库，其中包含每个餐厅的经度和纬度。当用户在地图上查看餐馆时，网站需要搜索用户正在查看的矩形地图区域内的所有餐馆。这需要一个二维范围查询，如下所示： 12SELECT * FROM restaurants WHERE latitude &gt; 51.4946 AND latitude &lt; 51.5079 AND longitude &gt; -0.1162 AND longitude &lt; -0.1004; 一个标准的 B 树或者 LSM 树索引不能够高效地处理这种查询：它可以返回一个纬度范围内的所有餐馆（但经度可能是任意值），或者返回在同一个经度范围内的所有餐馆（但纬度可能是北极和南极之间的任意地方），但不能同时满足两个条件。 一种选择是使用 空间填充曲线（space-filling curve） 将二维位置转换为单个数字，然后使用常规 B 树索引【34】。更普遍的是，使用特殊化的空间索引，例如 R 树。例如，PostGIS 使用 PostgreSQL 的通用 GiST 工具【35】将地理空间索引实现为 R 树。这里我们没有足够的地方来描述 R 树，但是有大量的文献可供参考。 有趣的是，多维索引不仅可以用于地理位置。例如，在电子商务网站上可以使用建立在（红，绿，蓝）维度上的三维索引来搜索特定颜色范围内的产品，也可以在天气观测数据库中建立（日期，温度）的二维索引，以便有效地搜索 2013 年内的温度在 25 至 30°C 之间的所有观测资料。如果使用一维索引，你将不得不扫描 2013 年的所有记录（不管温度如何），然后通过温度进行过滤，或者反之亦然。二维索引可以同时通过时间戳和温度来收窄数据集。这个技术被 HyperDex 所使用【36】。 全文搜索和模糊索引到目前为止所讨论的所有索引都假定你有确切的数据，并允许你查询键的确切值或具有排序顺序的键的值范围。他们不允许你做的是搜索类似的键，如拼写错误的单词。这种模糊的查询需要不同的技术。 例如，全文搜索引擎通常允许搜索目标从一个单词扩展为包括该单词的同义词，忽略单词的语法变体，搜索在相同文档中的近义词，并且支持各种其他取决于文本的语言分析功能。为了处理文档或查询中的拼写错误，Lucene 能够在一定的编辑距离内搜索文本【37】（编辑距离 1 意味着单词内发生了 1 个字母的添加、删除或替换）。 正如 “用 SSTables 制作 LSM 树” 中所提到的，Lucene 为其词典使用了一个类似于 SSTable 的结构。这个结构需要一个小的内存索引，告诉查询需要在排序文件中哪个偏移量查找键。在 LevelDB 中，这个内存中的索引是一些键的稀疏集合，但在 Lucene 中，内存中的索引是键中字符的有限状态自动机，类似于 trie 【38】。这个自动机可以转换成 Levenshtein 自动机，它支持在给定的编辑距离内有效地搜索单词【39】。 其他的模糊搜索技术正朝着文档分类和机器学习的方向发展。更多详细信息请参阅信息检索教科书，例如【40】。 在内存中存储一切本章到目前为止讨论的数据结构都是对硬盘限制的应对。与主内存相比，硬盘处理起来很麻烦。对于磁性硬盘和固态硬盘，如果要在读取和写入时获得良好性能，则需要仔细地布置硬盘上的数据。但是，我们能容忍这种麻烦，因为硬盘有两个显著的优点：它们是持久的（它们的内容在电源关闭时不会丢失），并且每 GB 的成本比 RAM 低。 随着 RAM 变得更便宜，每 GB 成本的论据被侵蚀了。许多数据集不是那么大，所以将它们全部保存在内存中是非常可行的，包括可能分布在多个机器上。这导致了内存数据库的发展。 某些内存中的键值存储（如 Memcached）仅用于缓存，在重新启动计算机时丢失的数据是可以接受的。但其他内存数据库的目标是持久性，可以通过特殊的硬件（例如电池供电的 RAM）来实现，也可以将更改日志写入硬盘，还可以将定时快照写入硬盘或者将内存中的状态复制到其他机器上。 内存数据库重新启动时，需要从硬盘或通过网络从副本重新加载其状态（除非使用特殊的硬件）。尽管写入硬盘，它仍然是一个内存数据库，因为硬盘仅出于持久性目的进行日志追加，读取请求完全由内存来处理。写入硬盘同时还有运维上的好处：硬盘上的文件可以很容易地由外部程序进行备份、检查和分析。 诸如 VoltDB、MemSQL 和 Oracle TimesTen 等产品是具有关系模型的内存数据库，供应商声称，通过消除与管理硬盘上的数据结构相关的所有开销，他们可以提供巨大的性能改进【41,42】。 RAM Cloud 是一个开源的内存键值存储器，具有持久性（对内存和硬盘上的数据都使用日志结构化方法）【43】。 Redis 和 Couchbase 通过异步写入硬盘提供了较弱的持久性。 反直觉的是，内存数据库的性能优势并不是因为它们不需要从硬盘读取的事实。只要有足够的内存即使是基于硬盘的存储引擎也可能永远不需要从硬盘读取，因为操作系统在内存中缓存了最近使用的硬盘块。相反，它们更快的原因在于省去了将内存数据结构编码为硬盘数据结构的开销【44】。 除了性能，内存数据库的另一个有趣的地方是提供了难以用基于硬盘的索引实现的数据模型。例如，Redis 为各种数据结构（如优先级队列和集合）提供了类似数据库的接口。因为它将所有数据保存在内存中，所以它的实现相对简单。 最近的研究表明，内存数据库体系结构可以扩展到支持比可用内存更大的数据集，而不必重新采用以硬盘为中心的体系结构【45】。所谓的 反缓存（anti-caching） 方法通过在内存不足的情况下将最近最少使用的数据从内存转移到硬盘，并在将来再次访问时将其重新加载到内存中。这与操作系统对虚拟内存和交换文件的操作类似，但数据库可以比操作系统更有效地管理内存，因为它可以按单个记录的粒度工作，而不是整个内存页面。尽管如此，这种方法仍然需要索引能完全放入内存中（就像本章开头的 Bitcask 例子）。 如果 非易失性存储器（non-volatile memory, NVM） 技术得到更广泛的应用，可能还需要进一步改变存储引擎设计【46】。目前这是一个新的研究领域，值得关注。 事务处理还是分析？在早期的业务数据处理过程中，一次典型的数据库写入通常与一笔 商业交易（commercial transaction） 相对应：卖个货、向供应商下订单、支付员工工资等等。但随着数据库开始应用到那些不涉及到钱的领域，术语 交易 &#x2F; 事务（transaction） 仍留了下来，用于指代一组读写操作构成的逻辑单元。 事务不一定具有 ACID（原子性，一致性，隔离性和持久性）属性。事务处理只是意味着允许客户端进行低延迟的读取和写入 —— 而不是只能定期运行（例如每天一次）的批处理作业。我们在 第七章 中讨论 ACID 属性，在 第十章 中讨论批处理。 即使数据库开始被用于许多不同类型的数据，比如博客文章的评论、游戏中的动作、地址簿中的联系人等等，基本的访问模式仍然类似于处理商业交易。应用程序通常使用索引通过某个键找少量记录。根据用户的输入来插入或更新记录。由于这些应用程序是交互式的，这种访问模式被称为 在线事务处理（OLTP, OnLine Transaction Processing）。 但是，数据库也开始越来越多地用于数据分析，这些数据分析具有非常不同的访问模式。通常，分析查询需要扫描大量记录，每个记录只读取几列，并计算汇总统计信息（如计数、总和或平均值），而不是将原始数据返回给用户。例如，如果你的数据是一个销售交易表，那么分析查询可能是： 一月份每个商店的总收入是多少？ 在最近的推广活动中多卖了多少香蕉？ 哪个牌子的婴儿食品最常与 X 品牌的尿布同时购买？ 这些查询通常由业务分析师编写，并提供报告以帮助公司管理层做出更好的决策（商业智能）。为了将这种使用数据库的模式和事务处理区分开，它被称为 在线分析处理（OLAP, OnLine Analytic Processing）【47】[^iv]。OLTP 和 OLAP 之间的区别并不总是清晰的，但是一些典型的特征在 表 3-1 中列出。 表 3-1 比较事务处理和分析系统的特点 属性 事务处理系统 OLTP 分析系统 OLAP 主要读取模式 查询少量记录，按键读取 在大批量记录上聚合 主要写入模式 随机访问，写入要求低延时 批量导入（ETL）或者事件流 主要用户 终端用户，通过 Web 应用 内部数据分析师，用于决策支持 处理的数据 数据的最新状态（当前时间点） 随时间推移的历史事件 数据集尺寸 GB ~ TB TB ~ PB [^iv]: OLAP 中的首字母 O（online）的含义并不明确，它可能是指查询并不是用来生成预定义好的报告的事实，也可能是指分析师通常是交互式地使用 OLAP 系统来进行探索式的查询。 起初，事务处理和分析查询使用了相同的数据库。 SQL 在这方面已证明是非常灵活的：对于 OLTP 类型的查询以及 OLAP 类型的查询来说效果都很好。尽管如此，在二十世纪八十年代末和九十年代初期，企业有停止使用 OLTP 系统进行分析的趋势，转而在单独的数据库上运行分析。这个单独的数据库被称为 数据仓库（data warehouse）。 数据仓库一个企业可能有几十个不同的交易处理系统：面向终端客户的网站、控制实体商店的收银系统、仓库库存跟踪、车辆路线规划、供应链管理、员工管理等。这些系统中每一个都很复杂，需要专人维护，所以最终这些系统互相之间都是独立运行的。 这些 OLTP 系统往往对业务运作至关重要，因而通常会要求 高可用 与 低延迟。所以 DBA 会密切关注他们的 OLTP 数据库，他们通常不愿意让业务分析人员在 OLTP 数据库上运行临时的分析查询，因为这些查询通常开销巨大，会扫描大部分数据集，这会损害同时在执行的事务的性能。 相比之下，数据仓库是一个独立的数据库，分析人员可以查询他们想要的内容而不影响 OLTP 操作【48】。数据仓库包含公司各种 OLTP 系统中所有的只读数据副本。从 OLTP 数据库中提取数据（使用定期的数据转储或连续的更新流），转换成适合分析的模式，清理并加载到数据仓库中。将数据存入仓库的过程称为 “抽取 - 转换 - 加载（ETL）”，如 图 3-8 所示。 图 3-8 ETL 至数据仓库的简化提纲 几乎所有的大型企业都有数据仓库，但在小型企业中几乎闻所未闻。这可能是因为大多数小公司没有这么多不同的 OLTP 系统，大多数小公司只有少量的数据 —— 可以在传统的 SQL 数据库中查询，甚至可以在电子表格中分析。在一家大公司里，要做一些在一家小公司很简单的事情，需要很多繁重的工作。 使用单独的数据仓库，而不是直接查询 OLTP 系统进行分析的一大优势是数据仓库可针对分析类的访问模式进行优化。事实证明，本章前半部分讨论的索引算法对于 OLTP 来说工作得很好，但对于处理分析查询并不是很好。在本章的其余部分中，我们将研究为分析而优化的存储引擎。 OLTP数据库和数据仓库之间的分歧数据仓库的数据模型通常是关系型的，因为 SQL 通常很适合分析查询。有许多图形数据分析工具可以生成 SQL 查询，可视化结果，并允许分析人员探索数据（通过下钻、切片和切块等操作）。 表面上，一个数据仓库和一个关系型 OLTP 数据库看起来很相似，因为它们都有一个 SQL 查询接口。然而，系统的内部看起来可能完全不同，因为它们针对非常不同的查询模式进行了优化。现在许多数据库供应商都只是重点支持事务处理负载和分析工作负载这两者中的一个，而不是都支持。 一些数据库（例如 Microsoft SQL Server 和 SAP HANA）支持在同一产品中进行事务处理和数据仓库。但是，它们也正日益发展为两套独立的存储和查询引擎，只是这些引擎正好可以通过一个通用的 SQL 接口访问【49,50,51】。 Teradata、Vertica、SAP HANA 和 ParAccel 等数据仓库供应商通常使用昂贵的商业许可证销售他们的系统。 Amazon RedShift 是 ParAccel 的托管版本。最近，大量的开源 SQL-on-Hadoop 项目已经出现，它们还很年轻，但是正在与商业数据仓库系统竞争，包括 Apache Hive、Spark SQL、Cloudera Impala、Facebook Presto、Apache Tajo 和 Apache Drill【52,53】。其中一些基于了谷歌 Dremel 的想法【54】。 星型和雪花型：分析的模式正如 第二章 所探讨的，根据应用程序的需要，在事务处理领域中使用了大量不同的数据模型。另一方面，在分析型业务中，数据模型的多样性则少得多。许多数据仓库都以相当公式化的方式使用，被称为星型模式（也称为维度建模【55】）。 图 3-9 中的示例模式显示了可能在食品零售商处找到的数据仓库。在模式的中心是一个所谓的事实表（在这个例子中，它被称为 fact_sales）。事实表的每一行代表在特定时间发生的事件（这里，每一行代表客户购买的产品）。如果我们分析的是网站流量而不是零售量，则每行可能代表一个用户的页面浏览或点击。 图 3-9 用于数据仓库的星型模式的示例 通常情况下，事实被视为单独的事件，因为这样可以在以后分析中获得最大的灵活性。但是，这意味着事实表可以变得非常大。像苹果、沃尔玛或 eBay 这样的大企业在其数据仓库中可能有几十 PB 的交易历史，其中大部分保存在事实表中【56】。 事实表中的一些列是属性，例如产品销售的价格和从供应商那里购买的成本（可以用来计算利润率）。事实表中的其他列是对其他表（称为维度表）的外键引用。由于事实表中的每一行都表示一个事件，因此这些维度代表事件发生的对象、内容、地点、时间、方式和原因。 例如，在 图 3-9 中，其中一个维度是已售出的产品。 dim_product 表中的每一行代表一种待售产品，包括库存单位（SKU）、产品描述、品牌名称、类别、脂肪含量、包装尺寸等。fact_sales 表中的每一行都使用外键表明在特定交易中销售了什么产品。 （简单起见，如果客户一次购买了几种不同的产品，则它们在事实表中被表示为单独的行）。 甚至日期和时间也通常使用维度表来表示，因为这允许对日期的附加信息（诸如公共假期）进行编码，从而允许区分假期和非假期的销售查询。 “星型模式” 这个名字来源于这样一个事实，即当我们对表之间的关系进行可视化时，事实表在中间，被维度表包围；与这些表的连接就像星星的光芒。 这个模板的变体被称为雪花模式，其中维度被进一步分解为子维度。例如，品牌和产品类别可能有单独的表格，并且 dim_product 表格中的每一行都可以将品牌和类别作为外键引用，而不是将它们作为字符串存储在 dim_product 表格中。雪花模式比星形模式更规范化，但是星形模式通常是首选，因为分析师使用它更简单【55】。 在典型的数据仓库中，表格通常非常宽：事实表通常有 100 列以上，有时甚至有数百列【51】。维度表也可以是非常宽的，因为它们包括了所有可能与分析相关的元数据 —— 例如，dim_store 表可以包括在每个商店提供哪些服务的细节、它是否具有店内面包房、店面面积、商店第一次开张的日期、最近一次改造的时间、离最近的高速公路的距离等等。 列式存储如果事实表中有万亿行和数 PB 的数据，那么高效地存储和查询它们就成为一个具有挑战性的问题。维度表通常要小得多（数百万行），所以在本节中我们将主要关注事实表的存储。 尽管事实表通常超过 100 列，但典型的数据仓库查询一次只会访问其中 4 个或 5 个列（ “SELECT *” 查询很少用于分析）【51】。以 例 3-1 中的查询为例：它访问了大量的行（在 2013 年中所有购买了水果或糖果的记录），但只需访问 fact_sales 表的三列：date_key, product_sk, quantity。该查询忽略了所有其他的列。 例 3-1 分析人们是否更倾向于在一周的某一天购买新鲜水果或糖果 123456789101112SELECT dim_date.weekday, dim_product.category, SUM(fact_sales.quantity) AS quantity_soldFROM fact_sales JOIN dim_date ON fact_sales.date_key = dim_date.date_key JOIN dim_product ON fact_sales.product_sk = dim_product.product_skWHERE dim_date.year = 2013 AND dim_product.category IN (&#x27;Fresh fruit&#x27;, &#x27;Candy&#x27;)GROUP BY dim_date.weekday, dim_product.category; 我们如何有效地执行这个查询？ 在大多数 OLTP 数据库中，存储都是以面向行的方式进行布局的：表格的一行中的所有值都相邻存储。文档数据库也是相似的：整个文档通常存储为一个连续的字节序列。你可以在 图 3-1 的 CSV 例子中看到这个。 为了处理像 例 3-1 这样的查询，你可能在 fact_sales.date_key、fact_sales.product_sk 上有索引，它们告诉存储引擎在哪里查找特定日期或特定产品的所有销售情况。但是，面向行的存储引擎仍然需要将所有这些行（每个包含超过 100 个属性）从硬盘加载到内存中，解析它们，并过滤掉那些不符合要求的属性。这可能需要很长时间。 列式存储背后的想法很简单：不要将所有来自一行的值存储在一起，而是将来自每一列的所有值存储在一起。如果每个列式存储在一个单独的文件中，查询只需要读取和解析查询中使用的那些列，这可以节省大量的工作。这个原理如 图 3-10 所示。 图 3-10 按列存储关系型数据，而不是行 列式存储在关系数据模型中是最容易理解的，但它同样适用于非关系数据。例如，Parquet【57】是一种列式存储格式，支持基于 Google 的 Dremel 的文档数据模型【54】。 列式存储布局依赖于每个列文件包含相同顺序的行。 因此，如果你需要重新组装完整的行，你可以从每个单独的列文件中获取第 23 项，并将它们放在一起形成表的第 23 行。 列压缩除了仅从硬盘加载查询所需的列以外，我们还可以通过压缩数据来进一步降低对硬盘吞吐量的需求。幸运的是，列式存储通常很适合压缩。 看看 图 3-10 中每一列的值序列：它们通常看起来是相当重复的，这是压缩的好兆头。根据列中的数据，可以使用不同的压缩技术。在数据仓库中特别有效的一种技术是位图编码，如 图 3-11 所示。 图 3-11 压缩的位图索引存储布局 通常情况下，一列中不同值的数量与行数相比要小得多（例如，零售商可能有数十亿的销售交易，但只有 100,000 个不同的产品）。现在我们可以拿一个有 n 个不同值的列，并把它转换成 n 个独立的位图：每个不同值对应一个位图，每行对应一个比特位。如果该行具有该值，则该位为 1，否则为 0。 如果 n 非常小（例如，国家 &#x2F; 地区列可能有大约 200 个不同的值），则这些位图可以将每行存储成一个比特位。但是，如果 n 更大，大部分位图中将会有很多的零（我们说它们是稀疏的）。在这种情况下，位图可以另外再进行游程编码（run-length encoding，一种无损数据压缩技术），如 图 3-11 底部所示。这可以使列的编码非常紧凑。 这些位图索引非常适合数据仓库中常见的各种查询。例如： 1WHERE product_sk IN（30，68，69） 加载 product_sk = 30、product_sk = 68 和 product_sk = 69 这三个位图，并计算三个位图的按位或（OR），这可以非常有效地完成。 1WHERE product_sk = 31 AND store_sk = 3 加载 product_sk = 31 和 store_sk = 3 的位图，并计算按位与（AND）。这是因为列按照相同的顺序包含行，因此一列的位图中的第 k 位和另一列的位图中的第 k 位对应相同的行。 对于不同种类的数据，也有各种不同的压缩方案，但我们不会详细讨论它们，请参阅【58】的概述。 列式存储和列族Cassandra 和 HBase 有一个列族（column families）的概念，他们从 Bigtable 继承【9】。然而，把它们称为列式（column-oriented）是非常具有误导性的：在每个列族中，它们将一行中的所有列与行键一起存储，并且不使用列压缩。因此，Bigtable 模型仍然主要是面向行的。 内存带宽和矢量化处理对于需要扫描数百万行的数据仓库查询来说，一个巨大的瓶颈是从硬盘获取数据到内存的带宽。但是，这不是唯一的瓶颈。分析型数据库的开发人员还需要有效地利用内存到 CPU 缓存的带宽，避免 CPU 指令处理流水线中的分支预测错误和闲置等待，以及在现代 CPU 上使用单指令多数据（SIMD）指令来加速运算【59,60】。 除了减少需要从硬盘加载的数据量以外，列式存储布局也可以有效利用 CPU 周期。例如，查询引擎可以将一整块压缩好的列数据放进 CPU 的 L1 缓存中，然后在紧密的循环（即没有函数调用）中遍历。相比于每条记录的处理都需要大量函数调用和条件判断的代码，CPU 执行这样一个循环要快得多。列压缩允许列中的更多行被同时放进容量有限的 L1 缓存。前面描述的按位 “与” 和 “或” 运算符可以被设计为直接在这样的压缩列数据块上操作。这种技术被称为矢量化处理（vectorized processing）【58,49】。 列式存储中的排序顺序在列式存储中，存储行的顺序并不关键。按插入顺序存储它们是最简单的，因为插入一个新行只需要追加到每个列文件。但是，我们也可以选择按某种顺序来排列数据，就像我们之前对 SSTables 所做的那样，并将其用作索引机制。 注意，对每列分别执行排序是没有意义的，因为那样就没法知道不同列中的哪些项属于同一行。我们只能在明确一列中的第 k 项与另一列中的第 k 项属于同一行的情况下，才能重建出完整的行。 相反，数据的排序需要对一整行统一操作，即使它们的存储方式是按列的。数据库管理员可以根据他们对常用查询的了解，来选择表格中用来排序的列。例如，如果查询通常以日期范围为目标，例如“上个月”，则可以将 date_key 作为第一个排序键。这样查询优化器就可以只扫描近1个月范围的行了，这比扫描所有行要快得多。 对于第一排序列中具有相同值的行，可以用第二排序列来进一步排序。例如，如果 date_key 是 图 3-10 中的第一个排序关键字，那么 product_sk 可能是第二个排序关键字，以便同一天的同一产品的所有销售数据都被存储在相邻位置。这将有助于需要在特定日期范围内按产品对销售进行分组或过滤的查询。 按顺序排序的另一个好处是它可以帮助压缩列。如果主要排序列没有太多个不同的值，那么在排序之后，将会得到一个相同的值连续重复多次的序列。一个简单的游程编码（就像我们用于 图 3-11 中的位图一样）可以将该列压缩到几 KB —— 即使表中有数十亿行。 第一个排序键的压缩效果最强。第二和第三个排序键会更混乱，因此不会有这么长的连续的重复值。排序优先级更低的列以几乎随机的顺序出现，所以可能不会被压缩。但对前几列做排序在整体上仍然是有好处的。 几个不同的排序顺序对这个想法，有一个巧妙的扩展被 C-Store 发现，并在商业数据仓库 Vertica 中被采用【61,62】：既然不同的查询受益于不同的排序顺序，为什么不以几种不同的方式来存储相同的数据呢？反正数据都需要做备份，以防单点故障时丢失数据。因此你可以用不同排序方式来存储冗余数据，以便在处理查询时，调用最适合查询模式的版本。 在一个列式存储中有多个排序顺序有点类似于在一个面向行的存储中有多个次级索引。但最大的区别在于面向行的存储将每一行保存在一个地方（在堆文件或聚集索引中），次级索引只包含指向匹配行的指针。在列式存储中，通常在其他地方没有任何指向数据的指针，只有包含值的列。 写入列式存储这些优化在数据仓库中是有意义的，因为其负载主要由分析人员运行的大型只读查询组成。列式存储、压缩和排序都有助于更快地读取这些查询。然而，他们的缺点是写入更加困难。 使用 B 树的就地更新方法对于压缩的列是不可能的。如果你想在排序表的中间插入一行，你很可能不得不重写所有的列文件。由于行由列中的位置标识，因此插入必须对所有列进行一致地更新。 幸运的是，本章前面已经看到了一个很好的解决方案：LSM 树。所有的写操作首先进入一个内存中的存储，在这里它们被添加到一个已排序的结构中，并准备写入硬盘。内存中的存储是面向行还是列的并不重要。当已经积累了足够的写入数据时，它们将与硬盘上的列文件合并，并批量写入新文件。这基本上是 Vertica 所做的【62】。 查询操作需要检查硬盘上的列数据和内存中的最近写入，并将两者的结果合并起来。但是，查询优化器对用户隐藏了这个细节。从分析师的角度来看，通过插入、更新或删除操作进行修改的数据会立即反映在后续的查询中。 聚合：数据立方体和物化视图并非所有数据仓库都需要采用列式存储：传统的面向行的数据库和其他一些架构也被使用。然而，列式存储可以显著加快专门的分析查询，所以它正在迅速变得流行起来【51,63】。 数据仓库的另一个值得一提的方面是物化聚合（materialized aggregates）。如前所述，数据仓库查询通常涉及一个聚合函数，如 SQL 中的 COUNT、SUM、AVG、MIN 或 MAX。如果相同的聚合被许多不同的查询使用，那么每次都通过原始数据来处理可能太浪费了。为什么不将一些查询使用最频繁的计数或总和缓存起来？ 创建这种缓存的一种方式是物化视图（Materialized View）。在关系数据模型中，它通常被定义为一个标准（虚拟）视图：一个类似于表的对象，其内容是一些查询的结果。不同的是，物化视图是查询结果的实际副本，会被写入硬盘，而虚拟视图只是编写查询的一个捷径。从虚拟视图读取时，SQL 引擎会将其展开到视图的底层查询中，然后再处理展开的查询。 当底层数据发生变化时，物化视图需要更新，因为它是数据的非规范化副本。数据库可以自动完成该操作，但是这样的更新使得写入成本更高，这就是在 OLTP 数据库中不经常使用物化视图的原因。在读取繁重的数据仓库中，它们可能更有意义（它们是否实际上改善了读取性能取决于使用场景）。 物化视图的常见特例称为数据立方体或 OLAP 立方【64】。它是按不同维度分组的聚合网格。图 3-12 显示了一个例子。 图 3-12 数据立方的两个维度，通过求和聚合 想象一下，现在每个事实都只有两个维度表的外键 —— 在 图 3-12 中分别是日期和产品。你现在可以绘制一个二维表格，一个轴线上是日期，另一个轴线上是产品。每个单元格包含具有该日期 - 产品组合的所有事实的属性（例如 net_price）的聚合（例如 SUM）。然后，你可以沿着每行或每列应用相同的汇总，并获得减少了一个维度的汇总（按产品的销售额，无论日期，或者按日期的销售额，无论产品）。 一般来说，事实往往有两个以上的维度。在图 3-9 中有五个维度：日期、产品、商店、促销和客户。要想象一个五维超立方体是什么样子是很困难的，但是原理是一样的：每个单元格都包含特定日期 - 产品 - 商店 - 促销 - 客户组合的销售额。这些值可以在每个维度上求和汇总。 物化数据立方体的优点是可以让某些查询变得非常快，因为它们已经被有效地预先计算了。例如，如果你想知道每个商店的总销售额，则只需查看合适维度的总计，而无需扫描数百万行的原始数据。 数据立方体的缺点是不具有查询原始数据的灵活性。例如，没有办法计算有多少比例的销售来自成本超过 100 美元的项目，因为价格不是其中的一个维度。因此，大多数数据仓库试图保留尽可能多的原始数据，并将聚合数据（如数据立方体）仅用作某些查询的性能提升手段。 本章小结在本章中，我们试图深入了解数据库是如何处理存储和检索的。将数据存储在数据库中会发生什么？稍后再次查询数据时数据库会做什么？ 在高层次上，我们看到存储引擎分为两大类：针对 事务处理（OLTP） 优化的存储引擎和针对 在线分析（OLAP） 优化的存储引擎。这两类使用场景的访问模式之间有很大的区别： OLTP 系统通常面向最终用户，这意味着系统可能会收到大量的请求。为了处理负载，应用程序在每个查询中通常只访问少量的记录。应用程序使用某种键来请求记录，存储引擎使用索引来查找所请求的键的数据。硬盘查找时间往往是这里的瓶颈。 数据仓库和类似的分析系统会少见一些，因为它们主要由业务分析人员使用，而不是最终用户。它们的查询量要比 OLTP 系统少得多，但通常每个查询开销高昂，需要在短时间内扫描数百万条记录。硬盘带宽（而不是查找时间）往往是瓶颈，列式存储是针对这种工作负载的日益流行的解决方案。 在 OLTP 这一边，我们能看到两派主流的存储引擎： 日志结构学派：只允许追加到文件和删除过时的文件，但不会更新已经写入的文件。Bitcask、SSTables、LSM 树、LevelDB、Cassandra、HBase、Lucene 等都属于这个类别。 就地更新学派：将硬盘视为一组可以覆写的固定大小的页面。 B 树是这种理念的典范，用在所有主要的关系数据库和许多非关系型数据库中。 日志结构的存储引擎是相对较新的技术。他们的主要想法是，通过系统性地将随机访问写入转换为硬盘上的顺序写入，由于硬盘驱动器和固态硬盘的性能特点，可以实现更高的写入吞吐量。 关于 OLTP，我们最后还介绍了一些更复杂的索引结构，以及针对所有数据都放在内存里而优化的数据库。 然后，我们暂时放下了存储引擎的内部细节，查看了典型数据仓库的高级架构，并说明了为什么分析工作负载与 OLTP 差别很大：当你的查询需要在大量行中顺序扫描时，索引的重要性就会降低很多。相反，非常紧凑地编码数据变得非常重要，以最大限度地减少查询需要从硬盘读取的数据量。我们讨论了列式存储如何帮助实现这一目标。 作为一名应用程序开发人员，如果你掌握了有关存储引擎内部的知识，那么你就能更好地了解哪种工具最适合你的特定应用程序。当你调整数据库的优化参数时，这种理解让你能够设想增减某个值会产生怎样的效果。 尽管本章不能让你成为一个特定存储引擎的调参专家，但它至少大概率使你有了足够的概念与词汇储备去读懂你所选择的数据库的文档。 参考文献 Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman: Data Structures and Algorithms. Addison-Wesley, 1983. ISBN: 978-0-201-00023-8 Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein: Introduction to Algorithms, 3rd edition. MIT Press, 2009. ISBN: 978-0-262-53305-8 Justin Sheehy and David Smith: “Bitcask: A Log-Structured Hash Table for Fast Key&#x2F;Value Data,” Basho Technologies, April 2010. Yinan Li, Bingsheng He, Robin Jun Yang, et al.: “Tree Indexing on Solid State Drives,” Proceedings of the VLDB Endowment, volume 3, number 1, pages 1195–1206, September 2010. Goetz Graefe: “Modern B-Tree Techniques,” Foundations and Trends in Databases, volume 3, number 4, pages 203–402, August 2011. doi:10.1561&#x2F;1900000028 Jeffrey Dean and Sanjay Ghemawat: “LevelDB Implementation Notes,” leveldb.googlecode.com. Dhruba Borthakur: “The History of RocksDB,” rocksdb.blogspot.com, November 24, 2013. Matteo Bertozzi: “Apache HBase I&#x2F;O – HFile,” blog.cloudera.com, June, 29 2012. Fay Chang, Jeffrey Dean, Sanjay Ghemawat, et al.: “Bigtable: A Distributed Storage System for Structured Data,” at 7th USENIX Symposium on Operating System Design and Implementation (OSDI), November 2006. Patrick O’Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O’Neil: “The Log-Structured Merge-Tree (LSM-Tree),” Acta Informatica, volume 33, number 4, pages 351–385, June 1996. doi:10.1007&#x2F;s002360050048 Mendel Rosenblum and John K. Ousterhout: “The Design and Implementation of a Log-Structured File System,” ACM Transactions on Computer Systems, volume 10, number 1, pages 26–52, February 1992. doi:10.1145&#x2F;146941.146943 Adrien Grand: “What Is in a Lucene Index?,” at Lucene&#x2F;Solr Revolution, November 14, 2013. Deepak Kandepet: “Hacking Lucene—The Index Format,” hackerlabs.org, October 1, 2011. Michael McCandless: “Visualizing Lucene’s Segment Merges,” blog.mikemccandless.com, February 11, 2011. Burton H. Bloom: “Space&#x2F;Time Trade-offs in Hash Coding with Allowable Errors,” Communications of the ACM, volume 13, number 7, pages 422–426, July 1970. doi:10.1145&#x2F;362686.362692 “Operating Cassandra: Compaction,” Apache Cassandra Documentation v4.0, 2016. Rudolf Bayer and Edward M. McCreight: “Organization and Maintenance of Large Ordered Indices,” Boeing Scientific Research Laboratories, Mathematical and Information Sciences Laboratory, report no. 20, July 1970. Douglas Comer: “The Ubiquitous B-Tree,” ACM Computing Surveys, volume 11, number 2, pages 121–137, June 1979. doi:10.1145&#x2F;356770.356776 Emmanuel Goossaert: “Coding for SSDs,” codecapsule.com, February 12, 2014. C. Mohan and Frank Levine: “ARIES&#x2F;IM: An Efficient and High Concurrency Index Management Method Using Write-Ahead Logging,” at ACM International Conference on Management of Data (SIGMOD), June 1992. doi:10.1145&#x2F;130283.130338 Howard Chu: “LDAP at Lightning Speed,” at Build Stuff ‘14, November 2014. Bradley C. Kuszmaul: “A Comparison of Fractal Trees to Log-Structured Merge (LSM) Trees,” tokutek.com, April 22, 2014. Manos Athanassoulis, Michael S. Kester, Lukas M. Maas, et al.: “Designing Access Methods: The RUM Conjecture,” at 19th International Conference on Extending Database Technology (EDBT), March 2016. doi:10.5441&#x2F;002&#x2F;edbt.2016.42 Peter Zaitsev: “Innodb Double Write,” percona.com, August 4, 2006. Tomas Vondra: “On the Impact of Full-Page Writes,” blog.2ndquadrant.com, November 23, 2016. Mark Callaghan: “The Advantages of an LSM vs a B-Tree,” smalldatum.blogspot.co.uk, January 19, 2016. Mark Callaghan: “Choosing Between Efficiency and Performance with RocksDB,” at Code Mesh, November 4, 2016. Michi Mutsuzaki: “MySQL vs. LevelDB,” github.com, August 2011. Benjamin Coverston, Jonathan Ellis, et al.: “CASSANDRA-1608: Redesigned Compaction, issues.apache.org, July 2011. Igor Canadi, Siying Dong, and Mark Callaghan: “RocksDB Tuning Guide,” github.com, 2016. MySQL 5.7 Reference Manual. Oracle, 2014. Books Online for SQL Server 2012. Microsoft, 2012. Joe Webb: “Using Covering Indexes to Improve Query Performance,” simple-talk.com, 29 September 2008. Frank Ramsak, Volker Markl, Robert Fenk, et al.: “Integrating the UB-Tree into a Database System Kernel,” at 26th International Conference on Very Large Data Bases (VLDB), September 2000. The PostGIS Development Group: “PostGIS 2.1.2dev Manual,” postgis.net, 2014. Robert Escriva, Bernard Wong, and Emin Gün Sirer: “HyperDex: A Distributed, Searchable Key-Value Store,” at ACM SIGCOMM Conference, August 2012. doi:10.1145&#x2F;2377677.2377681 Michael McCandless: “Lucene’s FuzzyQuery Is 100 Times Faster in 4.0,” blog.mikemccandless.com, March 24, 2011. Steffen Heinz, Justin Zobel, and Hugh E. Williams: “Burst Tries: A Fast, Efficient Data Structure for String Keys,” ACM Transactions on Information Systems, volume 20, number 2, pages 192–223, April 2002. doi:10.1145&#x2F;506309.506312 Klaus U. Schulz and Stoyan Mihov: “Fast String Correction with Levenshtein Automata,” International Journal on Document Analysis and Recognition, volume 5, number 1, pages 67–85, November 2002. doi:10.1007&#x2F;s10032-002-0082-8 Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze: Introduction to Information Retrieval. Cambridge University Press, 2008. ISBN: 978-0-521-86571-5, available online at nlp.stanford.edu&#x2F;IR-book Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.: “The End of an Architectural Era (It’s Time for a Complete Rewrite),” at 33rd International Conference on Very Large Data Bases (VLDB), September 2007. “VoltDB Technical Overview White Paper,” VoltDB, 2014. Stephen M. Rumble, Ankita Kejriwal, and John K. Ousterhout: “Log-Structured Memory for DRAM-Based Storage,” at 12th USENIX Conference on File and Storage Technologies (FAST), February 2014. Stavros Harizopoulos, Daniel J. Abadi, Samuel Madden, and Michael Stonebraker: “OLTP Through the Looking Glass, and What We Found There,” at ACM International Conference on Management of Data (SIGMOD), June 2008. doi:10.1145&#x2F;1376616.1376713 Justin DeBrabant, Andrew Pavlo, Stephen Tu, et al.: “Anti-Caching: A New Approach to Database Management System Architecture,” Proceedings of the VLDB Endowment, volume 6, number 14, pages 1942–1953, September 2013. Joy Arulraj, Andrew Pavlo, and Subramanya R. Dulloor: “Let’s Talk About Storage &amp; Recovery Methods for Non-Volatile Memory Database Systems,” at ACM International Conference on Management of Data (SIGMOD), June 2015. doi:10.1145&#x2F;2723372.2749441 Edgar F. Codd, S. B. Codd, and C. T. Salley: “Providing OLAP to User-Analysts: An IT Mandate,” E. F. Codd Associates, 1993. Surajit Chaudhuri and Umeshwar Dayal: “An Overview of Data Warehousing and OLAP Technology,” ACM SIGMOD Record, volume 26, number 1, pages 65–74, March 1997. doi:10.1145&#x2F;248603.248616 Per-Åke Larson, Cipri Clinciu, Campbell Fraser, et al.: “Enhancements to SQL Server Column Stores,” at ACM International Conference on Management of Data (SIGMOD), June 2013. Franz Färber, Norman May, Wolfgang Lehner, et al.: “The SAP HANA Database – An Architecture Overview,” IEEE Data Engineering Bulletin, volume 35, number 1, pages 28–33, March 2012. Michael Stonebraker: “The Traditional RDBMS Wisdom Is (Almost Certainly) All Wrong,” presentation at EPFL, May 2013. Daniel J. Abadi: “Classifying the SQL-on-Hadoop Solutions,” hadapt.com, October 2, 2013. Marcel Kornacker, Alexander Behm, Victor Bittorf, et al.: “Impala: A Modern, Open-Source SQL Engine for Hadoop,” at 7th Biennial Conference on Innovative Data Systems Research (CIDR), January 2015. Sergey Melnik, Andrey Gubarev, Jing Jing Long, et al.: “Dremel: Interactive Analysis of Web-Scale Datasets,” at 36th International Conference on Very Large Data Bases (VLDB), pages 330–339, September 2010. Ralph Kimball and Margy Ross: The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling, 3rd edition. John Wiley &amp; Sons, July 2013. ISBN: 978-1-118-53080-1 Derrick Harris: “Why Apple, eBay, and Walmart Have Some of the Biggest Data Warehouses You’ve Ever Seen,” gigaom.com, March 27, 2013. Julien Le Dem: “Dremel Made Simple with Parquet,” blog.twitter.com, September 11, 2013. Daniel J. Abadi, Peter Boncz, Stavros Harizopoulos, et al.: “The Design and Implementation of Modern Column-Oriented Database Systems,” Foundations and Trends in Databases, volume 5, number 3, pages 197–280, December 2013. doi:10.1561&#x2F;1900000024 Peter Boncz, Marcin Zukowski, and Niels Nes: “MonetDB&#x2F;X100: Hyper-Pipelining Query Execution,” at 2nd Biennial Conference on Innovative Data Systems Research (CIDR), January 2005. Jingren Zhou and Kenneth A. Ross: “Implementing Database Operations Using SIMD Instructions,” at ACM International Conference on Management of Data (SIGMOD), pages 145–156, June 2002. doi:10.1145&#x2F;564691.564709 Michael Stonebraker, Daniel J. Abadi, Adam Batkin, et al.: “C-Store: A Column-oriented DBMS,” at 31st International Conference on Very Large Data Bases (VLDB), pages 553–564, September 2005. Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, et al.: “The Vertica Analytic Database: C-Store 7 Years Later,” Proceedings of the VLDB Endowment, volume 5, number 12, pages 1790–1801, August 2012. Julien Le Dem and Nong Li: “Efficient Data Storage for Analytics with Apache Parquet 2.0,” at Hadoop Summit, San Jose, June 2014. Jim Gray, Surajit Chaudhuri, Adam Bosworth, et al.: “Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals,” Data Mining and Knowledge Discovery, volume 1, number 1, pages 29–53, March 2007. doi:10.1023&#x2F;A:1009726021843"},{"title":"第四章：编码与演化","path":"/wiki/ddia/ch4.html","content":"唯变所适 —— 以弗所的赫拉克利特，为柏拉图所引（公元前 360 年） 应用程序不可避免地随时间而变化。新产品的推出，对需求的深入理解，或者商业环境的变化，总会伴随着 功能（feature） 的增增改改。第一章 介绍了 可演化性（evolvability） 的概念：应该尽力构建能灵活适应变化的系统（请参阅 “可演化性：拥抱变化”）。 在大多数情况下，修改应用程序的功能也意味着需要更改其存储的数据：可能需要使用新的字段或记录类型，或者以新方式展示现有数据。 我们在 第二章 讨论的数据模型有不同的方法来应对这种变化。关系数据库通常假定数据库中的所有数据都遵循一个模式：尽管可以更改该模式（通过模式迁移，即 ALTER 语句），但是在任何时间点都有且仅有一个正确的模式。相比之下，读时模式（schema-on-read，或 无模式，即 schemaless）数据库不会强制一个模式，因此数据库可以包含在不同时间写入的新老数据格式的混合（请参阅 “文档模型中的模式灵活性” ）。 当数据 格式（format） 或 模式（schema） 发生变化时，通常需要对应用程序代码进行相应的更改（例如，为记录添加新字段，然后修改程序开始读写该字段）。但在大型应用程序中，代码变更通常不会立即完成： 对于 服务端（server-side） 应用程序，可能需要执行 滚动升级 （rolling upgrade） （也称为 阶段发布（staged rollout） ），一次将新版本部署到少数几个节点，检查新版本是否运行正常，然后逐渐部完所有的节点。这样无需中断服务即可部署新版本，为频繁发布提供了可行性，从而带来更好的可演化性。 对于 客户端（client-side） 应用程序，升不升级就要看用户的心情了。用户可能相当长一段时间里都不会去升级软件。 这意味着，新旧版本的代码，以及新旧数据格式可能会在系统中同时共处。系统想要继续顺利运行，就需要保持 双向兼容性： 向后兼容 (backward compatibility) 新的代码可以读取由旧的代码写入的数据。 向前兼容 (forward compatibility) 旧的代码可以读取由新的代码写入的数据。 向后兼容性通常并不难实现：新代码的作者当然知道由旧代码使用的数据格式，因此可以显示地处理它（最简单的办法是，保留旧代码即可读取旧数据）。 向前兼容性可能会更棘手，因为旧版的程序需要忽略新版数据格式中新增的部分。 本章中将介绍几种编码数据的格式，包括 JSON、XML、Protocol Buffers、Thrift 和 Avro。尤其将关注这些格式如何应对模式变化，以及它们如何对新旧代码数据需要共存的系统提供支持。然后将讨论如何使用这些格式进行数据存储和通信：在 Web 服务中，表述性状态传递（REST） 和 远程过程调用（RPC），以及 消息传递系统（如 Actor 和消息队列）。 编码数据的格式程序通常（至少）使用两种形式的数据： 在内存中，数据保存在对象、结构体、列表、数组、散列表、树等中。 这些数据结构针对 CPU 的高效访问和操作进行了优化（通常使用指针）。 如果要将数据写入文件，或通过网络发送，则必须将其 编码（encode） 为某种自包含的字节序列（例如，JSON 文档）。 由于每个进程都有自己独立的地址空间，一个进程中的指针对任何其他进程都没有意义，所以这个字节序列表示会与通常在内存中使用的数据结构完全不同 [^i]。 [^i]: 除一些特殊情况外，例如某些内存映射文件或直接在压缩数据上操作（如 “列压缩” 中所述）。 所以，需要在两种表示之间进行某种类型的翻译。 从内存中表示到字节序列的转换称为 编码（Encoding） （也称为 序列化（serialization） 或 编组（marshalling）），反过来称为 解码（Decoding）[^ii]（解析（Parsing），反序列化（deserialization），反编组 (unmarshalling））[^译i]。 [^ii]: 请注意，编码（encode） 与 加密（encryption） 无关。 本书不讨论加密。[^译i]: Marshal 与 Serialization 的区别：Marshal 不仅传输对象的状态，而且会一起传输对象的方法（相关代码）。 术语冲突不幸的是，在 第七章： 事务（Transaction） 的上下文里，序列化（Serialization） 这个术语也出现了，而且具有完全不同的含义。尽管序列化可能是更常见的术语，为了避免术语重载，本书中坚持使用 编码（Encoding） 表达此含义。 这是一个常见的问题，因而有许多库和编码格式可供选择。 首先让我们概览一下。 语言特定的格式许多编程语言都内建了将内存对象编码为字节序列的支持。例如，Java 有 java.io.Serializable 【1】，Ruby 有 Marshal【2】，Python 有 pickle【3】，等等。许多第三方库也存在，例如 Kryo for Java 【4】。 这些编码库非常方便，可以用很少的额外代码实现内存对象的保存与恢复。但是它们也有一些深层次的问题： 这类编码通常与特定的编程语言深度绑定，其他语言很难读取这种数据。如果以这类编码存储或传输数据，那你就和这门语言绑死在一起了。并且很难将系统与其他组织的系统（可能用的是不同的语言）进行集成。 为了恢复相同对象类型的数据，解码过程需要 实例化任意类 的能力，这通常是安全问题的一个来源【5】：如果攻击者可以让应用程序解码任意的字节序列，他们就能实例化任意的类，这会允许他们做可怕的事情，如远程执行任意代码【6,7】。 在这些库中，数据版本控制通常是事后才考虑的。因为它们旨在快速简便地对数据进行编码，所以往往忽略了前向后向兼容性带来的麻烦问题。 效率（编码或解码所花费的 CPU 时间，以及编码结构的大小）往往也是事后才考虑的。 例如，Java 的内置序列化由于其糟糕的性能和臃肿的编码而臭名昭著【8】。 因此，除非临时使用，采用语言内置编码通常是一个坏主意。 JSON、XML和二进制变体当我们谈到可以被多种编程语言读写的标准编码时，JSON 和 XML 是最显眼的角逐者。它们广为人知，广受支持，也 “广受憎恶”。 XML 经常收到批评：过于冗长与且过份复杂【9】。 JSON 的流行则主要源于（通过成为 JavaScript 的一个子集）Web 浏览器的内置支持，以及相对于 XML 的简单性。 CSV 是另一种流行的与语言无关的格式，尽管其功能相对较弱。 JSON，XML 和 CSV 属于文本格式，因此具有人类可读性（尽管它们的语法是一个热门争议话题）。除了表面的语法问题之外，它们也存在一些微妙的问题： 数字（numbers） 编码有很多模糊之处。在 XML 和 CSV 中，无法区分数字和碰巧由数字组成的字符串（除了引用外部模式）。 JSON 虽然区分字符串与数字，但并不区分整数和浮点数，并且不能指定精度。这在处理大数字时是个问题。例如大于 $2^{53}$ 的整数无法使用 IEEE 754 双精度浮点数精确表示，因此在使用浮点数（例如 JavaScript）的语言进行分析时，这些数字会变得不准确。 Twitter 有一个关于大于 $2^{53}$ 的数字的例子，它使用 64 位整数来标识每条推文。 Twitter API 返回的 JSON 包含了两个推特 ID，一个是 JSON 数字，另一个是十进制字符串，以解决 JavaScript 程序中无法正确解析数字的问题【10】。 JSON 和 XML 对 Unicode 字符串（即人类可读的文本）有很好的支持，但是它们不支持二进制数据（即不带 字符编码（character encoding） 的字节序列）。二进制串是很有用的功能，人们通过使用 Base64 将二进制数据编码为文本来绕过此限制。其特有的模式标识着这个值应当被解释为 Base64 编码的二进制数据。这种方案虽然管用，但比较 Hacky，并且会增加三分之一的数据大小。 XML 【11】和 JSON 【12】都有可选的模式支持。这些模式语言相当强大，所以学习和实现起来都相当复杂。 XML 模式的使用相当普遍，但许多基于 JSON 的工具才不会去折腾模式。对数据的正确解读（例如区分数值与二进制串）取决于模式中的信息，因此不使用 XML&#x2F;JSON 模式的应用程序可能需要对相应的编码 &#x2F; 解码逻辑进行硬编码。 CSV 没有任何模式，因此每行和每列的含义完全由应用程序自行定义。如果应用程序变更添加了新的行或列，那么这种变更必须通过手工处理。 CSV 也是一个相当模糊的格式（如果一个值包含逗号或换行符，会发生什么？）。尽管其转义规则已经被正式指定【13】，但并不是所有的解析器都正确的实现了标准。 尽管存在这些缺陷，但 JSON、XML 和 CSV 对很多需求来说已经足够好了。它们很可能会继续流行下去，特别是作为数据交换格式来说（即将数据从一个组织发送到另一个组织）。在这种情况下，只要人们对格式是什么意见一致，格式有多美观或者效率有多高效就无所谓了。让不同的组织就这些东西达成一致的难度超过了绝大多数问题。 二进制编码对于仅在组织内部使用的数据，使用最小公约数式的编码格式压力较小。例如，可以选择更紧凑或更快的解析格式。虽然对小数据集来说，收益可以忽略不计；但一旦达到 TB 级别，数据格式的选型就会产生巨大的影响。 JSON 比 XML 简洁，但与二进制格式相比还是太占空间。这一事实导致大量二进制编码版本 JSON（MessagePack、BSON、BJSON、UBJSON、BISON 和 Smile 等） 和 XML（例如 WBXML 和 Fast Infoset）的出现。这些格式已经在各种各样的领域中采用，但是没有一个能像文本版 JSON 和 XML 那样被广泛采用。 这些格式中的一些扩展了一组数据类型（例如，区分整数和浮点数，或者增加对二进制字符串的支持），另一方面，它们没有改变 JSON &#x2F; XML 的数据模型。特别是由于它们没有规定模式，所以它们需要在编码数据中包含所有的对象字段名称。也就是说，在 例 4-1 中的 JSON 文档的二进制编码中，需要在某处包含字符串 userName，favoriteNumber 和 interests。 例 4-1 本章中用于展示二进制编码的示例记录 12345&#123; &quot;userName&quot;: &quot;Martin&quot;, &quot;favoriteNumber&quot;: 1337, &quot;interests&quot;: [&quot;daydreaming&quot;, &quot;hacking&quot;]&#125; 我们来看一个 MessagePack 的例子，它是一个 JSON 的二进制编码。图 4-1 显示了如果使用 MessagePack 【14】对 例 4-1 中的 JSON 文档进行编码，则得到的字节序列。前几个字节如下： 第一个字节 0x83 表示接下来是 3 个字段（低四位 &#x3D; 0x03）的 对象 object（高四位 &#x3D; 0x80）。 （如果想知道如果一个对象有 15 个以上的字段会发生什么情况，字段的数量塞不进 4 个 bit 里，那么它会用另一个不同的类型标识符，字段的数量被编码两个或四个字节）。 第二个字节 0xa8 表示接下来是 8 字节长（低四位 &#x3D; 0x08）的字符串（高四位 &#x3D; 0x0a）。 接下来八个字节是 ASCII 字符串形式的字段名称 userName。由于之前已经指明长度，不需要任何标记来标识字符串的结束位置（或者任何转义）。 接下来的七个字节对前缀为 0xa6 的六个字母的字符串值 Martin 进行编码，依此类推。 二进制编码长度为 66 个字节，仅略小于文本 JSON 编码所取的 81 个字节（删除了空白）。所有的 JSON 的二进制编码在这方面是相似的。空间节省了一丁点（以及解析加速）是否能弥补可读性的损失，谁也说不准。 在下面的章节中，能达到比这好得多的结果，只用 32 个字节对相同的记录进行编码。 图 4-1 使用 MessagePack 编码的记录（例 4-1） Thrift与Protocol BuffersApache Thrift 【15】和 Protocol Buffers（protobuf）【16】是基于相同原理的二进制编码库。 Protocol Buffers 最初是在 Google 开发的，Thrift 最初是在 Facebook 开发的，并且都是在 2007~2008 开源的【17】。Thrift 和 Protocol Buffers 都需要一个模式来编码任何数据。要在 Thrift 的 例 4-1 中对数据进行编码，可以使用 Thrift 接口定义语言（IDL） 来描述模式，如下所示： 12345struct Person &#123; 1: required string userName, 2: optional i64 favoriteNumber, 3: optional list&lt;string&gt; interests&#125; Protocol Buffers 的等效模式定义看起来非常相似： 12345message Person &#123; required string user_name = 1; optional int64 favorite_number = 2; repeated string interests = 3;&#125; Thrift 和 Protocol Buffers 每一个都带有一个代码生成工具，它采用了类似于这里所示的模式定义，并且生成了以各种编程语言实现模式的类【18】。你的应用程序代码可以调用此生成的代码来对模式的记录进行编码或解码。用这个模式编码的数据是什么样的？令人困惑的是，Thrift 有两种不同的二进制编码格式 [^iii]，分别称为 BinaryProtocol 和 CompactProtocol。先来看看 BinaryProtocol。使用这种格式的编码来编码 例 4-1 中的消息只需要 59 个字节，如 图 4-2 所示【19】。 图 4-2 使用 Thrift 二进制协议编码的记录 [^iii]: 实际上，Thrift 有三种二进制协议：BinaryProtocol、CompactProtocol 和 DenseProtocol，尽管 DenseProtocol 只支持 C ++ 实现，所以不算作跨语言【18】。 除此之外，它还有两种不同的基于 JSON 的编码格式【19】。 真逗！ 与 图 4-1 类似，每个字段都有一个类型注释（用于指示它是一个字符串、整数、列表等），还可以根据需要指定长度（字符串的长度，列表中的项目数） 。出现在数据中的字符串 (“Martin”, “daydreaming”, “hacking”) 也被编码为 ASCII（或者说，UTF-8），与之前类似。 与 图 4-1 相比，最大的区别是没有字段名 (userName, favoriteNumber, interests)。相反，编码数据包含字段标签，它们是数字 (1, 2 和 3)。这些是模式定义中出现的数字。字段标记就像字段的别名 - 它们是说我们正在谈论的字段的一种紧凑的方式，而不必拼出字段名称。 Thrift CompactProtocol 编码在语义上等同于 BinaryProtocol，但是如 图 4-3 所示，它只将相同的信息打包成只有 34 个字节。它通过将字段类型和标签号打包到单个字节中，并使用可变长度整数来实现。数字 1337 不是使用全部八个字节，而是用两个字节编码，每个字节的最高位用来指示是否还有更多的字节。这意味着 - 64 到 63 之间的数字被编码为一个字节，-8192 和 8191 之间的数字以两个字节编码，等等。较大的数字使用更多的字节。 图 4-3 使用 Thrift 压缩协议编码的记录 最后，Protocol Buffers（只有一种二进制编码格式）对相同的数据进行编码，如 图 4-4 所示。 它的打包方式稍有不同，但与 Thrift 的 CompactProtocol 非常相似。 Protobuf 将同样的记录塞进了 33 个字节中。 图 4-4 使用 Protobuf 编码的记录 需要注意的一个细节：在前面所示的模式中，每个字段被标记为必需或可选，但是这对字段如何编码没有任何影响（二进制数据中没有任何字段指示某字段是否必须）。区别在于，如果字段设置为 required，但未设置该字段，则所需的运行时检查将失败，这对于捕获错误非常有用。 字段标签和模式演变我们之前说过，模式不可避免地需要随着时间而改变。我们称之为模式演变。 Thrift 和 Protocol Buffers 如何处理模式更改，同时保持向后兼容性？ 从示例中可以看出，编码的记录就是其编码字段的拼接。每个字段由其标签号码（样本模式中的数字 1,2,3）标识，并用数据类型（例如字符串或整数）注释。如果没有设置字段值，则简单地从编码记录中省略。从中可以看到，字段标记对编码数据的含义至关重要。你可以更改架构中字段的名称，因为编码的数据永远不会引用字段名称，但不能更改字段的标记，因为这会使所有现有的编码数据无效。 你可以添加新的字段到架构，只要你给每个字段一个新的标签号码。如果旧的代码（不知道你添加的新的标签号码）试图读取新代码写入的数据，包括一个新的字段，其标签号码不能识别，它可以简单地忽略该字段。数据类型注释允许解析器确定需要跳过的字节数。这保持了向前兼容性：旧代码可以读取由新代码编写的记录。 向后兼容性呢？只要每个字段都有一个唯一的标签号码，新的代码总是可以读取旧的数据，因为标签号码仍然具有相同的含义。唯一的细节是，如果你添加一个新的字段，你不能设置为必需。如果你要添加一个字段并将其设置为必需，那么如果新代码读取旧代码写入的数据，则该检查将失败，因为旧代码不会写入你添加的新字段。因此，为了保持向后兼容性，在模式的初始部署之后 添加的每个字段必须是可选的或具有默认值。 删除一个字段就像添加一个字段，只是这回要考虑的是向前兼容性。这意味着你只能删除可选的字段（必需字段永远不能删除），而且你不能再次使用相同的标签号码（因为你可能仍然有数据写在包含旧标签号码的地方，而该字段必须被新代码忽略）。 数据类型和模式演变如何改变字段的数据类型？这也许是可能的 —— 详细信息请查阅相关的文档 —— 但是有一个风险，值将失去精度或被截断。例如，假设你将一个 32 位的整数变成一个 64 位的整数。新代码可以轻松读取旧代码写入的数据，因为解析器可以用零填充任何缺失的位。但是，如果旧代码读取由新代码写入的数据，则旧代码仍使用 32 位变量来保存该值。如果解码的 64 位值不适合 32 位，则它将被截断。 Protobuf 的一个奇怪的细节是，它没有列表或数组数据类型，而是有一个字段的重复标记（repeated，这是除必需和可选之外的第三个选项）。如 图 4-4 所示，重复字段的编码正如它所说的那样：同一个字段标记只是简单地出现在记录中。这具有很好的效果，可以将可选（单值）字段更改为重复（多值）字段。读取旧数据的新代码会看到一个包含零个或一个元素的列表（取决于该字段是否存在）。读取新数据的旧代码只能看到列表的最后一个元素。 Thrift 有一个专用的列表数据类型，它使用列表元素的数据类型进行参数化。这不允许 Protocol Buffers 所做的从单值到多值的演变，但是它具有支持嵌套列表的优点。 AvroApache Avro 【20】是另一种二进制编码格式，与 Protocol Buffers 和 Thrift 有着有趣的不同。 它是作为 Hadoop 的一个子项目在 2009 年开始的，因为 Thrift 不适合 Hadoop 的用例【21】。 Avro 也使用模式来指定正在编码的数据的结构。 它有两种模式语言：一种（Avro IDL）用于人工编辑，一种（基于 JSON）更易于机器读取。 我们用 Avro IDL 编写的示例模式可能如下所示： 12345record Person &#123; string userName; union &#123; null, long &#125; favoriteNumber = null; array&lt;string&gt; interests;&#125; 等价的 JSON 表示： 123456789&#123; &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Person&quot;, &quot;fields&quot;: [ &#123;&quot;name&quot;: &quot;userName&quot;, &quot;type&quot;: &quot;string&quot;&#125;, &#123;&quot;name&quot;: &quot;favoriteNumber&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;long&quot;], &quot;default&quot;: null&#125;, &#123;&quot;name&quot;: &quot;interests&quot;, &quot;type&quot;: &#123;&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;&#125;&#125; ]&#125; 首先，请注意模式中没有标签号码。 如果我们使用这个模式编码我们的例子记录（例 4-1），Avro 二进制编码只有 32 个字节长，这是我们所见过的所有编码中最紧凑的。 编码字节序列的分解如 图 4-5 所示。 如果你检查字节序列，你可以看到没有什么可以识别字段或其数据类型。 编码只是由连在一起的值组成。 一个字符串只是一个长度前缀，后跟 UTF-8 字节，但是在被包含的数据中没有任何内容告诉你它是一个字符串。 它可以是一个整数，也可以是其他的整数。 整数使用可变长度编码（与 Thrift 的 CompactProtocol 相同）进行编码。 图 4-5 使用 Avro 编码的记录 为了解析二进制数据，你按照它们出现在模式中的顺序遍历这些字段，并使用模式来告诉你每个字段的数据类型。这意味着如果读取数据的代码使用与写入数据的代码完全相同的模式，才能正确解码二进制数据。Reader 和 Writer 之间的模式不匹配意味着错误地解码数据。 那么，Avro 如何支持模式演变呢？ Writer模式与Reader模式有了 Avro，当应用程序想要编码一些数据（将其写入文件或数据库、通过网络发送等）时，它使用它知道的任何版本的模式编码数据，例如，模式可能被编译到应用程序中。这被称为 Writer 模式。 当一个应用程序想要解码一些数据（从一个文件或数据库读取数据、从网络接收数据等）时，它希望数据在某个模式中，这就是 Reader 模式。这是应用程序代码所依赖的模式，在应用程序的构建过程中，代码可能已经从该模式生成。 Avro 的关键思想是 Writer 模式和 Reader 模式不必是相同的 - 他们只需要兼容。当数据解码（读取）时，Avro 库通过并排查看 Writer 模式和 Reader 模式并将数据从 Writer 模式转换到 Reader 模式来解决差异。 Avro 规范【20】确切地定义了这种解析的工作原理，如 图 4-6 所示。 例如，如果 Writer 模式和 Reader 模式的字段顺序不同，这是没有问题的，因为模式解析通过字段名匹配字段。如果读取数据的代码遇到出现在 Writer 模式中但不在 Reader 模式中的字段，则忽略它。如果读取数据的代码需要某个字段，但是 Writer 模式不包含该名称的字段，则使用在 Reader 模式中声明的默认值填充。 图 4-6 一个 Avro Reader 解决读写模式的差异 模式演变规则使用 Avro，向前兼容性意味着你可以将新版本的模式作为 Writer，并将旧版本的模式作为 Reader。相反，向后兼容意味着你可以有一个作为 Reader 的新版本模式和作为 Writer 的旧版本模式。 为了保持兼容性，你只能添加或删除具有默认值的字段（我们的 Avro 模式中的字段 favoriteNumber 的默认值为 null）。例如，假设你添加了一个有默认值的字段，这个新的字段将存在于新模式而不是旧模式中。当使用新模式的 Reader 读取使用旧模式写入的记录时，将为缺少的字段填充默认值。 如果你要添加一个没有默认值的字段，新的 Reader 将无法读取旧 Writer 写的数据，所以你会破坏向后兼容性。如果你要删除没有默认值的字段，旧的 Reader 将无法读取新 Writer 写入的数据，因此你会打破向前兼容性。在一些编程语言中，null 是任何变量可以接受的默认值，但在 Avro 中并不是这样：如果要允许一个字段为 null，则必须使用联合类型。例如，union &#123;null, long, string&#125; field; 表示 field 可以是数字或字符串，也可以是 null。如果要将 null 作为默认值，则它必须是 union 的分支之一 [^iv]。这样的写法比默认情况下就允许任何变量是 null 显得更加冗长，但是通过明确什么可以和什么不可以是 null，有助于防止出错【22】。 [^iv]: 确切地说，默认值必须是联合的第一个分支的类型，尽管这是 Avro 的特定限制，而不是联合类型的一般特征。 因此，Avro 没有像 Protocol Buffers 和 Thrift 那样的 optional 和 required 标记（但它有联合类型和默认值）。 只要 Avro 可以支持相应的类型转换，就可以改变字段的数据类型。更改字段的名称也是可能的，但有点棘手：Reader 模式可以包含字段名称的别名，所以它可以匹配旧 Writer 的模式字段名称与别名。这意味着更改字段名称是向后兼容的，但不能向前兼容。同样，向联合类型添加分支也是向后兼容的，但不能向前兼容。 但Writer模式到底是什么？到目前为止，我们一直跳过了一个重要的问题：对于一段特定的编码数据，Reader 如何知道其 Writer 模式？我们不能只将整个模式包括在每个记录中，因为模式可能比编码的数据大得多，从而使二进制编码节省的所有空间都是徒劳的。 答案取决于 Avro 使用的上下文。举几个例子： 有很多记录的大文件 Avro 的一个常见用途 - 尤其是在 Hadoop 环境中 - 用于存储包含数百万条记录的大文件，所有记录都使用相同的模式进行编码（我们将在 第十章 讨论这种情况）。在这种情况下，该文件的作者可以在文件的开头只包含一次 Writer 模式。 Avro 指定了一个文件格式（对象容器文件）来做到这一点。 支持独立写入的记录的数据库 在一个数据库中，不同的记录可能会在不同的时间点使用不同的 Writer 模式来写入 - 你不能假定所有的记录都有相同的模式。最简单的解决方案是在每个编码记录的开始处包含一个版本号，并在数据库中保留一个模式版本列表。Reader 可以获取记录，提取版本号，然后从数据库中获取该版本号的 Writer 模式。使用该 Writer 模式，它可以解码记录的其余部分（例如 Espresso 【23】就是这样工作的）。 通过网络连接发送记录 当两个进程通过双向网络连接进行通信时，他们可以在连接设置上协商模式版本，然后在连接的生命周期中使用该模式。 Avro RPC 协议（请参阅 “服务中的数据流：REST 与 RPC”）就是这样工作的。 具有模式版本的数据库在任何情况下都是非常有用的，因为它充当文档并为你提供了检查模式兼容性的机会【24】。作为版本号，你可以使用一个简单的递增整数，或者你可以使用模式的散列。 动态生成的模式与 Protocol Buffers 和 Thrift 相比，Avro 方法的一个优点是架构不包含任何标签号码。但为什么这很重要？在模式中保留一些数字有什么问题？ 不同之处在于 Avro 对动态生成的模式更友善。例如，假如你有一个关系数据库，你想要把它的内容转储到一个文件中，并且你想使用二进制格式来避免前面提到的文本格式（JSON，CSV，SQL）的问题。如果你使用 Avro，你可以很容易地从关系模式生成一个 Avro 模式（在我们之前看到的 JSON 表示中），并使用该模式对数据库内容进行编码，并将其全部转储到 Avro 对象容器文件【25】中。你为每个数据库表生成一个记录模式，每个列成为该记录中的一个字段。数据库中的列名称映射到 Avro 中的字段名称。 现在，如果数据库模式发生变化（例如，一个表中添加了一列，删除了一列），则可以从更新的数据库模式生成新的 Avro 模式，并在新的 Avro 模式中导出数据。数据导出过程不需要注意模式的改变 - 每次运行时都可以简单地进行模式转换。任何读取新数据文件的人都会看到记录的字段已经改变，但是由于字段是通过名字来标识的，所以更新的 Writer 模式仍然可以与旧的 Reader 模式匹配。 相比之下，如果你为此使用 Thrift 或 Protocol Buffers，则字段标签可能必须手动分配：每次数据库模式更改时，管理员都必须手动更新从数据库列名到字段标签的映射（这可能会自动化，但模式生成器必须非常小心，不要分配以前使用的字段标签）。这种动态生成的模式根本不是 Thrift 或 Protocol Buffers 的设计目标，而是 Avro 的。 代码生成和动态类型的语言Thrift 和 Protobuf 依赖于代码生成：在定义了模式之后，可以使用你选择的编程语言生成实现此模式的代码。这在 Java、C++ 或 C# 等静态类型语言中很有用，因为它允许将高效的内存中的数据结构用于解码的数据，并且在编写访问数据结构的程序时允许在 IDE 中进行类型检查和自动补全。 在动态类型编程语言（如 JavaScript、Ruby 或 Python）中，生成代码没有太多意义，因为没有编译时类型检查器来满足。代码生成在这些语言中经常被忽视，因为它们避免了显式的编译步骤。而且，对于动态生成的模式（例如从数据库表生成的 Avro 模式），代码生成对获取数据是一个不必要的障碍。 Avro 为静态类型编程语言提供了可选的代码生成功能，但是它也可以在不生成任何代码的情况下使用。如果你有一个对象容器文件（它嵌入了 Writer 模式），你可以简单地使用 Avro 库打开它，并以与查看 JSON 文件相同的方式查看数据。该文件是自描述的，因为它包含所有必要的元数据。 这个属性特别适用于动态类型的数据处理语言如 Apache Pig 【26】。在 Pig 中，你可以打开一些 Avro 文件，开始分析它们，并编写派生数据集以 Avro 格式输出文件，而无需考虑模式。 模式的优点正如我们所看到的，Protocol Buffers、Thrift 和 Avro 都使用模式来描述二进制编码格式。他们的模式语言比 XML 模式或者 JSON 模式简单得多，而后者支持更详细的验证规则（例如，“该字段的字符串值必须与该正则表达式匹配” 或 “该字段的整数值必须在 0 和 100 之间 “）。由于 Protocol Buffers，Thrift 和 Avro 实现起来更简单，使用起来也更简单，所以它们已经发展到支持相当广泛的编程语言。 这些编码所基于的想法绝不是新的。例如，它们与 ASN.1 有很多相似之处，它是 1984 年首次被标准化的模式定义语言【27】。它被用来定义各种网络协议，例如其二进制编码（DER）仍然被用于编码 SSL 证书（X.509）【28】。 ASN.1 支持使用标签号码的模式演进，类似于 Protocol Buffers 和 Thrift 【29】。然而，它也非常复杂，而且没有好的配套文档，所以 ASN.1 可能不是新应用程序的好选择。 许多数据系统也为其数据实现了某种专有的二进制编码。例如，大多数关系数据库都有一个网络协议，你可以通过该协议向数据库发送查询并获取响应。这些协议通常特定于特定的数据库，并且数据库供应商提供将来自数据库的网络协议的响应解码为内存数据结构的驱动程序（例如使用 ODBC 或 JDBC API）。 所以，我们可以看到，尽管 JSON、XML 和 CSV 等文本数据格式非常普遍，但基于模式的二进制编码也是一个可行的选择。他们有一些很好的属性： 它们可以比各种 “二进制 JSON” 变体更紧凑，因为它们可以省略编码数据中的字段名称。 模式是一种有价值的文档形式，因为模式是解码所必需的，所以可以确定它是最新的（而手动维护的文档可能很容易偏离现实）。 维护一个模式的数据库允许你在部署任何内容之前检查模式更改的向前和向后兼容性。 对于静态类型编程语言的用户来说，从模式生成代码的能力是有用的，因为它可以在编译时进行类型检查。 总而言之，模式演化保持了与 JSON 数据库提供的无模式 &#x2F; 读时模式相同的灵活性（请参阅 “文档模型中的模式灵活性”），同时还可以更好地保证你的数据并提供更好的工具。 数据流的类型在本章的开始部分，我们曾经说过，无论何时你想要将某些数据发送到不共享内存的另一个进程，例如，只要你想通过网络发送数据或将其写入文件，就需要将它编码为一个字节序列。然后我们讨论了做这个的各种不同的编码。 我们讨论了向前和向后的兼容性，这对于可演化性来说非常重要（通过允许你独立升级系统的不同部分，而不必一次改变所有内容，可以轻松地进行更改）。兼容性是编码数据的一个进程和解码它的另一个进程之间的一种关系。 这是一个相当抽象的概念 - 数据可以通过多种方式从一个流程流向另一个流程。谁编码数据，谁解码？在本章的其余部分中，我们将探讨数据如何在流程之间流动的一些最常见的方式： 通过数据库（请参阅 “数据库中的数据流”） 通过服务调用（请参阅 “服务中的数据流：REST 与 RPC”） 通过异步消息传递（请参阅 “消息传递中的数据流”） 数据库中的数据流在数据库中，写入数据库的过程对数据进行编码，从数据库读取的过程对数据进行解码。可能只有一个进程访问数据库，在这种情况下，读者只是相同进程的后续版本 - 在这种情况下，你可以考虑将数据库中的内容存储为向未来的自我发送消息。 向后兼容性显然是必要的。否则你未来的自己将无法解码你以前写的东西。 一般来说，几个不同的进程同时访问数据库是很常见的。这些进程可能是几个不同的应用程序或服务，或者它们可能只是几个相同服务的实例（为了可伸缩性或容错性而并行运行）。无论哪种方式，在应用程序发生变化的环境中，访问数据库的某些进程可能会运行较新的代码，有些进程可能会运行较旧的代码，例如，因为新版本当前正在部署滚动升级，所以有些实例已经更新，而其他实例尚未更新。 这意味着数据库中的一个值可能会被更新版本的代码写入，然后被仍旧运行的旧版本的代码读取。因此，数据库也经常需要向前兼容。 但是，还有一个额外的障碍。假设你将一个字段添加到记录模式，并且较新的代码将该新字段的值写入数据库。随后，旧版本的代码（尚不知道新字段）将读取记录，更新记录并将其写回。在这种情况下，理想的行为通常是旧代码保持新的字段不变，即使它不能被解释。 前面讨论的编码格式支持未知字段的保存，但是有时候需要在应用程序层面保持谨慎，如图 4-7 所示。例如，如果将数据库值解码为应用程序中的模型对象，稍后重新编码这些模型对象，那么未知字段可能会在该翻译过程中丢失。解决这个问题不是一个难题，你只需要意识到它。 图 4-7 当较旧版本的应用程序更新以前由较新版本的应用程序编写的数据时，如果不小心，数据可能会丢失。 在不同的时间写入不同的值数据库通常允许任何时候更新任何值。这意味着在一个单一的数据库中，可能有一些值是五毫秒前写的，而一些值是五年前写的。 在部署应用程序的新版本时，也许用不了几分钟就可以将所有的旧版本替换为新版本（至少服务器端应用程序是这样的）。但数据库内容并非如此：对于五年前的数据来说，除非对其进行显式重写，否则它仍然会以原始编码形式存在。这种现象有时被概括为：数据的生命周期超出代码的生命周期。 将数据重写（迁移）到一个新的模式当然是可能的，但是在一个大数据集上执行是一个昂贵的事情，所以大多数数据库如果可能的话就避免它。大多数关系数据库都允许简单的模式更改，例如添加一个默认值为空的新列，而不重写现有数据 [^v]。读取旧行时，对于磁盘上的编码数据缺少的任何列，数据库将填充空值。 LinkedIn 的文档数据库 Espresso 使用 Avro 存储，允许它使用 Avro 的模式演变规则【23】。 因此，模式演变允许整个数据库看起来好像是用单个模式编码的，即使底层存储可能包含用各种历史版本的模式编码的记录。 [^v]: 除了 MySQL，即使并非真的必要，它也经常会重写整个表，正如 “文档模型中的模式灵活性” 中所提到的。 归档存储也许你不时为数据库创建一个快照，例如备份或加载到数据仓库（请参阅 “数据仓库”）。在这种情况下，即使源数据库中的原始编码包含来自不同时代的模式版本的混合，数据转储通常也将使用最新模式进行编码。既然你不管怎样都要拷贝数据，那么你可以对这个数据拷贝进行一致的编码。 由于数据转储是一次写入的，而且以后是不可变的，所以 Avro 对象容器文件等格式非常适合。这也是一个很好的机会，可以将数据编码为面向分析的列式格式，例如 Parquet（请参阅 “列压缩”）。 在 第十章 中，我们将详细讨论使用档案存储中的数据。 服务中的数据流：REST与RPC当你需要通过网络进行进程间的通讯时，安排该通信的方式有几种。最常见的安排是有两个角色：客户端和服务器。服务器通过网络公开 API，并且客户端可以连接到服务器以向该 API 发出请求。服务器公开的 API 被称为服务。 Web 以这种方式工作：客户（Web 浏览器）向 Web 服务器发出请求，通过 GET 请求下载 HTML、CSS、JavaScript、图像等，并通过 POST 请求提交数据到服务器。 API 包含一组标准的协议和数据格式（HTTP、URL、SSL&#x2F;TLS、HTML 等）。由于网络浏览器、网络服务器和网站作者大多同意这些标准，你可以使用任何网络浏览器访问任何网站（至少在理论上！）。 Web 浏览器不是唯一的客户端类型。例如，在移动设备或桌面计算机上运行的本地应用程序也可以向服务器发出网络请求，并且在 Web 浏览器内运行的客户端 JavaScript 应用程序可以使用 XMLHttpRequest 成为 HTTP 客户端（该技术被称为 Ajax 【30】）。在这种情况下，服务器的响应通常不是用于显示给人的 HTML，而是便于客户端应用程序代码进一步处理的编码数据（如 JSON）。尽管 HTTP 可能被用作传输协议，但顶层实现的 API 是特定于应用程序的，客户端和服务器需要就该 API 的细节达成一致。 此外，服务器本身可以是另一个服务的客户端（例如，典型的 Web 应用服务器充当数据库的客户端）。这种方法通常用于将大型应用程序按照功能区域分解为较小的服务，这样当一个服务需要来自另一个服务的某些功能或数据时，就会向另一个服务发出请求。这种构建应用程序的方式传统上被称为 面向服务的体系结构（service-oriented architecture，SOA），最近被改进和更名为 微服务架构【31,32】。 在某些方面，服务类似于数据库：它们通常允许客户端提交和查询数据。但是，虽然数据库允许使用我们在 第二章 中讨论的查询语言进行任意查询，但是服务公开了一个特定于应用程序的 API，它只允许由服务的业务逻辑（应用程序代码）预定的输入和输出【33】。这种限制提供了一定程度的封装：服务能够对客户可以做什么和不可以做什么施加细粒度的限制。 面向服务 &#x2F; 微服务架构的一个关键设计目标是通过使服务独立部署和演化来使应用程序更易于更改和维护。例如，每个服务应该由一个团队拥有，并且该团队应该能够经常发布新版本的服务，而不必与其他团队协调。换句话说，我们应该期望服务器和客户端的旧版本和新版本同时运行，因此服务器和客户端使用的数据编码必须在不同版本的服务 API 之间兼容 —— 这正是我们在本章所一直在谈论的。 Web服务当服务使用 HTTP 作为底层通信协议时，可称之为 Web 服务。这可能是一个小错误，因为 Web 服务不仅在 Web 上使用，而且在几个不同的环境中使用。例如： 运行在用户设备上的客户端应用程序（例如，移动设备上的本地应用程序，或使用 Ajax 的 JavaScript web 应用程序）通过 HTTP 向服务发出请求。这些请求通常通过公共互联网进行。 一种服务向同一组织拥有的另一项服务提出请求，这些服务通常位于同一数据中心内，作为面向服务 &#x2F; 微服务架构的一部分。 （支持这种用例的软件有时被称为 中间件（middleware） ） 一种服务通过互联网向不同组织所拥有的服务提出请求。这用于不同组织后端系统之间的数据交换。此类别包括由在线服务（如信用卡处理系统）提供的公共 API，或用于共享访问用户数据的 OAuth。 有两种流行的 Web 服务方法：REST 和 SOAP。他们在哲学方面几乎是截然相反的，往往也是各自支持者之间的激烈辩论的主题 [^vi]。 [^vi]: 即使在每个阵营内也有很多争论。 例如，HATEOAS（超媒体作为应用程序状态的引擎） 就经常引发讨论【35】。 REST 不是一个协议，而是一个基于 HTTP 原则的设计哲学【34,35】。它强调简单的数据格式，使用 URL 来标识资源，并使用 HTTP 功能进行缓存控制，身份验证和内容类型协商。与 SOAP 相比，REST 已经越来越受欢迎，至少在跨组织服务集成的背景下【36】，并经常与微服务相关【31】。根据 REST 原则设计的 API 称为 RESTful。 相比之下，SOAP 是用于制作网络 API 请求的基于 XML 的协议 [^vii]。虽然它最常用于 HTTP，但其目的是独立于 HTTP，并避免使用大多数 HTTP 功能。相反，它带有庞大而复杂的多种相关标准（Web 服务框架，称为 WS-*），它们增加了各种功能【37】。 [^vii]: 尽管首字母缩写词相似，SOAP 并不是 SOA 的要求。 SOAP 是一种特殊的技术，而 SOA 是构建系统的一般方法。 SOAP Web 服务的 API 使用称为 Web 服务描述语言（WSDL）的基于 XML 的语言来描述。 WSDL 支持代码生成，客户端可以使用本地类和方法调用（编码为 XML 消息并由框架再次解码）访问远程服务。这在静态类型编程语言中非常有用，但在动态类型编程语言中很少（请参阅 “代码生成和动态类型的语言”）。 由于 WSDL 的设计不是人类可读的，而且由于 SOAP 消息通常因为过于复杂而无法手动构建，所以 SOAP 的用户在很大程度上依赖于工具支持，代码生成和 IDE【38】。对于 SOAP 供应商不支持的编程语言的用户来说，与 SOAP 服务的集成是困难的。 尽管 SOAP 及其各种扩展表面上是标准化的，但是不同厂商的实现之间的互操作性往往会造成问题【39】。由于所有这些原因，尽管许多大型企业仍然使用 SOAP，但在大多数小公司中已经不再受到青睐。 REST 风格的 API 倾向于更简单的方法，通常涉及较少的代码生成和自动化工具。定义格式（如 OpenAPI，也称为 Swagger 【40】）可用于描述 RESTful API 并生成文档。 远程过程调用（RPC）的问题Web 服务仅仅是通过网络进行 API 请求的一系列技术的最新版本，其中许多技术受到了大量的炒作，但是存在严重的问题。 Enterprise JavaBeans（EJB）和 Java 的 远程方法调用（RMI） 仅限于 Java。分布式组件对象模型（DCOM） 仅限于 Microsoft 平台。公共对象请求代理体系结构（CORBA） 过于复杂，不提供前向或后向兼容性【41】。 所有这些都是基于 远程过程调用（RPC） 的思想，该过程调用自 20 世纪 70 年代以来一直存在【42】。 RPC 模型试图向远程网络服务发出请求，看起来与在同一进程中调用编程语言中的函数或方法相同（这种抽象称为位置透明）。尽管 RPC 起初看起来很方便，但这种方法根本上是有缺陷的【43,44】。网络请求与本地函数调用非常不同： 本地函数调用是可预测的，并且成功或失败仅取决于受你控制的参数。网络请求是不可预测的：请求或响应可能由于网络问题会丢失，或者远程计算机可能很慢或不可用，这些问题完全不在你的控制范围之内。网络问题很常见，因此必须有所准备，例如重试失败的请求。 本地函数调用要么返回结果，要么抛出异常，或者永远不返回（因为进入无限循环或进程崩溃）。网络请求有另一个可能的结果：由于超时，它返回时可能没有结果。在这种情况下，你根本不知道发生了什么：如果你没有得到来自远程服务的响应，你无法知道请求是否通过（我们将在 第八章 更详细地讨论这个问题）。 如果你重试失败的网络请求，可能会发生请求实际上已经完成，只是响应丢失的情况。在这种情况下，重试将导致该操作被执行多次，除非你在协议中建立数据去重机制（幂等性，即 idempotence）。本地函数调用时没有这样的问题。 （在 第十一章 更详细地讨论幂等性） 每次调用本地函数时，通常需要大致相同的时间来执行。网络请求比函数调用要慢得多，而且其延迟也是非常可变的：好的时候它可能会在不到一毫秒的时间内完成，但是当网络拥塞或者远程服务超载时，可能需要几秒钟的时间才能完成相同的操作。 调用本地函数时，可以高效地将引用（指针）传递给本地内存中的对象。当你发出一个网络请求时，所有这些参数都需要被编码成可以通过网络发送的一系列字节。如果参数是像数字或字符串这样的基本类型倒是没关系，但是对于较大的对象很快就会出现问题。 客户端和服务可以用不同的编程语言实现，所以 RPC 框架必须将数据类型从一种语言翻译成另一种语言。这可能会变得很丑陋，因为不是所有的语言都具有相同的类型 —— 例如回想一下 JavaScript 的数字大于 $2^{53}$ 的问题（请参阅 “JSON、XML 和二进制变体”）。用单一语言编写的单个进程中不存在此问题。 所有这些因素意味着尝试使远程服务看起来像编程语言中的本地对象一样毫无意义，因为这是一个根本不同的事情。 REST 的部分吸引力在于，它并不试图隐藏它是一个网络协议的事实（尽管这似乎并没有阻止人们在 REST 之上构建 RPC 库）。 RPC的当前方向尽管有这样那样的问题，RPC 不会消失。在本章提到的所有编码的基础上构建了各种 RPC 框架：例如，Thrift 和 Avro 带有 RPC 支持，gRPC 是使用 Protocol Buffers 的 RPC 实现，Finagle 也使用 Thrift，Rest.li 使用 JSON over HTTP。 这种新一代的 RPC 框架更加明确的是，远程请求与本地函数调用不同。例如，Finagle 和 Rest.li 使用 futures（promises）来封装可能失败的异步操作。Futures 还可以简化需要并行发出多项服务并将其结果合并的情况【45】。 gRPC 支持流，其中一个调用不仅包括一个请求和一个响应，还可以是随时间的一系列请求和响应【46】。 其中一些框架还提供服务发现，即允许客户端找出在哪个 IP 地址和端口号上可以找到特定的服务。我们将在 “请求路由” 中回到这个主题。 使用二进制编码格式的自定义 RPC 协议可以实现比通用的 JSON over REST 更好的性能。但是，RESTful API 还有其他一些显著的优点：方便实验和调试（只需使用 Web 浏览器或命令行工具 curl，无需任何代码生成或软件安装即可向其请求），能被所有主流的编程语言和平台所支持，还有大量可用的工具（服务器、缓存、负载平衡器、代理、防火墙、监控、调试工具、测试工具等）的生态系统。 由于这些原因，REST 似乎是公共 API 的主要风格。 RPC 框架的主要重点在于同一组织拥有的服务之间的请求，通常在同一数据中心内。 数据编码与RPC的演化对于可演化性，重要的是可以独立更改和部署 RPC 客户端和服务器。与通过数据库流动的数据相比（如上一节所述），我们可以在通过服务进行数据流的情况下做一个简化的假设：假定所有的服务器都会先更新，其次是所有的客户端。因此，你只需要在请求上具有向后兼容性，并且对响应具有前向兼容性。 RPC 方案的前后向兼容性属性从它使用的编码方式中继承： Thrift、gRPC（Protobuf）和 Avro RPC 可以根据相应编码格式的兼容性规则进行演变。 在 SOAP 中，请求和响应是使用 XML 模式指定的。这些可以演变，但有一些微妙的陷阱【47】。 RESTful API 通常使用 JSON（没有正式指定的模式）用于响应，以及用于请求的 JSON 或 URI 编码 &#x2F; 表单编码的请求参数。添加可选的请求参数并向响应对象添加新的字段通常被认为是保持兼容性的改变。 由于 RPC 经常被用于跨越组织边界的通信，所以服务的兼容性变得更加困难，因此服务的提供者经常无法控制其客户，也不能强迫他们升级。因此，需要长期保持兼容性，也许是无限期的。如果需要进行兼容性更改，则服务提供商通常会并排维护多个版本的服务 API。 关于 API 版本化应该如何工作（即，客户端如何指示它想要使用哪个版本的 API）没有一致意见【48】）。对于 RESTful API，常用的方法是在 URL 或 HTTP Accept 头中使用版本号。对于使用 API 密钥来标识特定客户端的服务，另一种选择是将客户端请求的 API 版本存储在服务器上，并允许通过单独的管理界面更新该版本选项【49】。 消息传递中的数据流我们一直在研究从一个过程到另一个过程的编码数据流的不同方式。到目前为止，我们已经讨论了 REST 和 RPC（其中一个进程通过网络向另一个进程发送请求并期望尽可能快的响应）以及数据库（一个进程写入编码数据，另一个进程在将来再次读取）。 在最后一节中，我们将简要介绍一下 RPC 和数据库之间的异步消息传递系统。它们与 RPC 类似，因为客户端的请求（通常称为消息）以低延迟传送到另一个进程。它们与数据库类似，不是通过直接的网络连接发送消息，而是通过称为消息代理（也称为消息队列或面向消息的中间件）的中介来临时存储消息。 与直接 RPC 相比，使用消息代理有几个优点： 如果收件人不可用或过载，可以充当缓冲区，从而提高系统的可靠性。 它可以自动将消息重新发送到已经崩溃的进程，从而防止消息丢失。 避免发件人需要知道收件人的 IP 地址和端口号（这在虚拟机经常出入的云部署中特别有用）。 它允许将一条消息发送给多个收件人。 将发件人与收件人逻辑分离（发件人只是发布邮件，不关心使用者）。 然而，与 RPC 相比，差异在于消息传递通信通常是单向的：发送者通常不期望收到其消息的回复。一个进程可能发送一个响应，但这通常是在一个单独的通道上完成的。这种通信模式是异步的：发送者不会等待消息被传递，而只是发送它，然后忘记它。 消息代理过去，消息代理（Message Broker） 主要是 TIBCO、IBM WebSphere 和 webMethods 等公司的商业软件的秀场。最近像 RabbitMQ、ActiveMQ、HornetQ、NATS 和 Apache Kafka 这样的开源实现已经流行起来。我们将在 第十一章 中对它们进行更详细的比较。 详细的交付语义因实现和配置而异，但通常情况下，消息代理的使用方式如下：一个进程将消息发送到指定的队列或主题，代理确保将消息传递给那个队列或主题的一个或多个消费者或订阅者。在同一主题上可以有许多生产者和许多消费者。 一个主题只提供单向数据流。但是，消费者本身可能会将消息发布到另一个主题上（因此，可以将它们链接在一起，就像我们将在 第十一章 中看到的那样），或者发送给原始消息的发送者使用的回复队列（允许请求 &#x2F; 响应数据流，类似于 RPC）。 消息代理通常不会执行任何特定的数据模型 —— 消息只是包含一些元数据的字节序列，因此你可以使用任何编码格式。如果编码是向后和向前兼容的，你可以灵活地对发布者和消费者的编码进行独立的修改，并以任意顺序进行部署。 如果消费者重新发布消息到另一个主题，则可能需要小心保留未知字段，以防止前面在数据库环境中描述的问题（图 4-7）。 分布式的Actor框架Actor 模型是单个进程中并发的编程模型。逻辑被封装在 actor 中，而不是直接处理线程（以及竞争条件、锁定和死锁的相关问题）。每个 actor 通常代表一个客户或实体，它可能有一些本地状态（不与其他任何角色共享），它通过发送和接收异步消息与其他角色通信。不保证消息传送：在某些错误情况下，消息将丢失。由于每个角色一次只能处理一条消息，因此不需要担心线程，每个角色可以由框架独立调度。 在分布式 Actor 框架中，此编程模型用于跨多个节点伸缩应用程序。不管发送方和接收方是在同一个节点上还是在不同的节点上，都使用相同的消息传递机制。如果它们在不同的节点上，则该消息被透明地编码成字节序列，通过网络发送，并在另一侧解码。 位置透明在 actor 模型中比在 RPC 中效果更好，因为 actor 模型已经假定消息可能会丢失，即使在单个进程中也是如此。尽管网络上的延迟可能比同一个进程中的延迟更高，但是在使用 actor 模型时，本地和远程通信之间的基本不匹配是较少的。 分布式的 Actor 框架实质上是将消息代理和 actor 编程模型集成到一个框架中。但是，如果要执行基于 actor 的应用程序的滚动升级，则仍然需要担心向前和向后兼容性问题，因为消息可能会从运行新版本的节点发送到运行旧版本的节点，反之亦然。 三个流行的分布式 actor 框架处理消息编码如下： 默认情况下，Akka 使用 Java 的内置序列化，不提供前向或后向兼容性。 但是，你可以用类似 Prototol Buffers 的东西替代它，从而获得滚动升级的能力【50】。 Orleans 默认使用不支持滚动升级部署的自定义数据编码格式；要部署新版本的应用程序，你需要设置一个新的集群，将流量从旧集群迁移到新集群，然后关闭旧集群【51,52】。 像 Akka 一样，可以使用自定义序列化插件。 在 Erlang OTP 中，对记录模式进行更改是非常困难的（尽管系统具有许多为高可用性设计的功能）。 滚动升级是可能的，但需要仔细计划【53】。 一个新的实验性的 maps 数据类型（2014 年在 Erlang R17 中引入的类似于 JSON 的结构）可能使得这个数据类型在未来更容易【54】。 本章小结在本章中，我们研究了将数据结构转换为网络中的字节或磁盘上的字节的几种方法。我们看到了这些编码的细节不仅影响其效率，更重要的是也影响了应用程序的体系结构和部署它们的选项。 特别是，许多服务需要支持滚动升级，其中新版本的服务逐步部署到少数节点，而不是同时部署到所有节点。滚动升级允许在不停机的情况下发布新版本的服务（从而鼓励在罕见的大型版本上频繁发布小型版本），并使部署风险降低（允许在影响大量用户之前检测并回滚有故障的版本）。这些属性对于可演化性，以及对应用程序进行更改的容易性都是非常有利的。 在滚动升级期间，或出于各种其他原因，我们必须假设不同的节点正在运行我们的应用程序代码的不同版本。因此，在系统周围流动的所有数据都是以提供向后兼容性（新代码可以读取旧数据）和向前兼容性（旧代码可以读取新数据）的方式进行编码是重要的。 我们讨论了几种数据编码格式及其兼容性属性： 编程语言特定的编码仅限于单一编程语言，并且往往无法提供前向和后向兼容性。 JSON、XML 和 CSV 等文本格式非常普遍，其兼容性取决于你如何使用它们。他们有可选的模式语言，这有时是有用的，有时是一个障碍。这些格式对于数据类型有些模糊，所以你必须小心数字和二进制字符串。 像 Thrift、Protocol Buffers 和 Avro 这样的二进制模式驱动格式允许使用清晰定义的前向和后向兼容性语义进行紧凑，高效的编码。这些模式可以用于静态类型语言的文档和代码生成。但是，他们有一个缺点，就是在数据可读之前需要对数据进行解码。 我们还讨论了数据流的几种模式，说明了数据编码重要性的不同场景： 数据库，写入数据库的进程对数据进行编码，并从数据库读取进程对其进行解码 RPC 和 REST API，客户端对请求进行编码，服务器对请求进行解码并对响应进行编码，客户端最终对响应进行解码 异步消息传递（使用消息代理或参与者），其中节点之间通过发送消息进行通信，消息由发送者编码并由接收者解码 我们可以小心地得出这样的结论：前向兼容性和滚动升级在某种程度上是可以实现的。愿你的应用程序的演变迅速、敏捷部署。 参考文献 “Java Object Serialization Specification,” docs.oracle.com, 2010. “Ruby 2.2.0 API Documentation,” ruby-doc.org, Dec 2014. “The Python 3.4.3 Standard Library Reference Manual,” docs.python.org, February 2015. “EsotericSoftware&#x2F;kryo,” github.com, October 2014. “CWE-502: Deserialization of Untrusted Data,” Common Weakness Enumeration, cwe.mitre.org, July 30, 2014. Steve Breen: “What Do WebLogic, WebSphere, JBoss, Jenkins, OpenNMS, and Your Application Have in Common? This Vulnerability,” foxglovesecurity.com, November 6, 2015. Patrick McKenzie: “What the Rails Security Issue Means for Your Startup,” kalzumeus.com, January 31, 2013. Eishay Smith: “jvm-serializers wiki,” github.com, November 2014. “XML Is a Poor Copy of S-Expressions,” c2.com wiki. Matt Harris: “Snowflake: An Update and Some Very Important Information,” email to Twitter Development Talk mailing list, October 19, 2010. Shudi (Sandy) Gao, C. M. Sperberg-McQueen, and Henry S. Thompson: “XML Schema 1.1,” W3C Recommendation, May 2001. Francis Galiegue, Kris Zyp, and Gary Court: “JSON Schema,” IETF Internet-Draft, February 2013. Yakov Shafranovich: “RFC 4180: Common Format and MIME Type for Comma-Separated Values (CSV) Files,” October 2005. “MessagePack Specification,” msgpack.org. Mark Slee, Aditya Agarwal, and Marc Kwiatkowski: “Thrift: Scalable Cross-Language Services Implementation,” Facebook technical report, April 2007. “Protocol Buffers Developer Guide,” Google, Inc., developers.google.com. Igor Anishchenko: “Thrift vs Protocol Buffers vs Avro - Biased Comparison,” slideshare.net, September 17, 2012. “A Matrix of the Features Each Individual Language Library Supports,” wiki.apache.org. Martin Kleppmann: “Schema Evolution in Avro, Protocol Buffers and Thrift,” martin.kleppmann.com, December 5, 2012. “Apache Avro 1.7.7 Documentation,” avro.apache.org, July 2014. Doug Cutting, Chad Walters, Jim Kellerman, et al.: “&amp;#91;PROPOSAL&amp;#93; New Subproject: Avro,” email thread on hadoop-general mailing list, mail-archives.apache.org, April 2009. Tony Hoare: “Null References: The Billion Dollar Mistake,” at QCon London, March 2009. Aditya Auradkar and Tom Quiggle: “Introducing Espresso—LinkedIn’s Hot New Distributed Document Store,” engineering.linkedin.com, January 21, 2015. Jay Kreps: “Putting Apache Kafka to Use: A Practical Guide to Building a Stream Data Platform (Part 2),” blog.confluent.io, February 25, 2015. Gwen Shapira: “The Problem of Managing Schemas,” radar.oreilly.com, November 4, 2014. “Apache Pig 0.14.0 Documentation,” pig.apache.org, November 2014. John Larmouth: ASN.1Complete. Morgan Kaufmann, 1999. ISBN: 978-0-122-33435-1 Russell Housley, Warwick Ford, Tim Polk, and David Solo: “RFC 2459: Internet X.509 Public Key Infrastructure: Certificate and CRL Profile,” IETF Network Working Group, Standards Track, January 1999. Lev Walkin: “Question: Extensibility and Dropping Fields,” lionet.info, September 21, 2010. Jesse James Garrett: “Ajax: A New Approach to Web Applications,” adaptivepath.com, February 18, 2005. Sam Newman: Building Microservices. O’Reilly Media, 2015. ISBN: 978-1-491-95035-7 Chris Richardson: “Microservices: Decomposing Applications for Deployability and Scalability,” infoq.com, May 25, 2014. Pat Helland: “Data on the Outside Versus Data on the Inside,” at 2nd Biennial Conference on Innovative Data Systems Research (CIDR), January 2005. Roy Thomas Fielding: “Architectural Styles and the Design of Network-Based Software Architectures,” PhD Thesis, University of California, Irvine, 2000. Roy Thomas Fielding: “REST APIs Must Be Hypertext-Driven,” roy.gbiv.com, October 20 2008. “REST in Peace, SOAP,” royal.pingdom.com, October 15, 2010. “Web Services Standards as of Q1 2007,” innoq.com, February 2007. Pete Lacey: “The S Stands for Simple,” harmful.cat-v.org, November 15, 2006. Stefan Tilkov: “Interview: Pete Lacey Criticizes Web Services,” infoq.com, December 12, 2006. “OpenAPI Specification (fka Swagger RESTful API Documentation Specification) Version 2.0,” swagger.io, September 8, 2014. Michi Henning: “The Rise and Fall of CORBA,” ACM Queue, volume 4, number 5, pages 28–34, June 2006. doi:10.1145&#x2F;1142031.1142044 Andrew D. Birrell and Bruce Jay Nelson: “Implementing Remote Procedure Calls,” ACM Transactions on Computer Systems (TOCS), volume 2, number 1, pages 39–59, February 1984. doi:10.1145&#x2F;2080.357392 Jim Waldo, Geoff Wyant, Ann Wollrath, and Sam Kendall: “A Note on Distributed Computing,” Sun Microsystems Laboratories, Inc., Technical Report TR-94-29, November 1994. Steve Vinoski: “Convenience over Correctness,” IEEE Internet Computing, volume 12, number 4, pages 89–92, July 2008. doi:10.1109&#x2F;MIC.2008.75 Marius Eriksen: “Your Server as a Function,” at 7th Workshop on Programming Languages and Operating Systems (PLOS), November 2013. doi:10.1145&#x2F;2525528.2525538 “grpc-common Documentation,” Google, Inc., github.com, February 2015. Aditya Narayan and Irina Singh: “Designing and Versioning Compatible Web Services,” ibm.com, March 28, 2007. Troy Hunt: “Your API Versioning Is Wrong, Which Is Why I Decided to Do It 3 Different Wrong Ways,” troyhunt.com, February 10, 2014. “API Upgrades,” Stripe, Inc., April 2015. Jonas Bonér: “Upgrade in an Akka Cluster,” email to akka-user mailing list, grokbase.com, August 28, 2013. Philip A. Bernstein, Sergey Bykov, Alan Geller, et al.: “Orleans: Distributed Virtual Actors for Programmability and Scalability,” Microsoft Research Technical Report MSR-TR-2014-41, March 2014. “Microsoft Project Orleans Documentation,” Microsoft Research, dotnet.github.io, 2015. David Mercer, Sean Hinde, Yinso Chen, and Richard A O’Keefe: “beginner: Updating Data Structures,” email thread on erlang-questions mailing list, erlang.com, October 29, 2007. Fred Hebert: “Postscript: Maps,” learnyousomeerlang.com, April 9, 2014."},{"title":"第五章：复制","path":"/wiki/ddia/ch5.html","content":"与可能出错的东西比，“不可能”出错的东西最显著的特点就是：一旦真的出错，通常就彻底玩完了。 —— 道格拉斯・亚当斯（1992） 复制意味着在通过网络连接的多台机器上保留相同数据的副本。正如在 第二部分 的介绍中所讨论的那样，我们希望能复制数据，可能出于各种各样的原因： 使得数据与用户在地理上接近（从而减少延迟） 即使系统的一部分出现故障，系统也能继续工作（从而提高可用性） 伸缩可以接受读请求的机器数量（从而提高读取吞吐量） 本章将假设你的数据集非常小，每台机器都可以保存整个数据集的副本。在 第六章 中将放宽这个假设，讨论对单个机器来说太大的数据集的分割（分片）。在后面的章节中，我们将讨论复制数据系统中可能发生的各种故障，以及如何处理这些故障。 如果复制中的数据不会随时间而改变，那复制就很简单：将数据复制到每个节点一次就万事大吉。复制的困难之处在于处理复制数据的 变更（change），这就是本章所要讲的。我们将讨论三种流行的变更复制算法：单领导者（single leader，单主），多领导者（multi leader，多主） 和 无领导者（leaderless，无主）。几乎所有分布式数据库都使用这三种方法之一。 在复制时需要进行许多权衡：例如，使用同步复制还是异步复制？如何处理失败的副本？这些通常是数据库中的配置选项，细节因数据库而异，但原理在许多不同的实现中都类似。本章会讨论这些决策的后果。 数据库的复制算得上是老生常谈了 ——70 年代研究得出的基本原则至今没有太大变化【1】，因为网络的基本约束仍保持不变。然而在研究之外，许多开发人员仍然假设一个数据库只有一个节点。分布式数据库变为主流只是最近发生的事。许多程序员都是这一领域的新手，因此对于诸如 最终一致性（eventual consistency） 等问题存在许多误解。在 “复制延迟问题” 一节，我们将更加精确地了解最终一致性，并讨论诸如 读己之写（read-your-writes） 和 单调读（monotonic read） 等内容。 领导者与追随者存储了数据库拷贝的每个节点被称为 副本（replica） 。当存在多个副本时，会不可避免的出现一个问题：如何确保所有数据都落在了所有的副本上？ 每一次向数据库的写入操作都需要传播到所有副本上，否则副本就会包含不一样的数据。最常见的解决方案被称为 基于领导者的复制（leader-based replication） （也称 主动&#x2F;被动（active&#x2F;passive） 复制或 主&#x2F;从（master&#x2F;slave） 复制），如 图 5-1 所示。它的工作原理如下： 其中一个副本被指定为 领导者（leader），也称为 主库（master|primary） 。当客户端要向数据库写入时，它必须将请求发送给该 领导者，其会将新数据写入其本地存储。 其他副本被称为 追随者（followers），亦称为 只读副本（read replicas）、从库（slaves）、备库（ secondaries） 或 热备（hot-standby）[^i]。每当领导者将新数据写入本地存储时，它也会将数据变更发送给所有的追随者，称之为 复制日志（replication log） 或 变更流（change stream）。每个跟随者从领导者拉取日志，并相应更新其本地数据库副本，方法是按照与领导者相同的处理顺序来进行所有写入。 当客户想要从数据库中读取数据时，它可以向领导者或任一追随者进行查询。但只有领导者才能接受写入操作（从客户端的角度来看从库都是只读的）。 [^i]: 不同的人对 热（hot）、温（warm） 和 冷（cold） 备份服务器有不同的定义。例如在 PostgreSQL 中，热备（hot standby） 指的是能接受客户端读请求的副本。而 温备（warm standby） 只是追随领导者，但不处理客户端的任何查询。就本书而言，这些差异并不重要。 图 5-1 基于领导者的（主&#x2F;从）复制 这种复制模式是许多关系数据库的内置功能，如 PostgreSQL（从 9.0 版本开始）、MySQL、Oracle Data Guard【2】和 SQL Server 的 AlwaysOn 可用性组【3】。 它也被用于一些非关系数据库，包括 MongoDB、RethinkDB 和 Espresso【4】。最后，基于领导者的复制并不仅限于数据库：像 Kafka【5】和 RabbitMQ 高可用队列【6】这样的分布式消息代理也使用它。某些网络文件系统，例如 DRBD 这样的块复制设备也与之类似。 同步复制与异步复制复制系统的一个重要细节是：复制是 同步（synchronously） 发生的还是 异步（asynchronously） 发生的。（在关系型数据库中这通常是一个配置项，其他系统则通常硬编码为其中一个）。 想象一下 图 5-1 中发生的场景，即网站的用户更新他们的个人头像。在某个时间点，客户向主库发送更新请求；不久之后主库就收到了请求。在某个时间点，主库又会将数据变更转发给自己的从库。最终，主库通知客户更新成功。 图 5-2 显示了系统各个组件之间的通信：用户客户端、主库和两个从库。时间从左向右流动。请求或响应消息用粗箭头表示。 图 5-2 基于领导者的复制：一个同步从库和一个异步从库 在 图 5-2 的示例中，从库 1 的复制是同步的：在向用户报告写入成功并使结果对其他用户可见之前，主库需要等待从库 1 的确认，确保从库 1 已经收到写入操作。而从库 2 的复制是异步的：主库发送消息，但不等待该从库的响应。 在这幅图中，从库 2 处理消息前存在一个显著的延迟。通常情况下，复制的速度相当快：大多数数据库系统能在不到一秒内完成从库的同步，但它们不能提供复制用时的保证。有些情况下，从库可能落后主库几分钟或更久，例如：从库正在从故障中恢复，系统正在最大容量附近运行，或者当节点间存在网络问题时。 同步复制的优点是，从库能保证有与主库一致的最新数据副本。如果主库突然失效，我们可以确信这些数据仍然能在从库上找到。缺点是，如果同步从库没有响应（比如它已经崩溃，或者出现网络故障，或其它任何原因），主库就无法处理写入操作。主库必须阻止所有写入，并等待同步副本再次可用。 因此，将所有从库都设置为同步的是不切实际的：任何一个节点的中断都会导致整个系统停滞不前。实际上，如果在数据库上启用同步复制，通常意味着其中 一个 从库是同步的，而其他的从库则是异步的。如果该同步从库变得不可用或缓慢，则将一个异步从库改为同步运行。这保证你至少在两个节点上拥有最新的数据副本：主库和同步从库。 这种配置有时也被称为 半同步（semi-synchronous）【7】。 通常情况下，基于领导者的复制都配置为完全异步。在这种情况下，如果主库失效且不可恢复，则任何尚未复制给从库的写入都会丢失。这意味着即使已经向客户端确认成功，写入也不能保证是 持久（Durable） 的。然而，一个完全异步的配置也有优点：即使所有的从库都落后了，主库也可以继续处理写入。 弱化的持久性可能听起来像是一个坏的折衷，但异步复制其实已经被广泛使用了，特别是在有很多从库的场景下，或者当从库在地理上分布很广的时候。我们将在讨论 “复制延迟问题” 时回到这个问题。 关于复制的研究 对于异步复制系统而言，主库故障时会丢失数据可能是一个严重的问题，因此研究人员仍在研究不丢数据但仍能提供良好性能和可用性的复制方法。例如，链式复制（chain replication）【8,9】是同步复制的一种变体，已经在一些系统（如 Microsoft Azure Storage【10,11】）中成功实现。 复制的一致性与 共识（consensus，使几个节点就某个值达成一致）之间有着密切的联系，第九章 将详细地探讨这一领域的理论。本章主要讨论实践中的数据库常用的简单复制形式。 设置新从库有时候需要设置一个新的从库：也许是为了增加副本的数量，或替换失败的节点。如何确保新的从库拥有主库数据的精确副本？ 简单地将数据文件从一个节点复制到另一个节点通常是不够的：客户端不断向数据库写入数据，数据总是在不断地变化，标准的文件复制会看到数据库的不同部分在不同的时间点的内容，其结果可能没有任何意义。 可以通过锁定数据库（使其不可用于写入）来使磁盘上的文件保持一致，但是这会违背高可用的目标。幸运的是，设置新从库通常并不需要停机。从概念上讲，其过程如下所示： 在某个时刻获取主库的一致性快照（如果可能，不必锁定整个数据库）。大多数数据库都具有这个功能，因为它是备份必需的。对于某些场景，可能需要第三方工具，例如用于 MySQL 的 innobackupex【12】。 将快照复制到新的从库节点。 从库连接到主库，并拉取快照之后发生的所有数据变更。这要求快照与主库复制日志中的位置精确关联。该位置有不同的名称，例如 PostgreSQL 将其称为 日志序列号（log sequence number，LSN），MySQL 将其称为 二进制日志坐标（binlog coordinates）。 当从库处理完快照之后积累的数据变更，我们就说它 赶上（caught up） 了主库，现在它可以继续及时处理主库产生的数据变化了。 建立从库的实际步骤因数据库而异。在某些系统中，这个过程是完全自动化的，而在另外一些系统中，它可能是一个需要由管理员手动执行的、有点神秘的多步骤工作流。 处理节点宕机系统中的任何节点都可能宕机，可能因为意外的故障，也可能由于计划内的维护（例如，重启机器以安装内核安全补丁）。对运维而言，能在系统不中断服务的情况下重启单个节点好处多多。我们的目标是，即使个别节点失效，也能保持整个系统运行，并尽可能控制节点停机带来的影响。 如何通过基于领导者的复制实现高可用？ 从库失效：追赶恢复在其本地磁盘上，每个从库记录从主库收到的数据变更。如果从库崩溃并重新启动，或者，如果主库和从库之间的网络暂时中断，则比较容易恢复：从库可以从日志中知道，在发生故障之前处理的最后一个事务。因此，从库可以连接到主库，并请求在从库断开期间发生的所有数据变更。当应用完所有这些变更后，它就赶上了主库，并可以像以前一样继续接收数据变更流。 主库失效：故障切换主库失效处理起来相当棘手：其中一个从库需要被提升为新的主库，需要重新配置客户端，以将它们的写操作发送给新的主库，其他从库需要开始拉取来自新主库的数据变更。这个过程被称为 故障切换（failover）。 故障切换可以手动进行（通知管理员主库挂了，并采取必要的步骤来创建新的主库）或自动进行。自动的故障切换过程通常由以下步骤组成： 确认主库失效。有很多事情可能会出错：崩溃、停电、网络问题等等。没有万无一失的方法来检测出现了什么问题，所以大多数系统只是简单使用 超时（Timeout） ：节点频繁地相互来回传递消息，如果一个节点在一段时间内（例如 30 秒）没有响应，就认为它挂了（因为计划内维护而故意关闭主库不算）。 选择一个新的主库。这可以通过选举过程（主库由剩余副本以多数选举产生）来完成，或者可以由之前选定的 控制器节点（controller node） 来指定新的主库。主库的最佳人选通常是拥有旧主库最新数据副本的从库（以最小化数据损失）。让所有的节点同意一个新的领导者，是一个 共识 问题，将在 第九章 详细讨论。 重新配置系统以启用新的主库。客户端现在需要将它们的写请求发送给新主库（将在 “请求路由” 中讨论这个问题）。如果旧主库恢复，可能仍然认为自己是主库，而没有意识到其他副本已经让它失去领导权了。系统需要确保旧主库意识到新主库的存在，并成为一个从库。 故障切换的过程中有很多地方可能出错： 如果使用异步复制，则新主库可能没有收到老主库宕机前最后的写入操作。在选出新主库后，如果老主库重新加入集群，新主库在此期间可能会收到冲突的写入，那这些写入该如何处理？最常见的解决方案是简单丢弃老主库未复制的写入，这很可能打破客户对于数据持久性的期望。 如果数据库需要和其他外部存储相协调，那么丢弃写入内容是极其危险的操作。例如在 GitHub 【13】的一场事故中，一个过时的 MySQL 从库被提升为主库。数据库使用自增 ID 作为主键，因为新主库的计数器落后于老主库的计数器，所以新主库重新分配了一些已经被老主库分配掉的 ID 作为主键。这些主键也在 Redis 中使用，主键重用使得 MySQL 和 Redis 中的数据产生不一致，最后导致一些私有数据泄漏到错误的用户手中。 发生某些故障时（见 第八章）可能会出现两个节点都以为自己是主库的情况。这种情况称为 脑裂（split brain），非常危险：如果两个主库都可以接受写操作，却没有冲突解决机制（请参阅 “多主复制”），那么数据就可能丢失或损坏。一些系统采取了安全防范措施：当检测到两个主库节点同时存在时会关闭其中一个节点 [^ii]，但设计粗糙的机制可能最后会导致两个节点都被关闭【14】。 [^ii]: 这种机制称为 屏障（fencing），或者更充满感情的术语是：爆彼之头（Shoot The Other Node In The Head, STONITH）。我们将在 “领导者和锁” 中对屏障进行详细讨论。 主库被宣告死亡之前的正确超时应该怎么配置？在主库失效的情况下，超时时间越长意味着恢复时间也越长。但是如果超时设置太短，又可能会出现不必要的故障切换。例如，临时的负载峰值可能导致节点的响应时间增加到超出超时时间，或者网络故障也可能导致数据包延迟。如果系统已经处于高负载或网络问题的困扰之中，那么不必要的故障切换可能会让情况变得更糟糕。 这些问题没有简单的解决方案。因此，即使软件支持自动故障切换，不少运维团队还是更愿意手动执行故障切换。 节点故障、不可靠的网络、对副本一致性、持久性、可用性和延迟的权衡，这些问题实际上是分布式系统中的基本问题。第八章 和 第九章 将更深入地讨论它们。 复制日志的实现基于领导者的复制在底层是如何工作的？实践中有好几种不同的复制方式，所以先简要地看一下。 基于语句的复制在最简单的情况下，主库记录下它执行的每个写入请求（语句，即 statement）并将该语句日志发送给从库。对于关系数据库来说，这意味着每个 INSERT、UPDATE 或 DELETE 语句都被转发给每个从库，每个从库解析并执行该 SQL 语句，就像直接从客户端收到一样。 虽然听上去很合理，但有很多问题会搞砸这种复制方式： 任何调用 非确定性函数（nondeterministic） 的语句，可能会在每个副本上生成不同的值。例如，使用 NOW() 获取当前日期时间，或使用 RAND() 获取一个随机数。 如果语句使用了 自增列（auto increment），或者依赖于数据库中的现有数据（例如，UPDATE ... WHERE &lt;某些条件&gt;），则必须在每个副本上按照完全相同的顺序执行它们，否则可能会产生不同的效果。当有多个并发执行的事务时，这可能成为一个限制。 有副作用的语句（例如：触发器、存储过程、用户定义的函数）可能会在每个副本上产生不同的副作用，除非副作用是绝对确定性的。 的确有办法绕开这些问题 —— 例如，当语句被记录时，主库可以用固定的返回值替换掉任何不确定的函数调用，以便所有从库都能获得相同的值。但是由于边缘情况实在太多了，现在通常会选择其他的复制方法。 基于语句的复制在 5.1 版本前的 MySQL 中被使用到。因为它相当紧凑，现在有时候也还在用。但现在在默认情况下，如果语句中存在任何不确定性，MySQL 会切换到基于行的复制（稍后讨论）。 VoltDB 使用了基于语句的复制，但要求事务必须是确定性的，以此来保证安全【15】。 传输预写式日志（WAL）在 第三章 中，我们讨论了存储引擎如何在磁盘上表示数据，我们也发现了通常会将写操作追加到日志中： 对于日志结构存储引擎（请参阅 “SSTables 和 LSM 树”），日志是主要的存储位置。日志段在后台压缩，并进行垃圾回收。 对于覆写单个磁盘块的 B 树，每次修改都会先写入 预写式日志（Write Ahead Log, WAL），以便崩溃后索引可以恢复到一个一致的状态。 在任何一种情况下，该日志都是包含了所有数据库写入的仅追加字节序列。可以使用完全相同的日志在另一个节点上构建副本：除了将日志写入磁盘之外，主库还可以通过网络将其发送给从库。 通过使用这个日志，从库可以构建一个与主库一模一样的数据结构拷贝。 这种复制方法在 PostgreSQL 和 Oracle 等一些产品中被使用到【16】。其主要缺点是日志记录的数据非常底层：WAL 包含哪些磁盘块中的哪些字节发生了更改。这使复制与存储引擎紧密耦合。如果数据库将其存储格式从一个版本更改为另一个版本，通常不可能在主库和从库上运行不同版本的数据库软件。 看上去这可能只是一个小的实现细节，但却可能对运维产生巨大的影响。如果复制协议允许从库使用比主库更新的软件版本，则可以先升级从库，然后执行故障切换，使升级后的节点之一成为新的主库，从而允许数据库软件的零停机升级。如果复制协议不允许版本不匹配（传输 WAL 经常出现这种情况），则此类升级需要停机。 逻辑日志复制（基于行）另一种方法是对复制和存储引擎使用不同的日志格式，这样可以将复制日志从存储引擎的内部实现中解耦出来。这种复制日志被称为逻辑日志（logical log），以将其与存储引擎的（物理）数据表示区分开来。 关系数据库的逻辑日志通常是以行的粒度来描述对数据库表的写入记录的序列： 对于插入的行，日志包含所有列的新值。 对于删除的行，日志包含足够的信息来唯一标识被删除的行，这通常是主键，但如果表上没有主键，则需要记录所有列的旧值。 对于更新的行，日志包含足够的信息来唯一标识被更新的行，以及所有列的新值（或至少所有已更改的列的新值）。 修改多行的事务会生成多条这样的日志记录，后面跟着一条指明事务已经提交的记录。 MySQL 的二进制日志（当配置为使用基于行的复制时）使用了这种方法【17】。 由于逻辑日志与存储引擎的内部实现是解耦的，系统可以更容易地做到向后兼容，从而使主库和从库能够运行不同版本的数据库软件，或者甚至不同的存储引擎。 对于外部应用程序来说，逻辑日志格式也更容易解析。如果要将数据库的内容发送到外部系统，例如复制到数据仓库进行离线分析，或建立自定义索引和缓存【18】，这一点会很有用。这种技术被称为 数据变更捕获（change data capture），第十一章 将重新讲到它。 基于触发器的复制到目前为止描述的复制方法是由数据库系统实现的，不涉及任何应用程序代码。在很多情况下，这就是你想要的。但在某些情况下需要更多的灵活性。例如，如果你只想复制数据的一个子集，或者想从一种数据库复制到另一种数据库，或者如果你需要冲突解决逻辑（请参阅 “处理写入冲突”），则可能需要将复制操作上移到应用程序层。 一些工具，如 Oracle Golden Gate【19】，可以通过读取数据库日志，使得其他应用程序可以使用数据。另一种方法是使用许多关系数据库自带的功能：触发器和存储过程。 触发器允许你将数据更改（写入事务）发生时自动执行的自定义应用程序代码注册在数据库系统中。触发器有机会将更改记录到一个单独的表中，使用外部程序读取这个表，再加上一些必要的业务逻辑，就可以将数据变更复制到另一个系统去。例如，Databus for Oracle【20】和 Bucardo for Postgres【21】就是这样工作的。 基于触发器的复制通常比其他复制方法具有更高的开销，并且比数据库内置的复制更容易出错，也有很多限制。然而由于其灵活性，它仍然是很有用的。 复制延迟问题容忍节点故障只是需要复制的一个原因。正如在 第二部分 的介绍中提到的，其它原因还包括可伸缩性（处理比单个机器更多的请求）和延迟（让副本在地理位置上更接近用户）。 基于领导者的复制要求所有写入都由单个节点处理，但只读查询可以由任何一个副本来处理。所以对于读多写少的场景（Web 上的常见模式），一个有吸引力的选择是创建很多从库，并将读请求分散到所有的从库上去。这样能减小主库的负载，并允许由附近的副本来处理读请求。 在这种读伸缩（read-scaling）的体系结构中，只需添加更多的从库，就可以提高只读请求的服务容量。但是，这种方法实际上只适用于异步复制 —— 如果尝试同步复制到所有从库，则单个节点故障或网络中断将导致整个系统都无法写入。而且节点越多越有可能出现个别节点宕机的情况，所以完全同步的配置将是非常不可靠的。 不幸的是，当应用程序从异步从库读取时，如果从库落后，它可能会看到过时的信息。这会导致数据库中出现明显的不一致：同时对主库和从库执行相同的查询，可能得到不同的结果，因为并非所有的写入都反映在从库中。这种不一致只是一个暂时的状态 —— 如果停止写入数据库并等待一段时间，从库最终会赶上并与主库保持一致。出于这个原因，这种效应被称为 最终一致性（eventual consistency）【22,23】。[^iii] [^iii]: 道格拉斯・特里（Douglas Terry）等人【24】创造了最终一致性这个术语，并经由 Werner Vogels【22】的推广，成为了许多 NoSQL 项目的口号。然而，最终一致性并不只属于 NoSQL 数据库：关系型数据库中的异步复制从库也有相同的特性。 最终一致性中的 “最终” 一词有意进行了模糊化：总的来说，副本落后的程度是没有限制的。在正常的操作中，复制延迟（replication lag），即写入主库到反映至从库之间的延迟，可能仅仅是几分之一秒，在实践中并不显眼。但如果系统在接近极限的情况下运行，或网络中存在问题时，延迟可以轻而易举地超过几秒，甚至达到几分钟。 因为滞后时间太长引入的不一致性，不仅仅是一个理论问题，更是应用设计中会遇到的真实问题。本节将重点介绍三个在复制延迟时可能发生的问题实例，并简述解决这些问题的一些方法。 读己之写许多应用让用户提交一些数据，然后查看他们提交的内容。可能是用户数据库中的记录，也可能是对讨论主题的评论，或其他类似的内容。提交新数据时，必须将其发送给主库，但是当用户查看数据时，可以通过从库进行读取。如果数据经常被查看，但只是偶尔写入，这是非常合适的。 但对于异步复制，问题就来了。如 图 5-3 所示：如果用户在写入后马上就查看数据，则新数据可能尚未到达副本。对用户而言，看起来好像是刚提交的数据丢失了，所以他们不高兴是可以理解的。 图 5-3 用户写入后从旧副本中读取数据。需要写后读 (read-after-write) 的一致性来防止这种异常 在这种情况下，我们需要 写后读一致性（read-after-write consistency），也称为 读己之写一致性（read-your-writes consistency）【24】。这是一个保证，如果用户重新加载页面，他们总会看到他们自己提交的任何更新。它不会对其他用户的写入做出承诺：其他用户的更新可能稍等才会看到。它保证用户自己的输入已被正确保存。 如何在基于领导者的复制系统中实现写后读一致性？有各种可能的技术，这里说一些： 对于用户 可能修改过 的内容，总是从主库读取；这就要求得有办法不通过实际的查询就可以知道用户是否修改了某些东西。举个例子，社交网络上的用户个人资料信息通常只能由用户本人编辑，而不能由其他人编辑。因此一个简单的规则就是：总是从主库读取用户自己的档案，如果要读取其他用户的档案就去从库。 如果应用中的大部分内容都可能被用户编辑，那这种方法就没用了，因为大部分内容都必须从主库读取（读伸缩就没效果了）。在这种情况下可以使用其他标准来决定是否从主库读取。例如可以跟踪上次更新的时间，在上次更新后的一分钟内，从主库读。还可以监控从库的复制延迟，防止向任何滞后主库超过一分钟的从库发出查询。 客户端可以记住最近一次写入的时间戳，系统需要确保从库在处理该用户的读取请求时，该时间戳前的变更都已经传播到了本从库中。如果当前从库不够新，则可以从另一个从库读取，或者等待从库追赶上来。这里的时间戳可以是逻辑时间戳（表示写入顺序的东西，例如日志序列号）或实际的系统时钟（在这种情况下，时钟同步变得至关重要，请参阅 “不可靠的时钟”）。 如果你的副本分布在多个数据中心（为了在地理上接近用户或者出于可用性目的），还会有额外的复杂性。任何需要由主库提供服务的请求都必须路由到包含该主库的数据中心。 另一种复杂的情况发生在同一位用户从多个设备（例如桌面浏览器和移动 APP）请求服务的时候。这种情况下可能就需要提供跨设备的写后读一致性：如果用户在一个设备上输入了一些信息，然后在另一个设备上查看，则应该看到他们刚输入的信息。 在这种情况下，还有一些需要考虑的问题： 记住用户上次更新时间戳的方法变得更加困难，因为一个设备上运行的程序不知道另一个设备上发生了什么。需要对这些元数据进行中心化的存储。 如果副本分布在不同的数据中心，很难保证来自不同设备的连接会路由到同一数据中心。（例如，用户的台式计算机使用家庭宽带连接，而移动设备使用蜂窝数据网络，则设备的网络路由可能完全不同）。如果你的方法需要读主库，可能首先需要把来自该用户所有设备的请求都路由到同一个数据中心。 单调读在从异步从库读取时可能发生的异常的第二个例子是用户可能会遇到 时光倒流（moving backward in time）。 如果用户从不同从库进行多次读取，就可能发生这种情况。例如，图 5-4 显示了用户 2345 两次进行相同的查询，首先查询了一个延迟很小的从库，然后是一个延迟较大的从库（如果用户刷新网页时每个请求都被路由到一个随机的服务器，这种情况就很有可能发生）。第一个查询返回了最近由用户 1234 添加的评论，但是第二个查询不返回任何东西，因为滞后的从库还没有拉取到该写入内容。实际上可以认为第二个查询是在比第一个查询更早的时间点上观察系统。如果第一个查询没有返回任何内容，那问题并不大，因为用户 2345 可能不知道用户 1234 最近添加了评论。但如果用户 2345 先看见用户 1234 的评论，然后又看到它消失，这就会让人觉得非常困惑了。 图 5-4 用户首先从新副本读取，然后从旧副本读取。时间看上去回退了。为了防止这种异常，我们需要单调的读取。 单调读（monotonic reads）【23】可以保证这种异常不会发生。这是一个比 强一致性（strong consistency） 更弱，但比 最终一致性（eventual consistency） 更强的保证。当读取数据时，你可能会看到一个旧值；单调读仅意味着如果一个用户顺序地进行多次读取，则他们不会看到时间回退，也就是说，如果已经读取到较新的数据，后续的读取不会得到更旧的数据。 实现单调读的一种方式是确保每个用户总是从同一个副本进行读取（不同的用户可以从不同的副本读取）。例如，可以基于用户 ID 的散列来选择副本，而不是随机选择副本。但是，如果该副本出现故障，用户的查询将需要重新路由到另一个副本。 一致前缀读第三个复制延迟异常的例子违反了因果律。 想象一下 Poons 先生和 Cake 夫人之间的以下简短对话： Mr. Poons Mrs. Cake，你能看到多远的未来？ Mrs. Cake 通常约十秒钟，Mr. Poons. 这两句话之间有因果关系：Cake 夫人听到了 Poons 先生的问题并回答了这个问题。 现在，想象第三个人正在通过从库来听这个对话。 Cake 夫人说的内容是从一个延迟很低的从库读取的，但 Poons 先生所说的内容，从库的延迟要大的多（见 图 5-5 ）。于是，这个观察者会听到以下内容： Mrs. Cake 通常约十秒钟，Mr. Poons. Mr. Poons Mrs. Cake，你能看到多远的未来？ 对于观察者来说，看起来好像 Cake 夫人在 Poons 先生提问前就回答了这个问题。这种超能力让人印象深刻，但也会把人搞糊涂。【25】。 图 5-5 如果某些分区的复制速度慢于其他分区，那么观察者可能会在看到问题之前先看到答案。 要防止这种异常，需要另一种类型的保证：一致前缀读（consistent prefix reads）【23】。这个保证的意思是说：如果一系列写入按某个顺序发生，那么任何人读取这些写入时，也会看见它们以同样的顺序出现。 这是 分区（partitioned） 或 分片（sharded） 数据库中的一个特殊问题，我们将在 第六章 中讨论分区数据库。如果数据库总是以相同的顺序应用写入，而读取总是看到一致的前缀，那么这种异常不会发生。但是在许多分布式数据库中，不同的分区独立运行，因此不存在 全局的写入顺序：当用户从数据库中读取数据时，可能会看到数据库的某些部分处于较旧的状态，而某些则处于较新的状态。 一种解决方案是，确保任何因果相关的写入都写入相同的分区，但在一些应用中可能无法高效地完成这种操作。还有一些显式跟踪因果依赖关系的算法，我们将在 ““此前发生” 的关系和并发” 一节中回到这个话题。 复制延迟的解决方案在使用最终一致的系统时，如果复制延迟增加到几分钟甚至几小时，则应该考虑应用程序的行为。如果答案是 “没问题”，那很好。但如果结果对于用户来说是不好的体验，那么设计系统来提供更强的保证（例如 写后读）是很重要的。明明是异步复制却假设复制是同步的，这是很多麻烦的根源。 如前所述，应用程序可以提供比底层数据库更强有力的保证，例如通过主库进行某种读取。但在应用程序代码中处理这些问题是复杂的，容易出错。 如果应用程序开发人员不必担心微妙的复制问题，并可以信赖他们的数据库 “做了正确的事情”，那该多好呀。这就是 事务（transaction） 存在的原因：数据库通过事务提供强大的保证，所以应用程序可以更加简单。 单节点事务已经存在了很长时间。然而在走向分布式（复制和分区）数据库时，许多系统放弃了事务，声称事务在性能和可用性上的代价太高，并断言在可伸缩系统中最终一致性是不可避免的。这个叙述有一些道理，但过于简单了，本书其余部分将提出更为细致的观点。我们将在 第七章 和 第九章 回到事务的话题，并将在 第三部分 讨论一些替代机制。 多主复制本章到目前为止，我们只考虑了使用单个主库的复制架构。虽然这是一种常见的方法，但还有其它一些有趣的选择。 基于领导者的复制有一个主要的缺点：只有一个主库，而且所有的写入都必须通过它 [^iv]。如果出于任何原因（例如和主库之间的网络连接中断）无法连接到主库， 就无法向数据库写入。 [^iv]: 如果数据库被分区（见 第六章），每个分区都有一个主库。不同的分区的主库可能在不同的节点上，但是每个分区都必须有一个主库。 基于领导者的复制模型的自然延伸是允许多个节点接受写入。复制仍然以同样的方式发生：处理写入的每个节点都必须将该数据变更转发给所有其他节点。我们将其称之为 多领导者配置（multi-leader configuration，也称多主、多活复制，即 master-master replication 或 active&#x2F;active replication）。在这种情况下，每个主库同时是其他主库的从库。 多主复制的应用场景在单个数据中心内部使用多个主库的配置没有太大意义，因为其导致的复杂性已经超过了能带来的好处。但在一些情况下，这种配置也是合理的。 运维多个数据中心假如你有一个数据库，副本分散在好几个不同的数据中心（可能会用来容忍单个数据中心的故障，或者为了在地理上更接近用户）。如果使用常规的基于领导者的复制设置，主库必须位于其中一个数据中心，且所有写入都必须经过该数据中心。 多主配置中可以在每个数据中心都有主库。图 5-6 展示了这个架构。在每个数据中心内使用常规的主从复制；在数据中心之间，每个数据中心的主库都会将其更改复制到其他数据中心的主库中。 图 5-6 跨多个数据中心的多主复制 我们来比较一下在运维多个数据中心时，单主和多主的适应情况： 性能 在单主配置中，每个写入都必须穿过互联网，进入主库所在的数据中心。这可能会增加写入时间，并可能违背了设置多个数据中心的初心。在多主配置中，每个写操作都可以在本地数据中心进行处理，并与其他数据中心异步复制。因此，数据中心之间的网络延迟对用户来说是透明的，这意味着感觉到的性能可能会更好。 容忍数据中心停机 在单主配置中，如果主库所在的数据中心发生故障，故障切换必须使另一个数据中心里的从库成为主库。在多主配置中，每个数据中心可以独立于其他数据中心继续运行，并且当发生故障的数据中心归队时，复制会自动赶上。 容忍网络问题 数据中心之间的通信通常穿过公共互联网，这可能不如数据中心内的本地网络可靠。单主配置对数据中心之间的连接问题非常敏感，因为通过这个连接进行的写操作是同步的。采用异步复制功能的多主配置通常能更好地承受网络问题：临时的网络中断并不会妨碍正在处理的写入。 有些数据库默认情况下支持多主配置，但使用外部工具实现也很常见，例如用于 MySQL 的 Tungsten Replicator 【26】，用于 PostgreSQL 的 BDR【27】以及用于 Oracle 的 GoldenGate 【19】。 尽管多主复制有这些优势，但也有一个很大的缺点：两个不同的数据中心可能会同时修改相同的数据，写冲突是必须解决的（如 图 5-6 中的 “冲突解决（conflict resolution）”）。本书将在 “处理写入冲突” 中详细讨论这个问题。 由于多主复制在许多数据库中都属于改装的功能，所以常常存在微妙的配置缺陷，且经常与其他数据库功能之间出现意外的反应。比如自增主键、触发器、完整性约束等都可能会有麻烦。因此，多主复制往往被认为是危险的领域，应尽可能避免【28】。 需要离线操作的客户端多主复制的另一种适用场景是：应用程序在断网之后仍然需要继续工作。 例如，考虑手机，笔记本电脑和其他设备上的日历应用。无论设备目前是否有互联网连接，你需要能随时查看你的会议（发出读取请求），输入新的会议（发出写入请求）。如果在离线状态下进行任何更改，则设备下次上线时，需要与服务器和其他设备同步。 在这种情况下，每个设备都有一个充当主库的本地数据库（它接受写请求），并且在所有设备上的日历副本之间同步时，存在异步的多主复制过程。复制延迟可能是几小时甚至几天，具体取决于何时可以访问互联网。 从架构的角度来看，这种设置实际上与数据中心之间的多主复制类似，每个设备都是一个 “数据中心”，而它们之间的网络连接是极度不可靠的。从历史上各类日历同步功能的破烂实现可以看出，想把多主复制用好是多么困难的一件事。 有一些工具旨在使这种多主配置更容易。例如，CouchDB 就是为这种操作模式而设计的【29】。 协同编辑实时协作编辑应用程序允许多个人同时编辑文档。例如，Etherpad 【30】和 Google Docs 【31】允许多人同时编辑文本文档或电子表格（该算法在 “自动冲突解决” 中简要讨论）。我们通常不会将协作式编辑视为数据库复制问题，但它与前面提到的离线编辑用例有许多相似之处。当一个用户编辑文档时，所做的更改将立即应用到其本地副本（Web 浏览器或客户端应用程序中的文档状态），并异步复制到服务器和编辑同一文档的任何其他用户。 如果要保证不会发生编辑冲突，则应用程序必须先取得文档的锁定，然后用户才能对其进行编辑。如果另一个用户想要编辑同一个文档，他们首先必须等到第一个用户提交修改并释放锁定。这种协作模式相当于主从复制模型下在主节点上执行事务操作。 但是，为了加速协作，你可能希望将更改的单位设置得非常小（例如单次按键），并避免锁定。这种方法允许多个用户同时进行编辑，但同时也带来了多主复制的所有挑战，包括需要解决冲突【32】。 处理写入冲突多主复制的最大问题是可能发生写冲突，这意味着需要解决冲突。 例如，考虑一个由两个用户同时编辑的维基页面，如 图 5-7 所示。用户 1 将页面的标题从 A 更改为 B，并且用户 2 同时将标题从 A 更改为 C。每个用户的更改已成功应用到其本地主库。但当异步复制时，会发现冲突【33】。单主数据库中不会出现此问题。 图 5-7 两个主库同时更新同一记录引起的写入冲突 同步与异步冲突检测在单主数据库中，第二个写入将被阻塞并等待第一个写入完成，或者中止第二个写入事务并强制用户重试。另一方面，在多主配置中，两个写入都是成功的，在稍后的某个时间点才能异步地检测到冲突。那时再来要求用户解决冲突可能为时已晚。 原则上，可以使冲突检测同步 - 即等待写入被复制到所有副本，然后再告诉用户写入成功。但是，通过这样做，你将失去多主复制的主要优点：允许每个副本独立地接受写入。如果你想要同步冲突检测，那么你可能不如直接使用单主复制。 避免冲突处理冲突的最简单的策略就是避免它们：如果应用程序可以确保特定记录的所有写入都通过同一个主库，那么冲突就不会发生。由于许多的多主复制实现在处理冲突时处理得相当不好，避免冲突是一个经常被推荐的方法【34】。 例如，在一个用户可以编辑自己数据的应用程序中，可以确保来自特定用户的请求始终路由到同一数据中心，并使用该数据中心的主库进行读写。不同的用户可能有不同的 “主” 数据中心（可能根据用户的地理位置选择），但从任何一位用户的角度来看，本质上就是单主配置了。 但是，有时你可能需要更改被指定的主库 —— 可能是因为某个数据中心出现故障，你需要将流量重新路由到另一个数据中心，或者可能是因为用户已经迁移到另一个位置，现在更接近其它的数据中心。在这种情况下，冲突避免将失效，你必须处理不同主库同时写入的可能性。 收敛至一致的状态单主数据库按顺序进行写操作：如果同一个字段有多个更新，则最后一个写操作将决定该字段的最终值。 在多主配置中，没有明确的写入顺序，所以最终值应该是什么并不清楚。在 图 5-7 中，在主库 1 中标题首先更新为 B 而后更新为 C；在主库 2 中，首先更新为 C，然后更新为 B。两种顺序都不比另一种“更正确”。 如果每个副本只是按照它看到写入的顺序写入，那么数据库最终将处于不一致的状态：最终值将是在主库 1 的 C 和主库 2 的 B。这是不可接受的，每个复制方案都必须确保数据最终在所有副本中都是相同的。因此，数据库必须以一种 收敛（convergent） 的方式解决冲突，这意味着所有副本必须在所有变更复制完成时收敛至一个相同的最终值。 实现冲突合并解决有多种途径： 给每个写入一个唯一的 ID（例如时间戳、长随机数、UUID 或者键和值的哈希），挑选最高 ID 的写入作为胜利者，并丢弃其他写入。如果使用时间戳，这种技术被称为 最后写入胜利（LWW, last write wins）。虽然这种方法很流行，但是很容易造成数据丢失【35】。我们将在本章末尾的 检测并发写入 一节更详细地讨论 LWW。 为每个副本分配一个唯一的 ID，ID 编号更高的写入具有更高的优先级。这种方法也意味着数据丢失。 以某种方式将这些值合并在一起 - 例如，按字母顺序排序，然后连接它们（在 图 5-7 中，合并的标题可能类似于 “B&#x2F;C”）。 用一种可保留所有信息的显式数据结构来记录冲突，并编写解决冲突的应用程序代码（也许通过提示用户的方式）。 自定义冲突解决逻辑解决冲突的最合适的方法可能取决于应用程序，大多数多主复制工具允许使用应用程序代码编写冲突解决逻辑。该代码可以在写入或读取时执行： 写时执行 只要数据库系统检测到复制更改日志中存在冲突，就会调用冲突处理程序。例如，Bucardo 允许你为此编写一段 Perl 代码。这个处理程序通常不能提示用户 —— 它在后台进程中运行，并且必须快速执行。 读时执行 当检测到冲突时，所有冲突写入被存储。下一次读取数据时，会将这些多个版本的数据返回给应用程序。应用程序可以提示用户或自动解决冲突，并将结果写回数据库。例如 CouchDB 就以这种方式工作。 请注意，冲突解决通常适用于单行记录或单个文档的层面，而不是整个事务【36】。因此，如果你有一个事务会原子性地进行几次不同的写入（请参阅 第七章），对于冲突解决而言，每个写入仍需分开单独考虑。 自动冲突解决 冲突解决规则可能很容易变得越来越复杂，自定义代码可能也很容易出错。亚马逊是一个经常被引用的例子，由于冲突解决处理程序而产生了令人意外的效果：一段时间以来，购物车上的冲突解决逻辑将保留添加到购物车的物品，但不包括从购物车中移除的物品。因此，顾客有时会看到物品重新出现在他们的购物车中，即使他们之前已经被移走【37】。 已经有一些有趣的研究来自动解决由于数据修改引起的冲突。有几项研究值得一提： 无冲突复制数据类型（Conflict-free replicated datatypes，CRDT）【32,38】是可以由多个用户同时编辑的集合、映射、有序列表、计数器等一系列数据结构，它们以合理的方式自动解决冲突。一些 CRDT 已经在 Riak 2.0 中实现【39,40】。 可合并的持久数据结构（Mergeable persistent data structures）【41】显式跟踪历史记录，类似于 Git 版本控制系统，并使用三向合并功能（而 CRDT 使用双向合并）。 操作转换（operational transformation）[42] 是 Etherpad 【30】和 Google Docs 【31】等协同编辑应用背后的冲突解决算法。它是专为有序列表的并发编辑而设计的，例如构成文本文档的字符列表。 这些算法在数据库中的实现还很年轻，但很可能将来它们会被集成到更多的复制数据系统中。自动冲突解决方案可以使应用程序处理多主数据同步更为简单。 什么是冲突？有些冲突是显而易见的。在 图 5-7 的例子中，两个写操作并发地修改了同一条记录中的同一个字段，并将其设置为两个不同的值。毫无疑问这是一个冲突。 其他类型的冲突可能更为微妙而难以发现。例如，考虑一个会议室预订系统：它记录谁订了哪个时间段的哪个房间。应用程序需要确保每个房间在任意时刻都只能被一组人进行预定（即不得有相同房间的重叠预订）。在这种情况下，如果为同一个房间同时创建两个不同的预订，则可能会发生冲突。即使应用程序在允许用户进行预订之前先检查会议室的可用性，如果两次预订是由两个不同的主库进行的，则仍然可能会有冲突。 虽然现在还没有一个现成的答案，但在接下来的章节中，我们将更好地了解这个问题。我们将在 第七章 中看到更多的冲突示例，在 第十二章 中我们将讨论用于检测和解决复制系统中冲突的可伸缩方法。 多主复制拓扑复制拓扑（replication topology）用来描述写入操作从一个节点传播到另一个节点的通信路径。如果你有两个主库，如 图 5-7 所示，只有一个合理的拓扑结构：主库 1 必须把它所有的写入都发送到主库 2，反之亦然。当有两个以上的主库，多种不同的拓扑都是可能的。图 5-8 举例说明了一些例子。 图 5-8 三种可以在多主复制中使用的拓扑示例。 最常见的拓扑是全部到全部（all-to-all，如 图 5-8 (c)），其中每个主库都将其写入发送给其他所有的主库。然而，一些更受限的拓扑也会被使用到：例如，默认情况下 MySQL 仅支持 环形拓扑（circular topology）【34】，其中每个节点都从一个节点接收写入，并将这些写入（加上自己的写入）转发给另一个节点。另一种流行的拓扑结构具有星形的形状 [^v]：一个指定的根节点将写入转发给所有其他节点。星形拓扑可以推广到树。 [^v]: 不要与星型模式混淆（请参阅 “星型和雪花型：分析的模式”），其中描述了数据模型的结构，而不是节点之间的通信拓扑。 在环形和星形拓扑中，写入可能需要在到达所有副本之前通过多个节点。因此，节点需要转发从其他节点收到的数据更改。为了防止无限复制循环，每个节点被赋予一个唯一的标识符，并且在复制日志中，每次写入都会使用其经过的所有节点的标识符进行标记【43】。当一个节点收到用自己的标识符标记的数据更改时，该数据更改将被忽略，因为节点知道它已经被处理过。 环形和星形拓扑的问题是，如果只有一个节点发生故障，则可能会中断其他节点之间的复制消息流，导致它们无法通信，除非节点被修复。拓扑结构可以重新配置为跳过发生故障的节点，但在大多数部署中，这种重新配置必须手动完成。更密集连接的拓扑结构（例如全部到全部）的容错性更好，因为它允许消息沿着不同的路径传播，可以避免单点故障。 另一方面，全部到全部的拓扑也可能有问题。特别是，一些网络链接可能比其他网络链接更快（例如由于网络拥塞），结果是一些复制消息可能 “超越” 其他复制消息，如 图 5-9 所示。 图 5-9 使用多主复制时，写入可能会以错误的顺序到达某些副本。 在 图 5-9 中，客户端 A 向主库 1 的表中插入一行，客户端 B 在主库 3 上更新该行。然而，主库 2 可以以不同的顺序接收写入：它可能先接收到更新（从它的角度来看，是对数据库中不存在的行的更新），稍后才接收到相应的插入（其应该在更新之前）。 这是一个因果关系的问题，类似于我们在 “一致前缀读” 中看到的：更新取决于先前的插入，所以我们需要确保所有节点先处理插入，然后再处理更新。仅仅在每一次写入时添加一个时间戳是不够的，因为时钟不可能被充分地同步，所以主库 2 就无法正确地对这些事件进行排序（见 第八章）。 要正确排序这些事件，可以使用一种称为 版本向量（version vectors） 的技术，本章稍后将讨论这种技术（请参阅 “检测并发写入”）。然而，许多多主复制系统中的冲突检测技术实现得并不好。例如，在撰写本文时，PostgreSQL BDR 不提供写入的因果排序【27】，而 Tungsten Replicator for MySQL 甚至都不做检测冲突【34】。 如果你正在使用基于多主复制的系统，那么你应该多了解这些问题，仔细阅读文档，并彻底测试你的数据库，以确保它确实提供了你想要的保证。 无主复制我们在本章到目前为止所讨论的复制方法 —— 单主复制、多主复制 —— 都是这样的想法：客户端向一个主库发送写请求，而数据库系统负责将写入复制到其他副本。主库决定写入的顺序，而从库按相同顺序应用主库的写入。 一些数据存储系统采用不同的方法，放弃主库的概念，并允许任何副本直接接受来自客户端的写入。最早的一些的复制数据系统是 无主的（leaderless）【1,44】，但是在关系数据库主导的时代，这个想法几乎已被忘却。在亚马逊将其用于其内部的 Dynamo 系统 [^vi] 之后，它再一次成为数据库的一种时尚架构【37】。Riak，Cassandra 和 Voldemort 是受 Dynamo 启发的无主复制模型的开源数据存储，所以这类数据库也被称为 Dynamo 风格。 [^vi]: Dynamo 不适用于 Amazon 以外的用户。令人困惑的是，AWS 提供了一个名为 DynamoDB 的托管数据库产品，它使用了完全不同的体系结构：它基于单主复制。 在一些无主复制的实现中，客户端直接将写入发送到几个副本中，而另一些情况下，由一个 协调者（coordinator） 节点代表客户端进行写入。但与主库数据库不同，协调者不执行特定的写入顺序。我们将会看到，这种设计上的差异对数据库的使用方式有着深远的影响。 当节点故障时写入数据库假设你有一个带有三个副本的数据库，而其中一个副本目前不可用，或许正在重新启动以安装系统更新。在基于领导者的配置中，如果要继续处理写入，则可能需要执行故障切换（请参阅「处理节点宕机」）。 另一方面，在无主配置中，不存在故障转移。图 5-10 演示了会发生了什么事情：客户端（用户 1234）并行发送写入到所有三个副本，并且两个可用副本接受写入，但是不可用副本错过了它。假设三个副本中的两个承认写入是足够的：在用户 1234 已经收到两个确定的响应之后，我们认为写入成功。客户简单地忽略了其中一个副本错过了写入的事实。 图 5-10 法定写入，法定读取，并在节点中断后读修复。 现在想象一下，不可用的节点重新联机，客户端开始读取它。节点关闭期间发生的任何写入都不在该节点上。因此，如果你从该节点读取数据，则可能会从响应中拿到陈旧的（过时的）值。 为了解决这个问题，当一个客户端从数据库中读取数据时，它不仅仅把它的请求发送到一个副本：读请求将被并行地发送到多个节点。客户可能会从不同的节点获得不同的响应，即来自一个节点的最新值和来自另一个节点的陈旧值。版本号将被用于确定哪个值是更新的（请参阅 “检测并发写入”）。 读修复和反熵复制方案应确保最终将所有数据复制到每个副本。在一个不可用的节点重新联机之后，它如何赶上它错过的写入？ 在 Dynamo 风格的数据存储中经常使用两种机制： 读修复（Read repair） 当客户端并行读取多个节点时，它可以检测到任何陈旧的响应。例如，在 图 5-10 中，用户 2345 获得了来自副本 3 的版本 6 值和来自副本 1 和 2 的版本 7 值。客户端发现副本 3 具有陈旧值，并将新值写回到该副本。这种方法适用于读频繁的值。 反熵过程（Anti-entropy process） 此外，一些数据存储具有后台进程，该进程不断查找副本之间的数据差异，并将任何缺少的数据从一个副本复制到另一个副本。与基于领导者的复制中的复制日志不同，此反熵过程不会以任何特定的顺序复制写入，并且在复制数据之前可能会有显著的延迟。 并不是所有的系统都实现了这两种机制，例如，Voldemort 目前没有反熵过程。请注意，如果没有反熵过程，很少被读取的值可能会从某些副本中丢失，从而降低了持久性，因为只有在应用程序读取值时才执行读修复。 读写的法定人数在 图 5-10 的示例中，我们认为即使仅在三个副本中的两个上进行处理，写入仍然是成功的。如果三个副本中只有一个接受了写入，会怎样？以此类推，究竟多少个副本完成才可以认为写入成功？ 如果我们知道，每个成功的写操作意味着在三个副本中至少有两个出现，这意味着至多有一个副本可能是陈旧的。因此，如果我们从至少两个副本读取，我们可以确定至少有一个是最新的。如果第三个副本停机或响应速度缓慢，则读取仍可以继续返回最新值。 更一般地说，如果有 n 个副本，每个写入必须由 w 个节点确认才能被认为是成功的，并且我们必须至少为每个读取查询 r 个节点。 （在我们的例子中，$n &#x3D; 3，w &#x3D; 2，r &#x3D; 2$）。只要 $w + r &gt; n$，我们可以预期在读取时能获得最新的值，因为 r 个读取中至少有一个节点是最新的。遵循这些 r 值和 w 值的读写称为 法定人数（quorum）[^vii] 的读和写【44】。你可以认为，r 和 w 是有效读写所需的最低票数。 [^vii]: 有时候这种法定人数被称为严格的法定人数，其相对 “宽松的法定人数” 而言（见 “宽松的法定人数与提示移交”） 在 Dynamo 风格的数据库中，参数 n、w 和 r 通常是可配置的。一个常见的选择是使 n 为奇数（通常为 3 或 5）并设置 $w &#x3D; r &#x3D; (n + 1) &#x2F; 2$（向上取整）。但是你可以根据需要更改数字。例如，写入次数较少且读取次数较多的工作负载可以从设置 $w &#x3D; n$ 和 $r &#x3D; 1$中受益。这会使得读取速度更快，但缺点是只要有一个不可用的节点就会导致所有的数据库写入都失败。 集群中可能有多于 n 个的节点（集群的机器数可能多于副本数目）。但是任何给定的值只能存储在 n 个节点上。这允许对数据集进行分区，从而可以支持比单个节点的存储能力更大的数据集。我们将在 第六章 继续讨论分区。 法定人数条件 $w + r &gt; n$ 允许系统容忍不可用的节点，如下所示： 如果 $w &lt; n$，当节点不可用时，我们仍然可以处理写入。 如果 $r &lt; n$，当节点不可用时，我们仍然可以处理读取。 对于 $n &#x3D; 3，w &#x3D; 2，r &#x3D; 2$，我们可以容忍一个不可用的节点。 对于 $n &#x3D; 5，w &#x3D; 3，r &#x3D; 3$，我们可以容忍两个不可用的节点。 这个案例如 图 5-11 所示。 通常，读取和写入操作始终并行发送到所有 n 个副本。参数 w 和 r 决定我们等待多少个节点，即在我们认为读或写成功之前，有多少个节点需要报告成功。 图 5-11 如果 $w + r &gt; n$，读取 r 个副本，至少有一个副本必然包含了最近的成功写入。 如果可用的节点少于所需的 w 或 r，则写入或读取将返回错误。节点可能由于多种原因而不可用，比如：节点关闭（异常崩溃，电源关闭）、操作执行过程中的错误（由于磁盘已满而无法写入）、客户端和服务器节点之间的网络中断或任何其他原因。我们只需要关心节点是否返回了成功的响应，而不需要区分不同类型的错误。 法定人数一致性的局限性如果你有 n 个副本，并且你选择了满足 $w + r &gt; n$ 的 w 和 r，你通常可以期望每次读取都能返回最近写入的值。情况就是这样，因为你写入的节点集合和你读取的节点集合必然有重叠。也就是说，你读取的节点中必然至少有一个节点具有最新值（如 图 5-11 所示）。 通常，r 和 w 被选为多数（超过 $n&#x2F;2$ ）节点，因为这确保了 $w + r &gt; n$，同时仍然容忍多达 $n&#x2F;2$ 个节点故障。但是，法定人数不一定必须是大多数，重要的是读写使用的节点至少有一个节点的交集。其他法定人数的配置是可能的，这使得分布式算法的设计有一定的灵活性【45】。 你也可以将 w 和 r 设置为较小的数字，以使 $w + r ≤ n$（即法定条件不满足）。在这种情况下，读取和写入操作仍将被发送到 n 个节点，但操作成功只需要少量的成功响应。 较小的 w 和 r 更有可能会读取到陈旧的数据，因为你的读取更有可能未包含具有最新值的节点。另一方面，这种配置允许更低的延迟和更高的可用性：如果存在网络中断，并且许多副本变得无法访问，则有更大的机会可以继续处理读取和写入。只有当可达副本的数量低于 w 或 r 时，数据库才变得不可写入或读取。 但是，即使在 $w + r &gt; n$ 的情况下，也可能存在返回陈旧值的边缘情况。这取决于实现，但可能的情况包括： 如果使用宽松的法定人数（见 “宽松的法定人数与提示移交”），w 个写入和 r 个读取有可能落在完全不同的节点上，因此 r 节点和 w 之间不再保证有重叠节点【46】。 如果两个写入同时发生，不清楚哪一个先发生。在这种情况下，唯一安全的解决方案是合并并发写入（请参阅 “处理写入冲突”）。如果根据时间戳（最后写入胜利）挑选出一个胜者，则写入可能由于时钟偏差【35】而丢失。我们将在 “检测并发写入” 继续讨论此话题。 如果写操作与读操作同时发生，写操作可能仅反映在某些副本上。在这种情况下，不确定读取返回的是旧值还是新值。 如果写操作在某些副本上成功，而在其他节点上失败（例如，因为某些节点上的磁盘已满），在小于 w 个副本上写入成功。所以整体判定写入失败，但整体写入失败并没有在写入成功的副本上回滚。这意味着一个写入虽然报告失败，后续的读取仍然可能会读取这次失败写入的值【47】。 如果携带新值的节点发生故障，需要从其他带有旧值的副本进行恢复，则存储新值的副本数可能会低于 w，从而打破法定人数条件。 即使一切工作正常，有时也会不幸地出现关于 时序（timing） 的边缘情况，我们将在 “线性一致性和法定人数” 中看到这点。 因此，尽管法定人数似乎保证读取返回最新的写入值，但在实践中并不那么简单。 Dynamo 风格的数据库通常针对可以忍受最终一致性的用例进行优化。你可以通过参数 w 和 r 来调整读取到陈旧值的概率，但把它们当成绝对的保证是不明智的。 尤其是，因为通常得不到 “复制延迟问题” 中讨论的那些保证（读己之写，单调读，一致前缀读），前面提到的异常可能会发生在应用程序中。更强有力的保证通常需要 事务 或 共识。我们将在 第七章 和 第九章 回到这些话题。 监控陈旧度从运维的角度来看，监视你的数据库是否返回最新的结果是很重要的。即使应用可以容忍陈旧的读取，你也需要了解复制的健康状况。如果显著落后，它应该提醒你以便你可以调查原因（例如网络中的问题或过载的节点）。 对于基于领导者的复制，数据库通常会提供复制延迟的测量值，你可以将其提供给监视系统。这之所以能做到，是因为写入是按照相同的顺序应用于主库和从库，并且每个节点对应了复制日志中的一个位置（已经在本地应用的写入数量）。通过从主库的当前位置中减去从库的当前位置，你可以测量复制延迟的程度。 然而，在无主复制的系统中，没有固定的写入顺序，这使得监控变得更加困难。而且，如果数据库只使用读修复（没有反熵过程），那么对于一个值可能会有多陈旧其实是没有限制的 - 如果一个值很少被读取，那么由一个陈旧副本返回的值可能是古老的。 已经有一些关于衡量无主复制数据库中的复制陈旧度的研究，并根据参数 n、w 和 r 来预测陈旧读取的预期百分比【48】。不幸的是，这还不是很常见的做法，但是将陈旧测量值包含在数据库的标准度量集中是一件好事。虽然最终一致性是一种有意模糊的保证，但是从可操作性角度来说，能够量化 “最终” 也是很重要的。 宽松的法定人数与提示移交合理配置的法定人数可以使数据库无需故障切换即可容忍个别节点的故障。它也可以容忍个别节点变慢，因为请求不必等待所有 n 个节点响应 —— 当 w 或 r 个节点响应时它们就可以返回。对于需要高可用、低延时、且能够容忍偶尔读到陈旧值的应用场景来说，这些特性使无主复制的数据库很有吸引力。 然而，法定人数（如迄今为止所描述的）并不像它们可能的那样具有容错性。网络中断可以很容易地将客户端从大量的数据库节点上切断。虽然这些节点是活着的，而其他客户端可能也能够连接到它们，但是从数据库节点切断的客户端来看，它们也可能已经死亡。在这种情况下，剩余的可用节点可能会少于 w 或 r，因此客户端不再能达到法定人数。 在一个大型的集群中（节点数量明显多于 n 个），网络中断期间客户端可能仍能连接到一些数据库节点，但又不足以组成一个特定的法定人数。在这种情况下，数据库设计人员需要权衡一下： 对于所有无法达到 w 或 r 个节点法定人数的请求，是否返回错误是更好的？ 或者我们是否应该接受写入，然后将它们写入一些可达的节点，但不在这些值通常所存在的 n 个节点上？ 后者被认为是一个 宽松的法定人数（sloppy quorum）【37】：写和读仍然需要 w 和 r 个成功的响应，但这些响应可能来自不在指定的 n 个 “主” 节点中的其它节点。就好比说，如果你把自己锁在房子外面了，你可能会去敲开邻居的门，问是否可以暂时呆在他们的沙发上。 一旦网络中断得到解决，一个节点代表另一个节点临时接受的任何写入都将被发送到适当的 “主” 节点。这就是所谓的 提示移交（hinted handoff）（一旦你再次找到你的房子的钥匙，你的邻居可以礼貌地要求你离开沙发回家）。 宽松的法定人数对写入可用性的提高特别有用：只要有任何 w 个节点可用，数据库就可以接受写入。然而，这意味着即使当 $w + r &gt; n$ 时，也不能确保读取到某个键的最新值，因为最新的值可能已经临时写入了 n 之外的某些节点【47】。 因此，在传统意义上，宽松的法定人数实际上并不是法定人数。它只是一个持久性的保证，即数据已存储在某处的 w 个节点。但不能保证 r 个节点的读取能看到它，除非提示移交已经完成。 在所有常见的 Dynamo 实现中，宽松的法定人数是可选的。在 Riak 中，它们默认是启用的，而在 Cassandra 和 Voldemort 中它们默认是禁用的【46,49,50】。 运维多个数据中心我们先前讨论了跨数据中心复制，作为多主复制的用例（请参阅 “多主复制”）。其实无主复制也适用于多数据中心操作，既然它旨在容忍冲突的并发写入、网络中断和延迟尖峰。 Cassandra 和 Voldemort 在正常的无主模型中实现了他们的多数据中心支持：副本的数量 n 包括所有数据中心的节点，你可以在配置中指定每个数据中心所拥有的副本的数量。无论数据中心如何，每个来自客户端的写入都会发送到所有副本，但客户端通常只等待来自其本地数据中心内的法定节点的确认，从而不会受到跨数据中心链路延迟和中断的影响。对其他数据中心的高延迟写入通常被配置为异步执行，尽管该配置仍有一定的灵活性【50,51】。 Riak 将客户端和数据库节点之间的所有通信保持在一个本地的数据中心，因此 n 描述了一个数据中心内的副本数量。数据库集群之间的跨数据中心复制在后台异步发生，其风格类似于多主复制【52】。 检测并发写入Dynamo 风格的数据库允许多个客户端同时写入相同的键（Key），这意味着即使使用严格的法定人数也会发生冲突。这种情况与多主复制相似（请参阅 “处理写入冲突”），但在 Dynamo 风格的数据库中，在 读修复 或 提示移交 期间也可能会产生冲突。 其问题在于，由于可变的网络延迟和部分节点的故障，事件可能以不同的顺序到达不同的节点。例如，图 5-12 显示了两个客户机 A 和 B 同时写入三节点数据存储中的键 X： 节点 1 接收来自 A 的写入，但由于暂时中断，未接收到来自 B 的写入。 节点 2 首先接收来自 A 的写入，然后接收来自 B 的写入。 节点 3 首先接收来自 B 的写入，然后从 A 写入。 图 5-12 并发写入 Dynamo 风格的数据存储：没有明确定义的顺序。 如果每个节点只要接收到来自客户端的写入请求就简单地覆写某个键值，那么节点就会永久地不一致，如 图 5-12 中的最终获取请求所示：节点 2 认为 X 的最终值是 B，而其他节点认为值是 A 。 为了最终达成一致，副本应该趋于相同的值。如何做到这一点？有人可能希望复制的数据库能够自动处理，但不幸的是，大多数的实现都很糟糕：如果你想避免丢失数据，你（应用程序开发人员）需要知道很多有关数据库冲突处理的内部信息。 在 “处理写入冲突” 一节中已经简要介绍了一些解决冲突的技术。在总结本章之前，让我们来更详细地探讨这个问题。 最后写入胜利（丢弃并发写入）实现最终收敛的一种方法是声明每个副本只需要存储 “最近” 的值，并允许 “更旧” 的值被覆盖和抛弃。然后，只要我们有一种明确的方式来确定哪个写是 “最近的”，并且每个写入最终都被复制到每个副本，那么复制最终会收敛到相同的值。 正如 “最近” 的引号所表明的，这个想法其实颇具误导性。在 图 5-12 的例子中，当客户端向数据库节点发送写入请求时，两个客户端都不知道另一个客户端，因此不清楚哪一个先发送请求。事实上，说这两种情况谁先发送请求是没有意义的：既然我们说写入是 并发（concurrent） 的，那么它们的顺序就是不确定的。 即使写入没有自然的排序，我们也可以强制进行排序。例如，可以为每个写入附加一个时间戳，然后挑选最大的时间戳作为 “最近的”，并丢弃具有较早时间戳的任何写入。这种冲突解决算法被称为 最后写入胜利（LWW, last write wins），是 Cassandra 唯一支持的冲突解决方法【53】，也是 Riak 中的一个可选特征【35】。 LWW 实现了最终收敛的目标，但以 持久性 为代价：如果同一个键有多个并发写入，即使它们反馈给客户端的结果都是成功的（因为它们被写入 w 个副本），也只有一个写入将被保留，而其他写入将被默默地丢弃。此外，LWW 甚至可能会丢弃不是并发的写入，我们将在 “有序事件的时间戳” 中进行讨论。 在类似缓存的一些情况下，写入丢失可能是可以接受的。但如果数据丢失不可接受，LWW 是解决冲突的一个很烂的选择。 在数据库中使用 LWW 的唯一安全方法是确保一个键只写入一次，然后视为不可变，从而避免对同一个键进行并发更新。例如，Cassandra 推荐使用的方法是使用 UUID 作为键，从而为每个写操作提供一个唯一的键【53】。 “此前发生”的关系和并发我们如何判断两个操作是否是并发的？为了建立一个直觉，让我们看看一些例子： 在 图 5-9 中，两个写入不是并发的：A 的插入发生在 B 的递增之前，因为 B 递增的值是 A 插入的值。换句话说，B 的操作建立在 A 的操作上，所以 B 的操作必须后发生。我们也可以说 B 因果依赖（causally dependent） 于 A。 另一方面，图 5-12 中的两个写入是并发的：当每个客户端启动操作时，它不知道另一个客户端也正在对同样的键执行操作。因此，操作之间不存在因果关系。 如果操作 B 了解操作 A，或者依赖于 A，或者以某种方式构建于操作 A 之上，则操作 A 在操作 B 之前发生（happens before）。一个操作是否在另一个操作之前发生是定义并发含义的关键。事实上，我们可以简单地说，如果两个操作中的任何一个都不在另一个之前发生（即，两个操作都不知道对方），那么这两个操作是并发的【54】。 因此，只要有两个操作 A 和 B，就有三种可能性：A 在 B 之前发生，或者 B 在 A 之前发生，或者 A 和 B 并发。我们需要的是一个算法来告诉我们两个操作是否是并发的。如果一个操作发生在另一个操作之前，则后面的操作应该覆盖前面的操作，但是如果这些操作是并发的，则存在需要解决的冲突。 并发性、时间和相对性 如果两个操作 “同时” 发生，似乎应该称为并发 —— 但事实上，它们在字面时间上重叠与否并不重要。由于分布式系统中的时钟问题，现实中是很难判断两个事件是否是 同时 发生的，这个问题我们将在 第八章 中详细讨论。 为了定义并发性，确切的时间并不重要：如果两个操作都意识不到对方的存在，就称这两个操作 并发，而不管它们实际发生的物理时间。人们有时把这个原理和物理学中的狭义相对论联系起来【54】，该理论引入了信息不能比光速更快的思想。因此，如果两个事件发生的时间差小于光通过它们之间的距离所需要的时间，那么这两个事件不可能相互影响。 在计算机系统中，即使光速原则上允许一个操作影响另一个操作，但两个操作也可能是 并发的。例如，如果网络缓慢或中断，两个操作间可能会出现一段时间间隔，但仍然是并发的，因为网络问题阻止一个操作意识到另一个操作的存在。 捕获”此前发生”关系我们来看一个算法，它可以确定两个操作是否为并发的，还是一个在另一个之前。简单起见，我们从一个只有一个副本的数据库开始。一旦我们知道了如何在单个副本上完成这项工作，我们可以将该方法推广到具有多个副本的无主数据库。 图 5-13 显示了两个客户端同时向同一购物车添加项目。（如果这样的例子让你觉得无趣，那么可以想象一下两个空中交通管制员同时把飞机添加到他们正在跟踪的区域。）最初，购物车是空的。然后客户端向数据库发出五次写入： 客户端 1 将牛奶加入购物车。这是该键的第一次写入，服务器成功存储了它并为其分配版本号 1，最后将值与版本号一起回送给客户端。 客户端 2 将鸡蛋加入购物车，不知道客户端 1 同时添加了牛奶（客户端 2 认为它的鸡蛋是购物车中的唯一物品）。服务器为此写入分配版本号 2，并将鸡蛋和牛奶存储为两个单独的值。然后它将这两个值 都 返回给客户端 2 ，并附上版本号 2。 客户端 1 不知道客户端 2 的写入，想要将面粉加入购物车，因此认为当前的购物车内容应该是 [牛奶，面粉]。它将此值与服务器先前向客户端 1 提供的版本号 1 一起发送到服务器。服务器可以从版本号中知道 [牛奶，面粉] 的写入取代了 [牛奶] 的先前值，但与 [鸡蛋] 的值是 并发 的。因此，服务器将版本号 3 分配给 [牛奶，面粉]，覆盖版本 1 的值 [牛奶]，但保留版本 2 的值 [蛋]，并将所有的值返回给客户端 1。 同时，客户端 2 想要加入火腿，不知道客户端 1 刚刚加了面粉。客户端 2 在最近一次响应中从服务器收到了两个值 [牛奶] 和 [蛋]，所以客户端 2 现在合并这些值，并添加火腿形成一个新的值 [鸡蛋，牛奶，火腿]。它将这个值发送到服务器，带着之前的版本号 2 。服务器检测到新值会覆盖版本 2 的值 [鸡蛋]，但新值也会与版本 3 的值 [牛奶，面粉] 并发，所以剩下的两个值是版本 3 的 [牛奶，面粉]，和版本 4 的 [鸡蛋，牛奶，火腿]。 最后，客户端 1 想要加培根。它之前从服务器接收到了版本 3 的 [牛奶，面粉] 和 [鸡蛋]，所以它合并这些，添加培根，并将最终值 [牛奶，面粉，鸡蛋，培根] 连同版本号 3 发往服务器。这会覆盖版本 3 的值 [牛奶，面粉]（请注意 [鸡蛋] 已经在上一步被覆盖），但与版本 4 的值 [鸡蛋，牛奶，火腿] 并发，所以服务器将保留这两个并发值。 图 5-13 在同时编辑购物车时捕获两个客户端之间的因果关系。 图 5-13 中的操作之间的数据流如 图 5-14 所示。箭头表示哪个操作发生在其他操作之前，意味着后面的操作知道或依赖于较早的操作。在这个例子中，客户端永远不会完全拿到服务器上的最新数据，因为总是有另一个操作同时进行。但是旧版本的值最终会被覆盖，并且不会丢失任何写入。 图 5-14 图 5-13 中的因果依赖关系图。 请注意，服务器可以只通过查看版本号来确定两个操作是否是并发的 —— 它不需要对值本身进行解释（因此该值可以是任何数据结构）。该算法的工作原理如下： 服务器为每个键维护一个版本号，每次写入该键时都递增版本号，并将新版本号与写入的值一起存储。 当客户端读取键时，服务器将返回所有未覆盖的值以及最新的版本号。客户端在写入前必须先读取。 当客户端写入键时，必须包含之前读取的版本号，并且必须将之前读取的所有值合并在一起（针对写入请求的响应可以像读取请求一样，返回所有当前值，这使得我们可以像购物车示例那样将多个写入串联起来）。 当服务器接收到具有特定版本号的写入时，它可以覆盖该版本号或更低版本的所有值（因为它知道它们已经被合并到新的值中），但是它必须用更高的版本号来保存所有值（因为这些值与正在进行的其它写入是并发的）。 当一个写入包含前一次读取的版本号时，它会告诉我们的写入是基于之前的哪一种状态。如果在不包含版本号的情况下进行写操作，则与所有其他写操作并发，因此它不会覆盖任何内容 —— 只会在随后的读取中作为其中一个值返回。 合并并发写入的值这种算法可以确保没有数据被无声地丢弃，但不幸的是，客户端需要做一些额外的工作：客户端随后必须合并并发写入的值。 Riak 称这些并发值为 兄弟（siblings）。 合并并发值，本质上是与多主复制中的冲突解决问题相同，我们先前讨论过（请参阅 “处理写入冲突”）。一个简单的方法是根据版本号或时间戳（最后写入胜利）来选择一个值，但这意味着丢失数据。所以，你可能需要在应用程序代码中额外做些更聪明的事情。 以购物车为例，一种合理的合并值的方法就是做并集。在 图 5-14 中，最后的两个兄弟是 [牛奶，面粉，鸡蛋，熏肉] 和 [鸡蛋，牛奶，火腿]。注意牛奶和鸡蛋虽然同时出现在两个并发值里，但他们每个只被写过一次。合并的值可以是 [牛奶，面粉，鸡蛋，培根，火腿]，不再有重复了。 然而，如果你想让人们也可以从他们的购物车中 移除 东西，而不是仅仅添加东西，那么把并发值做并集可能不会产生正确的结果：如果你合并了两个客户端的购物车，并且只在其中一个客户端里面移除了一个项目，那么被移除的项目将会重新出现在这两个客户端的交集结果中【37】。为了防止这个问题，要移除一个项目时不能简单地直接从数据库中删除；相反，系统必须留下一个具有适当版本号的标记，以在兄弟合并时表明该项目已被移除。这种删除标记被称为 墓碑（tombstone）（我们上一次看到墓碑是在 “散列索引” 章节的日志压缩部分）。 因为在应用程序代码中做兄弟合并是复杂且容易出错的，所以有一些数据结构被设计出来用于自动执行这种合并，比如在 “自动冲突解决” 中讨论过的那些。举例来说，Riak 的数据类型就支持使用称为 CRDT 【38,39,55】的能以合理方式自动进行兄弟合并的数据结构家族，包括对保留删除的支持。 版本向量图 5-13 中的示例只使用了一个副本。当有多个副本但又没有主库时，算法该如何修改？ 图 5-13 使用单个版本号来捕获操作之间的依赖关系，但是当多个副本并发接受写入时，这是不够的。相反，除了对每个键，我们还需要对 每个副本 使用版本号。每个副本在处理写入时增加自己的版本号，并且跟踪从其他副本中看到的版本号。这个信息指出了要覆盖哪些并发值，以及要保留哪些并发值或兄弟值。 所有副本的版本号集合称为 版本向量（version vector）【56】。这个想法的一些变体正在被使用，但最有趣的可能是在 Riak 2.0 【58,59】中使用的 虚线版本向量（dotted version vector）【57】。我们不会深入细节，但是它的工作方式与我们在购物车示例中看到的非常相似。 与 图 5-13 中的版本号一样，当读取值时，版本向量会从数据库副本发送到客户端，并且随后写入值时需要将其发送回数据库。（Riak 将版本向量编码为一个字符串，并称其为 因果上下文，即 causal context）。版本向量允许数据库区分覆盖写入和并发写入。 另外，就像在单个副本中的情况一样，应用程序可能需要合并并发值。版本向量结构能够确保从一个副本读取并随后写回到另一个副本是安全的。这样做虽然可能会在其他副本上面创建数据，但只要能正确合并就不会丢失数据。 版本向量和向量时钟 版本向量有时也被称为向量时钟，即使它们不完全相同。其中的差别很微妙 —— 细节请参阅参考资料【57,60,61】。简而言之，在比较副本的状态时，版本向量才是正确的数据结构。 本章小结在本章中，我们考察了复制的问题。复制可以用于几个目的： 高可用性 即使在一台机器（或多台机器，或整个数据中心）停机的情况下也能保持系统正常运行 断开连接的操作 允许应用程序在网络中断时继续工作 延迟 将数据放置在地理上距离用户较近的地方，以便用户能够更快地与其交互 可伸缩性 通过在副本上读，能够处理比单机更大的读取量 尽管是一个简单的目标 - 在几台机器上保留相同数据的副本，但复制却是一个非常棘手的问题。它需要仔细考虑并发和所有可能出错的事情，并处理这些故障的后果。至少，我们需要处理不可用的节点和网络中断（这还不包括更隐蔽的故障，例如由于软件错误导致的静默数据损坏）。 我们讨论了复制的三种主要方法： 单主复制 客户端将所有写入操作发送到单个节点（主库），该节点将数据更改事件流发送到其他副本（从库）。读取可以在任何副本上执行，但从库的读取结果可能是陈旧的。 多主复制 客户端将每个写入发送到几个主库节点之一，其中任何一个主库都可以接受写入。主库将数据更改事件流发送给彼此以及任何从库节点。 无主复制 客户端将每个写入发送到几个节点，并从多个节点并行读取，以检测和纠正具有陈旧数据的节点。 每种方法都有优点和缺点。单主复制是非常流行的，因为它很容易理解，不需要担心冲突解决。在出现故障节点、网络中断和延迟峰值的情况下，多主复制和无主复制可以更加健壮，其代价是难以推理并且仅提供非常弱的一致性保证。 复制可以是同步的，也可以是异步的，这在发生故障时对系统行为有深远的影响。尽管在系统运行平稳时异步复制速度很快，但是要弄清楚在复制延迟增加和服务器故障时会发生什么，这一点很重要。如果主库失败后你将一个异步更新的从库提升为新的主库，那么最近提交的数据可能会丢失。 我们研究了一些可能由复制延迟引起的奇怪效应，我们也讨论了一些有助于决定应用程序在复制延迟时的行为的一致性模型： 写后读一致性 用户应该总是能看到自己提交的数据。 单调读 用户在看到某个时间点的数据后，他们不应该再看到该数据在更早时间点的情况。 一致前缀读 用户应该看到数据处于一种具有因果意义的状态：例如，按正确的顺序看到一个问题和对应的回答。 最后，我们讨论了多主复制和无主复制方法所固有的并发问题：因为他们允许多个写入并发发生，这可能会导致冲突。我们研究了一个数据库可以使用的算法来确定一个操作是否发生在另一个操作之前，或者它们是否并发发生。我们还谈到了通过合并并发更新来解决冲突的方法。 在下一章中，我们将继续考察数据分布在多台机器间的另一种不同于 复制 的形式：将大数据集分割成 分区。 参考文献 Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.: “Notes on Distributed Databases,” IBM Research, Research Report RJ2571(33471), July 1979. “Oracle Active Data Guard Real-Time Data Protection and Availability,” Oracle White Paper, June 2013. “AlwaysOn Availability Groups,” in SQL Server Books Online, Microsoft, 2012. Lin Qiao, Kapil Surlaker, Shirshanka Das, et al.: “On Brewing Fresh Espresso: LinkedIn’s Distributed Data Serving Platform,” at ACM International Conference on Management of Data (SIGMOD), June 2013. Jun Rao: “Intra-Cluster Replication for Apache Kafka,” at ApacheCon North America, February 2013. “Highly Available Queues,” in RabbitMQ Server Documentation, Pivotal Software, Inc., 2014. Yoshinori Matsunobu: “Semi-Synchronous Replication at Facebook,” yoshinorimatsunobu.blogspot.co.uk, April 1, 2014. Robbert van Renesse and Fred B. Schneider: “Chain Replication for Supporting High Throughput and Availability,” at 6th USENIX Symposium on Operating System Design and Implementation (OSDI), December 2004. Jeff Terrace and Michael J. Freedman: “Object Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads,” at USENIX Annual Technical Conference (ATC), June 2009. Brad Calder, Ju Wang, Aaron Ogus, et al.: “Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency,” at 23rd ACM Symposium on Operating Systems Principles (SOSP), October 2011. Andrew Wang: “Windows Azure Storage,” umbrant.com, February 4, 2016. “Percona Xtrabackup - Documentation,” Percona LLC, 2014. Jesse Newland: “GitHub Availability This Week,” github.com, September 14, 2012. Mark Imbriaco: “Downtime Last Saturday,” github.com, December 26, 2012. John Hugg: “‘All in’ with Determinism for Performance and Testing in Distributed Systems,” at Strange Loop, September 2015. Amit Kapila: “WAL Internals of PostgreSQL,” at PostgreSQL Conference (PGCon), May 2012. MySQL Internals Manual. Oracle, 2014. Yogeshwer Sharma, Philippe Ajoux, Petchean Ang, et al.: “Wormhole: Reliable Pub-Sub to Support Geo-Replicated Internet Services,” at 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI), May 2015. “Oracle GoldenGate 12c: Real-Time Access to Real-Time Information,” Oracle White Paper, October 2013. Shirshanka Das, Chavdar Botev, Kapil Surlaker, et al.: “All Aboard the Databus!,” at ACM Symposium on Cloud Computing (SoCC), October 2012. Greg Sabino Mullane: “Version 5 of Bucardo Database Replication System,” blog.endpoint.com, June 23, 2014. Werner Vogels: “Eventually Consistent,” ACM Queue, volume 6, number 6, pages 14–19, October 2008. doi:10.1145&#x2F;1466443.1466448 Douglas B. Terry: “Replicated Data Consistency Explained Through Baseball,” Microsoft Research, Technical Report MSR-TR-2011-137, October 2011. Douglas B. Terry, Alan J. Demers, Karin Petersen, et al.: “Session Guarantees for Weakly Consistent Replicated Data,” at 3rd International Conference on Parallel and Distributed Information Systems (PDIS), September 1994. doi:10.1109&#x2F;PDIS.1994.331722 Terry Pratchett: Reaper Man: A Discworld Novel. Victor Gollancz, 1991. ISBN: 978-0-575-04979-6 “Tungsten Replicator,” Continuent, Inc., 2014. “BDR 0.10.0 Documentation,” The PostgreSQL Global Development Group, bdr-project.org, 2015. Robert Hodges: “If You Must Deploy Multi-Master Replication, Read This First,” scale-out-blog.blogspot.co.uk, March 30, 2012. J. Chris Anderson, Jan Lehnardt, and Noah Slater: CouchDB: The Definitive Guide. O’Reilly Media, 2010. ISBN: 978-0-596-15589-6 AppJet, Inc.: “Etherpad and EasySync Technical Manual,” github.com, March 26, 2011. John Day-Richter: “What’s Different About the New Google Docs: Making Collaboration Fast,” googledrive.blogspot.com, 23 September 2010. Martin Kleppmann and Alastair R. Beresford: “A Conflict-Free Replicated JSON Datatype,” arXiv:1608.03960, August 13, 2016. Frazer Clement: “Eventual Consistency – Detecting Conflicts,” messagepassing.blogspot.co.uk, October 20, 2011. Robert Hodges: “State of the Art for MySQL Multi-Master Replication,” at Percona Live: MySQL Conference &amp; Expo, April 2013. John Daily: “Clocks Are Bad, or, Welcome to the Wonderful World of Distributed Systems,” basho.com, November 12, 2013. Riley Berton: “Is Bi-Directional Replication (BDR) in Postgres Transactional?,” sdf.org, January 4, 2016. Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: “Dynamo: Amazon’s Highly Available Key-Value Store,” at 21st ACM Symposium on Operating Systems Principles (SOSP), October 2007. Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski: “A Comprehensive Study of Convergent and Commutative Replicated Data Types,” INRIA Research Report no. 7506, January 2011. Sam Elliott: “CRDTs: An UPDATE (or Maybe Just a PUT),” at RICON West, October 2013. Russell Brown: “A Bluffers Guide to CRDTs in Riak,” gist.github.com, October 28, 2013. Benjamin Farinier, Thomas Gazagnaire, and Anil Madhavapeddy: “Mergeable Persistent Data Structures,” at 26es Journées Francophones des Langages Applicatifs (JFLA), January 2015. Chengzheng Sun and Clarence Ellis: “Operational Transformation in Real-Time Group Editors: Issues, Algorithms, and Achievements,” at ACM Conference on Computer Supported Cooperative Work (CSCW), November 1998. Lars Hofhansl: “HBASE-7709: Infinite Loop Possible in Master&#x2F;Master Replication,” issues.apache.org, January 29, 2013. David K. Gifford: “Weighted Voting for Replicated Data,” at 7th ACM Symposium on Operating Systems Principles (SOSP), December 1979. doi:10.1145&#x2F;800215.806583 Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman: “Flexible Paxos: Quorum Intersection Revisited,” arXiv:1608.06696, August 24, 2016. Joseph Blomstedt: “Re: Absolute Consistency,” email to riak-users mailing list, lists.basho.com, January 11, 2012. Joseph Blomstedt: “Bringing Consistency to Riak,” at RICON West, October 2012. Peter Bailis, Shivaram Venkataraman, Michael J. Franklin, et al.: “Quantifying Eventual Consistency with PBS,” Communications of the ACM, volume 57, number 8, pages 93–102, August 2014. doi:10.1145&#x2F;2632792 Jonathan Ellis: “Modern Hinted Handoff,” datastax.com, December 11, 2012. “Project Voldemort Wiki,” github.com, 2013. “Apache Cassandra 2.0 Documentation,” DataStax, Inc., 2014. “Riak Enterprise: Multi-Datacenter Replication.” Technical whitepaper, Basho Technologies, Inc., September 2014. Jonathan Ellis: “Why Cassandra Doesn’t Need Vector Clocks,” datastax.com, September 2, 2013. Leslie Lamport: “Time, Clocks, and the Ordering of Events in a Distributed System,” Communications of the ACM, volume 21, number 7, pages 558–565, July 1978. doi:10.1145&#x2F;359545.359563 Joel Jacobson: “Riak 2.0: Data Types,” blog.joeljacobson.com, March 23, 2014. D. Stott Parker Jr., Gerald J. Popek, Gerard Rudisin, et al.: “Detection of Mutual Inconsistency in Distributed Systems,” IEEE Transactions on Software Engineering, volume 9, number 3, pages 240–247, May 1983. doi:10.1109&#x2F;TSE.1983.236733 Nuno Preguiça, Carlos Baquero, Paulo Sérgio Almeida, et al.: “Dotted Version Vectors: Logical Clocks for Optimistic Replication,” arXiv:1011.5808, November 26, 2010. Sean Cribbs: “A Brief History of Time in Riak,” at RICON, October 2014. Russell Brown: “Vector Clocks Revisited Part 2: Dotted Version Vectors,” basho.com, November 10, 2015. Carlos Baquero: “Version Vectors Are Not Vector Clocks,” haslab.wordpress.com, July 8, 2011. Reinhard Schwarz and Friedemann Mattern: “Detecting Causal Relationships in Distributed Computations: In Search of the Holy Grail,” Distributed Computing, volume 7, number 3, pages 149–174, March 1994. doi:10.1007&#x2F;BF02277859"},{"title":"第六章：分区","path":"/wiki/ddia/ch6.html","content":"我们必须跳出电脑指令序列的窠臼。 叙述定义、描述元数据、梳理关系，而不是编写过程。 —— Grace Murray Hopper，未来的计算机及其管理（1962） 在 第五章 中，我们讨论了复制 —— 即数据在不同节点上的副本，对于非常大的数据集，或非常高的吞吐量，仅仅进行复制是不够的：我们需要将数据进行 分区（partitions），也称为 分片（sharding）[^i]。 [^i]: 正如本章所讨论的，分区是一种有意将大型数据库分解成小型数据库的方式。它与 网络分区（network partitions, netsplits） 无关，这是节点之间网络故障的一种。我们将在 第八章 讨论这些错误。 术语澄清上文中的 分区（partition），在 MongoDB，Elasticsearch 和 Solr Cloud 中被称为 分片（shard），在 HBase 中称之为 区域（Region），Bigtable 中则是 表块（tablet），Cassandra 和 Riak 中是 虚节点（vnode），Couchbase 中叫做 虚桶（vBucket）。但是 分区（partitioning） 是最约定俗成的叫法。 通常情况下，每条数据（每条记录，每行或每个文档）属于且仅属于一个分区。有很多方法可以实现这一点，本章将进行深入讨论。实际上，每个分区都是自己的小型数据库，尽管数据库可能支持同时进行多个分区的操作。 分区主要是为了 可伸缩性。不同的分区可以放在不共享集群中的不同节点上（请参阅 第二部分 关于 无共享架构 的定义）。因此，大数据集可以分布在多个磁盘上，并且查询负载可以分布在多个处理器上。 对于在单个分区上运行的查询，每个节点可以独立执行对自己的查询，因此可以通过添加更多的节点来扩大查询吞吐量。大型，复杂的查询可能会跨越多个节点并行处理，尽管这也带来了新的困难。 分区数据库在 20 世纪 80 年代由 Teradata 和 NonStop SQL【1】等产品率先推出，最近因为 NoSQL 数据库和基于 Hadoop 的数据仓库重新被关注。有些系统是为事务性工作设计的，有些系统则用于分析（请参阅 “事务处理还是分析”）：这种差异会影响系统的运作方式，但是分区的基本原理均适用于这两种工作方式。 在本章中，我们将首先介绍分割大型数据集的不同方法，并观察索引如何与分区配合。然后我们将讨论 分区再平衡（rebalancing），如果想要添加或删除集群中的节点，则必须进行再平衡。最后，我们将概述数据库如何将请求路由到正确的分区并执行查询。 分区与复制分区通常与复制结合使用，使得每个分区的副本存储在多个节点上。这意味着，即使每条记录属于一个分区，它仍然可以存储在多个不同的节点上以获得容错能力。 一个节点可能存储多个分区。如果使用主从复制模型，则分区和复制的组合如 图 6-1 所示。每个分区领导者（主库）被分配给一个节点，追随者（从库）被分配给其他节点。 每个节点可能是某些分区的主库，同时是其他分区的从库。 我们在 第五章 讨论的关于数据库复制的所有内容同样适用于分区的复制。大多数情况下，分区方案的选择与复制方案的选择是独立的，为简单起见，本章中将忽略复制。 图 6-1 组合使用复制和分区：每个节点充当某些分区的主库，其他分区充当从库。 键值数据的分区假设你有大量数据并且想要分区，如何决定在哪些节点上存储哪些记录呢？ 分区目标是将数据和查询负载均匀分布在各个节点上。如果每个节点公平分享数据和负载，那么理论上 10 个节点应该能够处理 10 倍的数据量和 10 倍的单个节点的读写吞吐量（暂时忽略复制）。 如果分区是不公平的，一些分区比其他分区有更多的数据或查询，我们称之为 偏斜（skew）。数据偏斜的存在使分区效率下降很多。在极端的情况下，所有的负载可能压在一个分区上，其余 9 个节点空闲的，瓶颈落在这一个繁忙的节点上。不均衡导致的高负载的分区被称为 热点（hot spot）。 避免热点最简单的方法是将记录随机分配给节点。这将在所有节点上平均分配数据，但是它有一个很大的缺点：当你试图读取一个特定的值时，你无法知道它在哪个节点上，所以你必须并行地查询所有的节点。 我们可以做得更好。现在假设你有一个简单的键值数据模型，其中你总是通过其主键访问记录。例如，在一本老式的纸质百科全书中，你可以通过标题来查找一个条目；由于所有条目按字母顺序排序，因此你可以快速找到你要查找的条目。 根据键的范围分区一种分区的方法是为每个分区指定一块连续的键范围（从最小值到最大值），如纸质百科全书的卷（图 6-2 ）。如果知道范围之间的边界，则可以轻松确定哪个分区包含某个值。如果你还知道分区所在的节点，那么可以直接向相应的节点发出请求（对于百科全书而言，就像从书架上选取正确的书籍）。 图 6-2 印刷版百科全书按照关键字范围进行分区 键的范围不一定均匀分布，因为数据也很可能不均匀分布。例如在 图 6-2 中，第 1 卷包含以 A 和 B 开头的单词，但第 12 卷则包含以 T、U、V、X、Y 和 Z 开头的单词。只是简单的规定每个卷包含两个字母会导致一些卷比其他卷大。为了均匀分配数据，分区边界需要依据数据调整。 分区边界可以由管理员手动选择，也可以由数据库自动选择（我们会在 “分区再平衡” 中更详细地讨论分区边界的选择）。 Bigtable 使用了这种分区策略，以及其开源等价物 HBase 【2, 3】，RethinkDB 和 2.4 版本之前的 MongoDB 【4】。 在每个分区中，我们可以按照一定的顺序保存键（请参阅 “SSTables 和 LSM 树”）。好处是进行范围扫描非常简单，你可以将键作为联合索引来处理，以便在一次查询中获取多个相关记录（请参阅 “多列索引”）。例如，假设我们有一个程序来存储传感器网络的数据，其中主键是测量的时间戳（年月日时分秒）。范围扫描在这种情况下非常有用，因为我们可以轻松获取某个月份的所有数据。 然而，Key Range 分区的缺点是某些特定的访问模式会导致热点。 如果主键是时间戳，则分区对应于时间范围，例如，给每天分配一个分区。 不幸的是，由于我们在测量发生时将数据从传感器写入数据库，因此所有写入操作都会转到同一个分区（即今天的分区），这样分区可能会因写入而过载，而其他分区则处于空闲状态【5】。 为了避免传感器数据库中的这个问题，需要使用除了时间戳以外的其他东西作为主键的第一个部分。 例如，可以在每个时间戳前添加传感器名称，这样会首先按传感器名称，然后按时间进行分区。 假设有多个传感器同时运行，写入负载将最终均匀分布在不同分区上。 现在，当想要在一个时间范围内获取多个传感器的值时，你需要为每个传感器名称执行一个单独的范围查询。 根据键的散列分区由于偏斜和热点的风险，许多分布式数据存储使用散列函数来确定给定键的分区。 一个好的散列函数可以将偏斜的数据均匀分布。假设你有一个 32 位散列函数，无论何时给定一个新的字符串输入，它将返回一个 0 到 $2^{32}$ -1 之间的 “随机” 数。即使输入的字符串非常相似，它们的散列也会均匀分布在这个数字范围内。 出于分区的目的，散列函数不需要多么强壮的加密算法：例如，Cassandra 和 MongoDB 使用 MD5，Voldemort 使用 Fowler-Noll-Vo 函数。许多编程语言都有内置的简单哈希函数（它们用于散列表），但是它们可能不适合分区：例如，在 Java 的 Object.hashCode() 和 Ruby 的 Object#hash，同一个键可能在不同的进程中有不同的哈希值【6】。 一旦你有一个合适的键散列函数，你可以为每个分区分配一个散列范围（而不是键的范围），每个通过哈希散列落在分区范围内的键将被存储在该分区中。如 图 6-3 所示。 图 6-3 按哈希键分区 这种技术擅长在分区之间公平地分配键。分区边界可以是均匀间隔的，也可以是伪随机选择的（在这种情况下，该技术有时也被称为 一致性哈希，即 consistent hashing）。 一致性哈希 一致性哈希由 Karger 等人定义。【7】 用于跨互联网级别的缓存系统，例如 CDN 中，是一种能均匀分配负载的方法。它使用随机选择的 分区边界（partition boundaries） 来避免中央控制或分布式共识的需要。 请注意，这里的一致性与复制一致性（请参阅 第五章）或 ACID 一致性（请参阅 第七章）无关，而只是描述了一种再平衡（rebalancing）的特定方法。 正如我们将在 “分区再平衡” 中所看到的，这种特殊的方法对于数据库实际上并不是很好，所以在实际中很少使用（某些数据库的文档仍然会使用一致性哈希的说法，但是它往往是不准确的）。 因为有可能产生混淆，所以最好避免使用一致性哈希这个术语，而只是把它称为 散列分区（hash partitioning）。 不幸的是，通过使用键散列进行分区，我们失去了键范围分区的一个很好的属性：高效执行范围查询的能力。曾经相邻的键现在分散在所有分区中，所以它们之间的顺序就丢失了。在 MongoDB 中，如果你使用了基于散列的分区模式，则任何范围查询都必须发送到所有分区【4】。Riak【9】、Couchbase 【10】或 Voldemort 不支持主键上的范围查询。 Cassandra 采取了折衷的策略【11, 12, 13】。 Cassandra 中的表可以使用由多个列组成的复合主键来声明。键中只有第一列会作为散列的依据，而其他列则被用作 Casssandra 的 SSTables 中排序数据的连接索引。尽管查询无法在复合主键的第一列中按范围扫表，但如果第一列已经指定了固定值，则可以对该键的其他列执行有效的范围扫描。 组合索引方法为一对多关系提供了一个优雅的数据模型。例如，在社交媒体网站上，一个用户可能会发布很多更新。如果更新的主键被选择为 (user_id, update_timestamp)，那么你可以有效地检索特定用户在某个时间间隔内按时间戳排序的所有更新。不同的用户可以存储在不同的分区上，对于每个用户，更新按时间戳顺序存储在单个分区上。 负载偏斜与热点消除如前所述，哈希分区可以帮助减少热点。但是，它不能完全避免它们：在极端情况下，所有的读写操作都是针对同一个键的，所有的请求都会被路由到同一个分区。 这种场景也许并不常见，但并非闻所未闻：例如，在社交媒体网站上，一个拥有数百万追随者的名人用户在做某事时可能会引发一场风暴【14】。这个事件可能导致同一个键的大量写入（键可能是名人的用户 ID，或者人们正在评论的动作的 ID）。哈希策略不起作用，因为两个相同 ID 的哈希值仍然是相同的。 如今，大多数数据系统无法自动补偿这种高度偏斜的负载，因此应用程序有责任减少偏斜。例如，如果一个主键被认为是非常火爆的，一个简单的方法是在主键的开始或结尾添加一个随机数。只要一个两位数的十进制随机数就可以将主键分散为 100 种不同的主键，从而存储在不同的分区中。 然而，将主键进行分割之后，任何读取都必须要做额外的工作，因为他们必须从所有 100 个主键分布中读取数据并将其合并。此技术还需要额外的记录：只需要对少量热点附加随机数；对于写入吞吐量低的绝大多数主键来说是不必要的开销。因此，你还需要一些方法来跟踪哪些键需要被分割。 也许在将来，数据系统将能够自动检测和补偿偏斜的工作负载；但现在，你需要自己来权衡。 分区与次级索引到目前为止，我们讨论的分区方案依赖于键值数据模型。如果只通过主键访问记录，我们可以从该键确定分区，并使用它来将读写请求路由到负责该键的分区。 如果涉及次级索引，情况会变得更加复杂（参考 “其他索引结构”）。次级索引通常并不能唯一地标识记录，而是一种搜索记录中出现特定值的方式：查找用户 123 的所有操作、查找包含词语 hogwash 的所有文章、查找所有颜色为红色的车辆等等。 次级索引是关系型数据库的基础，并且在文档数据库中也很普遍。许多键值存储（如 HBase 和 Volde-mort）为了减少实现的复杂度而放弃了次级索引，但是一些（如 Riak）已经开始添加它们，因为它们对于数据模型实在是太有用了。并且次级索引也是 Solr 和 Elasticsearch 等搜索服务器的基石。 次级索引的问题是它们不能整齐地映射到分区。有两种用次级索引对数据库进行分区的方法：基于文档的分区（document-based） 和 基于关键词（term-based）的分区。 基于文档的次级索引进行分区假设你正在经营一个销售二手车的网站（如 图 6-4 所示）。 每个列表都有一个唯一的 ID—— 称之为文档 ID—— 并且用文档 ID 对数据库进行分区（例如，分区 0 中的 ID 0 到 499，分区 1 中的 ID 500 到 999 等）。 你想让用户搜索汽车，允许他们通过颜色和厂商过滤，所以需要一个在颜色和厂商上的次级索引（文档数据库中这些是 字段（field），关系数据库中这些是 列（column） ）。 如果你声明了索引，则数据库可以自动执行索引 [^ii]。例如，无论何时将红色汽车添加到数据库，数据库分区都会自动将其添加到索引条目 color:red 的文档 ID 列表中。 [^ii]: 如果数据库仅支持键值模型，则你可能会尝试在应用程序代码中创建从值到文档 ID 的映射来实现次级索引。 如果沿着这条路线走下去，请万分小心，确保你的索引与底层数据保持一致。 竞争条件和间歇性写入失败（其中一些更改已保存，但其他更改未保存）很容易导致数据不同步 - 请参阅 “多对象事务的需求”。 图 6-4 基于文档的次级索引进行分区 在这种索引方法中，每个分区是完全独立的：每个分区维护自己的次级索引，仅覆盖该分区中的文档。它不关心存储在其他分区的数据。无论何时你需要写入数据库（添加，删除或更新文档），只需处理包含你正在编写的文档 ID 的分区即可。出于这个原因，文档分区索引 也被称为 本地索引（而不是将在下一节中描述的 全局索引）。 但是，从文档分区索引中读取需要注意：除非你对文档 ID 做了特别的处理，否则没有理由将所有具有特定颜色或特定品牌的汽车放在同一个分区中。在 图 6-4 中，红色汽车出现在分区 0 和分区 1 中。因此，如果要搜索红色汽车，则需要将查询发送到所有分区，并合并所有返回的结果。 这种查询分区数据库的方法有时被称为 分散 &#x2F; 聚集（scatter&#x2F;gather），并且可能会使次级索引上的读取查询相当昂贵。即使并行查询分区，分散 &#x2F; 聚集也容易导致尾部延迟放大（请参阅 “实践中的百分位点”）。然而，它被广泛使用：MongoDB，Riak 【15】，Cassandra 【16】，Elasticsearch 【17】，SolrCloud 【18】和 VoltDB 【19】都使用文档分区次级索引。大多数数据库供应商建议你构建一个能从单个分区提供次级索引查询的分区方案，但这并不总是可行，尤其是当在单个查询中使用多个次级索引时（例如同时需要按颜色和制造商查询）。 基于关键词(Term)的次级索引进行分区我们可以构建一个覆盖所有分区数据的 全局索引，而不是给每个分区创建自己的次级索引（本地索引）。但是，我们不能只把这个索引存储在一个节点上，因为它可能会成为瓶颈，违背了分区的目的。全局索引也必须进行分区，但可以采用与主键不同的分区方式。 图 6-5 描述了这可能是什么样子：来自所有分区的红色汽车在红色索引中，并且索引是分区的，首字母从 a 到 r 的颜色在分区 0 中，s 到 z 的在分区 1。汽车制造商的索引也与之类似（分区边界在 f 和 h 之间）。 图 6-5 基于关键词对次级索引进行分区 我们将这种索引称为 关键词分区（term-partitioned），因为我们寻找的关键词决定了索引的分区方式。例如，一个关键词可能是：color:red。关键词（Term） 这个名称来源于全文搜索索引（一种特殊的次级索引），指文档中出现的所有单词。 和之前一样，我们可以通过 关键词 本身或者它的散列进行索引分区。根据关键词本身来分区对于范围扫描非常有用（例如对于数值类的属性，像汽车的报价），而对关键词的哈希分区提供了负载均衡的能力。 关键词分区的全局索引优于文档分区索引的地方点是它可以使读取更有效率：不需要 分散 &#x2F; 收集 所有分区，客户端只需要向包含关键词的分区发出请求。全局索引的缺点在于写入速度较慢且较为复杂，因为写入单个文档现在可能会影响索引的多个分区（文档中的每个关键词可能位于不同的分区或者不同的节点上） 。 理想情况下，索引总是最新的，写入数据库的每个文档都会立即反映在索引中。但是，在关键词分区索引中，这需要跨分区的分布式事务，并不是所有数据库都支持（请参阅 第七章 和 第九章）。 在实践中，对全局次级索引的更新通常是 异步 的（也就是说，如果在写入之后不久读取索引，刚才所做的更改可能尚未反映在索引中）。例如，Amazon DynamoDB 声称在正常情况下，其全局次级索引会在不到一秒的时间内更新，但在基础架构出现故障的情况下可能会有延迟【20】。 全局关键词分区索引的其他用途包括 Riak 的搜索功能【21】和 Oracle 数据仓库，它允许你在本地和全局索引之间进行选择【22】。我们将在 第十二章 中继续关键词分区次级索引实现的话题。 分区再平衡随着时间的推移，数据库会有各种变化： 查询吞吐量增加，所以你想要添加更多的 CPU 来处理负载。 数据集大小增加，所以你想添加更多的磁盘和 RAM 来存储它。 机器出现故障，其他机器需要接管故障机器的责任。 所有这些更改都需要数据和请求从一个节点移动到另一个节点。 将负载从集群中的一个节点向另一个节点移动的过程称为 再平衡（rebalancing）。 无论使用哪种分区方案，再平衡通常都要满足一些最低要求： 再平衡之后，负载（数据存储，读取和写入请求）应该在集群中的节点之间公平地共享。 再平衡发生时，数据库应该继续接受读取和写入。 节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘 I&#x2F;O 负载。 再平衡策略有几种不同的分区分配方法【23】，让我们依次简要讨论一下。 反面教材：hash mod N我们在前面说过（图 6-3 ），最好将可能的散列分成不同的范围，并将每个范围分配给一个分区（例如，如果 $0 ≤ hash(key)&lt; b_0$，则将键分配给分区 0，如果 $b_0 ≤ hash(key) &lt; b_1$，则分配给分区 1） 也许你想知道为什么我们不使用 取模（mod）（许多编程语言中的 % 运算符）。例如，hash(key) mod 10 会返回一个介于 0 和 9 之间的数字（如果我们将散列写为十进制数，散列模 10 将是最后一个数字）。如果我们有 10 个节点，编号为 0 到 9，这似乎是将每个键分配给一个节点的简单方法。 模 N（$mod N$）方法的问题是，如果节点数量 N 发生变化，大多数键将需要从一个节点移动到另一个节点。例如，假设 $hash(key)&#x3D;123456$。如果最初有 10 个节点，那么这个键一开始放在节点 6 上（因为 $123456\\ mod\\ 10 &#x3D; 6$）。当你增长到 11 个节点时，键需要移动到节点 3（$123456\\ mod\\ 11 &#x3D; 3$），当你增长到 12 个节点时，需要移动到节点 0（$123456\\ mod\\ 12 &#x3D; 0$）。这种频繁的举动使得再平衡的成本过高。 我们需要一种只移动必需数据的方法。 固定数量的分区幸运的是，有一个相当简单的解决方案：创建比节点更多的分区，并为每个节点分配多个分区。例如，运行在 10 个节点的集群上的数据库可能会从一开始就被拆分为 1,000 个分区，因此大约有 100 个分区被分配给每个节点。 现在，如果一个节点被添加到集群中，新节点可以从当前每个节点中 窃取 一些分区，直到分区再次公平分配。这个过程如 图 6-6 所示。如果从集群中删除一个节点，则会发生相反的情况。 只有分区在节点之间的移动。分区的数量不会改变，键所指定的分区也不会改变。唯一改变的是分区所在的节点。这种变更并不是即时的 — 在网络上传输大量的数据需要一些时间 — 所以在传输过程中，原有分区仍然会接受读写操作。 图 6-6 将新节点添加到每个节点具有多个分区的数据库集群。 原则上，你甚至可以解决集群中的硬件不匹配问题：通过为更强大的节点分配更多的分区，可以强制这些节点承载更多的负载。在 Riak 【15】、Elasticsearch 【24】、Couchbase 【10】和 Voldemort 【25】中使用了这种再平衡的方法。 在这种配置中，分区的数量通常在数据库第一次建立时确定，之后不会改变。虽然原则上可以分割和合并分区（请参阅下一节），但固定数量的分区在操作上更简单，因此许多固定分区数据库选择不实施分区分割。因此，一开始配置的分区数就是你可以拥有的最大节点数量，所以你需要选择足够多的分区以适应未来的增长。但是，每个分区也有管理开销，所以选择太大的数字会适得其反。 如果数据集的总大小难以预估（例如，可能它开始很小，但随着时间的推移会变得更大），选择正确的分区数是困难的。由于每个分区包含了总数据量固定比率的数据，因此每个分区的大小与集群中的数据总量成比例增长。如果分区非常大，再平衡和从节点故障恢复变得昂贵。但是，如果分区太小，则会产生太多的开销。当分区大小 “恰到好处” 的时候才能获得很好的性能，如果分区数量固定，但数据量变动很大，则难以达到最佳性能。 动态分区对于使用键范围分区的数据库（请参阅 “根据键的范围分区”），具有固定边界的固定数量的分区将非常不便：如果出现边界错误，则可能会导致一个分区中的所有数据或者其他分区中的所有数据为空。手动重新配置分区边界将非常繁琐。 出于这个原因，按键的范围进行分区的数据库（如 HBase 和 RethinkDB）会动态创建分区。当分区增长到超过配置的大小时（在 HBase 上，默认值是 10GB），会被分成两个分区，每个分区约占一半的数据【26】。与之相反，如果大量数据被删除并且分区缩小到某个阈值以下，则可以将其与相邻分区合并。此过程与 B 树顶层发生的过程类似（请参阅 “B 树”）。 每个分区分配给一个节点，每个节点可以处理多个分区，就像固定数量的分区一样。大型分区拆分后，可以将其中的一半转移到另一个节点，以平衡负载。在 HBase 中，分区文件的传输通过 HDFS（底层使用的分布式文件系统）来实现【3】。 动态分区的一个优点是分区数量适应总数据量。如果只有少量的数据，少量的分区就足够了，所以开销很小；如果有大量的数据，每个分区的大小被限制在一个可配置的最大值【23】。 需要注意的是，一个空的数据库从一个分区开始，因为没有关于在哪里绘制分区边界的先验信息。数据集开始时很小，直到达到第一个分区的分割点，所有写入操作都必须由单个节点处理，而其他节点则处于空闲状态。为了解决这个问题，HBase 和 MongoDB 允许在一个空的数据库上配置一组初始分区（这被称为 预分割，即 pre-splitting）。在键范围分区的情况中，预分割需要提前知道键是如何进行分配的【4,26】。 动态分区不仅适用于数据的范围分区，而且也适用于散列分区。从版本 2.4 开始，MongoDB 同时支持范围和散列分区，并且都支持动态分割分区。 按节点比例分区通过动态分区，分区的数量与数据集的大小成正比，因为拆分和合并过程将每个分区的大小保持在固定的最小值和最大值之间。另一方面，对于固定数量的分区，每个分区的大小与数据集的大小成正比。在这两种情况下，分区的数量都与节点的数量无关。 Cassandra 和 Ketama 使用的第三种方法是使分区数与节点数成正比 —— 换句话说，每个节点具有固定数量的分区【23,27,28】。在这种情况下，每个分区的大小与数据集大小成比例地增长，而节点数量保持不变，但是当增加节点数时，分区将再次变小。由于较大的数据量通常需要较大数量的节点进行存储，因此这种方法也使每个分区的大小较为稳定。 当一个新节点加入集群时，它随机选择固定数量的现有分区进行拆分，然后占有这些拆分分区中每个分区的一半，同时将每个分区的另一半留在原地。随机化可能会产生不公平的分割，但是平均在更大数量的分区上时（在 Cassandra 中，默认情况下，每个节点有 256 个分区），新节点最终从现有节点获得公平的负载份额。 Cassandra 3.0 引入了另一种再平衡的算法来避免不公平的分割【29】。 随机选择分区边界要求使用基于散列的分区（可以从散列函数产生的数字范围中挑选边界）。实际上，这种方法最符合一致性哈希的原始定义【7】（请参阅 “一致性哈希”）。最新的哈希函数可以在较低元数据开销的情况下达到类似的效果【8】。 运维：手动还是自动再平衡关于再平衡有一个重要问题：自动还是手动进行？ 在全自动再平衡（系统自动决定何时将分区从一个节点移动到另一个节点，无须人工干预）和完全手动（分区指派给节点由管理员明确配置，仅在管理员明确重新配置时才会更改）之间有一个权衡。例如，Couchbase、Riak 和 Voldemort 会自动生成建议的分区分配，但需要管理员提交才能生效。 全自动再平衡可以很方便，因为正常维护的操作工作较少。然而，它可能是不可预测的。再平衡是一个昂贵的操作，因为它需要重新路由请求并将大量数据从一个节点移动到另一个节点。如果没有做好，这个过程可能会使网络或节点负载过重，降低其他请求的性能。 这种自动化与自动故障检测相结合可能十分危险。例如，假设一个节点过载，并且对请求的响应暂时很慢。其他节点得出结论：过载的节点已经死亡，并自动重新平衡集群，使负载离开它。这会对已经超负荷的节点，其他节点和网络造成额外的负载，从而使情况变得更糟，并可能导致级联失败。 出于这个原因，再平衡的过程中有人参与是一件好事。这比全自动的过程慢，但可以帮助防止运维意外。 请求路由现在我们已经将数据集分割到多个机器上运行的多个节点上。但是仍然存在一个悬而未决的问题：当客户想要发出请求时，如何知道要连接哪个节点？随着分区的重新平衡，分区对节点的分配也发生变化。为了回答这个问题，需要有人知晓这些变化：如果我想读或写键 “foo”，需要连接哪个 IP 地址和端口号？ 这个问题可以概括为 服务发现（service discovery） ，它不仅限于数据库。任何可通过网络访问的软件都有这个问题，特别是如果它的目标是高可用性（在多台机器上运行冗余配置）。许多公司已经编写了自己的内部服务发现工具，其中许多已经作为开源发布【30】。 概括来说，这个问题有几种不同的方案（如图 6-7 所示）: 允许客户联系任何节点（例如，通过 循环策略的负载均衡，即 Round-Robin Load Balancer）。如果该节点恰巧拥有请求的分区，则它可以直接处理该请求；否则，它将请求转发到适当的节点，接收回复并传递给客户端。 首先将所有来自客户端的请求发送到路由层，它决定了应该处理请求的节点，并相应地转发。此路由层本身不处理任何请求；它仅负责分区的负载均衡。 要求客户端知道分区和节点的分配。在这种情况下，客户端可以直接连接到适当的节点，而不需要任何中介。 以上所有情况中的关键问题是：作出路由决策的组件（可能是节点之一，还是路由层或客户端）如何了解分区 - 节点之间的分配关系变化？ 图 6-7 将请求路由到正确节点的三种不同方式。 这是一个具有挑战性的问题，因为重要的是所有参与者都达成共识 - 否则请求将被发送到错误的节点，得不到正确的处理。 在分布式系统中有达成共识的协议，但很难正确地实现（见 第九章）。 许多分布式数据系统都依赖于一个独立的协调服务，比如 ZooKeeper 来跟踪集群元数据，如 图 6-8 所示。 每个节点在 ZooKeeper 中注册自己，ZooKeeper 维护分区到节点的可靠映射。 其他参与者（如路由层或分区感知客户端）可以在 ZooKeeper 中订阅此信息。 只要分区分配发生了改变，或者集群中添加或删除了一个节点，ZooKeeper 就会通知路由层使路由信息保持最新状态。 图 6-8 使用 ZooKeeper 跟踪分区分配给节点。 例如，LinkedIn的Espresso使用Helix 【31】进行集群管理（依靠ZooKeeper），实现了如图6-8 所示的路由层。 HBase、SolrCloud和Kafka也使用ZooKeeper来跟踪分区分配。MongoDB具有类似的体系结构，但它依赖于自己的配置服务器（config server） 实现和mongos守护进程作为路由层。 Cassandra 和 Riak 采取不同的方法：他们在节点之间使用 流言协议（gossip protocol） 来传播集群状态的变化。请求可以发送到任意节点，该节点会转发到包含所请求的分区的适当节点（图 6-7 中的方法 1）。这个模型在数据库节点中增加了更多的复杂性，但是避免了对像 ZooKeeper 这样的外部协调服务的依赖。 Couchbase 不会自动进行再平衡，这简化了设计。通常情况下，它配置了一个名为 moxi 的路由层，它会从集群节点了解路由变化【32】。 当使用路由层或向随机节点发送请求时，客户端仍然需要找到要连接的 IP 地址。这些地址并不像分区的节点分布变化的那么快，所以使用 DNS 通常就足够了。 执行并行查询到目前为止，我们只关注读取或写入单个键的非常简单的查询（加上基于文档分区的次级索引场景下的分散 &#x2F; 聚集查询）。这也是大多数 NoSQL 分布式数据存储所支持的访问层级。 然而，通常用于分析的 大规模并行处理（MPP, Massively parallel processing） 关系型数据库产品在其支持的查询类型方面要复杂得多。一个典型的数据仓库查询包含多个连接，过滤，分组和聚合操作。 MPP 查询优化器将这个复杂的查询分解成许多执行阶段和分区，其中许多可以在数据库集群的不同节点上并行执行。涉及扫描大规模数据集的查询特别受益于这种并行执行。 数据仓库查询的快速并行执行是一个专门的话题，由于分析有很重要的商业意义，可以带来很多利益。我们将在 第十章 讨论并行查询执行的一些技巧。有关并行数据库中使用的技术的更详细的概述，请参阅参考文献【1,33】。 本章小结在本章中，我们探讨了将大数据集划分成更小的子集的不同方法。数据量非常大的时候，在单台机器上存储和处理不再可行，而分区则十分必要。分区的目标是在多台机器上均匀分布数据和查询负载，避免出现热点（负载不成比例的节点）。这需要选择适合于你的数据的分区方案，并在将节点添加到集群或从集群删除时重新平衡分区。 我们讨论了两种主要的分区方法： 键范围分区 其中键是有序的，并且分区拥有从某个最小值到某个最大值的所有键。排序的优势在于可以进行有效的范围查询，但是如果应用程序经常访问相邻的键，则存在热点的风险。 在这种方法中，当分区变得太大时，通常将分区分成两个子分区来动态地重新平衡分区。 散列分区 散列函数应用于每个键，分区拥有一定范围的散列。这种方法破坏了键的排序，使得范围查询效率低下，但可以更均匀地分配负载。 通过散列进行分区时，通常先提前创建固定数量的分区，为每个节点分配多个分区，并在添加或删除节点时将整个分区从一个节点移动到另一个节点。也可以使用动态分区。 两种方法搭配使用也是可行的，例如使用复合主键：使用键的一部分来标识分区，而使用另一部分作为排序顺序。 我们还讨论了分区和次级索引之间的相互作用。次级索引也需要分区，有两种方法： 基于文档分区（本地索引），其中次级索引存储在与主键和值相同的分区中。这意味着只有一个分区需要在写入时更新，但是读取次级索引需要在所有分区之间进行分散 &#x2F; 收集。 基于关键词分区（全局索引），其中次级索引存在不同的分区中。次级索引中的条目可以包括来自主键的所有分区的记录。当文档写入时，需要更新多个分区中的次级索引；但是可以从单个分区中进行读取。 最后，我们讨论了将查询路由到适当的分区的技术，从简单的分区负载平衡到复杂的并行查询执行引擎。 按照设计，多数情况下每个分区是独立运行的 — 这就是分区数据库可以伸缩到多台机器的原因。但是，需要写入多个分区的操作结果可能难以预料：例如，如果写入一个分区成功，但另一个分区失败，会发生什么情况？我们将在下面的章节中讨论这个问题。 参考文献 David J. DeWitt and Jim N. Gray: “Parallel Database Systems: The Future of High Performance Database Systems,” Communications of the ACM, volume 35, number 6, pages 85–98, June 1992. doi:10.1145&#x2F;129888.129894 Lars George: “HBase vs. BigTable Comparison,” larsgeorge.com, November 2009. “The Apache HBase Reference Guide,” Apache Software Foundation, hbase.apache.org, 2014. MongoDB, Inc.: “New Hash-Based Sharding Feature in MongoDB 2.4,” blog.mongodb.org, April 10, 2013. Ikai Lan: “App Engine Datastore Tip: Monotonically Increasing Values Are Bad,” ikaisays.com, January 25, 2011. Martin Kleppmann: “Java’s hashCode Is Not Safe for Distributed Systems,” martin.kleppmann.com, June 18, 2012. David Karger, Eric Lehman, Tom Leighton, et al.: “Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web,” at 29th Annual ACM Symposium on Theory of Computing (STOC), pages 654–663, 1997. doi:10.1145&#x2F;258533.258660 John Lamping and Eric Veach: “A Fast, Minimal Memory, Consistent Hash Algorithm,” arxiv.org, June 2014. Eric Redmond: “A Little Riak Book,” Version 1.4.0, Basho Technologies, September 2013. “Couchbase 2.5 Administrator Guide,” Couchbase, Inc., 2014. Avinash Lakshman and Prashant Malik: “Cassandra – A Decentralized Structured Storage System,” at 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware (LADIS), October 2009. Jonathan Ellis: “Facebook’s Cassandra Paper, Annotated and Compared to Apache Cassandra 2.0,” datastax.com, September 12, 2013. “Introduction to Cassandra Query Language,” DataStax, Inc., 2014. Samuel Axon: “3% of Twitter’s Servers Dedicated to Justin Bieber,” mashable.com, September 7, 2010. “Riak 1.4.8 Docs,” Basho Technologies, Inc., 2014. Richard Low: “The Sweet Spot for Cassandra Secondary Indexing,” wentnet.com, October 21, 2013. Zachary Tong: “Customizing Your Document Routing,” elasticsearch.org, June 3, 2013. “Apache Solr Reference Guide,” Apache Software Foundation, 2014. Andrew Pavlo: “H-Store Frequently Asked Questions,” hstore.cs.brown.edu, October 2013. “Amazon DynamoDB Developer Guide,” Amazon Web Services, Inc., 2014. Rusty Klophaus: “Difference Between 2I and Search,” email to riak-users mailing list, lists.basho.com, October 25, 2011. Donald K. Burleson: “Object Partitioning in Oracle,”dba-oracle.com, November 8, 2000. Eric Evans: “Rethinking Topology in Cassandra,” at ApacheCon Europe, November 2012. Rafał Kuć: “Reroute API Explained,” elasticsearchserverbook.com, September 30, 2013. “Project Voldemort Documentation,” project-voldemort.com. Enis Soztutar: “Apache HBase Region Splitting and Merging,” hortonworks.com, February 1, 2013. Brandon Williams: “Virtual Nodes in Cassandra 1.2,” datastax.com, December 4, 2012. Richard Jones: “libketama: Consistent Hashing Library for Memcached Clients,” metabrew.com, April 10, 2007. Branimir Lambov: “New Token Allocation Algorithm in Cassandra 3.0,” datastax.com, January 28, 2016. Jason Wilder: “Open-Source Service Discovery,” jasonwilder.com, February 2014. Kishore Gopalakrishna, Shi Lu, Zhen Zhang, et al.: “Untangling Cluster Management with Helix,” at ACM Symposium on Cloud Computing (SoCC), October 2012. doi:10.1145&#x2F;2391229.2391248 “Moxi 1.8 Manual,” Couchbase, Inc., 2014. Shivnath Babu and Herodotos Herodotou: “Massively Parallel Databases and MapReduce Systems,” Foundations and Trends in Databases, volume 5, number 1, pages 1–104, November 2013.doi:10.1561&#x2F;1900000036"},{"title":"第七章：事务","path":"/wiki/ddia/ch7.html","content":"一些作者声称，支持通用的两阶段提交代价太大，会带来性能与可用性的问题。让程序员来处理过度使用事务导致的性能问题，总比缺少事务编程好得多。 —— James Corbett 等人，Spanner：Google 的全球分布式数据库（2012） 在数据系统的残酷现实中，很多事情都可能出错： 数据库软件、硬件可能在任意时刻发生故障（包括写操作进行到一半时）。 应用程序可能在任意时刻崩溃（包括一系列操作的中间）。 网络中断可能会意外切断数据库与应用的连接，或数据库之间的连接。 多个客户端可能会同时写入数据库，覆盖彼此的更改。 客户端可能读取到无意义的数据，因为数据只更新了一部分。 客户端之间的竞争条件可能导致令人惊讶的错误。 为了实现可靠性，系统必须处理这些故障，确保它们不会导致整个系统的灾难性故障。但是实现容错机制工作量巨大。需要仔细考虑所有可能出错的事情，并进行大量的测试，以确保解决方案真正管用。 数十年来，事务（transaction） 一直是简化这些问题的首选机制。事务是应用程序将多个读写操作组合成一个逻辑单元的一种方式。从概念上讲，事务中的所有读写操作被视作单个操作来执行：整个事务要么成功 提交（commit），要么失败 中止（abort）或 回滚（rollback）。如果失败，应用程序可以安全地重试。对于事务来说，应用程序的错误处理变得简单多了，因为它不用再担心部分失败的情况了，即某些操作成功，某些失败（无论出于何种原因）。 和事务打交道时间长了，你可能会觉得它显而易见。但我们不应将其视为理所当然。事务不是天然存在的；它们是为了 简化应用编程模型 而创建的。通过使用事务，应用程序可以自由地忽略某些潜在的错误情况和并发问题，因为数据库会替应用处理好这些。（我们称之为 安全保证，即 safety guarantees）。 并不是所有的应用都需要事务，有时候弱化事务保证、或完全放弃事务也是有好处的（例如，为了获得更高性能或更高可用性）。一些安全属性也可以在没有事务的情况下实现。 怎样知道你是否需要事务？为了回答这个问题，首先需要确切理解事务可以提供的安全保障，以及它们的代价。尽管乍看事务似乎很简单，但实际上有许多微妙但重要的细节在起作用。 本章将研究许多出错案例，并探索数据库用于防范这些问题的算法。尤其会深入 并发控制 的领域，讨论各种可能发生的竞争条件，以及数据库如何实现 读已提交（read committed），快照隔离（snapshot isolation） 和 可串行化（serializability） 等隔离级别。 本章同时适用于单机数据库与分布式数据库；在 第八章 中将重点讨论仅出现在分布式系统中的特殊挑战。 事务的棘手概念现今，几乎所有的关系型数据库和一些非关系数据库都支持 事务。其中大多数遵循 IBM System R（第一个 SQL 数据库）在 1975 年引入的风格【1,2,3】。40 年里，尽管一些实现细节发生了变化，但总体思路大同小异：MySQL、PostgreSQL、Oracle 和 SQL Server 等数据库中的事务支持与 System R 异乎寻常地相似。 2000 年以后，非关系（NoSQL）数据库开始普及。它们的目标是在关系数据库的现状基础上，通过提供新的数据模型选择（请参阅 第二章）并默认包含复制（第五章）和分区（第六章）来进一步提升。事务是这次运动的主要牺牲品：这些新一代数据库中的许多数据库完全放弃了事务，或者重新定义了这个词，描述比以前所理解的更弱得多的一套保证【4】。 随着这种新型分布式数据库的炒作，人们普遍认为事务是可伸缩性的对立面，任何大型系统都必须放弃事务以保持良好的性能和高可用性【5,6】。另一方面，数据库厂商有时将事务保证作为 “重要应用” 和 “有价值数据” 的基本要求。这两种观点都是 纯粹的夸张。 事实并非如此简单：与其他技术设计选择一样，事务有其优势和局限性。为了理解这些权衡，让我们了解事务所提供保证的细节 —— 无论是在正常运行中还是在各种极端（但是现实存在）的情况下。 ACID的含义事务所提供的安全保证，通常由众所周知的首字母缩略词 ACID 来描述，ACID 代表 原子性（Atomicity），一致性（Consistency），隔离性（Isolation） 和 持久性（Durability）。它由 Theo Härder 和 Andreas Reuter 于 1983 年提出，旨在为数据库中的容错机制建立精确的术语。 但实际上，不同数据库的 ACID 实现并不相同。例如，我们将会看到，关于 隔离性 的含义就有许多含糊不清【8】。高层次上的想法很美好，但魔鬼隐藏在细节里。今天，当一个系统声称自己 “符合 ACID” 时，实际上能期待的是什么保证并不清楚。不幸的是，ACID 现在几乎已经变成了一个营销术语。 （不符合 ACID 标准的系统有时被称为 BASE，它代表 基本可用性（Basically Available），软状态（Soft State） 和 最终一致性（Eventual consistency）【9】，这比 ACID 的定义更加模糊，似乎 BASE 的唯一合理的定义是 “不是 ACID”，即它几乎可以代表任何你想要的东西。） 让我们深入了解原子性，一致性，隔离性和持久性的定义，这可以让我们提炼出事务的思想。 原子性一般来说，原子是指不能分解成小部分的东西。这个词在计算机的不同领域中意味着相似但又微妙不同的东西。例如，在多线程编程中，如果一个线程执行一个原子操作，这意味着另一个线程无法看到该操作的一半结果。系统只能处于操作之前或操作之后的状态，而不是介于两者之间的状态。 相比之下，ACID 的原子性并 不 是关于 并发（concurrent） 的。它并不是在描述如果几个进程试图同时访问相同的数据会发生什么情况，这种情况包含在 隔离性 中。 ACID 的原子性描述了当客户想进行多次写入，但在一些写操作处理完之后出现故障的情况。例如进程崩溃，网络连接中断，磁盘变满或者某种完整性约束被违反。如果这些写操作被分组到一个原子事务中，并且该事务由于错误而不能完成（提交），则该事务将被中止，并且数据库必须丢弃或撤消该事务中迄今为止所做的任何写入。 如果没有原子性，在多处更改进行到一半时发生错误，很难知道哪些更改已经生效，哪些没有生效。该应用程序可以再试一次，但冒着进行两次相同变更的风险，可能会导致数据重复或错误的数据。原子性简化了这个问题：如果事务被 中止（abort），应用程序可以确定它没有改变任何东西，所以可以安全地重试。 ACID 原子性的定义特征是：能够在错误时中止事务，丢弃该事务进行的所有写入变更的能力。 或许 可中止性（abortability） 是更好的术语，但本书将继续使用原子性，因为这是惯用词。 一致性一致性这个词被赋予太多含义： 在 第五章 中，我们讨论了副本一致性，以及异步复制系统中的最终一致性问题（请参阅 “复制延迟问题”）。 一致性哈希 是某些系统用于重新分区的一种分区方法。 在 CAP 定理 中，一致性一词用于表示 线性一致性。 在 ACID 的上下文中，一致性 是指数据库在应用程序的特定概念中处于 “良好状态”。 很不幸，这一个词就至少有四种不同的含义。 ACID 一致性的概念是，对数据的一组特定约束必须始终成立，即 不变式（invariants）。例如，在会计系统中，所有账户整体上必须借贷相抵。如果一个事务开始于一个满足这些不变式的有效数据库，且在事务处理期间的任何写入操作都保持这种有效性，那么可以确定，不变式总是满足的。 但是，一致性的这种概念取决于应用程序对不变式的理解，应用程序负责正确定义它的事务，并保持一致性。这并不是数据库可以保证的事情：如果你写入违反不变式的脏数据，数据库也无法阻止你（一些特定类型的不变式可以由数据库检查，例如外键约束或唯一约束，但是一般来说，是应用程序来定义什么样的数据是有效的，什么样是无效的。—— 数据库只管存储）。 原子性、隔离性和持久性是数据库的属性，而一致性（在 ACID 意义上）是应用程序的属性。应用可能依赖数据库的原子性和隔离性来实现一致性，但这并不仅取决于数据库。因此，字母 C 不属于 ACID [^i]。 [^i]: 乔・海勒斯坦（Joe Hellerstein）指出，在 Härder 与 Reuter 的论文中，“ACID 中的 C” 是被 “扔进去凑缩写单词的”【7】，而且那时候大家都不怎么在乎一致性。 隔离性大多数数据库都会同时被多个客户端访问。如果它们各自读写数据库的不同部分，这是没有问题的，但是如果它们访问相同的数据库记录，则可能会遇到 并发 问题（竞争条件，即 race conditions）。 图 7-1 是这类问题的一个简单例子。假设你有两个客户端同时在数据库中增长一个计数器。（假设数据库没有内建的自增操作）每个客户端需要读取计数器的当前值，加 1 ，再回写新值。图 7-1 中，因为发生了两次增长，计数器应该从 42 增至 44；但由于竞态条件，实际上只增至 43 。 ACID 意义上的隔离性意味着，同时执行的事务是相互隔离的：它们不能相互冒犯。传统的数据库教科书将隔离性形式化为 可串行化（Serializability），这意味着每个事务可以假装它是唯一在整个数据库上运行的事务。数据库确保当多个事务被提交时，结果与它们串行运行（一个接一个）是一样的，尽管实际上它们可能是并发运行的【10】。 图 7-1 两个客户之间的竞争状态同时递增计数器 然而实践中很少会使用可串行的隔离，因为它有性能损失。一些流行的数据库如 Oracle 11g，甚至没有实现它。在 Oracle 中有一个名为 “可串行的” 隔离级别，但实际上它实现了一种叫做 快照隔离（snapshot isolation） 的功能，这是一种比可串行化更弱的保证【8,11】。我们将在 “弱隔离级别” 中研究快照隔离和其他形式的隔离。 持久性数据库系统的目的是，提供一个安全的地方存储数据，而不用担心丢失。持久性 是一个承诺，即一旦事务成功完成，即使发生硬件故障或数据库崩溃，写入的任何数据也不会丢失。 在单节点数据库中，持久性通常意味着数据已被写入非易失性存储设备，如硬盘或 SSD。它通常还包括预写日志或类似的文件（请参阅 “让 B 树更可靠”），以便在磁盘上的数据结构损坏时进行恢复。在带复制的数据库中，持久性可能意味着数据已成功复制到一些节点。为了提供持久性保证，数据库必须等到这些写入或复制完成后，才能报告事务成功提交。 如 “可靠性” 一节所述，完美的持久性是不存在的 ：如果所有硬盘和所有备份同时被销毁，那显然没有任何数据库能救得了你。 复制与持久性 在历史上，持久性意味着写入归档磁带。后来它被理解为写入磁盘或 SSD。再后来它又有了新的内涵即 “复制（replication）”。哪种实现更好一些？ 真相是，没有什么是完美的： 如果你写入磁盘然后机器宕机，即使数据没有丢失，在修复机器或将磁盘转移到其他机器之前，也是无法访问的。这种情况下，复制系统可以保持可用性。 一个相关性故障（停电，或一个特定输入导致所有节点崩溃的 Bug）可能会一次性摧毁所有副本（请参阅「可靠性」），任何仅存储在内存中的数据都会丢失，故内存数据库仍然要和磁盘写入打交道。 在异步复制系统中，当主库不可用时，最近的写入操作可能会丢失（请参阅「处理节点宕机」）。 当电源突然断电时，特别是固态硬盘，有证据显示有时会违反应有的保证：甚至 fsync 也不能保证正常工作【12】。硬盘固件可能有错误，就像任何其他类型的软件一样【13,14】。 存储引擎和文件系统之间的微妙交互可能会导致难以追踪的错误，并可能导致磁盘上的文件在崩溃后被损坏【15,16】。 磁盘上的数据可能会在没有检测到的情况下逐渐损坏【17】。如果数据已损坏一段时间，副本和最近的备份也可能损坏。这种情况下，需要尝试从历史备份中恢复数据。 一项关于固态硬盘的研究发现，在运行的前四年中，30% 到 80% 的硬盘会产生至少一个坏块【18】。相比固态硬盘，磁盘的坏道率较低，但完全失效的概率更高。 如果 SSD 断电，可能会在几周内开始丢失数据，具体取决于温度【19】。 在实践中，没有一种技术可以提供绝对保证。只有各种降低风险的技术，包括写入磁盘，复制到远程机器和备份 —— 它们可以且应该一起使用。与往常一样，最好抱着怀疑的态度接受任何理论上的 “保证”。 单对象和多对象操作回顾一下，在 ACID 中，原子性和隔离性描述了客户端在同一事务中执行多次写入时，数据库应该做的事情： 原子性 如果在一系列写操作的中途发生错误，则应中止事务处理，并丢弃当前事务的所有写入。换句话说，数据库免去了用户对部分失败的担忧 —— 通过提供 “宁为玉碎，不为瓦全（all-or-nothing）” 的保证。 隔离性 同时运行的事务不应该互相干扰。例如，如果一个事务进行多次写入，则另一个事务要么看到全部写入结果，要么什么都看不到，但不应该是一些子集。 这些定义假设你想同时修改多个对象（行，文档，记录）。通常需要 多对象事务（multi-object transaction） 来保持多块数据同步。图 7-2 展示了一个来自电邮应用的例子。执行以下查询来显示用户未读邮件数量： 1SELECT COUNT（*）FROM emails WHERE recipient_id = 2 AND unread_flag = true 但如果邮件太多，你可能会觉得这个查询太慢，并决定用单独的字段存储未读邮件的数量（一种反规范化）。现在每当一个新消息写入时，必须也增长未读计数器，每当一个消息被标记为已读时，也必须减少未读计数器。 在 图 7-2 中，用户 2 遇到异常情况：邮件列表里显示有未读消息，但计数器显示为零未读消息，因为计数器增长还没有发生 ^ii。隔离性可以避免这个问题：通过确保用户 2 要么同时看到新邮件和增长后的计数器，要么都看不到，而不是一个前后矛盾的中间结果。 图 7-2 违反隔离性：一个事务读取另一个事务的未被执行的写入（“脏读”）。 图 7-3 说明了对原子性的需求：如果在事务过程中发生错误，邮箱和未读计数器的内容可能会失去同步。在原子事务中，如果对计数器的更新失败，事务将被中止，并且插入的电子邮件将被回滚。 图 7-3 原子性确保发生错误时，事务先前的任何写入都会被撤消，以避免状态不一致 多对象事务需要某种方式来确定哪些读写操作属于同一个事务。在关系型数据库中，通常基于客户端与数据库服务器的 TCP 连接：在任何特定连接上，BEGIN TRANSACTION 和 COMMIT 语句之间的所有内容，被认为是同一事务的一部分.[^iii] [^iii]: 这并不完美。如果 TCP 连接中断，则事务必须中止。如果中断发生在客户端请求提交之后，但在服务器确认提交发生之前，客户端并不知道事务是否已提交。为了解决这个问题，事务管理器可以通过一个唯一事务标识符来对操作进行分组，这个标识符并未绑定到特定 TCP 连接。后续再 “数据库的端到端原则” 一节将回到这个主题。 另一方面，许多非关系数据库并没有将这些操作组合在一起的方法。即使存在多对象 API（例如，某键值存储可能具有在一个操作中更新几个键的 multi-put 操作），但这并不一定意味着它具有事务语义：该命令可能在一些键上成功，在其他的键上失败，使数据库处于部分更新的状态。 单对象写入当单个对象发生改变时，原子性和隔离性也是适用的。例如，假设你正在向数据库写入一个 20 KB 的 JSON 文档： 如果在发送第一个 10 KB 之后网络连接中断，数据库是否存储了不可解析的 10KB JSON 片段？ 如果在数据库正在覆盖磁盘上的前一个值的过程中电源发生故障，是否最终将新旧值拼接在一起？ 如果另一个客户端在写入过程中读取该文档，是否会看到部分更新的值？ 这些问题非常让人头大，故存储引擎一个几乎普遍的目标是：对单节点上的单个对象（例如键值对）上提供原子性和隔离性。原子性可以通过使用日志来实现崩溃恢复（请参阅 “让 B 树更可靠”），并且可以使用每个对象上的锁来实现隔离（每次只允许一个线程访问对象） 。 一些数据库也提供更复杂的原子操作 [^iv]，例如自增操作，这样就不再需要像 图 7-1 那样的读取 - 修改 - 写入序列了。同样流行的是 比较和设置（CAS, compare-and-set） 操作，仅当值没有被其他并发修改过时，才允许执行写操作。 [^iv]: 严格地说，原子自增（atomic increment） 这个术语在多线程编程的意义上使用了原子这个词。 在 ACID 的情况下，它实际上应该被称为 隔离的（isolated） 的或 可串行的（serializable） 的增量。 但这就太吹毛求疵了。 这些单对象操作很有用，因为它们可以防止在多个客户端尝试同时写入同一个对象时丢失更新（请参阅 “防止丢失更新”）。但它们不是通常意义上的事务。CAS 以及其他单一对象操作被称为 “轻量级事务”，甚至出于营销目的被称为 “ACID”【20,21,22】，但是这个术语是误导性的。事务通常被理解为，将多个对象上的多个操作合并为一个执行单元的机制。 多对象事务的需求许多分布式数据存储已经放弃了多对象事务，因为多对象事务很难跨分区实现，而且在需要高可用性或高性能的情况下，它们可能会碍事。但说到底，在分布式数据库中实现事务，并没有什么根本性的障碍。第九章 将讨论分布式事务的实现。 但是我们是否需要多对象事务？是否有可能只用键值数据模型和单对象操作来实现任何应用程序？ 有一些场景中，单对象插入，更新和删除是足够的。但是许多其他场景需要协调写入几个不同的对象： 在关系数据模型中，一个表中的行通常具有对另一个表中的行的外键引用。（类似的是，在一个图数据模型中，一个顶点有着到其他顶点的边）。多对象事务使你确保这些引用始终有效：当插入几个相互引用的记录时，外键必须是正确的和最新的，不然数据就没有意义。 在文档数据模型中，需要一起更新的字段通常在同一个文档中，这被视为单个对象 —— 更新单个文档时不需要多对象事务。但是，缺乏连接功能的文档数据库会鼓励非规范化（请参阅 “关系型数据库与文档数据库在今日的对比”）。当需要更新非规范化的信息时，如 图 7-2 所示，需要一次更新多个文档。事务在这种情况下非常有用，可以防止非规范化的数据不同步。 在具有次级索引的数据库中（除了纯粹的键值存储以外几乎都有），每次更改值时都需要更新索引。从事务角度来看，这些索引是不同的数据库对象：例如，如果没有事务隔离性，记录可能出现在一个索引中，但没有出现在另一个索引中，因为第二个索引的更新还没有发生。 这些应用仍然可以在没有事务的情况下实现。然而，没有原子性，错误处理就要复杂得多，缺乏隔离性，就会导致并发问题。我们将在 “弱隔离级别” 中讨论这些问题，并在 第十二章 中探讨其他方法。 处理错误和中止事务的一个关键特性是，如果发生错误，它可以中止并安全地重试。 ACID 数据库基于这样的哲学：如果数据库有违反其原子性，隔离性或持久性的危险，则宁愿完全放弃事务，而不是留下半成品。 然而并不是所有的系统都遵循这个哲学。特别是具有 无主复制 的数据存储，主要是在 “尽力而为” 的基础上进行工作。可以概括为 “数据库将做尽可能多的事，运行遇到错误时，它不会撤消它已经完成的事情” —— 所以，从错误中恢复是应用程序的责任。 错误发生不可避免，但许多软件开发人员倾向于只考虑乐观情况，而不是错误处理的复杂性。例如，像 Rails 的 ActiveRecord 和 Django 这样的 对象关系映射（ORM, object-relation Mapping） 框架不会重试中断的事务 —— 这个错误通常会导致一个从堆栈向上传播的异常，所以任何用户输入都会被丢弃，用户拿到一个错误信息。这实在是太耻辱了，因为中止的重点就是允许安全的重试。 尽管重试一个中止的事务是一个简单而有效的错误处理机制，但它并不完美： 如果事务实际上成功了，但是在服务器试图向客户端确认提交成功时网络发生故障（所以客户端认为提交失败了），那么重试事务会导致事务被执行两次 —— 除非你有一个额外的应用级去重机制。 如果错误是由于负载过大造成的，则重试事务将使问题变得更糟，而不是更好。为了避免这种正反馈循环，可以限制重试次数，使用指数退避算法，并单独处理与过载相关的错误（如果允许）。 仅在临时性错误（例如，由于死锁，异常情况，临时性网络中断和故障切换）后才值得重试。在发生永久性错误（例如，违反约束）之后重试是毫无意义的。 如果事务在数据库之外也有副作用，即使事务被中止，也可能发生这些副作用。例如，如果你正在发送电子邮件，那你肯定不希望每次重试事务时都重新发送电子邮件。如果你想确保几个不同的系统一起提交或放弃，两阶段提交（2PC, two-phase commit） 可以提供帮助（“原子提交与两阶段提交” 中将讨论这个问题）。 如果客户端进程在重试中失效，任何试图写入数据库的数据都将丢失。 弱隔离级别如果两个事务不触及相同的数据，它们可以安全地 并行（parallel） 运行，因为两者都不依赖于另一个。当一个事务读取由另一个事务同时修改的数据时，或者当两个事务试图同时修改相同的数据时，并发问题（竞争条件）才会出现。 并发 BUG 很难通过测试找到，因为这样的错误只有在特殊时序下才会触发。这样的时序问题可能非常少发生，通常很难重现 [^译注i]。并发性也很难推理，特别是在大型应用中，你不一定知道哪些其他代码正在访问数据库。在一次只有一个用户时，应用开发已经很麻烦了，有许多并发用户使得它更加困难，因为任何一个数据都可能随时改变。 [^译注i]: 轶事：偶然出现的瞬时错误有时称为 Heisenbug，而确定性的问题对应地称为 Bohrbugs 出于这个原因，数据库一直试图通过提供 事务隔离（transaction isolation） 来隐藏应用程序开发者的并发问题。从理论上讲，隔离可以通过假装没有并发发生，让你的生活更加轻松：可串行的（serializable） 隔离等级意味着数据库保证事务的效果如同串行运行（即一次一个，没有任何并发）。 实际上不幸的是：隔离并没有那么简单。可串行的隔离 会有性能损失，许多数据库不愿意支付这个代价【8】。因此，系统通常使用较弱的隔离级别来防止一部分，而不是全部的并发问题。这些隔离级别难以理解，并且会导致微妙的错误，但是它们仍然在实践中被使用【23】。 弱事务隔离级别导致的并发性错误不仅仅是一个理论问题。它们造成了很多的资金损失【24,25】，耗费了财务审计人员的调查【26】，并导致客户数据被破坏【27】。关于这类问题的一个流行的评论是 “如果你正在处理财务数据，请使用 ACID 数据库！” —— 但是这一点没有提到。即使是很多流行的关系型数据库系统（通常被认为是 “ACID”）也使用弱隔离级别，所以它们也不一定能防止这些错误的发生。 比起盲目地依赖工具，我们需要对存在的各种并发问题，以及如何防止这些问题有深入的理解。然后就可以使用我们所掌握的工具来构建可靠和正确的应用程序。 在本节中，我们将看几个在实践中使用的弱（非串行的，即 nonserializable）隔离级别，并详细讨论哪种竞争条件可能发生也可能不发生，以便你可以决定什么级别适合你的应用程序。一旦我们完成了这个工作，我们将详细讨论可串行化（请参阅 “可串行化”）。我们讨论的隔离级别将是非正式的，通过示例来进行。如果你需要严格的定义和分析它们的属性，你可以在学术文献中找到它们【28,29,30】。 读已提交最基本的事务隔离级别是 读已提交（Read Committed）[^v]，它提供了两个保证： 从数据库读时，只能看到已提交的数据（没有 脏读，即 dirty reads）。 写入数据库时，只会覆盖已提交的数据（没有 脏写，即 dirty writes）。 我们来更详细地讨论这两个保证。 [^v]: 某些数据库支持甚至更弱的隔离级别，称为 读未提交（Read uncommitted）。它可以防止脏写，但不防止脏读。 没有脏读设想一个事务已经将一些数据写入数据库，但事务还没有提交或中止。另一个事务可以看到未提交的数据吗？如果是的话，那就叫做 脏读（dirty reads）【2】。 在 读已提交 隔离级别运行的事务必须防止脏读。这意味着事务的任何写入操作只有在该事务提交时才能被其他人看到（然后所有的写入操作都会立即变得可见）。如 图 7-4 所示，用户 1 设置了 x = 3，但用户 2 的 get x 仍旧返回旧值 2 （当用户 1 尚未提交时）。 图 7-4 没有脏读：用户 2 只有在用户 1 的事务已经提交后才能看到 x 的新值。 为什么要防止脏读，有几个原因： 如果事务需要更新多个对象，脏读取意味着另一个事务可能会只看到一部分更新。例如，在 图 7-2 中，用户看到新的未读电子邮件，但看不到更新的计数器。这就是电子邮件的脏读。看到处于部分更新状态的数据库会让用户感到困惑，并可能导致其他事务做出错误的决定。 如果事务中止，则所有写入操作都需要回滚（如 图 7-3 所示）。如果数据库允许脏读，那就意味着一个事务可能会看到稍后需要回滚的数据，即从未实际提交给数据库的数据。想想后果就让人头大。 没有脏写如果两个事务同时尝试更新数据库中的相同对象，会发生什么情况？我们不知道写入的顺序是怎样的，但是我们通常认为后面的写入会覆盖前面的写入。 但是，如果先前的写入是尚未提交事务的一部分，又会发生什么情况，后面的写入会覆盖一个尚未提交的值？这被称作 脏写（dirty write）【28】。在 读已提交 的隔离级别上运行的事务必须防止脏写，通常是延迟第二次写入，直到第一次写入事务提交或中止为止。 通过防止脏写，这个隔离级别避免了一些并发问题： 如果事务更新多个对象，脏写会导致不好的结果。例如，考虑 图 7-5，以一个二手车销售网站为例，Alice 和 Bob 两个人同时试图购买同一辆车。购买汽车需要两次数据库写入：网站上的商品列表需要更新，以反映买家的购买，销售发票需要发送给买家。在 图 7-5 的情况下，销售是属于 Bob 的（因为他成功更新了商品列表），但发票却寄送给了 Alice（因为她成功更新了发票表）。读已提交会防止这样的事故。 但是，读已提交并不能防止 图 7-1 中两个计数器增量之间的竞争状态。在这种情况下，第二次写入发生在第一个事务提交后，所以它不是一个脏写。这仍然是不正确的，但是出于不同的原因，在 “防止丢失更新” 中将讨论如何使这种计数器增量安全。 图 7-5 如果存在脏写，来自不同事务的冲突写入可能会混淆在一起 实现读已提交读已提交 是一个非常流行的隔离级别。这是 Oracle 11g、PostgreSQL、SQL Server 2012、MemSQL 和其他许多数据库的默认设置【8】。 最常见的情况是，数据库通过使用 行锁（row-level lock） 来防止脏写：当事务想要修改特定对象（行或文档）时，它必须首先获得该对象的锁。然后必须持有该锁直到事务被提交或中止。一次只有一个事务可持有任何给定对象的锁；如果另一个事务要写入同一个对象，则必须等到第一个事务提交或中止后，才能获取该锁并继续。这种锁定是读已提交模式（或更强的隔离级别）的数据库自动完成的。 如何防止脏读？一种选择是使用相同的锁，并要求任何想要读取对象的事务来简单地获取该锁，然后在读取之后立即再次释放该锁。这将确保在对象具有脏的、未提交的值时不会发生读取（因为在此期间，锁将由进行写入的事务持有）。 但是要求读锁的办法在实践中效果并不好。因为一个长时间运行的写入事务会迫使许多只读事务等到这个慢写入事务完成。这会影响只读事务的响应时间，并且不利于可操作性：因为等待锁，应用某个部分的迟缓可能由于连锁效应，导致其他部分出现问题。 出于这个原因，大多数数据库 [^vi] 使用 图 7-4 的方式防止脏读：对于写入的每个对象，数据库都会记住旧的已提交值，和由当前持有写入锁的事务设置的新值。当事务正在进行时，任何其他读取对象的事务都会拿到旧值。 只有当新值提交后，事务才会切换到读取新值。 [^vi]: 在撰写本文时，唯一在读已提交隔离级别使用读锁的主流数据库是 IBM DB2 和使用 read_committed_snapshot = off 配置的 Microsoft SQL Server【23,36】。 快照隔离和可重复读如果只从表面上看读已提交隔离级别，你可能就认为它完成了事务所需的一切，这是情有可原的。它允许 中止（原子性的要求）；它防止读取不完整的事务结果，并且防止并发写入造成的混乱。事实上这些功能非常有用，比起没有事务的系统来，可以提供更多的保证。 但是在使用此隔离级别时，仍然有很多地方可能会产生并发错误。例如 图 7-6 说明了读已提交时可能发生的问题。 图 7-6 读取偏差：Alice 观察数据库处于不一致的状态 Alice 在银行有 1000 美元的储蓄，分为两个账户，每个 500 美元。现在有一笔事务从她的一个账户转移了 100 美元到另一个账户。如果她非常不幸地在事务处理的过程中查看其账户余额列表，她可能会在收到付款之前先看到一个账户的余额（收款账户，余额仍为 500 美元），在发出转账之后再看到另一个账户的余额（付款账户，新余额为 400 美元）。对 Alice 来说，现在她的账户似乎总共只有 900 美元 —— 看起来有 100 美元已经凭空消失了。 这种异常被称为 不可重复读（nonrepeatable read） 或 读取偏差（read skew）：如果 Alice 在事务结束时再次读取账户 1 的余额，她将看到与她之前的查询中看到的不同的值（600 美元）。在读已提交的隔离条件下，不可重复读 被认为是可接受的：Alice 看到的帐户余额确实在阅读时已经提交了。 不幸的是，术语 偏差（skew） 这个词是过载的：以前使用它是因为热点的不平衡工作量（请参阅 “负载偏斜与热点消除”），而这里偏差意味着异常的时序。 对于 Alice 的情况，这不是一个长期持续的问题。因为如果她几秒钟后刷新银行网站的页面，她很可能会看到一致的帐户余额。但是有些情况下，不能容忍这种暂时的不一致： 备份 进行备份需要复制整个数据库，对大型数据库而言可能需要花费数小时才能完成。备份进程运行时，数据库仍然会接受写入操作。因此备份可能会包含一些旧的部分和一些新的部分。如果从这样的备份中恢复，那么不一致（如消失的钱）就会变成永久的。 分析查询和完整性检查 有时，你可能需要运行一个查询，扫描大部分的数据库。这样的查询在分析中很常见（请参阅 “事务处理还是分析？”），也可能是定期完整性检查（即监视数据损坏）的一部分。如果这些查询在不同时间点观察数据库的不同部分，则可能会返回毫无意义的结果。 快照隔离（snapshot isolation）【28】是这个问题最常见的解决方案。想法是，每个事务都从数据库的 一致快照（consistent snapshot） 中读取 —— 也就是说，事务可以看到事务开始时在数据库中提交的所有数据。即使这些数据随后被另一个事务更改，每个事务也只能看到该特定时间点的旧数据。 快照隔离对长时间运行的只读查询（如备份和分析）非常有用。如果查询的数据在查询执行的同时发生变化，则很难理解查询的含义。当一个事务可以看到数据库在某个特定时间点冻结时的一致快照，理解起来就很容易了。 快照隔离是一个流行的功能：PostgreSQL、使用 InnoDB 引擎的 MySQL、Oracle、SQL Server 等都支持【23,31,32】。 实现快照隔离与读取提交的隔离类似，快照隔离的实现通常使用写锁来防止脏写（请参阅 “读已提交”），这意味着进行写入的事务会阻止另一个事务修改同一个对象。但是读取则不需要加锁。从性能的角度来看，快照隔离的一个关键原则是：读不阻塞写，写不阻塞读。这允许数据库在处理一致性快照上的长时间查询时，可以正常地同时处理写入操作，且两者间没有任何锁争用。 为了实现快照隔离，数据库使用了我们看到的用于防止 图 7-4 中的脏读的机制的一般化。数据库必须可能保留一个对象的几个不同的提交版本，因为各种正在进行的事务可能需要看到数据库在不同的时间点的状态。因为它同时维护着单个对象的多个版本，所以这种技术被称为 多版本并发控制（MVCC, multi-version concurrency control）。 如果一个数据库只需要提供 读已提交 的隔离级别，而不提供 快照隔离，那么保留一个对象的两个版本就足够了：已提交的版本和被覆盖但尚未提交的版本。不过支持快照隔离的存储引擎通常也使用 MVCC 来实现 读已提交 隔离级别。一种典型的方法是 读已提交 为每个查询使用单独的快照，而 快照隔离 对整个事务使用相同的快照。 图 7-7 说明了 PostgreSQL 如何实现基于 MVCC 的快照隔离【31】（其他实现类似）。当一个事务开始时，它被赋予一个唯一的，永远增长 [^vii] 的事务 ID（txid）。每当事务向数据库写入任何内容时，它所写入的数据都会被标记上写入者的事务 ID。 [^vii]: 事实上，事务 ID 是 32 位整数，所以大约会在 40 亿次事务之后溢出。 PostgreSQL 的 Vacuum 过程会清理老旧的事务 ID，确保事务 ID 溢出（回卷）不会影响到数据。 图 7-7 使用多版本对象实现快照隔离 表中的每一行都有一个 created_by 字段，其中包含将该行插入到表中的的事务 ID。此外，每行都有一个 deleted_by 字段，最初是空的。如果某个事务删除了一行，那么该行实际上并未从数据库中删除，而是通过将 deleted_by 字段设置为请求删除的事务的 ID 来标记为删除。在稍后的时间，当确定没有事务可以再访问已删除的数据时，数据库中的垃圾收集过程会将所有带有删除标记的行移除，并释放其空间。[^译注ii] [^译注ii]: 在 PostgreSQL 中，created_by 的实际名称为 xmin，deleted_by 的实际名称为 xmax UPDATE 操作在内部翻译为 DELETE 和 INSERT 。例如，在 图 7-7 中，事务 13 从账户 2 中扣除 100 美元，将余额从 500 美元改为 400 美元。实际上包含两条账户 2 的记录：余额为 $500 的行被标记为 被事务 13 删除，余额为 $400 的行 由事务 13 创建。 观察一致性快照的可见性规则当一个事务从数据库中读取时，事务 ID 用于决定它可以看见哪些对象，看不见哪些对象。通过仔细定义可见性规则，数据库可以向应用程序呈现一致的数据库快照。工作如下： 在每次事务开始时，数据库列出当时所有其他（尚未提交或尚未中止）的事务清单，即使之后提交了，这些事务已执行的任何写入也都会被忽略。 被中止事务所执行的任何写入都将被忽略。 由具有较晚事务 ID（即，在当前事务开始之后开始的）的事务所做的任何写入都被忽略，而不管这些事务是否已经提交。 所有其他写入，对应用都是可见的。 这些规则适用于创建和删除对象。在 图 7-7 中，当事务 12 从账户 2 读取时，它会看到 $500 的余额，因为 $500 余额的删除是由事务 13 完成的（根据规则 3，事务 12 看不到事务 13 执行的删除），且 400 美元记录的创建也是不可见的（按照相同的规则）。 换句话说，如果以下两个条件都成立，则可见一个对象： 读事务开始时，创建该对象的事务已经提交。 对象未被标记为删除，或如果被标记为删除，请求删除的事务在读事务开始时尚未提交。 长时间运行的事务可能会长时间使用快照，并继续读取（从其他事务的角度来看）早已被覆盖或删除的值。由于从来不原地更新值，而是每次值改变时创建一个新的版本，数据库可以在提供一致快照的同时只产生很小的额外开销。 索引和快照隔离索引如何在多版本数据库中工作？一种选择是使索引简单地指向对象的所有版本，并且需要索引查询来过滤掉当前事务不可见的任何对象版本。当垃圾收集删除任何事务不再可见的旧对象版本时，相应的索引条目也可以被删除。 在实践中，许多实现细节决定了多版本并发控制的性能。例如，如果同一对象的不同版本可以放入同一个页面中，PostgreSQL 的优化可以避免更新索引【31】。 在 CouchDB、Datomic 和 LMDB 中使用另一种方法。虽然它们也使用 B 树，但它们使用的是一种 仅追加 &#x2F; 写时拷贝（append-only&#x2F;copy-on-write） 的变体，它们在更新时不覆盖树的页面，而为每个修改页面创建一份副本。从父页面直到树根都会级联更新，以指向它们子页面的新版本。任何不受写入影响的页面都不需要被复制，并且保持不变【33,34,35】。 使用仅追加的 B 树，每个写入事务（或一批事务）都会创建一棵新的 B 树，当创建时，从该特定树根生长的树就是数据库的一个一致性快照。没必要根据事务 ID 过滤掉对象，因为后续写入不能修改现有的 B 树；它们只能创建新的树根。但这种方法也需要一个负责压缩和垃圾收集的后台进程。 可重复读与命名混淆快照隔离是一个有用的隔离级别，特别对于只读事务而言。但是，许多数据库实现了它，却用不同的名字来称呼。在 Oracle 中称为 可串行化（Serializable） 的，在 PostgreSQL 和 MySQL 中称为 可重复读（repeatable read）【23】。 这种命名混淆的原因是 SQL 标准没有 快照隔离 的概念，因为标准是基于 System R 1975 年定义的隔离级别【2】，那时候 快照隔离 尚未发明。相反，它定义了 可重复读，表面上看起来与快照隔离很相似。 PostgreSQL 和 MySQL 称其 快照隔离 级别为 可重复读（repeatable read），因为这样符合标准要求，所以它们可以声称自己 “标准兼容”。 不幸的是，SQL 标准对隔离级别的定义是有缺陷的 —— 模糊，不精确，并不像标准应有的样子独立于实现【28】。有几个数据库实现了可重复读，但它们实际提供的保证存在很大的差异，尽管表面上是标准化的【23】。在研究文献【29,30】中已经有了可重复读的正式定义，但大多数的实现并不能满足这个正式定义。最后，IBM DB2 使用 “可重复读” 来引用可串行化【8】。 结果，没有人真正知道 可重复读 的意思。 防止丢失更新到目前为止已经讨论的 读已提交 和 快照隔离 级别，主要保证了 只读事务在并发写入时 可以看到什么。却忽略了两个事务并发写入的问题 —— 我们只讨论了脏写（请参阅 “没有脏写”），一种特定类型的写 - 写冲突是可能出现的。 并发的写入事务之间还有其他几种有趣的冲突。其中最著名的是 丢失更新（lost update） 问题，如 图 7-1 所示，以两个并发计数器增量为例。 如果应用从数据库中读取一些值，修改它并写回修改的值（读取 - 修改 - 写入序列），则可能会发生丢失更新的问题。如果两个事务同时执行，则其中一个的修改可能会丢失，因为第二个写入的内容并没有包括第一个事务的修改（有时会说后面写入 狠揍（clobber） 了前面的写入）这种模式发生在各种不同的情况下： 增加计数器或更新账户余额（需要读取当前值，计算新值并写回更新后的值） 将本地修改写入一个复杂值中：例如，将元素添加到 JSON 文档中的一个列表（需要解析文档，进行更改并写回修改的文档） 两个用户同时编辑 wiki 页面，每个用户通过将整个页面内容发送到服务器来保存其更改，覆写数据库中当前的任何内容。 这是一个普遍的问题，所以已经开发了各种解决方案。 原子写许多数据库提供了原子更新操作，从而消除了在应用程序代码中执行读取 - 修改 - 写入序列的需要。如果你的代码可以用这些操作来表达，那这通常是最好的解决方案。例如，下面的指令在大多数关系数据库中是并发安全的： 1UPDATE counters SET value = value + 1 WHERE key = &#x27;foo&#x27;; 类似地，像 MongoDB 这样的文档数据库提供了对 JSON 文档的一部分进行本地修改的原子操作，Redis 提供了修改数据结构（如优先级队列）的原子操作。并不是所有的写操作都可以用原子操作的方式来表达，例如 wiki 页面的更新涉及到任意文本编辑 [^viii]，但是在可以使用原子操作的情况下，它们通常是最好的选择。 [^viii]: 将文本文档的编辑表示为原子的变化流是可能的，尽管相当复杂。请参阅 “自动冲突解决”。 原子操作通常通过在读取对象时，获取其上的排它锁来实现。以便更新完成之前没有其他事务可以读取它。这种技术有时被称为 游标稳定性（cursor stability）【36,37】。另一个选择是简单地强制所有的原子操作在单一线程上执行。 不幸的是，ORM 框架很容易意外地执行不安全的读取 - 修改 - 写入序列，而不是使用数据库提供的原子操作【38】。如果你知道自己在做什么那当然不是问题，但它经常产生那种很难测出来的微妙 Bug。 显式锁定如果数据库的内置原子操作没有提供必要的功能，防止丢失更新的另一个选择是让应用程序显式地锁定将要更新的对象。然后应用程序可以执行读取 - 修改 - 写入序列，如果任何其他事务尝试同时读取同一个对象，则强制等待，直到第一个 读取 - 修改 - 写入序列 完成。 例如，考虑一个多人游戏，其中几个玩家可以同时移动相同的棋子。在这种情况下，一个原子操作可能是不够的，因为应用程序还需要确保玩家的移动符合游戏规则，这可能涉及到一些不能合理地用数据库查询实现的逻辑。但你可以使用锁来防止两名玩家同时移动相同的棋子，如例 7-1 所示。 例 7-1 显式锁定行以防止丢失更新 12345678BEGIN TRANSACTION;SELECT * FROM figures WHERE name = &#x27;robot&#x27; AND game_id = 222FOR UPDATE;-- 检查玩家的操作是否有效，然后更新先前 SELECT 返回棋子的位置。UPDATE figures SET position = &#x27;c4&#x27; WHERE id = 1234;COMMIT; FOR UPDATE 子句告诉数据库应该对该查询返回的所有行加锁。 这是有效的，但要做对，你需要仔细考虑应用逻辑。忘记在代码某处加锁很容易引入竞争条件。 自动检测丢失的更新原子操作和锁是通过强制 读取 - 修改 - 写入序列 按顺序发生，来防止丢失更新的方法。另一种方法是允许它们并行执行，如果事务管理器检测到丢失更新，则中止事务并强制它们重试其 读取 - 修改 - 写入序列。 这种方法的一个优点是，数据库可以结合快照隔离高效地执行此检查。事实上，PostgreSQL 的可重复读，Oracle 的可串行化和 SQL Server 的快照隔离级别，都会自动检测到丢失更新，并中止惹麻烦的事务。但是，MySQL&#x2F;InnoDB 的可重复读并不会检测 丢失更新【23】。一些作者【28,30】认为，数据库必须能防止丢失更新才称得上是提供了 快照隔离，所以在这个定义下，MySQL 下不提供快照隔离。 丢失更新检测是一个很好的功能，因为它不需要应用代码使用任何特殊的数据库功能，你可能会忘记使用锁或原子操作，从而引入错误；但丢失更新的检测是自动发生的，因此不太容易出错。 比较并设置（CAS）在不提供事务的数据库中，有时会发现一种原子操作：比较并设置（CAS, 即 Compare And Set，先前在 “单对象写入” 中提到）。此操作的目的是为了避免丢失更新：只有当前值从上次读取时一直未改变，才允许更新发生。如果当前值与先前读取的值不匹配，则更新不起作用，且必须重试读取 - 修改 - 写入序列。 例如，为了防止两个用户同时更新同一个 wiki 页面，可以尝试类似这样的方式，只有当用户开始编辑后页面内容未发生改变时，才会更新成功： 123-- 根据数据库的实现情况，这可能安全也可能不安全UPDATE wiki_pages SET content = &#x27;新内容&#x27; WHERE id = 1234 AND content = &#x27;旧内容&#x27;; 如果内容已经更改并且不再与 “旧内容” 相匹配，则此更新将不起作用，因此你需要检查更新是否生效，必要时重试。但是，如果数据库允许 WHERE 子句从旧快照中读取，则此语句可能无法防止丢失更新，因为即使发生了另一个并发写入，WHERE 条件也可能为真。在依赖数据库的 CAS 操作前要检查其是否安全。 冲突解决和复制在复制数据库中（请参阅 第五章），防止丢失的更新需要考虑另一个维度：由于在多个节点上存在数据副本，并且在不同节点上的数据可能被并发地修改，因此需要采取一些额外的步骤来防止丢失更新。 锁和 CAS 操作假定只有一个最新的数据副本。但是多主或无主复制的数据库通常允许多个写入并发执行，并异步复制到副本上，因此无法保证只有一个最新数据的副本。所以基于锁或 CAS 操作的技术不适用于这种情况（我们将在 “线性一致性” 中更详细地讨论这个问题）。 相反，如 “检测并发写入” 一节所述，这种复制数据库中的一种常见方法是允许并发写入创建多个冲突版本的值（也称为兄弟），并使用应用代码或特殊数据结构在事实发生之后解决和合并这些版本。 原子操作可以在复制的上下文中很好地工作，尤其当它们具有可交换性时（即，可以在不同的副本上以不同的顺序应用它们，且仍然可以得到相同的结果）。例如，递增计数器或向集合添加元素是可交换的操作。这是 Riak 2.0 数据类型背后的思想，它可以防止复制副本丢失更新。当不同的客户端同时更新一个值时，Riak 自动将更新合并在一起，以免丢失更新【39】。 另一方面，最后写入胜利（LWW）的冲突解决方法很容易丢失更新，如 “最后写入胜利（丢弃并发写入）” 中所述。不幸的是，LWW 是许多复制数据库中的默认方案。 写入偏差与幻读前面的章节中，我们看到了 脏写 和 丢失更新，当不同的事务并发地尝试写入相同的对象时，会出现这两种竞争条件。为了避免数据损坏，这些竞争条件需要被阻止 —— 既可以由数据库自动执行，也可以通过锁和原子写操作这类手动安全措施来防止。 但是，并发写入间可能发生的竞争条件还没有完。在本节中，我们将看到一些更微妙的冲突例子。 首先，想象一下这个例子：你正在为医院写一个医生轮班管理程序。医院通常会同时要求几位医生待命，但底线是至少有一位医生在待命。医生可以放弃他们的班次（例如，如果他们自己生病了），只要至少有一个同事在这一班中继续工作【40,41】。 现在想象一下，Alice 和 Bob 是两位值班医生。两人都感到不适，所以他们都决定请假。不幸的是，他们恰好在同一时间点击按钮下班。图 7-8 说明了接下来的事情。 图 7-8 写入偏差导致应用程序错误的示例 在两个事务中，应用首先检查是否有两个或以上的医生正在值班；如果是的话，它就假定一名医生可以安全地休班。由于数据库使用快照隔离，两次检查都返回 2 ，所以两个事务都进入下一个阶段。Alice 更新自己的记录休班了，而 Bob 也做了一样的事情。两个事务都成功提交了，现在没有医生值班了。违反了至少有一名医生在值班的要求。 写入偏差的特征这种异常称为 写入偏差【28】。它既不是 脏写，也不是 丢失更新，因为这两个事务正在更新两个不同的对象（Alice 和 Bob 各自的待命记录）。在这里发生的冲突并不是那么明显，但是这显然是一个竞争条件：如果两个事务一个接一个地运行，那么第二个医生就不能歇班了。异常行为只有在事务并发进行时才有可能发生。 可以将写入偏差视为丢失更新问题的一般化。如果两个事务读取相同的对象，然后更新其中一些对象（不同的事务可能更新不同的对象），则可能发生写入偏差。在多个事务更新同一个对象的特殊情况下，就会发生脏写或丢失更新（取决于时序）。 我们已经看到，有各种不同的方法来防止丢失的更新。但对于写入偏差，我们的选择更受限制： 由于涉及多个对象，单对象的原子操作不起作用。 不幸的是，在一些快照隔离的实现中，自动检测丢失更新对此并没有帮助。在 PostgreSQL 的可重复读，MySQL&#x2F;InnoDB 的可重复读，Oracle 可串行化或 SQL Server 的快照隔离级别中，都不会自动检测写入偏差【23】。自动防止写入偏差需要真正的可串行化隔离（请参阅 “可串行化”）。 某些数据库允许配置约束，然后由数据库强制执行（例如，唯一性，外键约束或特定值限制）。但是为了指定至少有一名医生必须在线，需要一个涉及多个对象的约束。大多数数据库没有内置对这种约束的支持，但是你可以使用触发器，或者物化视图来实现它们，这取决于不同的数据库【42】。 如果无法使用可串行化的隔离级别，则此情况下的次优选项可能是显式锁定事务所依赖的行。在例子中，你可以写下如下的代码： 1234567891011BEGIN TRANSACTION;SELECT * FROM doctors WHERE on_call = TRUE AND shift_id = 1234 FOR UPDATE;UPDATE doctors SET on_call = FALSE WHERE name = &#x27;Alice&#x27; AND shift_id = 1234; COMMIT; 和以前一样，FOR UPDATE 告诉数据库锁定返回的所有行以用于更新。 写入偏差的更多例子写入偏差乍看像是一个深奥的问题，但一旦意识到这一点，很容易会注意到它可能发生在更多场景下。以下是一些例子： 会议室预订系统 比如你想要规定不能在同一时间对同一个会议室进行多次的预订【43】。当有人想要预订时，首先检查是否存在相互冲突的预订（即预订时间范围重叠的同一房间），如果没有找到，则创建会议（请参阅示例 7-2）[^ix]。 [^ix]: 在 PostgreSQL 中，你可以使用范围类型优雅地执行此操作，但在其他数据库中并未得到广泛支持。 例 7-2 会议室预订系统试图避免重复预订（在快照隔离下不安全） 123456789101112BEGIN TRANSACTION;-- 检查所有现存的与 12:00~13:00 重叠的预定SELECT COUNT(*) FROM bookingsWHERE room_id = 123 AND end_time &gt; &#x27;2015-01-01 12:00&#x27; AND start_time &lt; &#x27;2015-01-01 13:00&#x27;;-- 如果之前的查询返回 0INSERT INTO bookings(room_id, start_time, end_time, user_id) VALUES (123, &#x27;2015-01-01 12:00&#x27;, &#x27;2015-01-01 13:00&#x27;, 666);COMMIT; 不幸的是，快照隔离并不能防止另一个用户同时插入冲突的会议。为了确保不会遇到调度冲突，你又需要可串行化的隔离级别了。 多人游戏 在 例 7-1 中，我们使用一个锁来防止丢失更新（也就是确保两个玩家不能同时移动同一个棋子）。但是锁定并不妨碍玩家将两个不同的棋子移动到棋盘上的相同位置，或者采取其他违反游戏规则的行为。取决于你正在执行的规则类型，也许可以使用唯一约束（unique constraint），否则你很容易发生写入偏差。 抢注用户名 在每个用户拥有唯一用户名的网站上，两个用户可能会尝试同时创建具有相同用户名的帐户。可以在事务检查名称是否被抢占，如果没有则使用该名称创建账户。但是像在前面的例子中那样，在快照隔离下这是不安全的。幸运的是，唯一约束是一个简单的解决办法（第二个事务在提交时会因为违反用户名唯一约束而被中止）。 防止双重开支 允许用户花钱或使用积分的服务，需要检查用户的支付数额不超过其余额。可以通过在用户的帐户中插入一个试探性的消费项目来实现这一点，列出帐户中的所有项目，并检查总和是否为正值【44】。在写入偏差场景下，可能会发生两个支出项目同时插入，一起导致余额变为负值，但这两个事务都不会注意到另一个。 导致写入偏差的幻读所有这些例子都遵循类似的模式： 一个 SELECT 查询找出符合条件的行，并检查是否符合一些要求。（例如：至少有两名医生在值班；不存在对该会议室同一时段的预定；棋盘上的位置没有被其他棋子占据；用户名还没有被抢注；账户里还有足够余额） 按照第一个查询的结果，应用代码决定是否继续。（可能会继续操作，也可能中止并报错） 如果应用决定继续操作，就执行写入（插入、更新或删除），并提交事务。 这个写入的效果改变了步骤 2 中的先决条件。换句话说，如果在提交写入后，重复执行一次步骤 1 的 SELECT 查询，将会得到不同的结果。因为写入改变了符合搜索条件的行集（现在少了一个医生值班，那时候的会议室现在已经被预订了，棋盘上的这个位置已经被占据了，用户名已经被抢注，账户余额不够了）。 这些步骤可能以不同的顺序发生。例如可以首先进行写入，然后进行 SELECT 查询，最后根据查询结果决定是放弃还是提交。 在医生值班的例子中，在步骤 3 中修改的行，是步骤 1 中返回的行之一，所以我们可以通过锁定步骤 1 中的行（SELECT FOR UPDATE）来使事务安全并避免写入偏差。但是其他四个例子是不同的：它们检查是否 不存在 某些满足条件的行，写入会 添加 一个匹配相同条件的行。如果步骤 1 中的查询没有返回任何行，则 SELECT FOR UPDATE 锁不了任何东西。 这种效应：一个事务中的写入改变另一个事务的搜索查询的结果，被称为 幻读【3】。快照隔离避免了只读查询中幻读，但是在像我们讨论的例子那样的读写事务中，幻读会导致特别棘手的写入偏差情况。 物化冲突如果幻读的问题是没有对象可以加锁，也许可以人为地在数据库中引入一个锁对象？ 例如，在会议室预订的场景中，可以想象创建一个关于时间槽和房间的表。此表中的每一行对应于特定时间段（例如 15 分钟）的特定房间。可以提前插入房间和时间的所有可能组合行（例如接下来的六个月）。 现在，要创建预订的事务可以锁定（SELECT FOR UPDATE）表中与所需房间和时间段对应的行。在获得锁定之后，它可以检查重叠的预订并像以前一样插入新的预订。请注意，这个表并不是用来存储预订相关的信息 —— 它完全就是一组锁，用于防止同时修改同一房间和时间范围内的预订。 这种方法被称为 物化冲突（materializing conflicts），因为它将幻读变为数据库中一组具体行上的锁冲突【11】。不幸的是，弄清楚如何物化冲突可能很难，也很容易出错，并且让并发控制机制泄漏到应用数据模型是很丑陋的做法。出于这些原因，如果没有其他办法可以实现，物化冲突应被视为最后的手段。在大多数情况下。可串行化（Serializable） 的隔离级别是更可取的。 可串行化在本章中，已经看到了几个易于出现竞争条件的事务例子。读已提交 和 快照隔离 级别会阻止某些竞争条件，但不会阻止另一些。我们遇到了一些特别棘手的例子，写入偏差 和 幻读。这是一个可悲的情况： 隔离级别难以理解，并且在不同的数据库中实现的不一致（例如，“可重复读” 的含义天差地别）。 光检查应用代码很难判断在特定的隔离级别运行是否安全。 特别是在大型应用程序中，你可能并不知道并发发生的所有事情。 没有检测竞争条件的好工具。原则上来说，静态分析可能会有帮助【26】，但研究中的技术还没法实际应用。并发问题的测试是很难的，因为它们通常是非确定性的 —— 只有在倒霉的时序下才会出现问题。 这不是一个新问题，从 20 世纪 70 年代以来就一直是这样了，当时首先引入了较弱的隔离级别【2】。一直以来，研究人员的答案都很简单：使用 可串行化（serializable） 的隔离级别！ 可串行化（Serializability） 隔离通常被认为是最强的隔离级别。它保证即使事务可以并行执行，最终的结果也是一样的，就好像它们没有任何并发性，连续挨个执行一样。因此数据库保证，如果事务在单独运行时正常运行，则它们在并发运行时继续保持正确 —— 换句话说，数据库可以防止 所有 可能的竞争条件。 但如果可串行化隔离级别比弱隔离级别的烂摊子要好得多，那为什么没有人见人爱？为了回答这个问题，我们需要看看实现可串行化的选项，以及它们如何执行。目前大多数提供可串行化的数据库都使用了三种技术之一，本章的剩余部分将会介绍这些技术： 字面意义上地串行顺序执行事务（请参阅 “真的串行执行”） 两阶段锁定（2PL, two-phase locking），几十年来唯一可行的选择（请参阅 “两阶段锁定”） 乐观并发控制技术，例如 可串行化快照隔离（serializable snapshot isolation，请参阅 “可串行化快照隔离”） 现在将主要在单节点数据库的背景下讨论这些技术；在 第九章 中，我们将研究如何将它们推广到涉及分布式系统中多个节点的事务。 真的串行执行避免并发问题的最简单方法就是完全不要并发：在单个线程上按顺序一次只执行一个事务。这样做就完全绕开了检测 &#x2F; 防止事务间冲突的问题，由此产生的隔离，正是可串行化的定义。 尽管这似乎是一个明显的主意，但数据库设计人员只是在 2007 年左右才决定，单线程循环执行事务是可行的【45】。如果多线程并发在过去的 30 年中被认为是获得良好性能的关键所在，那么究竟是什么改变致使单线程执行变为可能呢？ 两个进展引发了这个反思： RAM 足够便宜了，许多场景现在都可以将完整的活跃数据集保存在内存中（请参阅 “在内存中存储一切”）。当事务需要访问的所有数据都在内存中时，事务处理的执行速度要比等待数据从磁盘加载时快得多。 数据库设计人员意识到 OLTP 事务通常很短，而且只进行少量的读写操作（请参阅 “事务处理还是分析？”）。相比之下，长时间运行的分析查询通常是只读的，因此它们可以在串行执行循环之外的一致快照（使用快照隔离）上运行。 串行执行事务的方法在 VoltDB&#x2F;H-Store、Redis 和 Datomic 中实现【46,47,48】。设计用于单线程执行的系统有时可以比支持并发的系统性能更好，因为它可以避免锁的协调开销。但是其吞吐量仅限于单个 CPU 核的吞吐量。为了充分利用单一线程，需要有与传统形式的事务不同的结构。 在存储过程中封装事务在数据库的早期阶段，意图是数据库事务可以包含整个用户活动流程。例如，预订机票是一个多阶段的过程（搜索路线，票价和可用座位，决定行程，在每段行程的航班上订座，输入乘客信息，付款）。数据库设计者认为，如果整个过程是一个事务，那么它就可以被原子化地执行。 不幸的是，人类做出决定和回应的速度非常缓慢。如果数据库事务需要等待来自用户的输入，则数据库需要支持潜在的大量并发事务，其中大部分是空闲的。大多数数据库不能高效完成这项工作，因此几乎所有的 OLTP 应用程序都避免在事务中等待交互式的用户输入，以此来保持事务的简短。在 Web 上，这意味着事务在同一个 HTTP 请求中被提交 —— 一个事务不会跨越多个请求。一个新的 HTTP 请求开始一个新的事务。 即使已经将人类从关键路径中排除，事务仍然以交互式的客户端 &#x2F; 服务器风格执行，一次一个语句。应用程序进行查询，读取结果，可能根据第一个查询的结果进行另一个查询，依此类推。查询和结果在应用程序代码（在一台机器上运行）和数据库服务器（在另一台机器上）之间来回发送。 在这种交互式的事务方式中，应用程序和数据库之间的网络通信耗费了大量的时间。如果不允许在数据库中进行并发处理，且一次只处理一个事务，则吞吐量将会非常糟糕，因为数据库大部分的时间都花费在等待应用程序发出当前事务的下一个查询。在这种数据库中，为了获得合理的性能，需要同时处理多个事务。 出于这个原因，具有单线程串行事务处理的系统不允许交互式的多语句事务。取而代之，应用程序必须提前将整个事务代码作为存储过程提交给数据库。这些方法之间的差异如 图 7-9 所示。如果事务所需的所有数据都在内存中，则存储过程可以非常快地执行，而不用等待任何网络或磁盘 I&#x2F;O。 图 7-9 交互式事务和存储过程之间的区别（使用图 7-8 的示例事务） 存储过程的优点和缺点存储过程在关系型数据库中已经存在了一段时间了，自 1999 年以来它们一直是 SQL 标准（SQL&#x2F;PSM）的一部分。出于各种原因，它们的名声有点不太好： 每个数据库厂商都有自己的存储过程语言（Oracle 有 PL&#x2F;SQL，SQL Server 有 T-SQL，PostgreSQL 有 PL&#x2F;pgSQL，等等）。这些语言并没有跟上通用编程语言的发展，所以从今天的角度来看，它们看起来相当丑陋和陈旧，而且缺乏大多数编程语言中能找到的库的生态系统。 在数据库中运行的代码难以管理：与应用服务器相比，它更难调试，更难以保持版本控制和部署，更难测试，并且难以集成到指标收集系统来进行监控。 数据库通常比应用服务器对性能敏感的多，因为单个数据库实例通常由许多应用服务器共享。数据库中一个写得不好的存储过程（例如，占用大量内存或 CPU 时间）会比在应用服务器中相同的代码造成更多的麻烦。 但是这些问题都是可以克服的。现代的存储过程实现放弃了 PL&#x2F;SQL，而是使用现有的通用编程语言：VoltDB 使用 Java 或 Groovy，Datomic 使用 Java 或 Clojure，而 Redis 使用 Lua。 存储过程与内存存储，使得在单个线程上执行所有事务变得可行。由于不需要等待 I&#x2F;O，且避免了并发控制机制的开销，它们可以在单个线程上实现相当好的吞吐量。 VoltDB 还使用存储过程进行复制：但不是将事务的写入结果从一个节点复制到另一个节点，而是在每个节点上执行相同的存储过程。因此 VoltDB 要求存储过程是 确定性的（在不同的节点上运行时，它们必须产生相同的结果）。举个例子，如果事务需要使用当前的日期和时间，则必须通过特殊的确定性 API 来实现。 分区顺序执行所有事务使并发控制简单多了，但数据库的事务吞吐量被限制为单机单核的速度。只读事务可以使用快照隔离在其它地方执行，但对于写入吞吐量较高的应用，单线程事务处理器可能成为一个严重的瓶颈。 为了伸缩至多个 CPU 核心和多个节点，可以对数据进行分区（请参阅 第六章），在 VoltDB 中支持这样做。如果你可以找到一种对数据集进行分区的方法，以便每个事务只需要在单个分区中读写数据，那么每个分区就可以拥有自己独立运行的事务处理线程。在这种情况下可以为每个分区指派一个独立的 CPU 核，事务吞吐量就可以与 CPU 核数保持线性伸缩【47】。 但是，对于需要访问多个分区的任何事务，数据库必须在触及的所有分区之间协调事务。存储过程需要跨越所有分区锁定执行，以确保整个系统的可串行性。 由于跨分区事务具有额外的协调开销，所以它们比单分区事务慢得多。 VoltDB 报告的吞吐量大约是每秒 1000 个跨分区写入，比单分区吞吐量低几个数量级，并且不能通过增加更多的机器来增加吞吐量【49】。 事务是否可以是划分至单个分区很大程度上取决于应用数据的结构。简单的键值数据通常可以非常容易地进行分区，但是具有多个次级索引的数据可能需要大量的跨分区协调（请参阅 “分区与次级索引”）。 串行执行小结在特定约束条件下，真的串行执行事务，已经成为一种实现可串行化隔离等级的可行办法。 每个事务都必须小而快，只要有一个缓慢的事务，就会拖慢所有事务处理。 仅限于活跃数据集可以放入内存的情况。很少访问的数据可能会被移动到磁盘，但如果需要在单线程执行的事务中访问这些磁盘中的数据，系统就会变得非常慢 [^x]。 写入吞吐量必须低到能在单个 CPU 核上处理，如若不然，事务需要能划分至单个分区，且不需要跨分区协调。 跨分区事务是可能的，但是它们能被使用的程度有很大的限制。 [^x]: 如果事务需要访问不在内存中的数据，最好的解决方案可能是中止事务，异步地将数据提取到内存中，同时继续处理其他事务，然后在数据加载完毕时重新启动事务。这种方法被称为 反缓存（anti-caching），正如前面在 “在内存中存储一切” 中所述。 两阶段锁定大约 30 年来，在数据库中只有一种广泛使用的串行化算法：两阶段锁定（2PL，two-phase locking） [^xi] [^xi]: 有时也称为 严格两阶段锁定（SS2PL, strong strict two-phase locking），以便和其他 2PL 变体区分。 2PL不是2PC请注意，虽然两阶段锁定（2PL）听起来非常类似于两阶段提交（2PC），但它们是完全不同的东西。我们将在 第九章 讨论 2PC。 之前我们看到锁通常用于防止脏写（请参阅 “没有脏写” 一节）：如果两个事务同时尝试写入同一个对象，则锁可确保第二个写入必须等到第一个写入完成事务（中止或提交），然后才能继续。 两阶段锁定类似，但是锁的要求更强得多。只要没有写入，就允许多个事务同时读取同一个对象。但对象只要有写入（修改或删除），就需要 独占访问（exclusive access） 权限： 如果事务 A 读取了一个对象，并且事务 B 想要写入该对象，那么 B 必须等到 A 提交或中止才能继续（这确保 B 不能在 A 底下意外地改变对象）。 如果事务 A 写入了一个对象，并且事务 B 想要读取该对象，则 B 必须等到 A 提交或中止才能继续（像 图 7-1 那样读取旧版本的对象在 2PL 下是不可接受的）。 在 2PL 中，写入不仅会阻塞其他写入，也会阻塞读，反之亦然。快照隔离使得 读不阻塞写，写也不阻塞读（请参阅 “实现快照隔离”），这是 2PL 和快照隔离之间的关键区别。另一方面，因为 2PL 提供了可串行化的性质，它可以防止早先讨论的所有竞争条件，包括丢失更新和写入偏差。 实现两阶段锁2PL 用于 MySQL（InnoDB）和 SQL Server 中的可串行化隔离级别，以及 DB2 中的可重复读隔离级别【23,36】。 读与写的阻塞是通过为数据库中每个对象添加锁来实现的。锁可以处于 共享模式（shared mode） 或 独占模式（exclusive mode）。锁使用如下： 若事务要读取对象，则须先以共享模式获取锁。允许多个事务同时持有共享锁。但如果另一个事务已经在对象上持有排它锁，则这些事务必须等待。 若事务要写入一个对象，它必须首先以独占模式获取该锁。没有其他事务可以同时持有锁（无论是共享模式还是独占模式），所以如果对象上存在任何锁，该事务必须等待。 如果事务先读取再写入对象，则它可能会将其共享锁升级为独占锁。升级锁的工作与直接获得独占锁相同。 事务获得锁之后，必须继续持有锁直到事务结束（提交或中止）。这就是 “两阶段” 这个名字的来源：第一阶段（当事务正在执行时）获取锁，第二阶段（在事务结束时）释放所有的锁。 由于使用了这么多的锁，因此很可能会发生：事务 A 等待事务 B 释放它的锁，反之亦然。这种情况叫做 死锁（Deadlock）。数据库会自动检测事务之间的死锁，并中止其中一个，以便另一个继续执行。被中止的事务需要由应用程序重试。 两阶段锁定的性能两阶段锁定的巨大缺点，以及 70 年代以来没有被所有人使用的原因，是其性能问题。两阶段锁定下的事务吞吐量与查询响应时间要比弱隔离级别下要差得多。 这一部分是由于获取和释放所有这些锁的开销，但更重要的是由于并发性的降低。按照设计，如果两个并发事务试图做任何可能导致竞争条件的事情，那么必须等待另一个完成。 传统的关系数据库不限制事务的持续时间，因为它们是为等待人类输入的交互式应用而设计的。因此，当一个事务需要等待另一个事务时，等待的时长并没有限制。即使你保证所有的事务都很短，如果有多个事务想要访问同一个对象，那么可能会形成一个队列，所以事务可能需要等待几个其他事务才能完成。 因此，运行 2PL 的数据库可能具有相当不稳定的延迟，如果在工作负载中存在争用，那么可能高百分位点处的响应会非常的慢（请参阅 “描述性能”）。可能只需要一个缓慢的事务，或者一个访问大量数据并获取许多锁的事务，就能把系统的其他部分拖慢，甚至迫使系统停机。当需要稳健的操作时，这种不稳定性是有问题的。 基于锁实现的读已提交隔离级别可能发生死锁，但在基于 2PL 实现的可串行化隔离级别中，它们会出现的频繁的多（取决于事务的访问模式）。这可能是一个额外的性能问题：当事务由于死锁而被中止并被重试时，它需要从头重做它的工作。如果死锁很频繁，这可能意味着巨大的浪费。 谓词锁在前面关于锁的描述中，我们掩盖了一个微妙而重要的细节。在 “导致写入偏差的幻读” 中，我们讨论了 幻读（phantoms） 的问题。即一个事务改变另一个事务的搜索查询的结果。具有可串行化隔离级别的数据库必须防止 幻读。 在会议室预订的例子中，这意味着如果一个事务在某个时间窗口内搜索了一个房间的现有预订（见 例 7-2），则另一个事务不能同时插入或更新同一时间窗口与同一房间的另一个预订 （可以同时插入其他房间的预订，或在不影响另一个预定的条件下预定同一房间的其他时间段）。 如何实现这一点？从概念上讲，我们需要一个 谓词锁（predicate lock）【3】。它类似于前面描述的共享 &#x2F; 排它锁，但不属于特定的对象（例如，表中的一行），它属于所有符合某些搜索条件的对象，如： 1234SELECT * FROM bookingsWHERE room_id = 123 AND end_time &gt; &#x27;2018-01-01 12:00&#x27; AND start_time &lt; &#x27;2018-01-01 13:00&#x27;; 谓词锁限制访问，如下所示： 如果事务 A 想要读取匹配某些条件的对象，就像在这个 SELECT 查询中那样，它必须获取查询条件上的 共享谓词锁（shared-mode predicate lock）。如果另一个事务 B 持有任何满足这一查询条件对象的排它锁，那么 A 必须等到 B 释放它的锁之后才允许进行查询。 如果事务 A 想要插入，更新或删除任何对象，则必须首先检查旧值或新值是否与任何现有的谓词锁匹配。如果事务 B 持有匹配的谓词锁，那么 A 必须等到 B 已经提交或中止后才能继续。 这里的关键思想是，谓词锁甚至适用于数据库中尚不存在，但将来可能会添加的对象（幻象）。如果两阶段锁定包含谓词锁，则数据库将阻止所有形式的写入偏差和其他竞争条件，因此其隔离实现了可串行化。 索引范围锁不幸的是谓词锁性能不佳：如果活跃事务持有很多锁，检查匹配的锁会非常耗时。 因此，大多数使用 2PL 的数据库实际上实现了索引范围锁（index-range locking，也称为 next-key locking），这是一个简化的近似版谓词锁【41,50】。 通过使谓词匹配到一个更大的集合来简化谓词锁是安全的。例如，如果你有在中午和下午 1 点之间预订 123 号房间的谓词锁，则锁定 123 号房间的所有时间段，或者锁定 12:00~13:00 时间段的所有房间（不只是 123 号房间）是一个安全的近似，因为任何满足原始谓词的写入也一定会满足这种更松散的近似。 在房间预订数据库中，你可能会在 room_id 列上有一个索引，并且 &#x2F; 或者在 start_time 和 end_time 上有索引（否则前面的查询在大型数据库上的速度会非常慢）： 假设你的索引位于 room_id 上，并且数据库使用此索引查找 123 号房间的现有预订。现在数据库可以简单地将共享锁附加到这个索引项上，指示事务已搜索 123 号房间用于预订。 或者，如果数据库使用基于时间的索引来查找现有预订，那么它可以将共享锁附加到该索引中的一系列值，指示事务已经将 12:00~13:00 时间段标记为用于预定。 无论哪种方式，搜索条件的近似值都附加到其中一个索引上。现在，如果另一个事务想要插入、更新或删除同一个房间和 &#x2F; 或重叠时间段的预订，则它将不得不更新索引的相同部分。在这样做的过程中，它会遇到共享锁，它将被迫等到锁被释放。 这种方法能够有效防止幻读和写入偏差。索引范围锁并不像谓词锁那样精确（它们可能会锁定更大范围的对象，而不是维持可串行化所必需的范围），但是由于它们的开销较低，所以是一个很好的折衷。 如果没有可以挂载范围锁的索引，数据库可以退化到使用整个表上的共享锁。这对性能不利，因为它会阻止所有其他事务写入表格，但这是一个安全的回退位置。 可串行化快照隔离本章描绘了数据库中并发控制的黯淡画面。一方面，我们实现了性能不好（2PL）或者伸缩性不好（串行执行）的可串行化隔离级别。另一方面，我们有性能良好的弱隔离级别，但容易出现各种竞争条件（丢失更新、写入偏差、幻读等）。串行化的隔离级别和高性能是从根本上相互矛盾的吗？ 也许不是：一个称为 可串行化快照隔离（SSI, serializable snapshot isolation） 的算法是非常有前途的。它提供了完整的可串行化隔离级别，但与快照隔离相比只有很小的性能损失。 SSI 是相当新的：它在 2008 年首次被描述【40】，并且是 Michael Cahill 的博士论文【51】的主题。 今天，SSI 既用于单节点数据库（PostgreSQL9.1 以后的可串行化隔离级别），也用于分布式数据库（FoundationDB 使用类似的算法）。由于 SSI 与其他并发控制机制相比还很年轻，还处于在实践中证明自己表现的阶段。但它有可能因为足够快而在未来成为新的默认选项。 悲观与乐观的并发控制两阶段锁是一种所谓的 悲观并发控制机制（pessimistic） ：它是基于这样的原则：如果有事情可能出错（如另一个事务所持有的锁所表示的），最好等到情况安全后再做任何事情。这就像互斥，用于保护多线程编程中的数据结构。 从某种意义上说，串行执行可以称为悲观到了极致：在事务持续期间，每个事务对整个数据库（或数据库的一个分区）具有排它锁，作为对悲观的补偿，我们让每笔事务执行得非常快，所以只需要短时间持有 “锁”。 相比之下，串行化快照隔离 是一种 乐观（optimistic） 的并发控制技术。在这种情况下，乐观意味着，如果存在潜在的危险也不阻止事务，而是继续执行事务，希望一切都会好起来。当一个事务想要提交时，数据库检查是否有什么不好的事情发生（即隔离是否被违反）；如果是的话，事务将被中止，并且必须重试。只有可串行化的事务才被允许提交。 乐观并发控制是一个古老的想法【52】，其优点和缺点已经争论了很长时间【53】。如果存在很多 争用（contention，即很多事务试图访问相同的对象），则表现不佳，因为这会导致很大一部分事务需要中止。如果系统已经接近最大吞吐量，来自重试事务的额外负载可能会使性能变差。 但是，如果有足够的空闲容量，并且事务之间的争用不是太高，乐观的并发控制技术往往比悲观的性能要好。可交换的原子操作可以减少争用：例如，如果多个事务同时要增加一个计数器，那么应用增量的顺序（只要计数器不在同一个事务中读取）就无关紧要了，所以并发增量可以全部应用且不会有冲突。 顾名思义，SSI 基于快照隔离 —— 也就是说，事务中的所有读取都是来自数据库的一致性快照（请参阅 “快照隔离和可重复读取”）。与早期的乐观并发控制技术相比这是主要的区别。在快照隔离的基础上，SSI 添加了一种算法来检测写入之间的串行化冲突，并确定要中止哪些事务。 基于过时前提的决策先前讨论了快照隔离中的写入偏差（请参阅 “写入偏差与幻读”）时，我们观察到一个循环模式：事务从数据库读取一些数据，检查查询的结果，并根据它看到的结果决定采取一些操作（写入数据库）。但是，在快照隔离的情况下，原始查询的结果在事务提交时可能不再是最新的，因为数据可能在同一时间被修改。 换句话说，事务基于一个 前提（premise） 采取行动（事务开始时候的事实，例如：“目前有两名医生正在值班”）。之后当事务要提交时，原始数据可能已经改变 —— 前提可能不再成立。 当应用程序进行查询时（例如，“当前有多少医生正在值班？”），数据库不知道应用逻辑如何使用该查询结果。在这种情况下为了安全，数据库需要假设任何对该结果集的变更都可能会使该事务中的写入变得无效。 换而言之，事务中的查询与写入可能存在因果依赖。为了提供可串行化的隔离级别，如果事务在过时的前提下执行操作，数据库必须能检测到这种情况，并中止事务。 数据库如何知道查询结果是否可能已经改变？有两种情况需要考虑： 检测对旧 MVCC 对象版本的读取（读之前存在未提交的写入） 检测影响先前读取的写入（读之后发生写入） 检测旧MVCC读取回想一下，快照隔离通常是通过多版本并发控制（MVCC；见 图 7-10 ）来实现的。当一个事务从 MVCC 数据库中的一致快照读时，它将忽略取快照时尚未提交的任何其他事务所做的写入。在 图 7-10 中，事务 43 认为 Alice 的 on_call = true ，因为事务 42（修改 Alice 的待命状态）未被提交。然而，在事务 43 想要提交时，事务 42 已经提交。这意味着在读一致性快照时被忽略的写入已经生效，事务 43 的前提不再为真。 图 7-10 检测事务何时从 MVCC 快照读取过时的值 为了防止这种异常，数据库需要跟踪一个事务由于 MVCC 可见性规则而忽略另一个事务的写入。当事务想要提交时，数据库检查是否有任何被忽略的写入现在已经被提交。如果是这样，事务必须中止。 为什么要等到提交？当检测到陈旧的读取时，为什么不立即中止事务 43 ？因为如果事务 43 是只读事务，则不需要中止，因为没有写入偏差的风险。当事务 43 进行读取时，数据库还不知道事务是否要稍后执行写操作。此外，事务 42 可能在事务 43 被提交的时候中止或者可能仍然未被提交，因此读取可能终究不是陈旧的。通过避免不必要的中止，SSI 保留了快照隔离从一致快照中长时间读取的能力。 检测影响之前读取的写入第二种情况要考虑的是另一个事务在读取数据之后修改数据。这种情况如 图 7-11 所示。 图 7-11 在可串行化快照隔离中，检测一个事务何时修改另一个事务的读取。 在两阶段锁定的上下文中，我们讨论了索引范围锁（请参阅 “索引范围锁”），它允许数据库锁定与某个搜索查询匹配的所有行的访问权，例如 WHERE shift_id = 1234。可以在这里使用类似的技术，除了 SSI 锁不会阻塞其他事务。 在 图 7-11 中，事务 42 和 43 都在班次 1234 查找值班医生。如果在 shift_id 上有索引，则数据库可以使用索引项 1234 来记录事务 42 和 43 读取这个数据的事实。（如果没有索引，这个信息可以在表级别进行跟踪）。这个信息只需要保留一段时间：在一个事务完成（提交或中止），并且所有的并发事务完成之后，数据库就可以忘记它读取的数据了。 当事务写入数据库时，它必须在索引中查找最近曾读取受影响数据的其他事务。这个过程类似于在受影响的键范围上获取写锁，但锁并不会阻塞事务直到其他读事务完成，而是像警戒线一样只是简单通知其他事务：你们读过的数据可能不是最新的啦。 在 图 7-11 中，事务 43 通知事务 42 其先前读已过时，反之亦然。事务 42 首先提交并成功，尽管事务 43 的写影响了 42 ，但因为事务 43 尚未提交，所以写入尚未生效。然而当事务 43 想要提交时，来自事务 42 的冲突写入已经被提交，所以事务 43 必须中止。 可串行化快照隔离的性能与往常一样，许多工程细节会影响算法的实际表现。例如一个权衡是跟踪事务的读取和写入的 粒度（granularity）。如果数据库详细地跟踪每个事务的活动（细粒度），那么可以准确地确定哪些事务需要中止，但是簿记开销可能变得很显著。简略的跟踪速度更快（粗粒度），但可能会导致更多不必要的事务中止。 在某些情况下，事务可以读取被另一个事务覆盖的信息：这取决于发生了什么，有时可以证明执行结果无论如何都是可串行化的。 PostgreSQL 使用这个理论来减少不必要的中止次数【11,41】。 与两阶段锁定相比，可串行化快照隔离的最大优点是一个事务不需要阻塞等待另一个事务所持有的锁。就像在快照隔离下一样，写不会阻塞读，反之亦然。这种设计原则使得查询延迟更可预测，波动更少。特别是，只读查询可以运行在一致快照上，而不需要任何锁定，这对于读取繁重的工作负载非常有吸引力。 与串行执行相比，可串行化快照隔离并不局限于单个 CPU 核的吞吐量：FoundationDB 将串行化冲突的检测分布在多台机器上，允许扩展到很高的吞吐量。即使数据可能跨多台机器进行分区，事务也可以在保证可串行化隔离等级的同时读写多个分区中的数据【54】。 中止率显著影响 SSI 的整体表现。例如，长时间读取和写入数据的事务很可能会发生冲突并中止，因此 SSI 要求同时读写的事务尽量短（只读的长事务可能没问题）。对于慢事务，SSI 可能比两阶段锁定或串行执行更不敏感。 本章小结事务是一个抽象层，允许应用程序假装某些并发问题和某些类型的硬件和软件故障不存在。各式各样的错误被简化为一种简单情况：事务中止（transaction abort），而应用需要的仅仅是重试。 在本章中介绍了很多问题，事务有助于防止这些问题发生。并非所有应用都易受此类问题影响：具有非常简单访问模式的应用（例如每次读写单条记录）可能无需事务管理。但是对于更复杂的访问模式，事务可以大大减少需要考虑的潜在错误情景数量。 如果没有事务处理，各种错误情况（进程崩溃、网络中断、停电、磁盘已满、意外并发等）意味着数据可能以各种方式变得不一致。例如，非规范化的数据可能很容易与源数据不同步。如果没有事务处理，就很难推断复杂的交互访问可能对数据库造成的影响。 本章深入讨论了 并发控制 的话题。我们讨论了几个广泛使用的隔离级别，特别是 读已提交、快照隔离（有时称为可重复读）和 可串行化。并通过研究竞争条件的各种例子，来描述这些隔离等级： 脏读 一个客户端读取到另一个客户端尚未提交的写入。读已提交 或更强的隔离级别可以防止脏读。 脏写 一个客户端覆盖写入了另一个客户端尚未提交的写入。几乎所有的事务实现都可以防止脏写。 读取偏差（不可重复读） 在同一个事务中，客户端在不同的时间点会看见数据库的不同状态。快照隔离 经常用于解决这个问题，它允许事务从一个特定时间点的一致性快照中读取数据。快照隔离通常使用 多版本并发控制（MVCC） 来实现。 丢失更新 两个客户端同时执行 读取 - 修改 - 写入序列。其中一个写操作，在没有合并另一个写入变更情况下，直接覆盖了另一个写操作的结果。所以导致数据丢失。快照隔离的一些实现可以自动防止这种异常，而另一些实现则需要手动锁定（SELECT FOR UPDATE）。 写入偏差 一个事务读取一些东西，根据它所看到的值作出决定，并将该决定写入数据库。但是，写入时，该决定的前提不再是真实的。只有可串行化的隔离才能防止这种异常。 幻读 事务读取符合某些搜索条件的对象。另一个客户端进行写入，影响搜索结果。快照隔离可以防止直接的幻像读取，但是写入偏差上下文中的幻读需要特殊处理，例如索引范围锁定。 弱隔离级别可以防止其中一些异常情况，但要求你，也就是应用程序开发人员手动处理剩余那些（例如，使用显式锁定）。只有可串行化的隔离才能防范所有这些问题。我们讨论了实现可串行化事务的三种不同方法： 字面意义上的串行执行 如果每个事务的执行速度非常快，并且事务吞吐量足够低，足以在单个 CPU 核上处理，这是一个简单而有效的选择。 两阶段锁定 数十年来，两阶段锁定一直是实现可串行化的标准方式，但是许多应用出于性能问题的考虑避免使用它。 可串行化快照隔离（SSI） 一个相当新的算法，避免了先前方法的大部分缺点。它使用乐观的方法，允许事务执行而无需阻塞。当一个事务想要提交时，它会进行检查，如果执行不可串行化，事务就会被中止。 本章中的示例主要是在关系数据模型的上下文中。但是，正如在 “多对象事务的需求” 中所讨论的，无论使用哪种数据模型，事务都是有价值的数据库功能。 本章主要是在单机数据库的上下文中，探讨了各种想法和算法。分布式数据库中的事务，则引入了一系列新的困难挑战，我们将在接下来的两章中讨论。 参考文献 Donald D. Chamberlin, Morton M. Astrahan, Michael W. Blasgen, et al.: “A History and Evaluation of System R,” Communications of the ACM, volume 24, number 10, pages 632–646, October 1981. doi:10.1145&#x2F;358769.358784 Jim N. Gray, Raymond A. Lorie, Gianfranco R. Putzolu, and Irving L. Traiger: “Granularity of Locks and Degrees of Consistency in a Shared Data Base,” in Modelling in Data Base Management Systems: Proceedings of the IFIP Working Conference on Modelling in Data Base Management Systems, edited by G. M. Nijssen, pages 364–394, Elsevier&#x2F;North Holland Publishing, 1976. Also in Readings in Database Systems, 4th edition, edited by Joseph M. Hellerstein and Michael Stonebraker, MIT Press, 2005. ISBN: 978-0-262-69314-1 Kapali P. Eswaran, Jim N. Gray, Raymond A. Lorie, and Irving L. Traiger: “The Notions of Consistency and Predicate Locks in a Database System,” Communications of the ACM, volume 19, number 11, pages 624–633, November 1976. “ACID Transactions Are Incredibly Helpful,” FoundationDB, LLC, 2013. John D. Cook: “ACID Versus BASE for Database Transactions,” johndcook.com, July 6, 2009. Gavin Clarke: “NoSQL’s CAP Theorem Busters: We Don’t Drop ACID,” theregister.co.uk, November 22, 2012. Theo Härder and Andreas Reuter: “Principles of Transaction-Oriented Database Recovery,” ACM Computing Surveys, volume 15, number 4, pages 287–317, December 1983. doi:10.1145&#x2F;289.291 Peter Bailis, Alan Fekete, Ali Ghodsi, et al.: “HAT, not CAP: Towards Highly Available Transactions,” at 14th USENIX Workshop on Hot Topics in Operating Systems (HotOS), May 2013. Armando Fox, Steven D. Gribble, Yatin Chawathe, et al.: “Cluster-Based Scalable Network Services,” at 16th ACM Symposium on Operating Systems Principles (SOSP), October 1997. Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: Concurrency Control and Recovery in Database Systems. Addison-Wesley, 1987. ISBN: 978-0-201-10715-9, available online at research.microsoft.com. Alan Fekete, Dimitrios Liarokapis, Elizabeth O’Neil, et al.: “Making Snapshot Isolation Serializable,” ACM Transactions on Database Systems, volume 30, number 2, pages 492–528, June 2005. doi:10.1145&#x2F;1071610.1071615 Mai Zheng, Joseph Tucek, Feng Qin, and Mark Lillibridge: “Understanding the Robustness of SSDs Under Power Fault,” at 11th USENIX Conference on File and Storage Technologies (FAST), February 2013. Laurie Denness: “SSDs: A Gift and a Curse,” laur.ie, June 2, 2015. Adam Surak: “When Solid State Drives Are Not That Solid,” blog.algolia.com, June 15, 2015. Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram, Ramnatthan Alagappan, et al.: “All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications,” at 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2014. Chris Siebenmann: “Unix’s File Durability Problem,” utcc.utoronto.ca, April 14, 2016. Lakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder, et al.: “An Analysis of Data Corruption in the Storage Stack,” at 6th USENIX Conference on File and Storage Technologies (FAST), February 2008. Bianca Schroeder, Raghav Lagisetty, and Arif Merchant: “Flash Reliability in Production: The Expected and the Unexpected,” at 14th USENIX Conference on File and Storage Technologies (FAST), February 2016. Don Allison: “SSD Storage – Ignorance of Technology Is No Excuse,” blog.korelogic.com, March 24, 2015. Dave Scherer: “Those Are Not Transactions (Cassandra 2.0),” blog.foundationdb.com, September 6, 2013. Kyle Kingsbury: “Call Me Maybe: Cassandra,” aphyr.com, September 24, 2013. “ACID Support in Aerospike,” Aerospike, Inc., June 2014. Martin Kleppmann: “Hermitage: Testing the ‘I’ in ACID,” martin.kleppmann.com, November 25, 2014. Tristan D’Agosta: “BTC Stolen from Poloniex,” bitcointalk.org, March 4, 2014. bitcointhief2: “How I Stole Roughly 100 BTC from an Exchange and How I Could Have Stolen More!,” reddit.com, February 2, 2014. Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan: “Automating the Detection of Snapshot Isolation Anomalies,” at 33rd International Conference on Very Large Data Bases (VLDB), September 2007. Michael Melanson: “Transactions: The Limits of Isolation,” michaelmelanson.net, March 20, 2014. Hal Berenson, Philip A. Bernstein, Jim N. Gray, et al.: “A Critique of ANSI SQL Isolation Levels,” at ACM International Conference on Management of Data (SIGMOD), May 1995. Atul Adya: “Weak Consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions,” PhD Thesis, Massachusetts Institute of Technology, March 1999. Peter Bailis, Aaron Davidson, Alan Fekete, et al.: “Highly Available Transactions: Virtues and Limitations (Extended Version),” at 40th International Conference on Very Large Data Bases (VLDB), September 2014. Bruce Momjian: “MVCC Unmasked,” momjian.us, July 2014. Annamalai Gurusami: “Repeatable Read Isolation Level in InnoDB – How Consistent Read View Works,” blogs.oracle.com, January 15, 2013. Nikita Prokopov: “Unofficial Guide to Datomic Internals,” tonsky.me, May 6, 2014. Baron Schwartz: “Immutability, MVCC, and Garbage Collection,” xaprb.com, December 28, 2013. J. Chris Anderson, Jan Lehnardt, and Noah Slater: CouchDB: The Definitive Guide. O’Reilly Media, 2010. ISBN: 978-0-596-15589-6 Rikdeb Mukherjee: “Isolation in DB2 (Repeatable Read, Read Stability, Cursor Stability, Uncommitted Read) with Examples,” mframes.blogspot.co.uk, July 4, 2013. Steve Hilker: “Cursor Stability (CS) – IBM DB2 Community,” toadworld.com, March 14, 2013. Nate Wiger: “An Atomic Rant,” nateware.com, February 18, 2010. Joel Jacobson: “Riak 2.0: Data Types,” blog.joeljacobson.com, March 23, 2014. Michael J. Cahill, Uwe Röhm, and Alan Fekete: “Serializable Isolation for Snapshot Databases,” at ACM International Conference on Management of Data (SIGMOD), June 2008. doi:10.1145&#x2F;1376616.1376690 Dan R. K. Ports and Kevin Grittner: “Serializable Snapshot Isolation in PostgreSQL,” at 38th International Conference on Very Large Databases (VLDB), August 2012. Tony Andrews: “Enforcing Complex Constraints in Oracle,” tonyandrews.blogspot.co.uk, October 15, 2004. Douglas B. Terry, Marvin M. Theimer, Karin Petersen, et al.: “Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System,” at 15th ACM Symposium on Operating Systems Principles (SOSP), December 1995. doi:10.1145&#x2F;224056.224070 Gary Fredericks: “Postgres Serializability Bug,” github.com, September 2015. Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.: “The End of an Architectural Era (It’s Time for a Complete Rewrite),” at 33rd International Conference on Very Large Data Bases (VLDB), September 2007. John Hugg: “H-Store&#x2F;VoltDB Architecture vs. CEP Systems and Newer Streaming Architectures,” at Data @Scale Boston, November 2014. Robert Kallman, Hideaki Kimura, Jonathan Natkins, et al.: “H-Store: A High-Performance, Distributed Main Memory Transaction Processing System,” Proceedings of the VLDB Endowment, volume 1, number 2, pages 1496–1499, August 2008. Rich Hickey: “The Architecture of Datomic,” infoq.com, November 2, 2012. John Hugg: “Debunking Myths About the VoltDB In-Memory Database,” voltdb.com, May 12, 2014. Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton: “Architecture of a Database System,” Foundations and Trends in Databases, volume 1, number 2, pages 141–259, November 2007. doi:10.1561&#x2F;1900000002 Michael J. Cahill: “Serializable Isolation for Snapshot Databases,” PhD Thesis, University of Sydney, July 2009. D. Z. Badal: “Correctness of Concurrency Control and Implications in Distributed Databases,” at 3rd International IEEE Computer Software and Applications Conference (COMPSAC), November 1979. Rakesh Agrawal, Michael J. Carey, and Miron Livny: “Concurrency Control Performance Modeling: Alternatives and Implications,” ACM Transactions on Database Systems (TODS), volume 12, number 4, pages 609–654, December 1987. doi:10.1145&#x2F;32204.32220 Dave Rosenthal: “Databases at 14.4MHz,” blog.foundationdb.com, December 10, 2014."},{"title":"第八章：分布式系统的麻烦","path":"/wiki/ddia/ch8.html","content":"邂逅相遇 网络延迟 存之为吾 无食我数 —— Kyle Kingsbury, Carly Rae Jepsen 《网络分区的危害》（2013 年）[^译著1] 最近几章中反复出现的主题是，系统如何处理错误的事情。例如，我们讨论了 副本故障切换（“处理节点中断”），复制延迟（“复制延迟问题”）和事务控制（“弱隔离级别”）。当我们了解可能在实际系统中出现的各种边缘情况时，我们会更好地处理它们。 但是，尽管我们已经谈了很多错误，但之前几章仍然过于乐观。现实更加黑暗。我们现在将悲观主义最大化，假设任何可能出错的东西 都会 出错 [^i]。（经验丰富的系统运维会告诉你，这是一个合理的假设。如果你问得好，他们可能会一边治疗心理创伤一边告诉你一些可怕的故事） [^i]: 除了一个例外：我们将假定故障是非拜占庭式的（请参阅 “拜占庭故障”）。 使用分布式系统与在一台计算机上编写软件有着根本的区别，主要的区别在于，有许多新颖和刺激的方法可以使事情出错【1,2】。在这一章中，我们将了解实践中出现的问题，理解我们能够依赖，和不可以依赖的东西。 最后，作为工程师，我们的任务是构建能够完成工作的系统（即满足用户期望的保证），尽管一切都出错了。 在 第九章 中，我们将看看一些可以在分布式系统中提供这种保证的算法的例子。 但首先，在本章中，我们必须了解我们面临的挑战。 本章对分布式系统中可能出现的问题进行彻底的悲观和沮丧的总结。 我们将研究网络的问题（“不可靠的网络”）; 时钟和时序问题（“不可靠的时钟”）; 我们将讨论他们可以避免的程度。 所有这些问题的后果都是困惑的，所以我们将探索如何思考一个分布式系统的状态，以及如何推理发生的事情（“知识、真相与谎言”）。 故障与部分失效当你在一台计算机上编写一个程序时，它通常会以一种相当可预测的方式运行：无论是工作还是不工作。充满错误的软件可能会让人觉得电脑有时候也会有 “糟糕的一天”（这种问题通常是重新启动就恢复了），但这主要是软件写得不好的结果。 单个计算机上的软件没有根本性的不可靠原因：当硬件正常工作时，相同的操作总是产生相同的结果（这是确定性的）。如果存在硬件问题（例如，内存损坏或连接器松动），其后果通常是整个系统故障（例如，内核恐慌，“蓝屏死机”，启动失败）。装有良好软件的个人计算机通常要么功能完好，要么完全失效，而不是介于两者之间。 这是计算机设计中的一个有意的选择：如果发生内部错误，我们宁愿电脑完全崩溃，而不是返回错误的结果，因为错误的结果很难处理。因为计算机隐藏了模糊不清的物理实现，并呈现出一个理想化的系统模型，并以数学一样的完美的方式运作。 CPU 指令总是做同样的事情；如果你将一些数据写入内存或磁盘，那么这些数据将保持不变，并且不会被随机破坏。从第一台数字计算机开始，始终正确地计算 这个设计目标贯穿始终【3】。 当你编写运行在多台计算机上的软件时，情况有本质上的区别。在分布式系统中，我们不再处于理想化的系统模型中，我们别无选择，只能面对现实世界的混乱现实。而在现实世界中，各种各样的事情都可能会出现问题【4】，如下面的轶事所述： 在我有限的经验中，我已经和很多东西打过交道：单个 数据中心（DC） 中长期存在的网络分区，配电单元 PDU 故障，交换机故障，整个机架的意外重启，整个数据中心主干网络故障，整个数据中心的电源故障，以及一个低血糖的司机把他的福特皮卡撞在数据中心的 HVAC（加热，通风和空调）系统上。而且我甚至不是一个运维。 —— 柯达黑尔 在分布式系统中，尽管系统的其他部分工作正常，但系统的某些部分可能会以某种不可预知的方式被破坏。这被称为 部分失效（partial failure）。难点在于部分失效是 不确定性的（nondeterministic）：如果你试图做任何涉及多个节点和网络的事情，它有时可能会工作，有时会出现不可预知的失败。正如我们将要看到的，你甚至不知道是否成功了，因为消息通过网络传播的时间也是不确定的！ 这种不确定性和部分失效的可能性，使得分布式系统难以工作【5】。 云计算与超级计算机关于如何构建大型计算系统有一系列的哲学： 一个极端是高性能计算（HPC）领域。具有数千个 CPU 的超级计算机通常用于计算密集型科学计算任务，如天气预报或分子动力学（模拟原子和分子的运动）。 另一个极端是 云计算（cloud computing），云计算并不是一个良好定义的概念【6】，但通常与多租户数据中心，连接 IP 网络（通常是以太网）的商用计算机，弹性 &#x2F; 按需资源分配以及计量计费等相关联。 传统企业数据中心位于这两个极端之间。 不同的哲学会导致不同的故障处理方式。在超级计算机中，作业通常会不时地将计算的状态存盘到持久存储中。如果一个节点出现故障，通常的解决方案是简单地停止整个集群的工作负载。故障节点修复后，计算从上一个检查点重新开始【7,8】。因此，超级计算机更像是一个单节点计算机而不是分布式系统：通过让部分失败升级为完全失败来处理部分失败 —— 如果系统的任何部分发生故障，只是让所有的东西都崩溃（就像单台机器上的内核恐慌一样）。 在本书中，我们将重点放在实现互联网服务的系统上，这些系统通常与超级计算机看起来有很大不同： 许多与互联网有关的应用程序都是 在线（online） 的，因为它们需要能够随时以低延迟服务用户。使服务不可用（例如，停止集群以进行修复）是不可接受的。相比之下，像天气模拟这样的离线（批处理）工作可以停止并重新启动，影响相当小。 超级计算机通常由专用硬件构建而成，每个节点相当可靠，节点通过共享内存和 远程直接内存访问（RDMA） 进行通信。另一方面，云服务中的节点是由商用机器构建而成的，由于规模经济，可以以较低的成本提供相同的性能，而且具有较高的故障率。 大型数据中心网络通常基于 IP 和以太网，以 CLOS 拓扑排列，以提供更高的对分（bisection）带宽【9】。超级计算机通常使用专门的网络拓扑结构，例如多维网格和 Torus 网络 【10】，这为具有已知通信模式的 HPC 工作负载提供了更好的性能。 系统越大，其组件之一就越有可能坏掉。随着时间的推移，坏掉的东西得到修复，新的东西又坏掉，但是在一个有成千上万个节点的系统中，有理由认为总是有一些东西是坏掉的【7】。当错误处理的策略只由简单放弃组成时，一个大的系统最终会花费大量时间从错误中恢复，而不是做有用的工作【8】。 如果系统可以容忍发生故障的节点，并继续保持整体工作状态，那么这对于运营和维护非常有用：例如，可以执行滚动升级（请参阅 第四章），一次重新启动一个节点，同时继续给用户提供不中断的服务。在云环境中，如果一台虚拟机运行不佳，可以杀死它并请求一台新的虚拟机（希望新的虚拟机速度更快）。 在地理位置分散的部署中（保持数据在地理位置上接近用户以减少访问延迟），通信很可能通过互联网进行，与本地网络相比，通信速度缓慢且不可靠。超级计算机通常假设它们的所有节点都靠近在一起。 如果要使分布式系统工作，就必须接受部分故障的可能性，并在软件中建立容错机制。换句话说，我们需要从不可靠的组件构建一个可靠的系统（正如 “可靠性” 中所讨论的那样，没有完美的可靠性，所以我们需要理解我们可以实际承诺的极限）。 即使在只有少数节点的小型系统中，考虑部分故障也是很重要的。在一个小系统中，很可能大部分组件在大部分时间都正常工作。然而，迟早会有一部分系统出现故障，软件必须以某种方式处理。故障处理必须是软件设计的一部分，并且作为软件的运维，你需要知道在发生故障的情况下，软件可能会表现出怎样的行为。 简单地假设缺陷很罕见并希望始终保持最好的状况是不明智的。考虑一系列可能的错误（甚至是不太可能的错误），并在测试环境中人为地创建这些情况来查看会发生什么是非常重要的。在分布式系统中，怀疑，悲观和偏执狂是值得的。 从不可靠的组件构建可靠的系统 你可能想知道这是否有意义 —— 直观地看来，系统只能像其最不可靠的组件（最薄弱的环节）一样可靠。事实并非如此：事实上，从不太可靠的潜在基础构建更可靠的系统是计算机领域的一个古老思想【11】。例如： 纠错码允许数字数据在通信信道上准确传输，偶尔会出现一些错误，例如由于无线网络上的无线电干扰【12】。 互联网协议（Internet Protocol, IP） 不可靠：可能丢弃、延迟、重复或重排数据包。 传输控制协议（Transmission Control Protocol, TCP）在互联网协议（IP）之上提供了更可靠的传输层：它确保丢失的数据包被重新传输，消除重复，并且数据包被重新组装成它们被发送的顺序。 虽然这个系统可以比它的底层部分更可靠，但它的可靠性总是有限的。例如，纠错码可以处理少量的单比特错误，但是如果你的信号被干扰所淹没，那么通过信道可以得到多少数据，是有根本性的限制的【13】。 TCP 可以隐藏数据包的丢失，重复和重新排序，但是它不能神奇地消除网络中的延迟。 虽然更可靠的高级系统并不完美，但它仍然有用，因为它处理了一些棘手的低级错误，所以其余的错误通常更容易推理和处理。我们将在 “数据库的端到端原则” 中进一步探讨这个问题。 不可靠的网络正如在 第二部分 的介绍中所讨论的那样，我们在本书中关注的分布式系统是无共享的系统，即通过网络连接的一堆机器。网络是这些机器可以通信的唯一途径 —— 我们假设每台机器都有自己的内存和磁盘，一台机器不能访问另一台机器的内存或磁盘（除了通过网络向服务器发出请求）。 无共享 并不是构建系统的唯一方式，但它已经成为构建互联网服务的主要方式，其原因如下：相对便宜，因为它不需要特殊的硬件，可以利用商品化的云计算服务，通过跨多个地理分布的数据中心进行冗余可以实现高可靠性。 互联网和数据中心（通常是以太网）中的大多数内部网络都是 异步分组网络（asynchronous packet networks）。在这种网络中，一个节点可以向另一个节点发送一个消息（一个数据包），但是网络不能保证它什么时候到达，或者是否到达。如果你发送请求并期待响应，则很多事情可能会出错（其中一些如 图 8-1 所示）： 请求可能已经丢失（可能有人拔掉了网线）。 请求可能正在排队，稍后将交付（也许网络或接收方过载）。 远程节点可能已经失效（可能是崩溃或关机）。 远程节点可能暂时停止了响应（可能会遇到长时间的垃圾回收暂停；请参阅 “进程暂停”），但稍后会再次响应。 远程节点可能已经处理了请求，但是网络上的响应已经丢失（可能是网络交换机配置错误）。 远程节点可能已经处理了请求，但是响应已经被延迟，并且稍后将被传递（可能是网络或者你自己的机器过载）。 图 8-1 如果发送请求并没有得到响应，则无法区分（a）请求是否丢失，（b）远程节点是否关闭，或（c）响应是否丢失。 发送者甚至不能分辨数据包是否被发送：唯一的选择是让接收者发送响应消息，这可能会丢失或延迟。这些问题在异步网络中难以区分：你所拥有的唯一信息是，你尚未收到响应。如果你向另一个节点发送请求并且没有收到响应，则不可能判断是什么原因。 处理这个问题的通常方法是 超时（Timeout）：在一段时间之后放弃等待，并且认为响应不会到达。但是，当发生超时时，你仍然不知道远程节点是否收到了请求（如果请求仍然在某个地方排队，那么即使发送者已经放弃了该请求，仍然可能会将其发送给接收者）。 真实世界的网络故障我们几十年来一直在建设计算机网络 —— 有人可能希望现在我们已经找出了使网络变得可靠的方法。但是现在似乎还没有成功。 有一些系统的研究和大量的轶事证据表明，即使在像一家公司运营的数据中心那样的受控环境中，网络问题也可能出乎意料地普遍。在一家中型数据中心进行的一项研究发现，每个月大约有 12 个网络故障，其中一半断开一台机器，一半断开整个机架【15】。另一项研究测量了架顶式交换机，汇聚交换机和负载平衡器等组件的故障率【16】。它发现添加冗余网络设备不会像你所希望的那样减少故障，因为它不能防范人为错误（例如，错误配置的交换机），这是造成中断的主要原因。 诸如 EC2 之类的公有云服务因频繁的暂态网络故障而臭名昭著【14】，管理良好的私有数据中心网络可能是更稳定的环境。尽管如此，没有人不受网络问题的困扰：例如，交换机软件升级过程中的一个问题可能会引发网络拓扑重构，在此期间网络数据包可能会延迟超过一分钟【17】。鲨鱼可能咬住海底电缆并损坏它们 【18】。其他令人惊讶的故障包括网络接口有时会丢弃所有入站数据包，但是成功发送出站数据包 【19】：仅仅因为网络链接在一个方向上工作，并不能保证它也在相反的方向工作。 网络分区 当网络的一部分由于网络故障而被切断时，有时称为 网络分区（network partition） 或 网络断裂（netsplit）。在本书中，我们通常会坚持使用更一般的术语 网络故障（network fault），以避免与 第六章 讨论的存储系统的分区（分片）相混淆。 即使网络故障在你的环境中非常罕见，故障可能发生的事实，意味着你的软件需要能够处理它们。无论何时通过网络进行通信，都可能会失败，这是无法避免的。 如果网络故障的错误处理没有定义与测试，武断地讲，各种错误可能都会发生：例如，即使网络恢复【20】，集群可能会发生 死锁，永久无法为请求提供服务，甚至可能会删除所有的数据【21】。如果软件被置于意料之外的情况下，它可能会做出出乎意料的事情。 处理网络故障并不意味着容忍它们：如果你的网络通常是相当可靠的，一个有效的方法可能是当你的网络遇到问题时，简单地向用户显示一条错误信息。但是，你确实需要知道你的软件如何应对网络问题，并确保系统能够从中恢复。有意识地触发网络问题并测试系统响应（这是 Chaos Monkey 背后的想法；请参阅 “可靠性”）。 检测故障许多系统需要自动检测故障节点。例如： 负载平衡器需要停止向已死亡的节点转发请求（从轮询列表移出，即 out of rotation）。 在单主复制功能的分布式数据库中，如果主库失效，则需要将从库之一升级为新主库（请参阅 “处理节点宕机”）。 不幸的是，网络的不确定性使得很难判断一个节点是否工作。在某些特定的情况下，你可能会收到一些反馈信息，明确告诉你某些事情没有成功： 如果你可以连接到运行节点的机器，但没有进程正在侦听目标端口（例如，因为进程崩溃），操作系统将通过发送 FIN 或 RST 来关闭并重用 TCP 连接。但是，如果节点在处理请求时发生崩溃，则无法知道远程节点实际处理了多少数据【22】。 如果节点进程崩溃（或被管理员杀死），但节点的操作系统仍在运行，则脚本可以通知其他节点有关该崩溃的信息，以便另一个节点可以快速接管，而无需等待超时到期。例如，HBase 就是这么做的【23】。 如果你有权访问数据中心网络交换机的管理界面，则可以通过它们检测硬件级别的链路故障（例如，远程机器是否关闭电源）。如果你通过互联网连接，或者如果你处于共享数据中心而无法访问交换机，或者由于网络问题而无法访问管理界面，则排除此选项。 如果路由器确认你尝试连接的 IP 地址不可用，则可能会使用 ICMP 目标不可达数据包回复你。但是，路由器不具备神奇的故障检测能力 —— 它受到与网络其他参与者相同的限制。 关于远程节点关闭的快速反馈很有用，但是你不能指望它。即使 TCP 确认已经传送了一个数据包，应用程序在处理之前可能已经崩溃。如果你想确保一个请求是成功的，你需要应用程序本身的正确响应【24】。 相反，如果出了什么问题，你可能会在堆栈的某个层次上得到一个错误响应，但总的来说，你必须假设你可能根本就得不到任何回应。你可以重试几次（TCP 重试是透明的，但是你也可以在应用程序级别重试），等待超时过期，并且如果在超时时间内没有收到响应，则最终声明节点已经死亡。 超时与无穷的延迟如果超时是检测故障的唯一可靠方法，那么超时应该等待多久？不幸的是没有简单的答案。 长时间的超时意味着长时间等待，直到一个节点被宣告死亡（在这段时间内，用户可能不得不等待，或者看到错误信息）。短的超时可以更快地检测到故障，但有更高地风险误将一个节点宣布为失效，而该节点实际上只是暂时地变慢了（例如由于节点或网络上的负载峰值）。 过早地声明一个节点已经死了是有问题的：如果这个节点实际上是活着的，并且正在执行一些动作（例如，发送一封电子邮件），而另一个节点接管，那么这个动作可能会最终执行两次。我们将在 “知识、真相与谎言” 以及 第九章 和 第十一章 中更详细地讨论这个问题。 当一个节点被宣告死亡时，它的职责需要转移到其他节点，这会给其他节点和网络带来额外的负担。如果系统已经处于高负荷状态，则过早宣告节点死亡会使问题更严重。特别是如果节点实际上没有死亡，只是由于过载导致其响应缓慢；这时将其负载转移到其他节点可能会导致 级联失效（即 cascading failure，表示在极端情况下，所有节点都宣告对方死亡，所有节点都将停止工作）。 设想一个虚构的系统，其网络可以保证数据包的最大延迟 —— 每个数据包要么在一段时间内传送，要么丢失，但是传递永远不会比 $d$ 更长。此外，假设你可以保证一个非故障节点总是在一段时间 $r$ 内处理一个请求。在这种情况下，你可以保证每个成功的请求在 $2d + r$ 时间内都能收到响应，如果你在此时间内没有收到响应，则知道网络或远程节点不工作。如果这是成立的，$2d + r$ 会是一个合理的超时设置。 不幸的是，我们所使用的大多数系统都没有这些保证：异步网络具有无限的延迟（即尽可能快地传送数据包，但数据包到达可能需要的时间没有上限），并且大多数服务器实现并不能保证它们可以在一定的最大时间内处理请求（请参阅 “响应时间保证”）。对于故障检测，即使系统大部分时间快速运行也是不够的：如果你的超时时间很短，往返时间只需要一个瞬时尖峰就可以使系统失衡。 网络拥塞和排队在驾驶汽车时，由于交通拥堵，道路交通网络的通行时间往往不尽相同。同样，计算机网络上数据包延迟的可变性通常是由于排队【25】： 如果多个不同的节点同时尝试将数据包发送到同一目的地，则网络交换机必须将它们排队并将它们逐个送入目标网络链路（如 图 8-2 所示）。在繁忙的网络链路上，数据包可能需要等待一段时间才能获得一个插槽（这称为网络拥塞）。如果传入的数据太多，交换机队列填满，数据包将被丢弃，因此需要重新发送数据包 - 即使网络运行良好。 当数据包到达目标机器时，如果所有 CPU 内核当前都处于繁忙状态，则来自网络的传入请求将被操作系统排队，直到应用程序准备好处理它为止。根据机器上的负载，这可能需要一段任意的时间。 在虚拟化环境中，正在运行的操作系统经常暂停几十毫秒，因为另一个虚拟机正在使用 CPU 内核。在这段时间内，虚拟机不能从网络中消耗任何数据，所以传入的数据被虚拟机监视器 【26】排队（缓冲），进一步增加了网络延迟的可变性。 TCP 执行 流量控制（flow control，也称为 拥塞避免，即 congestion avoidance，或 背压，即 backpressure），其中节点会限制自己的发送速率以避免网络链路或接收节点过载【27】。这意味着甚至在数据进入网络之前，在发送者处就需要进行额外的排队。 图 8-2 如果有多台机器将网络流量发送到同一目的地，则其交换机队列可能会被填满。在这里，端口 1,2 和 4 都试图发送数据包到端口 3 而且，如果 TCP 在某个超时时间内没有被确认（这是根据观察的往返时间计算的），则认为数据包丢失，丢失的数据包将自动重新发送。尽管应用程序没有看到数据包丢失和重新传输，但它看到了延迟（等待超时到期，然后等待重新传输的数据包得到确认）。 TCP与UDP 一些对延迟敏感的应用程序，比如视频会议和 IP 语音（VoIP），使用了 UDP 而不是 TCP。这是在可靠性和和延迟变化之间的折衷：由于 UDP 不执行流量控制并且不重传丢失的分组，所以避免了网络延迟变化的一些原因（尽管它仍然易受切换队列和调度延迟的影响）。 在延迟数据毫无价值的情况下，UDP 是一个不错的选择。例如，在 VoIP 电话呼叫中，可能没有足够的时间重新发送丢失的数据包，并在扬声器上播放数据。在这种情况下，重发数据包没有意义 —— 应用程序必须使用静音填充丢失数据包的时隙（导致声音短暂中断），然后在数据流中继续。重试发生在人类层（“你能再说一遍吗？声音刚刚断了一会儿。”）。 所有这些因素都会造成网络延迟的变化。当系统接近其最大容量时，排队延迟的变化范围特别大：拥有足够备用容量的系统可以轻松排空队列，而在高利用率的系统中，很快就能积累很长的队列。 在公共云和多租户数据中心中，资源被许多客户共享：网络链接和交换机，甚至每个机器的网卡和 CPU（在虚拟机上运行时）。批处理工作负载（如 MapReduce，请参阅 第十章）能够很容易使网络链接饱和。由于无法控制或了解其他客户对共享资源的使用情况，如果附近的某个人（嘈杂的邻居）正在使用大量资源，则网络延迟可能会发生剧烈变化【28,29】。 在这种环境下，你只能通过实验方式选择超时：在一段较长的时期内、在多台机器上测量网络往返时间的分布，以确定延迟的预期变化。然后，考虑到应用程序的特性，可以确定 故障检测延迟 与 过早超时风险 之间的适当折衷。 更好的一种做法是，系统不是使用配置的常量超时时间，而是连续测量响应时间及其变化（抖动），并根据观察到的响应时间分布自动调整超时时间。这可以通过 Phi Accrual 故障检测器【30】来完成，该检测器在例如 Akka 和 Cassandra 【31】中使用。 TCP 的超时重传机制也是以类似的方式工作【27】。 同步网络与异步网络如果我们可以依靠网络来传递一些 最大延迟固定 的数据包，而不是丢弃数据包，那么分布式系统就会简单得多。为什么我们不能在硬件层面上解决这个问题，使网络可靠，使软件不必担心呢？ 为了回答这个问题，将数据中心网络与非常可靠的传统固定电话网络（非蜂窝，非 VoIP）进行比较是很有趣的：延迟音频帧和掉话是非常罕见的。一个电话需要一个很低的端到端延迟，以及足够的带宽来传输你声音的音频采样数据。在计算机网络中有类似的可靠性和可预测性不是很好吗？ 当你通过电话网络拨打电话时，它会建立一个电路：在两个呼叫者之间的整个路线上为呼叫分配一个固定的，有保证的带宽量。这个电路会保持至通话结束【32】。例如，ISDN 网络以每秒 4000 帧的固定速率运行。呼叫建立时，每个帧内（每个方向）分配 16 位空间。因此，在通话期间，每一方都保证能够每 250 微秒发送一个精确的 16 位音频数据【33,34】。 这种网络是同步的：即使数据经过多个路由器，也不会受到排队的影响，因为呼叫的 16 位空间已经在网络的下一跳中保留了下来。而且由于没有排队，网络的最大端到端延迟是固定的。我们称之为 有限延迟（bounded delay）。 我们不能简单地使网络延迟可预测吗？请注意，电话网络中的电路与 TCP 连接有很大不同：电路是固定数量的预留带宽，在电路建立时没有其他人可以使用，而 TCP 连接的数据包 机会性地 使用任何可用的网络带宽。你可以给 TCP 一个可变大小的数据块（例如，一个电子邮件或一个网页），它会尽可能在最短的时间内传输它。 TCP 连接空闲时，不使用任何带宽 [^ii]。 [^ii]: 除了偶尔的 keepalive 数据包，如果 TCP keepalive 被启用。 如果数据中心网络和互联网是电路交换网络，那么在建立电路时就可以建立一个受保证的最大往返时间。但是，它们并不能这样：以太网和 IP 是 分组交换协议，不得不忍受排队的折磨和因此导致的网络无限延迟，这些协议没有电路的概念。 为什么数据中心网络和互联网使用分组交换？答案是，它们针对 突发流量（bursty traffic） 进行了优化。一个电路适用于音频或视频通话，在通话期间需要每秒传送相当数量的比特。另一方面，请求网页，发送电子邮件或传输文件没有任何特定的带宽要求 —— 我们只是希望它尽快完成。 如果想通过电路传输文件，你得预测一个带宽分配。如果你猜的太低，传输速度会不必要的太慢，导致网络容量闲置。如果你猜的太高，电路就无法建立（因为如果无法保证其带宽分配，网络不能建立电路）。因此，将电路用于突发数据传输会浪费网络容量，并且使传输不必要地缓慢。相比之下，TCP 动态调整数据传输速率以适应可用的网络容量。 已经有一些尝试去建立同时支持电路交换和分组交换的混合网络，比如 ATM [^iii]。InfiniBand 有一些相似之处【35】：它在链路层实现了端到端的流量控制，从而减少了在网络中排队的需要，尽管它仍然可能因链路拥塞而受到延迟【36】。通过仔细使用 服务质量（quality of service，即 QoS，数据包的优先级和调度）和 准入控制（admission control，限速发送器），可以在分组网络上模拟电路交换，或提供统计上的 有限延迟【25,32】。 [^iii]: 异步传输模式（Asynchronous Transfer Mode, ATM） 在 20 世纪 80 年代是以太网的竞争对手【32】，但在电话网核心交换机之外并没有得到太多的采用。它与自动柜员机（也称为自动取款机）无关，尽管共用一个缩写词。或许，在一些平行的世界里，互联网是基于像 ATM 这样的东西，因此它们的互联网视频通话可能比我们的更可靠，因为它们不会遭受包的丢失和延迟。 但是，目前在多租户数据中心和公共云或通过互联网 [^iv] 进行通信时，此类服务质量尚未启用。当前部署的技术不允许我们对网络的延迟或可靠性作出任何保证：我们必须假设网络拥塞，排队和无限的延迟总是会发生。因此，超时时间没有 “正确” 的值 —— 它需要通过实验来确定。 [^iv]: 互联网服务提供商之间的对等协议和通过 BGP 网关协议（BGP） 建立的路由，与 IP 协议相比，更接近于电路交换。在这个级别上，可以购买专用带宽。但是，互联网路由在网络级别运行，而不是主机之间的单独连接，而且运行时间要长得多。 延迟和资源利用 更一般地说，可以将 延迟变化 视为 动态资源分区 的结果。 假设两台电话交换机之间有一条线路，可以同时进行 10,000 个呼叫。通过此线路切换的每个电路都占用其中一个呼叫插槽。因此，你可以将线路视为可由多达 10,000 个并发用户共享的资源。资源以静态方式分配：即使你现在是线路上唯一的呼叫，并且所有其他 9,999 个插槽都未使用，你的电路仍将分配与线路充分利用时相同的固定数量的带宽。 相比之下，互联网动态分享网络带宽。发送者互相推挤和争夺，以让他们的数据包尽可能快地通过网络，并且网络交换机决定从一个时刻到另一个时刻发送哪个分组（即，带宽分配）。这种方法有排队的缺点，但其优点是它最大限度地利用了线路。线路固定成本，所以如果你更好地利用它，你通过线路发送的每个字节都会更便宜。 CPU 也会出现类似的情况：如果你在多个线程间动态共享每个 CPU 内核，则一个线程有时必须在操作系统的运行队列里等待，而另一个线程正在运行，这样每个线程都有可能被暂停一个不定的时间长度。但是，与为每个线程分配静态数量的 CPU 周期相比，这会更好地利用硬件（请参阅 “响应时间保证”）。更好的硬件利用率也是使用虚拟机的重要动机。 如果资源是静态分区的（例如，专用硬件和专用带宽分配），则在某些环境中可以实现 延迟保证。但是，这是以降低利用率为代价的 —— 换句话说，它是更昂贵的。另一方面，动态资源分配的多租户提供了更好的利用率，所以它更便宜，但它具有可变延迟的缺点。 网络中的可变延迟不是一种自然规律，而只是成本 &#x2F; 收益权衡的结果。 不可靠的时钟时钟和时间很重要。应用程序以各种方式依赖于时钟来回答以下问题： 这个请求是否超时了？ 这项服务的第 99 百分位响应时间是多少？ 在过去五分钟内，该服务平均每秒处理多少个查询？ 用户在我们的网站上花了多长时间？ 这篇文章在何时发布？ 在什么时间发送提醒邮件？ 这个缓存条目何时到期？ 日志文件中此错误消息的时间戳是什么？ 例 1-4 测量了 持续时间（durations，例如，请求发送与响应接收之间的时间间隔），而 例 5-8 描述了 时间点（point in time，在特定日期和和特定时间发生的事件）。 在分布式系统中，时间是一件棘手的事情，因为通信不是即时的：消息通过网络从一台机器传送到另一台机器需要时间。收到消息的时间总是晚于发送的时间，但是由于网络中的可变延迟，我们不知道晚了多少时间。这个事实导致有时很难确定在涉及多台机器时发生事情的顺序。 而且，网络上的每台机器都有自己的时钟，这是一个实际的硬件设备：通常是石英晶体振荡器。这些设备不是完全准确的，所以每台机器都有自己的时间概念，可能比其他机器稍快或更慢。可以在一定程度上同步时钟：最常用的机制是 网络时间协议（NTP），它允许根据一组服务器报告的时间来调整计算机时钟【37】。服务器则从更精确的时间源（如 GPS 接收机）获取时间。 单调钟与日历时钟现代计算机至少有两种不同的时钟：日历时钟（time-of-day clock）和单调钟（monotonic clock）。尽管它们都衡量时间，但区分这两者很重要，因为它们有不同的目的。 日历时钟日历时钟是你直观地了解时钟的依据：它根据某个日历（也称为 挂钟时间，即 wall-clock time）返回当前日期和时间。例如，Linux 上的 clock_gettime(CLOCK_REALTIME)[^v] 和 Java 中的 System.currentTimeMillis() 返回自 epoch（UTC 时间 1970 年 1 月 1 日午夜）以来的秒数（或毫秒），根据公历（Gregorian）日历，不包括闰秒。有些系统使用其他日期作为参考点。 [^v]: 虽然该时钟被称为实时时钟，但它与实时操作系统无关，如 “响应时间保证” 中所述。 日历时钟通常与 NTP 同步，这意味着来自一台机器的时间戳（理想情况下）与另一台机器上的时间戳相同。但是如下节所述，日历时钟也具有各种各样的奇特之处。特别是，如果本地时钟在 NTP 服务器之前太远，则它可能会被强制重置，看上去好像跳回了先前的时间点。这些跳跃以及他们经常忽略闰秒的事实，使日历时钟不能用于测量经过时间（elapsed time）【38】。 历史上的日历时钟还具有相当粗略的分辨率，例如，在较早的 Windows 系统上以 10 毫秒为单位前进【39】。在最近的系统中这已经不是一个问题了。 单调钟单调钟适用于测量持续时间（时间间隔），例如超时或服务的响应时间：Linux 上的 clock_gettime(CLOCK_MONOTONIC)，和 Java 中的 System.nanoTime() 都是单调时钟。这个名字来源于他们保证总是往前走的事实（而日历时钟可以往回跳）。 你可以在某个时间点检查单调钟的值，做一些事情，且稍后再次检查它。这两个值之间的差异告诉你两次检查之间经过了多长时间。但单调钟的绝对值是毫无意义的：它可能是计算机启动以来的纳秒数，或类似的任意值。特别是比较来自两台不同计算机的单调钟的值是没有意义的，因为它们并不是一回事。 在具有多个 CPU 插槽的服务器上，每个 CPU 可能有一个单独的计时器，但不一定与其他 CPU 同步。操作系统会补偿所有的差异，并尝试向应用线程表现出单调钟的样子，即使这些线程被调度到不同的 CPU 上。当然，明智的做法是不要太把这种单调性保证当回事【40】。 如果 NTP 协议检测到计算机的本地石英钟比 NTP 服务器要更快或更慢，则可以调整单调钟向前走的频率（这称为 偏移（skewing） 时钟）。默认情况下，NTP 允许时钟速率增加或减慢最高至 0.05%，但 NTP 不能使单调时钟向前或向后跳转。单调时钟的分辨率通常相当好：在大多数系统中，它们能在几微秒或更短的时间内测量时间间隔。 在分布式系统中，使用单调钟测量 经过时间（elapsed time，比如超时）通常很好，因为它不假定不同节点的时钟之间存在任何同步，并且对测量的轻微不准确性不敏感。 时钟同步与准确性单调钟不需要同步，但是日历时钟需要根据 NTP 服务器或其他外部时间源来设置才能有用。不幸的是，我们获取时钟的方法并不像你所希望的那样可靠或准确 —— 硬件时钟和 NTP 可能会变幻莫测。举几个例子： 计算机中的石英钟不够精确：它会 漂移（drifts，即运行速度快于或慢于预期）。时钟漂移取决于机器的温度。 Google 假设其服务器时钟漂移为 200 ppm（百万分之一）【41】，相当于每 30 秒与服务器重新同步一次的时钟漂移为 6 毫秒，或者每天重新同步的时钟漂移为 17 秒。即使一切工作正常，此漂移也会限制可以达到的最佳准确度。 如果计算机的时钟与 NTP 服务器的时钟差别太大，可能会拒绝同步，或者本地时钟将被强制重置【37】。任何观察重置前后时间的应用程序都可能会看到时间倒退或突然跳跃。 如果某个节点被 NTP 服务器的防火墙意外阻塞，有可能会持续一段时间都没有人会注意到。有证据表明，这在实践中确实发生过。 NTP 同步只能和网络延迟一样好，所以当你在拥有可变数据包延迟的拥塞网络上时，NTP 同步的准确性会受到限制。一个实验表明，当通过互联网同步时，35 毫秒的最小误差是可以实现的，尽管偶尔的网络延迟峰值会导致大约一秒的误差。根据配置，较大的网络延迟会导致 NTP 客户端完全放弃。 一些 NTP 服务器是错误的或者配置错误的，报告的时间可能相差几个小时【43,44】。还好 NTP 客户端非常健壮，因为他们会查询多个服务器并忽略异常值。无论如何，依赖于互联网上的陌生人所告诉你的时间来保证你的系统的正确性，这还挺让人担忧的。 闰秒导致一分钟可能有 59 秒或 61 秒，这会打破一些在设计之时未考虑闰秒的系统的时序假设【45】。闰秒已经使许多大型系统崩溃的事实【38,46】说明了，关于时钟的错误假设是多么容易偷偷溜入系统中。处理闰秒的最佳方法可能是让 NTP 服务器 “撒谎”，并在一天中逐渐执行闰秒调整（这被称为 拖尾，即 smearing）【47,48】，虽然实际的 NTP 服务器表现各异【49】。 在虚拟机中，硬件时钟被虚拟化，这对于需要精确计时的应用程序提出了额外的挑战【50】。当一个 CPU 核心在虚拟机之间共享时，每个虚拟机都会暂停几十毫秒，与此同时另一个虚拟机正在运行。从应用程序的角度来看，这种停顿表现为时钟突然向前跳跃【26】。 如果你在没有完整控制权的设备（例如，移动设备或嵌入式设备）上运行软件，则可能完全不能信任该设备的硬件时钟。一些用户故意将其硬件时钟设置为不正确的日期和时间，例如，为了规避游戏中的时间限制，时钟可能会被设置到很远的过去或将来。 如果你足够在乎这件事并投入大量资源，就可以达到非常好的时钟精度。例如，针对金融机构的欧洲法规草案 MiFID II 要求所有高频率交易基金在 UTC 时间 100 微秒内同步时钟，以便调试 “闪崩” 等市场异常现象，并帮助检测市场操纵【51】。 通过 GPS 接收机，精确时间协议（PTP）【52】以及仔细的部署和监测可以实现这种精确度。然而，这需要很多努力和专业知识，而且有很多东西都会导致时钟同步错误。如果你的 NTP 守护进程配置错误，或者防火墙阻止了 NTP 通信，由漂移引起的时钟误差可能很快就会变大。 依赖同步时钟时钟的问题在于，虽然它们看起来简单易用，但却具有令人惊讶的缺陷：一天可能不会有精确的 86,400 秒，日历时钟 可能会前后跳跃，而一个节点上的时间可能与另一个节点上的时间完全不同。 本章早些时候，我们讨论了网络丢包和任意延迟包的问题。尽管网络在大多数情况下表现良好，但软件的设计必须假定网络偶尔会出现故障，而软件必须正常处理这些故障。时钟也是如此：尽管大多数时间都工作得很好，但需要准备健壮的软件来处理不正确的时钟。 有一部分问题是，不正确的时钟很容易被视而不见。如果一台机器的 CPU 出现故障或者网络配置错误，很可能根本无法工作，所以很快就会被注意和修复。另一方面，如果它的石英时钟有缺陷，或者它的 NTP 客户端配置错误，大部分事情似乎仍然可以正常工作，即使它的时钟逐渐偏离现实。如果某个软件依赖于精确同步的时钟，那么结果更可能是悄无声息的，仅有微量的数据丢失，而不是一次惊天动地的崩溃【53,54】。 因此，如果你使用需要同步时钟的软件，必须仔细监控所有机器之间的时钟偏移。时钟偏离其他时钟太远的节点应当被宣告死亡，并从集群中移除。这样的监控可以确保你在损失发生之前注意到破损的时钟。 有序事件的时间戳让我们考虑一个特别的情况，一件很有诱惑但也很危险的事情：依赖时钟，在多个节点上对事件进行排序。 例如，如果两个客户端写入分布式数据库，谁先到达？ 哪一个更近？ 图 8-3 显示了在具有多主复制的数据库中对时钟的危险使用（该例子类似于 图 5-9）。 客户端 A 在节点 1 上写入 x = 1；写入被复制到节点 3；客户端 B 在节点 3 上增加 x（我们现在有 x = 2）；最后这两个写入都被复制到节点 2。 图 8-3 客户端 B 的写入比客户端 A 的写入要晚，但是 B 的写入具有较早的时间戳。 在 图 8-3 中，当一个写入被复制到其他节点时，它会根据发生写入的节点上的日历时钟标记一个时间戳。在这个例子中，时钟同步是非常好的：节点 1 和节点 3 之间的偏差小于 3ms，这可能比你在实践中能预期的更好。 尽管如此，图 8-3 中的时间戳却无法正确排列事件：写入 x = 1 的时间戳为 42.004 秒，但写入 x = 2 的时间戳为 42.003 秒，即使 x = 2 在稍后出现。当节点 2 接收到这两个事件时，会错误地推断出 x = 1 是最近的值，而丢弃写入 x = 2。效果上表现为，客户端 B 的增量操作会丢失。 这种冲突解决策略被称为 最后写入胜利（LWW），它在多主复制和无主数据库（如 Cassandra 【53】和 Riak 【54】）中被广泛使用（请参阅 “最后写入胜利（丢弃并发写入）” 一节）。有些实现会在客户端而不是服务器上生成时间戳，但这并不能改变 LWW 的基本问题： 数据库写入可能会神秘地消失：具有滞后时钟的节点无法覆盖之前具有快速时钟的节点写入的值，直到节点之间的时钟偏差消逝【54,55】。此方案可能导致一定数量的数据被悄悄丢弃，而未向应用报告任何错误。 LWW 无法区分 高频顺序写入（在 图 8-3 中，客户端 B 的增量操作 一定 发生在客户端 A 的写入之后）和 真正并发写入（写入者意识不到其他写入者）。需要额外的因果关系跟踪机制（例如版本向量），以防止违背因果关系（请参阅 “检测并发写入”）。 两个节点很可能独立地生成具有相同时间戳的写入，特别是在时钟仅具有毫秒分辨率的情况下。为了解决这样的冲突，还需要一个额外的 决胜值（tiebreaker，可以简单地是一个大随机数），但这种方法也可能会导致违背因果关系【53】。 因此，尽管通过保留 “最近” 的值并放弃其他值来解决冲突是很诱惑人的，但是要注意，“最近” 的定义取决于本地的 日历时钟，这很可能是不正确的。即使用严格同步的 NTP 时钟，一个数据包也可能在时间戳 100 毫秒（根据发送者的时钟）时发送，并在时间戳 99 毫秒（根据接收者的时钟）处到达 —— 看起来好像数据包在发送之前已经到达，这是不可能的。 NTP 同步是否能足够准确，以至于这种不正确的排序不会发生？也许不能，因为 NTP 的同步精度本身，除了石英钟漂移这类误差源之外，还受到网络往返时间的限制。为了进行正确的排序，你需要一个比测量对象（即网络延迟）要精确得多的时钟。 所谓的 逻辑时钟（logic clock）【56,57】是基于递增计数器而不是振荡石英晶体，对于排序事件来说是更安全的选择（请参阅 “检测并发写入”）。逻辑时钟不测量一天中的时间或经过的秒数，而仅测量事件的相对顺序（无论一个事件发生在另一个事件之前还是之后）。相反，用来测量实际经过时间的 日历时钟 和 单调钟 也被称为 物理时钟（physical clock）。我们将在 “顺序保证” 中来看顺序问题。 时钟读数存在置信区间你可能能够以微秒或甚至纳秒的精度读取机器的时钟。但即使可以得到如此细致的测量结果，这并不意味着这个值对于这样的精度实际上是准确的。实际上，大概率是不准确的 —— 如前所述，即使你每分钟与本地网络上的 NTP 服务器进行同步，几毫秒的时间漂移也很容易在不精确的石英时钟上发生。使用公共互联网上的 NTP 服务器，最好的准确度可能达到几十毫秒，而且当网络拥塞时，误差可能会超过 100 毫秒【57】。 因此，将时钟读数视为一个时间点是没有意义的 —— 它更像是一段时间范围：例如，一个系统可能以 95% 的置信度认为当前时间处于本分钟内的第 10.3 秒和 10.5 秒之间，它可能没法比这更精确了【58】。如果我们只知道 ±100 毫秒的时间，那么时间戳中的微秒数字部分基本上是没有意义的。 不确定性界限可以根据你的时间源来计算。如果你的 GPS 接收器或原子（铯）时钟直接连接到你的计算机上，预期的错误范围由制造商告知。如果从服务器获得时间，则不确定性取决于自上次与服务器同步以来的石英钟漂移的期望值，加上 NTP 服务器的不确定性，再加上到服务器的网络往返时间（只是获取粗略近似值，并假设服务器是可信的）。 不幸的是，大多数系统不公开这种不确定性：例如，当调用 clock_gettime() 时，返回值不会告诉你时间戳的预期错误，所以你不知道其置信区间是 5 毫秒还是 5 年。 一个有趣的例外是 Spanner 中的 Google TrueTime API 【41】，它明确地报告了本地时钟的置信区间。当你询问当前时间时，你会得到两个值：[最早，最晚]，这是最早可能的时间戳和最晚可能的时间戳。在不确定性估计的基础上，时钟知道当前的实际时间落在该区间内。区间的宽度取决于自从本地石英钟最后与更精确的时钟源同步以来已经过了多长时间。 全局快照的同步时钟在 “快照隔离和可重复读” 中，我们讨论了快照隔离，这是数据库中非常有用的功能，需要支持小型快速读写事务和大型长时间运行的只读事务（用于备份或分析）。它允许只读事务看到特定时间点的处于一致状态的数据库，且不会锁定和干扰读写事务。 快照隔离最常见的实现需要单调递增的事务 ID。如果写入比快照晚（即，写入具有比快照更大的事务 ID），则该写入对于快照事务是不可见的。在单节点数据库上，一个简单的计数器就足以生成事务 ID。 但是当数据库分布在许多机器上，也许可能在多个数据中心中时，由于需要协调，（跨所有分区）全局单调递增的事务 ID 会很难生成。事务 ID 必须反映因果关系：如果事务 B 读取由事务 A 写入的值，则 B 必须具有比 A 更大的事务 ID，否则快照就无法保持一致。在有大量的小规模、高频率的事务情景下，在分布式系统中创建事务 ID 成为一个难以处理的瓶颈 [^vi]。 [^vi]: 存在分布式序列号生成器，例如 Twitter 的雪花（Snowflake），其以可伸缩的方式（例如，通过将 ID 空间的块分配给不同节点）近似单调地增加唯一 ID。但是，它们通常无法保证与因果关系一致的排序，因为分配的 ID 块的时间范围比数据库读取和写入的时间范围要长。另请参阅 “顺序保证”。 我们可以使用同步时钟的时间戳作为事务 ID 吗？如果我们能够获得足够好的同步性，那么这种方法将具有很合适的属性：更晚的事务会有更大的时间戳。当然，问题在于时钟精度的不确定性。 Spanner 以这种方式实现跨数据中心的快照隔离【59，60】。它使用 TrueTime API 报告的时钟置信区间，并基于以下观察结果：如果你有两个置信区间，每个置信区间包含最早和最晚可能的时间戳（$A &#x3D; [A_{earliest}, A_{latest}]$， $B&#x3D;[B_{earliest}, B_{latest}]$），这两个区间不重叠（即：$A_{earliest} &lt;A_{latest} &lt;B_{earliest} &lt;B_{latest}$）的话，那么 B 肯定发生在 A 之后 —— 这是毫无疑问的。只有当区间重叠时，我们才不确定 A 和 B 发生的顺序。 为了确保事务时间戳反映因果关系，在提交读写事务之前，Spanner 在提交读写事务时，会故意等待置信区间长度的时间。通过这样，它可以确保任何可能读取数据的事务处于足够晚的时间，因此它们的置信区间不会重叠。为了保持尽可能短的等待时间，Spanner 需要保持尽可能小的时钟不确定性，为此，Google 在每个数据中心都部署了一个 GPS 接收器或原子钟，这允许时钟同步到大约 7 毫秒以内【41】。 对分布式事务语义使用时钟同步是一个活跃的研究领域【57,61,62】。这些想法很有趣，但是它们还没有在谷歌之外的主流数据库中实现。 进程暂停让我们考虑在分布式系统中使用危险时钟的另一个例子。假设你有一个数据库，每个分区只有一个领导者。只有领导被允许接受写入。一个节点如何知道它仍然是领导者（它并没有被别人宣告为死亡），并且它可以安全地接受写入？ 一种选择是领导者从其他节点获得一个 租约（lease），类似一个带超时的锁【63】。任一时刻只有一个节点可以持有租约 —— 因此，当一个节点获得一个租约时，它知道它在某段时间内自己是领导者，直到租约到期。为了保持领导地位，节点必须周期性地在租约过期前续期。 如果节点发生故障，就会停止续期，所以当租约过期时，另一个节点可以接管。 可以想象，请求处理循环看起来像这样： 1234567891011while (true) &#123; request = getIncomingRequest(); // 确保租约还剩下至少 10 秒 if (lease.expiryTimeMillis - System.currentTimeMillis() &lt; 10000)&#123; lease = lease.renew(); &#125; if (lease.isValid()) &#123; process(request); &#125;&#125; 这个代码有什么问题？首先，它依赖于同步时钟：租约到期时间由另一台机器设置（例如，当前时间加上 30 秒，计算到期时间），并将其与本地系统时钟进行比较。如果时钟不同步超过几秒，这段代码将开始做奇怪的事情。 其次，即使我们将协议更改为仅使用本地单调时钟，也存在另一个问题：代码假定在执行剩余时间检查 System.currentTimeMillis() 和实际执行请求 process(request) 中间的时间间隔非常短。通常情况下，这段代码运行得非常快，所以 10 秒的缓冲区已经足够确保 租约 在请求处理到一半时不会过期。 但是，如果程序执行中出现了意外的停顿呢？例如，想象一下，线程在 lease.isValid() 行周围停止 15 秒，然后才继续。在这种情况下，在请求被处理的时候，租约可能已经过期，而另一个节点已经接管了领导。然而，没有什么可以告诉这个线程已经暂停了这么长时间了，所以这段代码不会注意到租约已经到期了，直到循环的下一个迭代 —— 到那个时候它可能已经做了一些不安全的处理请求。 假设一个线程可能会暂停很长时间，这是疯了吗？不幸的是，这种情况发生的原因有很多种： 许多编程语言运行时（如 Java 虚拟机）都有一个垃圾收集器（GC），偶尔需要停止所有正在运行的线程。这些 “停止所有处理（stop-the-world）”GC 暂停有时会持续几分钟【64】！甚至像 HotSpot JVM 的 CMS 这样的所谓的 “并行” 垃圾收集器也不能完全与应用程序代码并行运行，它需要不时地停止所有处理【65】。尽管通常可以通过改变分配模式或调整 GC 设置来减少暂停【66】，但是如果我们想要提供健壮的保证，就必须假设最坏的情况发生。 在虚拟化环境中，可以 挂起（suspend） 虚拟机（暂停执行所有进程并将内存内容保存到磁盘）并恢复（恢复内存内容并继续执行）。这个暂停可以在进程执行的任何时候发生，并且可以持续任意长的时间。这个功能有时用于虚拟机从一个主机到另一个主机的实时迁移，而不需要重新启动，在这种情况下，暂停的长度取决于进程写入内存的速率【67】。 在最终用户的设备（如笔记本电脑）上，执行也可能被暂停并随意恢复，例如当用户关闭笔记本电脑的盖子时。 当操作系统上下文切换到另一个线程时，或者当管理程序切换到另一个虚拟机时（在虚拟机中运行时），当前正在运行的线程可能在代码中的任意点处暂停。在虚拟机的情况下，在其他虚拟机中花费的 CPU 时间被称为 窃取时间（steal time）。如果机器处于沉重的负载下（即，如果等待运行的线程队列很长），暂停的线程再次运行可能需要一些时间。 如果应用程序执行同步磁盘访问，则线程可能暂停，等待缓慢的磁盘 I&#x2F;O 操作完成【68】。在许多语言中，即使代码没有包含文件访问，磁盘访问也可能出乎意料地发生 —— 例如，Java 类加载器在第一次使用时惰性加载类文件，这可能在程序执行过程中随时发生。 I&#x2F;O 暂停和 GC 暂停甚至可能合谋组合它们的延迟【69】。如果磁盘实际上是一个网络文件系统或网络块设备（如亚马逊的 EBS），I&#x2F;O 延迟进一步受到网络延迟变化的影响【29】。 如果操作系统配置为允许交换到磁盘（页面交换），则简单的内存访问可能导致 页面错误（page fault），要求将磁盘中的页面装入内存。当这个缓慢的 I&#x2F;O 操作发生时，线程暂停。如果内存压力很高，则可能需要将另一个页面换出到磁盘。在极端情况下，操作系统可能花费大部分时间将页面交换到内存中，而实际上完成的工作很少（这被称为 抖动，即 thrashing）。为了避免这个问题，通常在服务器机器上禁用页面调度（如果你宁愿干掉一个进程来释放内存，也不愿意冒抖动风险）。 可以通过发送 SIGSTOP 信号来暂停 Unix 进程，例如通过在 shell 中按下 Ctrl-Z。 这个信号立即阻止进程继续执行更多的 CPU 周期，直到 SIGCONT 恢复为止，此时它将继续运行。 即使你的环境通常不使用 SIGSTOP，也可能由运维工程师意外发送。 所有这些事件都可以随时 抢占（preempt） 正在运行的线程，并在稍后的时间恢复运行，而线程甚至不会注意到这一点。这个问题类似于在单个机器上使多线程代码线程安全：你不能对时序做任何假设，因为随时可能发生上下文切换，或者出现并行运行。 当在一台机器上编写多线程代码时，我们有相当好的工具来实现线程安全：互斥量、信号量、原子计数器、无锁数据结构、阻塞队列等等。不幸的是，这些工具并不能直接转化为分布式系统操作，因为分布式系统没有共享内存，只有通过不可靠网络发送的消息。 分布式系统中的节点，必须假定其执行可能在任意时刻暂停相当长的时间，即使是在一个函数的中间。在暂停期间，世界的其它部分在继续运转，甚至可能因为该节点没有响应，而宣告暂停节点的死亡。最终暂停的节点可能会继续运行，在再次检查自己的时钟之前，甚至可能不会意识到自己进入了睡眠。 响应时间保证在许多编程语言和操作系统中，线程和进程可能暂停一段无限制的时间，正如讨论的那样。如果你足够努力，导致暂停的原因是 可以 消除的。 某些软件的运行环境要求很高，不能在特定时间内响应可能会导致严重的损失：控制飞机、火箭、机器人、汽车和其他物体的计算机必须对其传感器输入做出快速而可预测的响应。在这些系统中，软件必须有一个特定的 截止时间（deadline），如果截止时间不满足，可能会导致整个系统的故障。这就是所谓的 硬实时（hard real-time） 系统。 实时是真的吗？ 在嵌入式系统中，实时是指系统经过精心设计和测试，以满足所有情况下的特定时间保证。这个含义与 Web 上对实时术语的模糊使用相反，后者描述了服务器将数据推送到客户端以及没有严格的响应时间限制的流处理（见 第十一章）。 例如，如果车载传感器检测到当前正在经历碰撞，你肯定不希望安全气囊释放系统因为 GC 暂停而延迟弹出。 在系统中提供 实时保证 需要各级软件栈的支持：一个实时操作系统（RTOS），允许在指定的时间间隔内保证 CPU 时间的分配。库函数必须申明最坏情况下的执行时间；动态内存分配可能受到限制或完全不允许（实时垃圾收集器存在，但是应用程序仍然必须确保它不会给 GC 太多的负担）；必须进行大量的测试和测量，以确保达到保证。 所有这些都需要大量额外的工作，严重限制了可以使用的编程语言、库和工具的范围（因为大多数语言和工具不提供实时保证）。由于这些原因，开发实时系统非常昂贵，并且它们通常用于安全关键的嵌入式设备。而且，“实时” 与 “高性能” 不一样 —— 事实上，实时系统可能具有较低的吞吐量，因为他们必须让及时响应的优先级高于一切（另请参阅 “延迟和资源利用“）。 对于大多数服务器端数据处理系统来说，实时保证是不经济或不合适的。因此，这些系统必须承受在非实时环境中运行的暂停和时钟不稳定性。 限制垃圾收集的影响进程暂停的负面影响可以在不诉诸昂贵的实时调度保证的情况下得到缓解。语言运行时在计划垃圾回收时具有一定的灵活性，因为它们可以跟踪对象分配的速度和随着时间的推移剩余的空闲内存。 一个新兴的想法是将 GC 暂停视为一个节点的短暂计划中断，并在这个节点收集其垃圾的同时，让其他节点处理来自客户端的请求。如果运行时可以警告应用程序一个节点很快需要 GC 暂停，那么应用程序可以停止向该节点发送新的请求，等待它完成处理未完成的请求，然后在没有请求正在进行时执行 GC。这个技巧向客户端隐藏了 GC 暂停，并降低了响应时间的高百分比【70,71】。一些对延迟敏感的金融交易系统【72】使用这种方法。 这个想法的一个变种是只用垃圾收集器来处理短命对象（这些对象可以快速收集），并定期在积累大量长寿对象（因此需要完整 GC）之前重新启动进程【65,73】。一次可以重新启动一个节点，在计划重新启动之前，流量可以从该节点移开，就像 第四章 里描述的滚动升级一样。 这些措施不能完全阻止垃圾回收暂停，但可以有效地减少它们对应用的影响。 知识、真相与谎言本章到目前为止，我们已经探索了分布式系统与运行在单台计算机上的程序的不同之处：没有共享内存，只有通过可变延迟的不可靠网络传递的消息，系统可能遭受部分失效，不可靠的时钟和处理暂停。 如果你不习惯于分布式系统，那么这些问题的后果就会让人迷惑不解。网络中的一个节点无法确切地知道任何事情 —— 它只能根据它通过网络接收到（或没有接收到）的消息进行猜测。节点只能通过交换消息来找出另一个节点所处的状态（存储了哪些数据，是否正确运行等等）。如果远程节点没有响应，则无法知道它处于什么状态，因为网络中的问题不能可靠地与节点上的问题区分开来。 这些系统的讨论与哲学有关：在系统中什么是真什么是假？如果感知和测量的机制都是不可靠的，那么关于这些知识我们又能多么确定呢？软件系统应该遵循我们对物理世界所期望的法则，如因果关系吗？ 幸运的是，我们不需要去搞清楚生命的意义。在分布式系统中，我们可以陈述关于行为（系统模型）的假设，并以满足这些假设的方式设计实际系统。算法可以被证明在某个系统模型中正确运行。这意味着即使底层系统模型提供了很少的保证，也可以实现可靠的行为。 但是，尽管可以使软件在不可靠的系统模型中表现良好，但这并不是可以直截了当实现的。在本章的其余部分中，我们将进一步探讨分布式系统中的知识和真相的概念，这将有助于我们思考我们可以做出的各种假设以及我们可能希望提供的保证。在 第九章 中，我们将着眼于分布式系统的一些例子，这些算法在特定的假设条件下提供了特定的保证。 真相由多数所定义设想一个具有不对称故障的网络：一个节点能够接收发送给它的所有消息，但是来自该节点的任何传出消息被丢弃或延迟【19】。即使该节点运行良好，并且正在接收来自其他节点的请求，其他节点也无法听到其响应。经过一段时间后，其他节点宣布它已经死亡，因为他们没有听到节点的消息。这种情况就像梦魇一样：半断开（semi-disconnected） 的节点被拖向墓地，敲打尖叫道 “我没死！” —— 但是由于没有人能听到它的尖叫，葬礼队伍继续以坚忍的决心继续行进。 在一个稍微不那么梦魇的场景中，半断开的节点可能会注意到它发送的消息没有被其他节点确认，因此意识到网络中必定存在故障。尽管如此，节点被其他节点错误地宣告为死亡，而半连接的节点对此无能为力。 第三种情况，想象一个正在经历长时间 垃圾收集暂停（stop-the-world GC Pause） 的节点，节点的所有线程被 GC 抢占并暂停一分钟，因此没有请求被处理，也没有响应被发送。其他节点等待，重试，不耐烦，并最终宣布节点死亡，并将其丢到灵车上。最后，GC 完成，节点的线程继续，好像什么也没有发生。其他节点感到惊讶，因为所谓的死亡节点突然从棺材中抬起头来，身体健康，开始和旁观者高兴地聊天。GC 后的节点最初甚至没有意识到已经经过了整整一分钟，而且自己已被宣告死亡。从它自己的角度来看，从最后一次与其他节点交谈以来，几乎没有经过任何时间。 这些故事的寓意是，节点不一定能相信自己对于情况的判断。分布式系统不能完全依赖单个节点，因为节点可能随时失效，可能会使系统卡死，无法恢复。相反，许多分布式算法都依赖于法定人数，即在节点之间进行投票（请参阅 “读写的法定人数“）：决策需要来自多个节点的最小投票数，以减少对于某个特定节点的依赖。 这也包括关于宣告节点死亡的决定。如果法定数量的节点宣告另一个节点已经死亡，那么即使该节点仍感觉自己活着，它也必须被认为是死的。个体节点必须遵守法定决定并下台。 最常见的法定人数是超过一半的绝对多数（尽管其他类型的法定人数也是可能的）。多数法定人数允许系统继续工作，如果单个节点发生故障（三个节点可以容忍单节点故障；五个节点可以容忍双节点故障）。系统仍然是安全的，因为在这个制度中只能有一个多数 —— 不能同时存在两个相互冲突的多数决定。当我们在 第九章 中讨论 共识算法（consensus algorithms） 时，我们将更详细地讨论法定人数的应用。 领导者和锁通常情况下，一些东西在一个系统中只能有一个。例如： 数据库分区的领导者只能有一个节点，以避免 脑裂（即 split brain，请参阅 “处理节点宕机”）。 特定资源的锁或对象只允许一个事务 &#x2F; 客户端持有，以防同时写入和损坏。 一个特定的用户名只能被一个用户所注册，因为用户名必须唯一标识一个用户。 在分布式系统中实现这一点需要注意：即使一个节点认为它是 “天选者（the choosen one）”（分区的负责人，锁的持有者，成功获取用户名的用户的请求处理程序），但这并不一定意味着有法定人数的节点同意！一个节点可能以前是领导者，但是如果其他节点在此期间宣布它死亡（例如，由于网络中断或 GC 暂停），则它可能已被降级，且另一个领导者可能已经当选。 如果一个节点继续表现为 天选者，即使大多数节点已经声明它已经死了，则在考虑不周的系统中可能会导致问题。这样的节点能以自己赋予的权能向其他节点发送消息，如果其他节点相信，整个系统可能会做一些不正确的事情。 例如，图 8-4 显示了由于不正确的锁实现导致的数据损坏错误。 （这个错误不仅仅是理论上的：HBase 曾经有这个问题【74,75】）假设你要确保一个存储服务中的文件一次只能被一个客户访问，因为如果多个客户试图对此写入，该文件将被损坏。你尝试通过在访问文件之前要求客户端从锁定服务获取租约来实现此目的。 图 8-4 分布式锁的实现不正确：客户端 1 认为它仍然具有有效的租约，即使它已经过期，从而破坏了存储中的文件 这个问题就是我们先前在 “进程暂停” 中讨论过的一个例子：如果持有租约的客户端暂停太久，它的租约将到期。另一个客户端可以获得同一文件的租约，并开始写入文件。当暂停的客户端回来时，它认为（不正确）它仍然有一个有效的租约，并继续写入文件。结果，客户的写入将产生冲突并损坏文件。 防护令牌当使用锁或租约来保护对某些资源（如 图 8-4 中的文件存储）的访问时，需要确保一个被误认为自己是 “天选者” 的节点不能扰乱系统的其它部分。实现这一目标的一个相当简单的技术就是 防护（fencing），如 图 8-5 所示 图 8-5 只允许以增加防护令牌的顺序进行写操作，从而保证存储安全 我们假设每次锁定服务器授予锁或租约时，它还会返回一个 防护令牌（fencing token），这个数字在每次授予锁定时都会增加（例如，由锁定服务增加）。然后，我们可以要求客户端每次向存储服务发送写入请求时，都必须包含当前的防护令牌。 在 图 8-5 中，客户端 1 以 33 的令牌获得租约，但随后进入一个长时间的停顿并且租约到期。客户端 2 以 34 的令牌（该数字总是增加）获取租约，然后将其写入请求发送到存储服务，包括 34 的令牌。稍后，客户端 1 恢复生机并将其写入存储服务，包括其令牌值 33。但是，存储服务器会记住它已经处理了一个具有更高令牌编号（34）的写入，因此它会拒绝带有令牌 33 的请求。 如果将 ZooKeeper 用作锁定服务，则可将事务标识 zxid 或节点版本 cversion 用作防护令牌。由于它们保证单调递增，因此它们具有所需的属性【74】。 请注意，这种机制要求资源本身在检查令牌方面发挥积极作用，通过拒绝使用旧的令牌，而不是已经被处理的令牌来进行写操作 —— 仅仅依靠客户端检查自己的锁状态是不够的。对于不明确支持防护令牌的资源，可能仍然可以解决此限制（例如，在文件存储服务的情况下，可以将防护令牌包含在文件名中）。但是，为了避免在锁的保护之外处理请求，需要进行某种检查。 在服务器端检查一个令牌可能看起来像是一个缺点，但这可以说是一件好事：一个服务假定它的客户总是守规矩并不明智，因为使用客户端的人与运行服务的人优先级非常不一样【76】。因此，任何服务保护自己免受意外客户的滥用是一个好主意。 拜占庭故障防护令牌可以检测和阻止无意中发生错误的节点（例如，因为它尚未发现其租约已过期）。但是，如果节点有意破坏系统的保证，则可以通过使用假防护令牌发送消息来轻松完成此操作。 在本书中，我们假设节点是不可靠但诚实的：它们可能很慢或者从不响应（由于故障），并且它们的状态可能已经过时（由于 GC 暂停或网络延迟），但是我们假设如果节点它做出了回应，它正在说出 “真相”：尽其所知，它正在按照协议的规则扮演其角色。 如果存在节点可能 “撒谎”（发送任意错误或损坏的响应）的风险，则分布式系统的问题变得更困难了 —— 例如，如果节点可能声称其实际上没有收到特定的消息。这种行为被称为 拜占庭故障（Byzantine fault），在不信任的环境中达成共识的问题被称为拜占庭将军问题【77】。 拜占庭将军问题 拜占庭将军问题是对所谓 “两将军问题” 的泛化【78】，它想象两个将军需要就战斗计划达成一致的情况。由于他们在两个不同的地点建立了营地，他们只能通过信使进行沟通，信使有时会被延迟或丢失（就像网络中的信息包一样）。我们将在 第九章 讨论这个共识问题。 在这个问题的拜占庭版本里，有 n 位将军需要同意，他们的努力因为有一些叛徒在他们中间而受到阻碍。大多数的将军都是忠诚的，因而发出了真实的信息，但是叛徒可能会试图通过发送虚假或不真实的信息来欺骗和混淆他人（在试图保持未被发现的同时）。事先并不知道叛徒是谁。 拜占庭是后来成为君士坦丁堡的古希腊城市，现在在土耳其的伊斯坦布尔。没有任何历史证据表明拜占庭将军比其他地方更容易出现诡计和阴谋。相反，这个名字来源于拜占庭式的过度复杂，官僚，迂回等意义，早在计算机之前就已经在政治中被使用了【79】。Lamport 想要选一个不会冒犯任何读者的国家，他被告知将其称为阿尔巴尼亚将军问题并不是一个好主意【80】。 当一个系统在部分节点发生故障、不遵守协议、甚至恶意攻击、扰乱网络时仍然能继续正确工作，称之为 拜占庭容错（Byzantine fault-tolerant） 的，这种担忧在某些特定情况下是有意义的： 在航空航天环境中，计算机内存或 CPU 寄存器中的数据可能被辐射破坏，导致其以任意不可预知的方式响应其他节点。由于系统故障非常昂贵（例如，飞机撞毁和炸死船上所有人员，或火箭与国际空间站相撞），飞行控制系统必须容忍拜占庭故障【81,82】。 在多个参与组织的系统中，一些参与者可能会试图欺骗或诈骗他人。在这种情况下，节点仅仅信任另一个节点的消息是不安全的，因为它们可能是出于恶意的目的而被发送的。例如，像比特币和其他区块链一样的对等网络可以被认为是让互不信任的各方同意交易是否发生的一种方式，而不依赖于中心机构（central authority）【83】。 然而，在本书讨论的那些系统中，我们通常可以安全地假设没有拜占庭式的错误。在你的数据中心里，所有的节点都是由你的组织控制的（所以他们可以信任），辐射水平足够低，内存损坏不是一个大问题。制作拜占庭容错系统的协议相当复杂【84】，而容错嵌入式系统依赖于硬件层面的支持【81】。在大多数服务器端数据系统中，部署拜占庭容错解决方案的成本使其变得不切实际。 Web 应用程序确实需要预期受终端用户控制的客户端（如 Web 浏览器）的任意和恶意行为。这就是为什么输入验证，数据清洗和输出转义如此重要：例如，防止 SQL 注入和跨站点脚本。然而，我们通常不在这里使用拜占庭容错协议，而只是让服务器有权决定是否允许客户端行为。但在没有这种中心机构的对等网络中，拜占庭容错更为重要。 软件中的一个错误（bug）可能被认为是拜占庭式的错误，但是如果你将相同的软件部署到所有节点上，那么拜占庭式的容错算法帮不到你。大多数拜占庭式容错算法要求超过三分之二的节点能够正常工作（即，如果有四个节点，最多只能有一个故障）。要使用这种方法对付 bug，你必须有四个独立的相同软件的实现，并希望一个 bug 只出现在四个实现之一中。 同样，如果一个协议可以保护我们免受漏洞，安全渗透和恶意攻击，那么这将是有吸引力的。不幸的是，这也是不现实的：在大多数系统中，如果攻击者可以渗透一个节点，那他们可能会渗透所有这些节点，因为它们可能都运行着相同的软件。因此，传统机制（认证，访问控制，加密，防火墙等）仍然是抵御攻击者的主要保护措施。 弱谎言形式尽管我们假设节点通常是诚实的，但值得向软件中添加防止 “撒谎” 弱形式的机制 —— 例如，由硬件问题导致的无效消息，软件错误和错误配置。这种保护机制并不是完全的拜占庭容错，因为它们不能抵挡决心坚定的对手，但它们仍然是简单而实用的步骤，以提高可靠性。例如： 由于硬件问题或操作系统、驱动程序、路由器等中的错误，网络数据包有时会受到损坏。通常，损坏的数据包会被内建于 TCP 和 UDP 中的校验和所俘获，但有时它们也会逃脱检测【85,86,87】 。要对付这种破坏通常使用简单的方法就可以做到，例如应用程序级协议中的校验和。 可公开访问的应用程序必须仔细清理来自用户的任何输入，例如检查值是否在合理的范围内，并限制字符串的大小以防止通过大内存分配的拒绝服务。防火墙后面的内部服务对于输入也许可以只采取一些不那么严格的检查，但是采取一些基本的合理性检查（例如，在协议解析中）仍然是一个好主意。 NTP 客户端可以配置多个服务器地址。同步时，客户端联系所有的服务器，估计它们的误差，并检查大多数服务器是否对某个时间范围达成一致。只要大多数的服务器没问题，一个配置错误的 NTP 服务器报告的时间会被当成特异值从同步中排除【37】。使用多个服务器使 NTP 更健壮（比起只用单个服务器来）。 系统模型与现实已经有很多算法被设计以解决分布式系统问题 —— 例如，我们将在 第九章 讨论共识问题的解决方案。为了有用，这些算法需要容忍我们在本章中讨论的分布式系统的各种故障。 算法的编写方式不应该过分依赖于运行的硬件和软件配置的细节。这就要求我们以某种方式将我们期望在系统中发生的错误形式化。我们通过定义一个系统模型来做到这一点，这个模型是一个抽象，描述一个算法可以假设的事情。 关于时序假设，三种系统模型是常用的： 同步模型 同步模型（synchronous model） 假设网络延迟、进程暂停和和时钟误差都是受限的。这并不意味着完全同步的时钟或零网络延迟；这只意味着你知道网络延迟、暂停和时钟漂移将永远不会超过某个固定的上限【88】。同步模型并不是大多数实际系统的现实模型，因为（如本章所讨论的）无限延迟和暂停确实会发生。 部分同步模型 部分同步（partial synchronous） 意味着一个系统在大多数情况下像一个同步系统一样运行，但有时候会超出网络延迟，进程暂停和时钟漂移的界限【88】。这是很多系统的现实模型：大多数情况下，网络和进程表现良好，否则我们永远无法完成任何事情，但是我们必须承认，在任何时刻都存在时序假设偶然被破坏的事实。发生这种情况时，网络延迟、暂停和时钟错误可能会变得相当大。 异步模型 在这个模型中，一个算法不允许对时序做任何假设 —— 事实上它甚至没有时钟（所以它不能使用超时）。一些算法被设计为可用于异步模型，但非常受限。 进一步来说，除了时序问题，我们还要考虑 节点失效。三种最常见的节点系统模型是： 崩溃 - 停止故障 在 崩溃停止（crash-stop） 模型中，算法可能会假设一个节点只能以一种方式失效，即通过崩溃。这意味着节点可能在任意时刻突然停止响应，此后该节点永远消失 —— 它永远不会回来。 崩溃 - 恢复故障 我们假设节点可能会在任何时候崩溃，但也许会在未知的时间之后再次开始响应。在 崩溃 - 恢复（crash-recovery） 模型中，假设节点具有稳定的存储（即，非易失性磁盘存储）且会在崩溃中保留，而内存中的状态会丢失。 拜占庭（任意）故障 节点可以做（绝对意义上的）任何事情，包括试图戏弄和欺骗其他节点，如上一节所述。 对于真实系统的建模，具有 崩溃 - 恢复故障（crash-recovery） 的 部分同步模型（partial synchronous） 通常是最有用的模型。分布式算法如何应对这种模型？ 算法的正确性为了定义算法是正确的，我们可以描述它的属性。例如，排序算法的输出具有如下特性：对于输出列表中的任何两个不同的元素，左边的元素比右边的元素小。这只是定义对列表进行排序含义的一种形式方式。 同样，我们可以写下我们想要的分布式算法的属性来定义它的正确含义。例如，如果我们正在为一个锁生成防护令牌（请参阅 “防护令牌”），我们可能要求算法具有以下属性： 唯一性（uniqueness） 没有两个防护令牌请求返回相同的值。 单调序列（monotonic sequence） 如果请求 $x$ 返回了令牌 $t_x$，并且请求 $y$ 返回了令牌 $t_y$，并且 $x$ 在 $y$ 开始之前已经完成，那么 $t_x &lt; t_y$。 可用性（availability） 请求防护令牌并且不会崩溃的节点，最终会收到响应。 如果一个系统模型中的算法总是满足它在所有我们假设可能发生的情况下的性质，那么这个算法是正确的。但这如何有意义？如果所有的节点崩溃，或者所有的网络延迟突然变得无限长，那么没有任何算法能够完成任何事情。 安全性和活性为了澄清这种情况，有必要区分两种不同的属性：安全（safety）属性 和 活性（liveness）属性。在刚刚给出的例子中，唯一性 和 单调序列 是安全属性，而 可用性 是活性属性。 这两种性质有什么区别？一个试金石就是，活性属性通常在定义中通常包括 “最终” 一词（是的，你猜对了 —— 最终一致性是一个活性属性【89】）。 安全通常被非正式地定义为：没有坏事发生，而活性通常就类似：最终好事发生。但是，最好不要过多地阅读那些非正式的定义，因为好与坏的含义是主观的。安全和活性的实际定义是精确的和数学的【90】： 如果安全属性被违反，我们可以指向一个特定的安全属性被破坏的时间点（例如，如果违反了唯一性属性，我们可以确定重复的防护令牌被返回的特定操作）。违反安全属性后，违规行为不能被撤销 —— 损失已经发生。 活性属性反过来：在某个时间点（例如，一个节点可能发送了一个请求，但还没有收到响应），它可能不成立，但总是希望在未来能成立（即通过接受答复）。 区分安全属性和活性属性的一个优点是可以帮助我们处理困难的系统模型。对于分布式算法，在系统模型的所有可能情况下，要求 始终 保持安全属性是常见的【88】。也就是说，即使所有节点崩溃，或者整个网络出现故障，算法仍然必须确保它不会返回错误的结果（即保证安全属性得到满足）。 但是，对于活性属性，我们可以提出一些注意事项：例如，只有在大多数节点没有崩溃的情况下，只有当网络最终从中断中恢复时，我们才可以说请求需要接收响应。部分同步模型的定义要求系统最终返回到同步状态 —— 即任何网络中断的时间段只会持续一段有限的时间，然后进行修复。 将系统模型映射到现实世界安全属性和活性属性以及系统模型对于推理分布式算法的正确性非常有用。然而，在实践中实施算法时，现实的混乱事实再一次地让你咬牙切齿，很明显系统模型是对现实的简化抽象。 例如，在崩溃 - 恢复（crash-recovery）模型中的算法通常假设稳定存储器中的数据在崩溃后可以幸存。但是，如果磁盘上的数据被破坏，或者由于硬件错误或错误配置导致数据被清除，会发生什么情况【91】？如果服务器存在固件错误并且在重新启动时无法识别其硬盘驱动器，即使驱动器已正确连接到服务器，那又会发生什么情况【92】？ 法定人数算法（请参阅 “读写的法定人数”）依赖节点来记住它声称存储的数据。如果一个节点可能患有健忘症，忘记了以前存储的数据，这会打破法定条件，从而破坏算法的正确性。也许需要一个新的系统模型，在这个模型中，我们假设稳定的存储大多能在崩溃后幸存，但有时也可能会丢失。但是那个模型就变得更难以推理了。 算法的理论描述可以简单宣称一些事是不会发生的 —— 在非拜占庭式系统中，我们确实需要对可能发生和不可能发生的故障做出假设。然而，真实世界的实现，仍然会包括处理 “假设上不可能” 情况的代码，即使代码可能就是 printf(&quot;Sucks to be you&quot;) 和 exit(666)，实际上也就是留给运维来擦屁股【93】。（这可以说是计算机科学和软件工程间的一个差异）。 这并不是说理论上抽象的系统模型是毫无价值的，恰恰相反。它们对于将实际系统的复杂性提取成一个个我们可以推理的可处理的错误类型是非常有帮助的，以便我们能够理解这个问题，并试图系统地解决这个问题。我们可以证明算法是正确的，通过表明它们的属性在某个系统模型中总是成立的。 证明算法正确并不意味着它在真实系统上的实现必然总是正确的。但这迈出了很好的第一步，因为理论分析可以发现算法中的问题，这种问题可能会在现实系统中长期潜伏，直到你的假设（例如，时序）因为不寻常的情况被打破。理论分析与经验测试同样重要。 本章小结在本章中，我们讨论了分布式系统中可能发生的各种问题，包括： 当你尝试通过网络发送数据包时，数据包可能会丢失或任意延迟。同样，答复可能会丢失或延迟，所以如果你没有得到答复，你不知道消息是否发送成功了。 节点的时钟可能会与其他节点显著不同步（尽管你尽最大努力设置 NTP），它可能会突然跳转或跳回，依靠它是很危险的，因为你很可能没有好的方法来测量你的时钟的错误间隔。 一个进程可能会在其执行的任何时候暂停一段相当长的时间（可能是因为停止所有处理的垃圾收集器），被其他节点宣告死亡，然后再次复活，却没有意识到它被暂停了。 这类 部分失效（partial failure） 可能发生的事实是分布式系统的决定性特征。每当软件试图做任何涉及其他节点的事情时，偶尔就有可能会失败，或者随机变慢，或者根本没有响应（最终超时）。在分布式系统中，我们试图在软件中建立 部分失效 的容错机制，这样整个系统在即使某些组成部分被破坏的情况下，也可以继续运行。 为了容忍错误，第一步是 检测 它们，但即使这样也很难。大多数系统没有检测节点是否发生故障的准确机制，所以大多数分布式算法依靠 超时 来确定远程节点是否仍然可用。但是，超时无法区分网络失效和节点失效，并且可变的网络延迟有时会导致节点被错误地怀疑发生故障。此外，有时一个节点可能处于降级状态：例如，由于驱动程序错误，千兆网卡可能突然下降到 1 Kb&#x2F;s 的吞吐量【94】。这样一个 “跛行” 而不是死掉的节点可能比一个干净的失效节点更难处理。 一旦检测到故障，使系统容忍它也并不容易：没有全局变量，没有共享内存，没有共同的知识，或机器之间任何其他种类的共享状态。节点甚至不能就现在是什么时间达成一致，就不用说更深奥的了。信息从一个节点流向另一个节点的唯一方法是通过不可靠的网络发送信息。重大决策不能由一个节点安全地完成，因此我们需要一个能从其他节点获得帮助的协议，并争取达到法定人数以达成一致。 如果你习惯于在理想化的数学完美的单机环境（同一个操作总能确定地返回相同的结果）中编写软件，那么转向分布式系统的凌乱的物理现实可能会有些令人震惊。相反，如果能够在单台计算机上解决一个问题，那么分布式系统工程师通常会认为这个问题是平凡的【5】，现在单个计算机确实可以做很多事情【95】。如果你可以避免打开潘多拉的盒子，把东西放在一台机器上，那么通常是值得的。 但是，正如在 第二部分 的介绍中所讨论的那样，可伸缩性并不是使用分布式系统的唯一原因。容错和低延迟（通过将数据放置在距离用户较近的地方）是同等重要的目标，而这些不能用单个节点实现。 在本章中，我们也转换了几次话题，探讨了网络、时钟和进程的不可靠性是否是不可避免的自然规律。我们看到这并不是：有可能给网络提供硬实时的响应保证和有限的延迟，但是这样做非常昂贵，且导致硬件资源的利用率降低。大多数非安全关键系统会选择 便宜而不可靠，而不是 昂贵和可靠。 我们还谈到了超级计算机，它们采用可靠的组件，因此当组件发生故障时必须完全停止并重新启动。相比之下，分布式系统可以永久运行而不会在服务层面中断，因为所有的错误和维护都可以在节点级别进行处理 —— 至少在理论上是如此。 （实际上，如果一个错误的配置变更被应用到所有的节点，仍然会使分布式系统瘫痪）。 本章一直在讲存在的问题，给我们展现了一幅黯淡的前景。在 下一章 中，我们将继续讨论解决方案，并讨论一些旨在解决分布式系统中所有问题的算法。 参考文献 Mark Cavage: Just No Getting Around It: You’re Building a Distributed System](http://queue.acm.org/detail.cfm?id=2482856),” ACM Queue, volume 11, number 4, pages 80-89, April 2013. doi:10.1145&#x2F;2466486.2482856 Jay Kreps: “Getting Real About Distributed System Reliability,” blog.empathybox.com, March 19, 2012. Sydney Padua: The Thrilling Adventures of Lovelace and Babbage: The (Mostly) True Story of the First Computer. Particular Books, April ISBN: 978-0-141-98151-2 Coda Hale: “You Can’t Sacrifice Partition Tolerance,” codahale.com, October 7, 2010. Jeff Hodges: “Notes on Distributed Systems for Young Bloods,” somethingsimilar.com, January 14, 2013. Antonio Regalado: “Who Coined ‘Cloud Computing&amp;#x2019;?,” technologyreview.com, October 31, 2011. Luiz André Barroso, Jimmy Clidaras, and Urs Hölzle: “The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, Second Edition,” Synthesis Lectures on Computer Architecture, volume 8, number 3, Morgan &amp; Claypool Publishers, July 2013.doi:10.2200&#x2F;S00516ED2V01Y201306CAC024, ISBN: 978-1-627-05010-4 David Fiala, Frank Mueller, Christian Engelmann, et al.: “Detection and Correction of Silent Data Corruption for Large-Scale High-Performance Computing,” at International Conference for High Performance Computing, Networking, Storage and Analysis (SC12), November 2012. Arjun Singh, Joon Ong, Amit Agarwal, et al.: “Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network,” at Annual Conference of the ACM Special Interest Group on Data Communication (SIGCOMM), August 2015. doi:10.1145&#x2F;2785956.2787508 Glenn K. Lockwood: “Hadoop’s Uncomfortable Fit in HPC,” glennklockwood.blogspot.co.uk, May 16, 2014. John von Neumann: “Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components,” in Automata Studies (AM-34), edited by Claude E. Shannon and John McCarthy, Princeton University Press, 1956. ISBN: 978-0-691-07916-5 Richard W. Hamming: The Art of Doing Science and Engineering. Taylor &amp; Francis, 1997. ISBN: 978-9-056-99500-3 Claude E. Shannon: “A Mathematical Theory of Communication,” The Bell System Technical Journal, volume 27, number 3, pages 379–423 and 623–656, July 1948. Peter Bailis and Kyle Kingsbury: “The Network Is Reliable,” ACM Queue, volume 12, number 7, pages 48-55, July 2014. doi:10.1145&#x2F;2639988.2639988 Joshua B. Leners, Trinabh Gupta, Marcos K. Aguilera, and Michael Walfish: “Taming Uncertainty in Distributed Systems with Help from the Network,” at 10th European Conference on Computer Systems (EuroSys), April 2015. doi:10.1145&#x2F;2741948.2741976 Phillipa Gill, Navendu Jain, and Nachiappan Nagappan: “Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications,” at ACM SIGCOMM Conference, August 2011. doi:10.1145&#x2F;2018436.2018477 Mark Imbriaco: “Downtime Last Saturday,” github.com, December 26, 2012. Will Oremus: “The Global Internet Is Being Attacked by Sharks, Google Confirms,” slate.com, August 15, 2014. Marc A. Donges: “Re: bnx2 cards Intermittantly Going Offline,” Message to Linux netdev mailing list, spinics.net, September 13, 2012. Kyle Kingsbury: “Call Me Maybe: Elasticsearch,” aphyr.com, June 15, 2014. Salvatore Sanfilippo: “A Few Arguments About Redis Sentinel Properties and Fail Scenarios,” antirez.com, October 21, 2014. Bert Hubert: “The Ultimate SO_LINGER Page, or: Why Is My TCP Not Reliable,” blog.netherlabs.nl, January 18, 2009. Nicolas Liochon: “CAP: If All You Have Is a Timeout, Everything Looks Like a Partition,” blog.thislongrun.com, May 25, 2015. Jerome H. Saltzer, David P. Reed, and David D. Clark: “End-To-End Arguments in System Design,” ACM Transactions on Computer Systems, volume 2, number 4, pages 277–288, November 1984. doi:10.1145&#x2F;357401.357402 Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, et al.: “Queues Don’t Matter When You Can JUMP Them!,” at 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI), May 2015. Guohui Wang and T. S. Eugene Ng: “The Impact of Virtualization on Network Performance of Amazon EC2 Data Center,” at 29th IEEE International Conference on Computer Communications (INFOCOM), March 2010. doi:10.1109&#x2F;INFCOM.2010.5461931 Van Jacobson: “Congestion Avoidance and Control,” at ACM Symposium on Communications Architectures and Protocols (SIGCOMM), August 1988. doi:10.1145&#x2F;52324.52356 Brandon Philips: “etcd: Distributed Locking and Service Discovery,” at Strange Loop, September 2014. Steve Newman: “A Systematic Look at EC2 I&#x2F;O,” blog.scalyr.com, October 16, 2012. Naohiro Hayashibara, Xavier Défago, Rami Yared, and Takuya Katayama: “The ϕ Accrual Failure Detector,” Japan Advanced Institute of Science and Technology, School of Information Science, Technical Report IS-RR-2004-010, May 2004. Jeffrey Wang: “Phi Accrual Failure Detector,” ternarysearch.blogspot.co.uk, August 11, 2013. Srinivasan Keshav: An Engineering Approach to Computer Networking: ATM Networks, the Internet, and the Telephone Network. Addison-Wesley Professional, May 1997. ISBN: 978-0-201-63442-6 Cisco, “Integrated Services Digital Network,” docwiki.cisco.com. Othmar Kyas: ATM Networks. International Thomson Publishing, 1995. ISBN: 978-1-850-32128-6 “InfiniBand FAQ,” Mellanox Technologies, December 22, 2014. Jose Renato Santos, Yoshio Turner, and G. (John) Janakiraman: “End-to-End Congestion Control for InfiniBand,” at 22nd Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM), April 2003. Also published by HP Laboratories Palo Alto, Tech Report HPL-2002-359. doi:10.1109&#x2F;INFCOM.2003.1208949 Ulrich Windl, David Dalton, Marc Martinec, and Dale R. Worley: “The NTP FAQ and HOWTO,” ntp.org, November 2006. John Graham-Cumming: “How and why the leap second affected Cloudflare DNS,” blog.cloudflare.com, January 1, 2017. David Holmes: “Inside the Hotspot VM: Clocks, Timers and Scheduling Events – Part I – Windows,” blogs.oracle.com, October 2, 2006. Steve Loughran: “Time on Multi-Core, Multi-Socket Servers,” steveloughran.blogspot.co.uk, September 17, 2015. James C. Corbett, Jeffrey Dean, Michael Epstein, et al.: “Spanner: Google’s Globally-Distributed Database,” at 10th USENIX Symposium on Operating System Design and Implementation (OSDI), October 2012. M. Caporaloni and R. Ambrosini: “How Closely Can a Personal Computer Clock Track the UTC Timescale Via the Internet?,” European Journal of Physics, volume 23, number 4, pages L17–L21, June 2012. doi:10.1088&#x2F;0143-0807&#x2F;23&#x2F;4&#x2F;103 Nelson Minar: “A Survey of the NTP Network,” alumni.media.mit.edu, December 1999. Viliam Holub: “Synchronizing Clocks in a Cassandra Cluster Pt. 1 – The Problem,” blog.logentries.com, March 14, 2014. Poul-Henning Kamp: “The One-Second War (What Time Will You Die?),” ACM Queue, volume 9, number 4, pages 44–48, April 2011. doi:10.1145&#x2F;1966989.1967009 Nelson Minar: “Leap Second Crashes Half the Internet,” somebits.com, July 3, 2012. Christopher Pascoe: “Time, Technology and Leaping Seconds,” googleblog.blogspot.co.uk, September 15, 2011. Mingxue Zhao and Jeff Barr: “Look Before You Leap – The Coming Leap Second and AWS,” aws.amazon.com, May 18, 2015. Darryl Veitch and Kanthaiah Vijayalayan: “Network Timing and the 2015 Leap Second,” at 17th International Conference on Passive and Active Measurement (PAM), April 2016. doi:10.1007&#x2F;978-3-319-30505-9_29 “Timekeeping in VMware Virtual Machines,” Information Guide, VMware, Inc., December 2011. “MiFID II &#x2F; MiFIR: Regulatory Technical and Implementing Standards – Annex I (Draft),” European Securities and Markets Authority, Report ESMA&#x2F;2015&#x2F;1464, September 2015. Luke Bigum: “Solving MiFID II Clock Synchronisation With Minimum Spend (Part 1),” lmax.com, November 27, 2015. Kyle Kingsbury: “Call Me Maybe: Cassandra,” aphyr.com, September 24, 2013. John Daily: “Clocks Are Bad, or, Welcome to the Wonderful World of Distributed Systems,” basho.com, November 12, 2013. Kyle Kingsbury: “The Trouble with Timestamps,” aphyr.com, October 12, 2013. Leslie Lamport: “Time, Clocks, and the Ordering of Events in a Distributed System,” Communications of the ACM, volume 21, number 7, pages 558–565, July 1978. doi:10.1145&#x2F;359545.359563 Sandeep Kulkarni, Murat Demirbas, Deepak Madeppa, et al.: “Logical Physical Clocks and Consistent Snapshots in Globally Distributed Databases,” State University of New York at Buffalo, Computer Science and Engineering Technical Report 2014-04, May 2014. Justin Sheehy: “There Is No Now: Problems With Simultaneity in Distributed Systems,” ACM Queue, volume 13, number 3, pages 36–41, March 2015. doi:10.1145&#x2F;2733108 Murat Demirbas: “Spanner: Google’s Globally-Distributed Database,” muratbuffalo.blogspot.co.uk, July 4, 2013. Dahlia Malkhi and Jean-Philippe Martin: “Spanner’s Concurrency Control,” ACM SIGACT News, volume 44, number 3, pages 73–77, September 2013. doi:10.1145&#x2F;2527748.2527767 Manuel Bravo, Nuno Diegues, Jingna Zeng, et al.: “On the Use of Clocks to Enforce Consistency in the Cloud,” IEEE Data Engineering Bulletin, volume 38, number 1, pages 18–31, March 2015. Spencer Kimball: “Living Without Atomic Clocks,” cockroachlabs.com, February 17, 2016. Cary G. Gray and David R. Cheriton:“Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency,” at 12th ACM Symposium on Operating Systems Principles (SOSP), December 1989. doi:10.1145&#x2F;74850.74870 Todd Lipcon: “Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1,” blog.cloudera.com, February 24, 2011. Martin Thompson: “Java Garbage Collection Distilled,” mechanical-sympathy.blogspot.co.uk, July 16, 2013. Alexey Ragozin: “How to Tame Java GC Pauses? Surviving 16GiB Heap and Greater,” java.dzone.com, June 28, 2011. Christopher Clark, Keir Fraser, Steven Hand, et al.: “Live Migration of Virtual Machines,” at 2nd USENIX Symposium on Symposium on Networked Systems Design &amp; Implementation (NSDI), May 2005. Mike Shaver: “fsyncers and Curveballs,” shaver.off.net, May 25, 2008. Zhenyun Zhuang and Cuong Tran: “Eliminating Large JVM GC Pauses Caused by Background IO Traffic,” engineering.linkedin.com, February 10, 2016. David Terei and Amit Levy: “Blade: A Data Center Garbage Collector,” arXiv:1504.02578, April 13, 2015. Martin Maas, Tim Harris, Krste Asanović, and John Kubiatowicz: “Trash Day: Coordinating Garbage Collection in Distributed Systems,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS), May 2015. “Predictable Low Latency,” Cinnober Financial Technology AB, cinnober.com, November 24, 2013. Martin Fowler: “The LMAX Architecture,” martinfowler.com, July 12, 2011. Flavio P. Junqueira and Benjamin Reed: ZooKeeper: Distributed Process Coordination. O’Reilly Media, 2013. ISBN: 978-1-449-36130-3 Enis Söztutar: “HBase and HDFS: Understanding Filesystem Usage in HBase,” at HBaseCon, June 2013. Caitie McCaffrey: “Clients Are Jerks: AKA How Halo 4 DoSed the Services at Launch &amp; How We Survived,” caitiem.com, June 23, 2015. Leslie Lamport, Robert Shostak, and Marshall Pease: “The Byzantine Generals Problem,” ACM Transactions on Programming Languages and Systems (TOPLAS), volume 4, number 3, pages 382–401, July 1982. doi:10.1145&#x2F;357172.357176 Jim N. Gray: “Notes on Data Base Operating Systems,” in Operating Systems: An Advanced Course, Lecture Notes in Computer Science, volume 60, edited by R. Bayer, R. M. Graham, and G. Seegmüller, pages 393–481, Springer-Verlag, 1978. ISBN: 978-3-540-08755-7 Brian Palmer: “How Complicated Was the Byzantine Empire?,” slate.com, October 20, 2011. Leslie Lamport: “My Writings,” research.microsoft.com, December 16, 2014. This page can be found by searching the web for the 23-character string obtained by removing the hyphens from the string allla-mport-spubso-ntheweb. John Rushby: “Bus Architectures for Safety-Critical Embedded Systems,” at 1st International Workshop on Embedded Software (EMSOFT), October 2001. Jake Edge: “ELC: SpaceX Lessons Learned,” lwn.net, March 6, 2013. Andrew Miller and Joseph J. LaViola, Jr.: “Anonymous Byzantine Consensus from Moderately-Hard Puzzles: A Model for Bitcoin,” University of Central Florida, Technical Report CS-TR-14-01, April 2014. James Mickens: “The Saddest Moment,” USENIX ;login: logout, May 2013. Evan Gilman: “The Discovery of Apache ZooKeeper’s Poison Packet,” pagerduty.com, May 7, 2015. Jonathan Stone and Craig Partridge: “When the CRC and TCP Checksum Disagree,” at ACM Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM), August 2000. doi:10.1145&#x2F;347059.347561 Evan Jones: “How Both TCP and Ethernet Checksums Fail,” evanjones.ca, October 5, 2015. Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: “Consensus in the Presence of Partial Synchrony,” Journal of the ACM, volume 35, number 2, pages 288–323, April 1988. doi:10.1145&#x2F;42282.42283 Peter Bailis and Ali Ghodsi: “Eventual Consistency Today: Limitations, Extensions, and Beyond,” ACM Queue, volume 11, number 3, pages 55-63, March 2013. doi:10.1145&#x2F;2460276.2462076 Bowen Alpern and Fred B. Schneider: “Defining Liveness,” Information Processing Letters, volume 21, number 4, pages 181–185, October 1985. doi:10.1016&#x2F;0020-0190(85)90056-0 Flavio P. Junqueira: “Dude, Where’s My Metadata?,” fpj.me, May 28, 2015. Scott Sanders: “January 28th Incident Report,” github.com, February 3, 2016. Jay Kreps: “A Few Notes on Kafka and Jepsen,” blog.empathybox.com, September 25, 2013. Thanh Do, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: “Limplock: Understanding the Impact of Limpware on Scale-out Cloud Systems,” at 4th ACM Symposium on Cloud Computing (SoCC), October 2013. doi:10.1145&#x2F;2523616.2523627 Frank McSherry, Michael Isard, and Derek G. Murray: “Scalability! But at What COST?,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS), May 2015. [^译著1]: 原诗为：Hey I just met you. The network’s laggy. But here’s my data. So store it maybe.Hey, 应改编自《Call Me Maybe》歌词：I just met you, And this is crazy, But here’s my number, So call me, maybe?"},{"title":"第九章：一致性与共识","path":"/wiki/ddia/ch9.html","content":"好死还是赖活着？—— Jay Kreps, 关于 Kafka 与 Jepsen 的若干笔记 (2013) 正如 第八章 所讨论的，分布式系统中的许多事情可能会出错。处理这种故障的最简单方法是简单地让整个服务失效，并向用户显示错误消息。如果无法接受这个解决方案，我们就需要找到容错的方法 —— 即使某些内部组件出现故障，服务也能正常运行。 在本章中，我们将讨论构建容错分布式系统的算法和协议的一些例子。我们将假设 第八章 的所有问题都可能发生：网络中的数据包可能会丢失、重新排序、重复推送或任意延迟；时钟只是尽其所能地近似；且节点可以暂停（例如，由于垃圾收集）或随时崩溃。 构建容错系统的最好方法，是找到一些带有实用保证的通用抽象，实现一次，然后让应用依赖这些保证。这与 第七章 中的事务处理方法相同：通过使用事务，应用可以假装没有崩溃（原子性），没有其他人同时访问数据库（隔离），存储设备是完全可靠的（持久性）。即使发生崩溃，竞态条件和磁盘故障，事务抽象隐藏了这些问题，因此应用不必担心它们。 现在我们将继续沿着同样的路线前进，寻求可以让应用忽略分布式系统部分问题的抽象概念。例如，分布式系统最重要的抽象之一就是 共识（consensus）：就是让所有的节点对某件事达成一致。正如我们在本章中将会看到的那样，要可靠地达成共识，且不被网络故障和进程故障所影响，是一个令人惊讶的棘手问题。 一旦达成共识，应用可以将其用于各种目的。例如，假设你有一个单主复制的数据库。如果主库挂掉，并且需要故障切换到另一个节点，剩余的数据库节点可以使用共识来选举新的领导者。正如在 “处理节点宕机” 中所讨论的那样，重要的是只有一个领导者，且所有的节点都认同其领导。如果两个节点都认为自己是领导者，这种情况被称为 脑裂（split brain），它经常会导致数据丢失。正确实现共识有助于避免这种问题。 在本章后面的 “分布式事务与共识” 中，我们将研究解决共识和相关问题的算法。但首先，我们首先需要探索可以在分布式系统中提供的保证和抽象的范围。 我们需要了解可以做什么和不可以做什么的范围：在某些情况下，系统可以容忍故障并继续工作；在其他情况下，这是不可能的。我们将深入研究什么可能而什么不可能的限制，既通过理论证明，也通过实际实现。我们将在本章中概述这些基本限制。 分布式系统领域的研究人员几十年来一直在研究这些主题，所以有很多资料 —— 我们只能介绍一些皮毛。在本书中，我们没有空间去详细介绍形式模型和证明的细节，所以我们会按照直觉来介绍。如果你有兴趣，参考文献可以提供更多的深度。 一致性保证在 “复制延迟问题” 中，我们看到了数据库复制中发生的一些时序问题。如果你在同一时刻查看两个数据库节点，则可能在两个节点上看到不同的数据，因为写请求在不同的时间到达不同的节点。无论数据库使用何种复制方法（单主复制，多主复制或无主复制），都会出现这些不一致情况。 大多数复制的数据库至少提供了 最终一致性，这意味着如果你停止向数据库写入数据并等待一段不确定的时间，那么最终所有的读取请求都会返回相同的值【1】。换句话说，不一致性是暂时的，最终会自行解决（假设网络中的任何故障最终都会被修复）。最终一致性的一个更好的名字可能是 收敛（convergence），因为我们预计所有的副本最终会收敛到相同的值【2】。 然而，这是一个非常弱的保证 —— 它并没有说什么时候副本会收敛。在收敛之前，读操作可能会返回任何东西或什么都没有【1】。例如，如果你写入了一个值，然后立即再次读取，这并不能保证你能看到刚才写入的值，因为读请求可能会被路由到另外的副本上。（请参阅 “读己之写” ）。 对于应用开发人员而言，最终一致性是很困难的，因为它与普通单线程程序中变量的行为有很大区别。对于后者，如果将一个值赋给一个变量，然后很快地再次读取，不可能读到旧的值，或者读取失败。数据库表面上看起来像一个你可以读写的变量，但实际上它有更复杂的语义【3】。 在与只提供弱保证的数据库打交道时，你需要始终意识到它的局限性，而不是意外地作出太多假设。错误往往是微妙的，很难找到，也很难测试，因为应用可能在大多数情况下运行良好。当系统出现故障（例如网络中断）或高并发时，最终一致性的边缘情况才会显现出来。 本章将探索数据系统可能选择提供的更强一致性模型。它不是免费的：具有较强保证的系统可能会比保证较差的系统具有更差的性能或更少的容错性。尽管如此，更强的保证能够吸引人，因为它们更容易用对。只有见过不同的一致性模型后，才能更好地决定哪一个最适合自己的需求。 分布式一致性模型 和我们之前讨论的事务隔离级别的层次结构有一些相似之处【4,5】（请参阅 “弱隔离级别”）。尽管两者有一部分内容重叠，但它们大多是无关的问题：事务隔离主要是为了 避免由于同时执行事务而导致的竞争状态，而分布式一致性主要关于 在面对延迟和故障时如何协调副本间的状态。 本章涵盖了广泛的话题，但我们将会看到这些领域实际上是紧密联系在一起的： 首先看一下常用的 最强一致性模型 之一，线性一致性（linearizability），并考察其优缺点。 然后我们将检查分布式系统中 事件顺序 的问题，特别是因果关系和全局顺序的问题。 在第三节的（“分布式事务与共识”）中将探讨如何原子地提交分布式事务，这将最终引领我们走向共识问题的解决方案。 线性一致性在 最终一致 的数据库，如果你在同一时刻问两个不同副本相同的问题，可能会得到两个不同的答案。这很让人困惑。如果数据库可以提供只有一个副本的假象（即，只有一个数据副本），那么事情就简单太多了。那么每个客户端都会有相同的数据视图，且不必担心复制滞后了。 这就是 线性一致性（linearizability） 背后的想法【6】（也称为 原子一致性（atomic consistency）【7】，强一致性（strong consistency），立即一致性（immediate consistency） 或 外部一致性（external consistency ）【8】）。线性一致性的精确定义相当微妙，我们将在本节的剩余部分探讨它。但是基本的想法是让一个系统看起来好像只有一个数据副本，而且所有的操作都是原子性的。有了这个保证，即使实际中可能有多个副本，应用也不需要担心它们。 在一个线性一致的系统中，只要一个客户端成功完成写操作，所有客户端从数据库中读取数据必须能够看到刚刚写入的值。要维护数据的单个副本的假象，系统应保障读到的值是最近的、最新的，而不是来自陈旧的缓存或副本。换句话说，线性一致性是一个 新鲜度保证（recency guarantee）。为了阐明这个想法，我们来看看一个非线性一致系统的例子。 图 9-1 这个系统是非线性一致的，导致了球迷的困惑 图 9-1 展示了一个关于体育网站的非线性一致例子【9】。Alice 和 Bob 正坐在同一个房间里，都盯着各自的手机，关注着 2014 年 FIFA 世界杯决赛的结果。在最后得分公布后，Alice 刷新页面，看到宣布了获胜者，并兴奋地告诉 Bob。Bob 难以置信地刷新了自己的手机，但他的请求路由到了一个落后的数据库副本上，手机显示比赛仍在进行。 如果 Alice 和 Bob 在同一时间刷新并获得了两个不同的查询结果，也许就没有那么令人惊讶了。因为他们不知道服务器处理他们请求的精确时刻。然而 Bob 是在听到 Alice 惊呼最后得分 之后，点击了刷新按钮（启动了他的查询），因此他希望查询结果至少与爱丽丝一样新鲜。但他的查询返回了陈旧结果，这一事实违背了线性一致性的要求。 什么使得系统线性一致？线性一致性背后的基本思想很简单：使系统看起来好像只有一个数据副本。然而确切来讲，实际上有更多要操心的地方。为了更好地理解线性一致性，让我们再看几个例子。 图 9-2 显示了三个客户端在线性一致数据库中同时读写相同的键 x。在分布式系统文献中，x 被称为 寄存器（register），例如，它可以是键值存储中的一个 键，关系数据库中的一 行，或文档数据库中的一个 文档。 图 9-2 如果读取请求与写入请求并发，则可能会返回旧值或新值 为了简单起见，图 9-2 采用了用户请求的视角，而不是数据库内部的视角。每个柱都是由客户端发出的请求，其中柱头是请求发送的时刻，柱尾是客户端收到响应的时刻。因为网络延迟变化无常，客户端不知道数据库处理其请求的精确时间 —— 只知道它发生在发送请求和接收响应之间的某个时刻。[^i] [^i]: 这个图的一个微妙的细节是它假定存在一个全局时钟，由水平轴表示。即使真实的系统通常没有准确的时钟（请参阅 “不可靠的时钟”），但这种假设是允许的：为了分析分布式算法，我们可以假设一个精确的全局时钟存在，不过算法无法访问它【47】。算法只能看到由石英振荡器和 NTP 产生的实时逼近。 在这个例子中，寄存器有两种类型的操作： $read(x)⇒v$表示客户端请求读取寄存器 x 的值，数据库返回值 v。 $write(x,v)⇒r$ 表示客户端请求将寄存器 x 设置为值 v ，数据库返回响应 r （可能正确，可能错误）。 在 图 9-2 中，x 的值最初为 0，客户端 C 执行写请求将其设置为 1。发生这种情况时，客户端 A 和 B 反复轮询数据库以读取最新值。 A 和 B 的请求可能会收到怎样的响应？ 客户端 A 的第一个读操作，完成于写操作开始之前，因此必须返回旧值 0。 客户端 A 的最后一个读操作，开始于写操作完成之后。如果数据库是线性一致性的，它必然返回新值 1：因为读操作和写操作一定是在其各自的起止区间内的某个时刻被处理。如果在写入结束后开始读取，则读取处理一定发生在写入完成之后，因此它必须看到写入的新值。 与写操作在时间上重叠的任何读操作，可能会返回 0 或 1 ，因为我们不知道读取时，写操作是否已经生效。这些操作是 并发（concurrent） 的。 但是，这还不足以完全描述线性一致性：如果与写入同时发生的读取可以返回旧值或新值，那么读者可能会在写入期间看到数值在旧值和新值之间来回翻转。这不是我们所期望的仿真 “单一数据副本” 的系统。[^ii] [^ii]: 如果读取（与写入同时发生时）可能返回旧值或新值，则称该寄存器为 常规寄存器（regular register）【7,25】 为了使系统线性一致，我们需要添加另一个约束，如 图 9-3 所示 图 9-3 任何一个读取返回新值后，所有后续读取（在相同或其他客户端上）也必须返回新值。 在一个线性一致的系统中，我们可以想象，在 x 的值从 0 自动翻转到 1 的时候（在写操作的开始和结束之间）必定有一个时间点。因此，如果一个客户端的读取返回新的值 1，即使写操作尚未完成，所有后续读取也必须返回新值。 图 9-3 中的箭头说明了这个时序依赖关系。客户端 A 是第一个读取新的值 1 的位置。在 A 的读取返回之后，B 开始新的读取。由于 B 的读取严格在发生于 A 的读取之后，因此即使 C 的写入仍在进行中，也必须返回 1（与 图 9-1 中的 Alice 和 Bob 的情况相同：在 Alice 读取新值之后，Bob 也希望读取新的值）。 我们可以进一步细化这个时序图，展示每个操作是如何在特定时刻原子性生效的。图 9-4 显示了一个更复杂的例子【10】。 在 图 9-4 中，除了读写之外，还增加了第三种类型的操作： $cas(x, v_{old}, v_{new})⇒r$ 表示客户端请求进行原子性的 比较与设置 操作。如果寄存器 $x$ 的当前值等于 $v_{old}$ ，则应该原子地设置为 $v_{new}$ 。如果 $x$ 不等于 $v_{old}$ ，则操作应该保持寄存器不变并返回一个错误。 $r$ 是数据库的响应（正确或错误）。 图 9-4 中的每个操作都在我们认为执行操作的时候用竖线标出（在每个操作的条柱之内）。这些标记按顺序连在一起，其结果必须是一个有效的寄存器读写序列（每次读取都必须返回最近一次写入设置的值）。 线性一致性的要求是，操作标记的连线总是按时间（从左到右）向前移动，而不是向后移动。这个要求确保了我们之前讨论的新鲜度保证：一旦新的值被写入或读取，所有后续的读都会看到写入的值，直到它被再次覆盖。 图 9-4 可视化读取和写入看起来已经生效的时间点。 B 的最后读取不是线性一致性的 图 9-4 中有一些有趣的细节需要指出： 第一个客户端 B 发送一个读取 x 的请求，然后客户端 D 发送一个请求将 x 设置为 0，然后客户端 A 发送请求将 x 设置为 1。尽管如此，返回到 B 的读取值为 1（由 A 写入的值）。这是可以的：这意味着数据库首先处理 D 的写入，然后是 A 的写入，最后是 B 的读取。虽然这不是请求发送的顺序，但这是一个可以接受的顺序，因为这三个请求是并发的。也许 B 的读请求在网络上略有延迟，所以它在两次写入之后才到达数据库。 在客户端 A 从数据库收到响应之前，客户端 B 的读取返回 1 ，表示写入值 1 已成功。这也是可以的：这并不意味着在写之前读到了值，这只是意味着从数据库到客户端 A 的正确响应在网络中略有延迟。 此模型不假设有任何事务隔离：另一个客户端可能随时更改值。例如，C 首先读取 1 ，然后读取 2 ，因为两次读取之间的值由 B 更改。可以使用原子 比较并设置（cas） 操作来检查该值是否未被另一客户端同时更改：B 和 C 的 cas 请求成功，但是 D 的 cas 请求失败（在数据库处理它时，x 的值不再是 0 ）。 客户 B 的最后一次读取（阴影条柱中）不是线性一致性的。 该操作与 C 的 cas 写操作并发（它将 x 从 2 更新为 4 ）。在没有其他请求的情况下，B 的读取返回 2 是可以的。然而，在 B 的读取开始之前，客户端 A 已经读取了新的值 4 ，因此不允许 B 读取比 A 更旧的值。再次，与 图 9-1 中的 Alice 和 Bob 的情况相同。 这就是线性一致性背后的直觉。 正式的定义【6】更准确地描述了它。 通过记录所有请求和响应的时序，并检查它们是否可以排列成有效的顺序，以测试一个系统的行为是否线性一致性是可能的（尽管在计算上是昂贵的）【11】。 线性一致性与可串行化 线性一致性 容易和 可串行化 相混淆，因为两个词似乎都是类似 “可以按顺序排列” 的东西。但它们是两种完全不同的保证，区分两者非常重要： 可串行化 可串行化（Serializability） 是事务的隔离属性，每个事务可以读写多个对象（行，文档，记录）—— 请参阅 “单对象和多对象操作”。它确保事务的行为，与它们按照 某种 顺序依次执行的结果相同（每个事务在下一个事务开始之前运行完成）。这种执行顺序可以与事务实际执行的顺序不同。【12】。 线性一致性 线性一致性（Linearizability） 是读取和写入寄存器（单个对象）的 新鲜度保证。它不会将操作组合为事务，因此它也不会阻止写入偏差等问题（请参阅 “写入偏差和幻读”），除非采取其他措施（例如 物化冲突）。 一个数据库可以提供可串行化和线性一致性，这种组合被称为严格的可串行化或 强的单副本可串行化（strong-1SR）【4,13】。基于两阶段锁定的可串行化实现（请参阅 “两阶段锁定” 一节）或 真的串行执行（请参阅 “真的串行执行”一节）通常是线性一致性的。 但是，可串行化的快照隔离（请参阅 “可串行化快照隔离”）不是线性一致性的：按照设计，它从一致的快照中进行读取，以避免读者和写者之间的锁竞争。一致性快照的要点就在于 它不会包括该快照之后的写入，因此从快照读取不是线性一致性的。 依赖线性一致性线性一致性在什么情况下有用？观看体育比赛的最后得分可能是一个轻率的例子：滞后了几秒钟的结果不太可能在这种情况下造成任何真正的伤害。然而对于少数领域，线性一致性是系统正确工作的一个重要条件。 锁定和领导选举一个使用单主复制的系统，需要确保领导者真的只有一个，而不是几个（脑裂）。一种选择领导者的方法是使用锁：每个节点在启动时尝试获取锁，成功者成为领导者【14】。不管这个锁是如何实现的，它必须是线性一致的：所有节点必须就哪个节点拥有锁达成一致，否则就没用了。 诸如 Apache ZooKeeper 【15】和 etcd 【16】之类的协调服务通常用于实现分布式锁和领导者选举。它们使用一致性算法，以容错的方式实现线性一致的操作（在本章后面的 “容错共识” 中讨论此类算法）[^iii]。还有许多微妙的细节来正确地实现锁和领导者选举（例如，请参阅 “领导者和锁” 中的防护问题），而像 Apache Curator 【17】这样的库则通过在 ZooKeeper 之上提供更高级别的配方来提供帮助。但是，线性一致性存储服务是这些协调任务的基础。 [^iii]: 严格地说，ZooKeeper 和 etcd 提供线性一致性的写操作，但读取可能是陈旧的，因为默认情况下，它们可以由任何一个副本提供服务。你可以选择请求线性一致性读取：etcd 称之为 法定人数读取（quorum read）【16】，而在 ZooKeeper 中，你需要在读取之前调用 sync()【15】。请参阅 “使用全序广播实现线性一致的存储”。 分布式锁也在一些分布式数据库（如 Oracle Real Application Clusters（RAC）【18】）中有更细粒度级别的使用。RAC 对每个磁盘页面使用一个锁，多个节点共享对同一个磁盘存储系统的访问权限。由于这些线性一致的锁处于事务执行的关键路径上，RAC 部署通常具有用于数据库节点之间通信的专用集群互连网络。 约束和唯一性保证唯一性约束在数据库中很常见：例如，用户名或电子邮件地址必须唯一标识一个用户，而在文件存储服务中，不能有两个具有相同路径和文件名的文件。如果要在写入数据时强制执行此约束（例如，如果两个人试图同时创建一个具有相同名称的用户或文件，其中一个将返回一个错误），则需要线性一致性。 这种情况实际上类似于一个锁：当一个用户注册你的服务时，可以认为他们获得了所选用户名的 “锁”。该操作与原子性的比较与设置（CAS）非常相似：将用户名赋予声明它的用户，前提是用户名尚未被使用。 如果想要确保银行账户余额永远不会为负数，或者不会出售比仓库里的库存更多的物品，或者两个人不会都预定了航班或剧院里同一时间的同一个位置。这些约束条件都要求所有节点都同意一个最新的值（账户余额，库存水平，座位占用率）。 在实际应用中，宽松地处理这些限制有时是可以接受的（例如，如果航班超额预订，你可以将客户转移到不同的航班并为其提供补偿）。在这种情况下，可能不需要线性一致性，我们将在 “及时性与完整性” 中讨论这种宽松的约束。 然而，一个硬性的唯一性约束（关系型数据库中常见的那种）需要线性一致性。其他类型的约束，如外键或属性约束，可以不需要线性一致性【19】。 跨信道的时序依赖注意 图 9-1 中的一个细节：如果 Alice 没有惊呼得分，Bob 就不会知道他的查询结果是陈旧的。他会在几秒钟之后再次刷新页面，并最终看到最后的分数。由于系统中存在额外的信道（Alice 的声音传到了 Bob 的耳朵中），线性一致性的违背才被注意到。 计算机系统也会出现类似的情况。例如，假设有一个网站，用户可以上传照片，一个后台进程会调整照片大小，降低分辨率以加快下载速度（缩略图）。该系统的架构和数据流如 图 9-5 所示。 图像缩放器需要明确的指令来执行尺寸缩放作业，指令是 Web 服务器通过消息队列发送的（请参阅 第十一章）。 Web 服务器不会将整个照片放在队列中，因为大多数消息代理都是针对较短的消息而设计的，而一张照片的空间占用可能达到几兆字节。取而代之的是，首先将照片写入文件存储服务，写入完成后再将给缩放器的指令放入消息队列。 图 9-5 Web 服务器和图像缩放器通过文件存储和消息队列进行通信，打开竞争条件的可能性。 如果文件存储服务是线性一致的，那么这个系统应该可以正常工作。如果它不是线性一致的，则存在竞争条件的风险：消息队列（图 9-5 中的步骤 3 和 4）可能比存储服务内部的复制（replication）更快。在这种情况下，当缩放器读取图像（步骤 5）时，可能会看到图像的旧版本，或者什么都没有。如果它处理的是旧版本的图像，则文件存储中的全尺寸图和缩略图就产生了永久性的不一致。 出现这个问题是因为 Web 服务器和缩放器之间存在两个不同的信道：文件存储与消息队列。没有线性一致性的新鲜性保证，这两个信道之间的竞争条件是可能的。这种情况类似于 图 9-1，数据库复制与 Alice 的嘴到 Bob 耳朵之间的真人音频信道之间也存在竞争条件。 线性一致性并不是避免这种竞争条件的唯一方法，但它是最容易理解的。如果你可以控制额外信道（例如消息队列的例子，而不是在 Alice 和 Bob 的例子），则可以使用在 “读己之写” 讨论过的类似方法，不过会有额外的复杂度代价。 实现线性一致的系统我们已经见到了几个线性一致性有用的例子，让我们思考一下，如何实现一个提供线性一致语义的系统。 由于线性一致性本质上意味着 “表现得好像只有一个数据副本，而且所有的操作都是原子的”，所以最简单的答案就是，真的只用一个数据副本。但是这种方法无法容错：如果持有该副本的节点失效，数据将会丢失，或者至少无法访问，直到节点重新启动。 使系统容错最常用的方法是使用复制。我们再来回顾 第五章 中的复制方法，并比较它们是否可以满足线性一致性： 单主复制（可能线性一致） 在具有单主复制功能的系统中（请参阅 “领导者与追随者”），主库具有用于写入的数据的主副本，而追随者在其他节点上保留数据的备份副本。如果从主库或同步更新的从库读取数据，它们 可能（potential） 是线性一致性的 [^iv]。然而，实际上并不是每个单主数据库都是线性一致性的，无论是因为设计的原因（例如，因为使用了快照隔离）还是因为在并发处理上存在错误【10】。 [^iv]: 对单主数据库进行分区（分片），使得每个分区有一个单独的领导者，不会影响线性一致性，因为线性一致性只是对单一对象的保证。 交叉分区事务是一个不同的问题（请参阅 “分布式事务与共识”）。 从主库读取依赖一个假设，你确切地知道领导者是谁。正如在 “真相由多数所定义” 中所讨论的那样，一个节点很可能会认为它是领导者，而事实上并非如此 —— 如果具有错觉的领导者继续为请求提供服务，可能违反线性一致性【20】。使用异步复制，故障切换时甚至可能会丢失已提交的写入（请参阅 “处理节点宕机”），这同时违反了持久性和线性一致性。 共识算法（线性一致） 一些在本章后面讨论的共识算法，与单主复制类似。然而，共识协议包含防止脑裂和陈旧副本的措施。正是由于这些细节，共识算法可以安全地实现线性一致性存储。例如，Zookeeper 【21】和 etcd 【22】就是这样工作的。 多主复制（非线性一致） 具有多主程序复制的系统通常不是线性一致的，因为它们同时在多个节点上处理写入，并将其异步复制到其他节点。因此，它们可能会产生需要被解决的写入冲突（请参阅 “处理写入冲突”）。这种冲突是因为缺少单一数据副本所导致的。 无主复制（也许不是线性一致的） 对于无主复制的系统（Dynamo 风格；请参阅 “无主复制”），有时候人们会声称通过要求法定人数读写（ $w + r &gt; n$ ）可以获得 “强一致性”。这取决于法定人数的具体配置，以及强一致性如何定义（通常不完全正确）。 基于日历时钟（例如，在 Cassandra 中；请参阅 “依赖同步时钟”）的 “最后写入胜利” 冲突解决方法几乎可以确定是非线性一致的，由于时钟偏差，不能保证时钟的时间戳与实际事件顺序一致。宽松的法定人数（请参阅 “宽松的法定人数与提示移交”）也破坏了线性一致的可能性。即使使用严格的法定人数，非线性一致的行为也是可能的，如下节所示。 线性一致性和法定人数直觉上在 Dynamo 风格的模型中，严格的法定人数读写应该是线性一致性的。但是当我们有可变的网络延迟时，就可能存在竞争条件，如 图 9-6 所示。 图 9-6 非线性一致的执行，尽管使用了严格的法定人数 在 图 9-6 中，$x$ 的初始值为 0，写入客户端通过向所有三个副本（ $n &#x3D; 3, w &#x3D; 3$ ）发送写入将 $x$ 更新为 1。客户端 A 并发地从两个节点组成的法定人群（ $r &#x3D; 2$ ）中读取数据，并在其中一个节点上看到新值 1 。客户端 B 也并发地从两个不同的节点组成的法定人数中读取，并从两个节点中取回了旧值 0 。 法定人数条件满足（ $w + r&gt; n$ ），但是这个执行是非线性一致的：B 的请求在 A 的请求完成后开始，但是 B 返回旧值，而 A 返回新值。 （又一次，如同 Alice 和 Bob 的例子 图 9-1） 有趣的是，通过牺牲性能，可以使 Dynamo 风格的法定人数线性化：读取者必须在将结果返回给应用之前，同步执行读修复（请参阅 “读修复和反熵”） ，并且写入者必须在发送写入之前，读取法定数量节点的最新状态【24,25】。然而，由于性能损失，Riak 不执行同步读修复【26】。 Cassandra 在进行法定人数读取时，确实 在等待读修复完成【27】；但是由于使用了最后写入胜利的冲突解决方案，当同一个键有多个并发写入时，将不能保证线性一致性。 而且，这种方式只能实现线性一致的读写；不能实现线性一致的比较和设置（CAS）操作，因为它需要一个共识算法【28】。 总而言之，最安全的做法是：假设采用 Dynamo 风格无主复制的系统不能提供线性一致性。 线性一致性的代价一些复制方法可以提供线性一致性，另一些复制方法则不能，因此深入地探讨线性一致性的优缺点是很有趣的。 我们已经在 第五章 中讨论了不同复制方法的一些用例。例如对多数据中心的复制而言，多主复制通常是理想的选择（请参阅 “运维多个数据中心”）。图 9-7 说明了这种部署的一个例子。 图 9-7 网络中断迫使在线性一致性和可用性之间做出选择。 考虑这样一种情况：如果两个数据中心之间发生网络中断会发生什么？我们假设每个数据中心内的网络正在工作，客户端可以访问数据中心，但数据中心之间彼此无法互相连接。 使用多主数据库，每个数据中心都可以继续正常运行：由于在一个数据中心写入的数据是异步复制到另一个数据中心的，所以在恢复网络连接时，写入操作只是简单地排队并交换。 另一方面，如果使用单主复制，则主库必须位于其中一个数据中心。任何写入和任何线性一致的读取请求都必须发送给该主库，因此对于连接到从库所在数据中心的客户端，这些读取和写入请求必须通过网络同步发送到主库所在的数据中心。 在单主配置的条件下，如果数据中心之间的网络被中断，则连接到从库数据中心的客户端无法联系到主库，因此它们无法对数据库执行任何写入，也不能执行任何线性一致的读取。它们仍能从从库读取，但结果可能是陈旧的（非线性一致）。如果应用需要线性一致的读写，却又位于与主库网络中断的数据中心，则网络中断将导致这些应用不可用。 如果客户端可以直接连接到主库所在的数据中心，这就不是问题了，那些应用可以继续正常工作。但只能访问从库数据中心的客户端会中断运行，直到网络连接得到修复。 CAP定理这个问题不仅仅是单主复制和多主复制的后果：任何线性一致的数据库都有这个问题，不管它是如何实现的。这个问题也不仅仅局限于多数据中心部署，而可能发生在任何不可靠的网络上，即使在同一个数据中心内也是如此。问题面临的权衡如下：[^v] 如果应用需要线性一致性，且某些副本因为网络问题与其他副本断开连接，那么这些副本掉线时不能处理请求。请求必须等到网络问题解决，或直接返回错误。（无论哪种方式，服务都 不可用）。 如果应用不需要线性一致性，那么某个副本即使与其他副本断开连接，也可以独立处理请求（例如多主复制）。在这种情况下，应用可以在网络问题解决前保持可用，但其行为不是线性一致的。 [^v]: 这两种选择有时分别称为 CP（在网络分区下一致但不可用）和 AP（在网络分区下可用但不一致）。 但是，这种分类方案存在一些缺陷【9】，所以最好不要这样用。 因此，不需要线性一致性的应用对网络问题有更强的容错能力。这种见解通常被称为 CAP 定理【29,30,31,32】，由 Eric Brewer 于 2000 年命名，尽管 70 年代的分布式数据库设计者早就知道了这种权衡【33,34,35,36】。 CAP 最初是作为一个经验法则提出的，没有准确的定义，目的是开始讨论数据库的权衡。那时候许多分布式数据库侧重于在共享存储的集群上提供线性一致性的语义【18】，CAP 定理鼓励数据库工程师向分布式无共享系统的设计领域深入探索，这类架构更适合实现大规模的网络服务【37】。 对于这种文化上的转变，CAP 值得赞扬 —— 它见证了自 00 年代中期以来新数据库的技术爆炸（即 NoSQL）。 CAP定理没有帮助 CAP 有时以这种面目出现：一致性，可用性和分区容错性：三者只能择其二。不幸的是这种说法很有误导性【32】，因为网络分区是一种故障类型，所以它并不是一个选项：不管你喜不喜欢它都会发生【38】。 在网络正常工作的时候，系统可以提供一致性（线性一致性）和整体可用性。发生网络故障时，你必须在线性一致性和整体可用性之间做出选择。因此，CAP 更好的表述成：在分区时要么选择一致，要么选择可用【39】。一个更可靠的网络需要减少这个选择，但是在某些时候选择是不可避免的。 在 CAP 的讨论中，术语可用性有几个相互矛盾的定义，形式化作为一个定理【30】并不符合其通常的含义【40】。许多所谓的 “高可用”（容错）系统实际上不符合 CAP 对可用性的特殊定义。总而言之，围绕着 CAP 有很多误解和困惑，并不能帮助我们更好地理解系统，所以最好避免使用 CAP。 CAP 定理的正式定义仅限于很狭隘的范围【30】，它只考虑了一个一致性模型（即线性一致性）和一种故障（网络分区 [^vi]，或活跃但彼此断开的节点）。它没有讨论任何关于网络延迟，死亡节点或其他权衡的事。 因此，尽管 CAP 在历史上有一些影响力，但对于设计系统而言并没有实际价值【9,40】。 在分布式系统中有更多有趣的 “不可能” 的结果【41】，且 CAP 定理现在已经被更精确的结果取代【2,42】，所以它现在基本上成了历史古迹了。 [^vi]: 正如 “真实世界的网络故障” 中所讨论的，本书使用 分区（partition） 指代将大数据集细分为小数据集的操作（分片；请参阅 第六章）。与之对应的是，网络分区（network partition） 是一种特定类型的网络故障，我们通常不会将其与其他类型的故障分开考虑。但是，由于它是 CAP 的 P，所以这种情况下我们无法避免混乱。 线性一致性和网络延迟虽然线性一致是一个很有用的保证，但实际上，线性一致的系统惊人的少。例如，现代多核 CPU 上的内存甚至都不是线性一致的【43】：如果一个 CPU 核上运行的线程写入某个内存地址，而另一个 CPU 核上运行的线程不久之后读取相同的地址，并没有保证一定能读到第一个线程写入的值（除非使用了 内存屏障（memory barrier） 或 围栏（fence）【44】）。 这种行为的原因是每个 CPU 核都有自己的内存缓存和存储缓冲区。默认情况下，内存访问首先走缓存，任何变更会异步写入主存。因为缓存访问比主存要快得多【45】，所以这个特性对于现代 CPU 的良好性能表现至关重要。但是现在就有几个数据副本（一个在主存中，也许还有几个在不同缓存中的其他副本），而且这些副本是异步更新的，所以就失去了线性一致性。 为什么要做这个权衡？对多核内存一致性模型而言，CAP 定理是没有意义的：在同一台计算机中，我们通常假定通信都是可靠的。并且我们并不指望一个 CPU 核能在脱离计算机其他部分的条件下继续正常工作。牺牲线性一致性的原因是 性能（performance），而不是容错。 许多分布式数据库也是如此：它们是 为了提高性能 而选择了牺牲线性一致性，而不是为了容错【46】。线性一致的速度很慢 —— 这始终是事实，而不仅仅是网络故障期间。 能找到一个更高效的线性一致存储实现吗？看起来答案是否定的：Attiya 和 Welch 【47】证明，如果你想要线性一致性，读写请求的响应时间至少与网络延迟的不确定性成正比。在像大多数计算机网络一样具有高度可变延迟的网络中（请参阅 “超时与无穷的延迟”），线性读写的响应时间不可避免地会很高。更快地线性一致算法不存在，但更弱的一致性模型可以快得多，所以对延迟敏感的系统而言，这类权衡非常重要。在 第十二章 中将讨论一些在不牺牲正确性的前提下，绕开线性一致性的方法。 顺序保证之前说过，线性一致寄存器的行为就好像只有单个数据副本一样，且每个操作似乎都是在某个时间点以原子性的方式生效的。这个定义意味着操作是按照某种良好定义的顺序执行的。我们将操作以看上去被执行的顺序连接起来，以此说明了 图 9-4 中的顺序。 顺序（ordering） 这一主题在本书中反复出现，这表明它可能是一个重要的基础性概念。让我们简要回顾一下其它曾经出现过 顺序 的上下文： 在 第五章 中我们看到，领导者在单主复制中的主要目的就是，在复制日志中确定 写入顺序（order of write）—— 也就是从库应用这些写入的顺序。如果不存在一个领导者，则并发操作可能导致冲突（请参阅 “处理写入冲突”）。 在 第七章 中讨论的 可串行化，是关于事务表现的像按 某种先后顺序（some sequential order） 执行的保证。它可以字面意义上地以 串行顺序（serial order） 执行事务来实现，或者允许并行执行，但同时防止序列化冲突来实现（通过锁或中止事务）。 在 第八章 讨论过的在分布式系统中使用时间戳和时钟（请参阅 “依赖同步时钟”）是另一种将顺序引入无序世界的尝试，例如，确定两个写入操作哪一个更晚发生。 事实证明，顺序、线性一致性和共识之间有着深刻的联系。尽管这个概念比本书其他部分更加理论化和抽象，但对于明确系统的能力范围（可以做什么和不可以做什么）而言是非常有帮助的。我们将在接下来的几节中探讨这个话题。 顺序与因果关系顺序 反复出现有几个原因，其中一个原因是，它有助于保持 因果关系（causality）。在本书中我们已经看到了几个例子，其中因果关系是很重要的： 在 “一致前缀读”（图 5-5）中，我们看到一个例子：一个对话的观察者首先看到问题的答案，然后才看到被回答的问题。这是令人困惑的，因为它违背了我们对 因（cause） 与 果（effect） 的直觉：如果一个问题被回答，显然问题本身得先在那里，因为给出答案的人必须先看到这个问题（假如他们并没有预见未来的超能力）。我们认为在问题和答案之间存在 因果依赖（causal dependency）。 图 5-9 中出现了类似的模式，我们看到三位领导者之间的复制，并注意到由于网络延迟，一些写入可能会 “压倒” 其他写入。从其中一个副本的角度来看，好像有一个对尚不存在的记录的更新操作。这里的因果意味着，一条记录必须先被创建，然后才能被更新。 在 “检测并发写入” 中我们观察到，如果有两个操作 A 和 B，则存在三种可能性：A 发生在 B 之前，或 B 发生在 A 之前，或者 A 和 B并发。这种 此前发生（happened before） 关系是因果关系的另一种表述：如果 A 在 B 前发生，那么意味着 B 可能已经知道了 A，或者建立在 A 的基础上，或者依赖于 A。如果 A 和 B 是 并发 的，那么它们之间并没有因果联系；换句话说，我们确信 A 和 B 不知道彼此。 在事务快照隔离的上下文中（“快照隔离和可重复读”），我们说事务是从一致性快照中读取的。但此语境中 “一致” 到底又是什么意思？这意味着 与因果关系保持一致（consistent with causality）：如果快照包含答案，它也必须包含被回答的问题【48】。在某个时间点观察整个数据库，与因果关系保持一致意味着：因果上在该时间点之前发生的所有操作，其影响都是可见的，但因果上在该时间点之后发生的操作，其影响对观察者不可见。读偏差（read skew） 意味着读取的数据处于违反因果关系的状态（不可重复读，如 图 7-6 所示）。 事务之间 写偏差（write skew） 的例子（请参阅 “写入偏差与幻读”）也说明了因果依赖：在 图 7-8 中，爱丽丝被允许离班，因为事务认为鲍勃仍在值班，反之亦然。在这种情况下，离班的动作因果依赖于对当前值班情况的观察。可串行化快照隔离 通过跟踪事务之间的因果依赖来检测写偏差。 在爱丽丝和鲍勃看球的例子中（图 9-1），在听到爱丽丝惊呼比赛结果后，鲍勃从服务器得到陈旧结果的事实违背了因果关系：爱丽丝的惊呼因果依赖于得分宣告，所以鲍勃应该也能在听到爱丽斯惊呼后查询到比分。相同的模式在 “跨信道的时序依赖” 一节中，以 “图像大小调整服务” 的伪装再次出现。 因果关系对事件施加了一种 顺序：因在果之前；消息发送在消息收取之前。而且就像现实生活中一样，一件事会导致另一件事：某个节点读取了一些数据然后写入一些结果，另一个节点读取其写入的内容，并依次写入一些其他内容，等等。这些因果依赖的操作链定义了系统中的因果顺序，即，什么在什么之前发生。 如果一个系统服从因果关系所规定的顺序，我们说它是 因果一致（causally consistent） 的。例如，快照隔离提供了因果一致性：当你从数据库中读取到一些数据时，你一定还能够看到其因果前驱（假设在此期间这些数据还没有被删除）。 因果顺序不是全序的全序（total order） 允许任意两个元素进行比较，所以如果有两个元素，你总是可以说出哪个更大，哪个更小。例如，自然数集是全序的：给定两个自然数，比如说 5 和 13，那么你可以告诉我，13 大于 5。 然而数学集合并不完全是全序的：&#123;a, b&#125; 比 &#123;b, c&#125; 更大吗？好吧，你没法真正比较它们，因为二者都不是对方的子集。我们说它们是 无法比较（incomparable） 的，因此数学集合是 偏序（partially order） 的：在某些情况下，可以说一个集合大于另一个（如果一个集合包含另一个集合的所有元素），但在其他情况下它们是无法比较的 [^译注i]。 [^译注i]: 设 R 为非空集合 A 上的关系，如果 R 是自反的、反对称的和可传递的，则称 R 为 A 上的偏序关系。简称偏序，通常记作≦。一个集合 A 与 A 上的偏序关系 R 一起叫作偏序集，记作 $(A,R)$ 或 $(A, ≦)$。全序、偏序、关系、集合，这些概念的精确定义可以参考任意一本离散数学教材。 全序和偏序之间的差异反映在不同的数据库一致性模型中： 线性一致性 在线性一致的系统中，操作是全序的：如果系统表现的就好像只有一个数据副本，并且所有操作都是原子性的，这意味着对任何两个操作，我们总是能判定哪个操作先发生。这个全序在 图 9-4 中以时间线表示。 因果性 我们说过，如果两个操作都没有在彼此 之前发生，那么这两个操作是并发的（请参阅 “此前发生” 的关系和并发）。换句话说，如果两个事件是因果相关的（一个发生在另一个事件之前），则它们之间是有序的，但如果它们是并发的，则它们之间的顺序是无法比较的。这意味着因果关系定义了一个偏序，而不是一个全序：一些操作相互之间是有顺序的，但有些则是无法比较的。 因此，根据这个定义，在线性一致的数据存储中是不存在并发操作的：必须有且仅有一条时间线，所有的操作都在这条时间线上，构成一个全序关系。可能有几个请求在等待处理，但是数据存储确保了每个请求都是在唯一时间线上的某个时间点自动处理的，不存在任何并发。 并发意味着时间线会分岔然后合并 —— 在这种情况下，不同分支上的操作是无法比较的（即并发操作）。在 第五章 中我们看到了这种现象：例如，图 5-14 并不是一条直线的全序关系，而是一堆不同的操作并发进行。图中的箭头指明了因果依赖 —— 操作的偏序。 如果你熟悉像 Git 这样的分布式版本控制系统，那么其版本历史与因果关系图极其相似。通常，一个 提交（Commit） 发生在另一个提交之后，在一条直线上。但是有时你会遇到分支（当多个人同时在一个项目上工作时），合并（Merge） 会在这些并发创建的提交相融合时创建。 线性一致性强于因果一致性那么因果顺序和线性一致性之间的关系是什么？答案是线性一致性 隐含着（implies） 因果关系：任何线性一致的系统都能正确保持因果性【7】。特别是，如果系统中有多个通信通道（如 图 9-5 中的消息队列和文件存储服务），线性一致性可以自动保证因果性，系统无需任何特殊操作（如在不同组件间传递时间戳）。 线性一致性确保因果性的事实使线性一致系统变得简单易懂，更有吸引力。然而，正如 “线性一致性的代价” 中所讨论的，使系统线性一致可能会损害其性能和可用性，尤其是在系统具有严重的网络延迟的情况下（例如，如果系统在地理上散布）。出于这个原因，一些分布式数据系统已经放弃了线性一致性，从而获得更好的性能，但它们用起来也更为困难。 好消息是存在折衷的可能性。线性一致性并不是保持因果性的唯一途径 —— 还有其他方法。一个系统可以是因果一致的，而无需承担线性一致带来的性能折损（尤其对于 CAP 定理不适用的情况）。实际上在所有的不会被网络延迟拖慢的一致性模型中，因果一致性是可行的最强的一致性模型。而且在网络故障时仍能保持可用【2,42】。 在许多情况下，看上去需要线性一致性的系统，实际上需要的只是因果一致性，因果一致性可以更高效地实现。基于这种观察结果，研究人员正在探索新型的数据库，既能保证因果一致性，且性能与可用性与最终一致的系统类似【49,50,51】。 这方面的研究相当新鲜，其中很多尚未应用到生产系统，仍然有不少挑战需要克服【52,53】。但对于未来的系统而言，这是一个有前景的方向。 捕获因果关系我们不会在这里讨论非线性一致的系统如何保证因果性的细节，而只是简要地探讨一些关键的思想。 为了维持因果性，你需要知道哪个操作发生在哪个其他操作之前（happened before）。这是一个偏序：并发操作可以以任意顺序进行，但如果一个操作发生在另一个操作之前，那它们必须在所有副本上以那个顺序被处理。因此，当一个副本处理一个操作时，它必须确保所有因果前驱的操作（之前发生的所有操作）已经被处理；如果前面的某个操作丢失了，后面的操作必须等待，直到前面的操作被处理完毕。 为了确定因果依赖，我们需要一些方法来描述系统中节点的 “知识”。如果节点在发出写入 Y 的请求时已经看到了 X 的值，则 X 和 Y 可能存在因果关系。这个分析使用了那些在欺诈指控刑事调查中常见的问题：CEO 在做出决定 Y 时是否 知道 X ？ 用于确定 哪些操作发生在其他操作之前 的技术，与我们在 “检测并发写入” 中所讨论的内容类似。那一节讨论了无领导者数据存储中的因果性：为了防止丢失更新，我们需要检测到对同一个键的并发写入。因果一致性则更进一步：它需要跟踪整个数据库中的因果依赖，而不仅仅是一个键。可以推广版本向量以解决此类问题【54】。 为了确定因果顺序，数据库需要知道应用读取了哪个版本的数据。这就是为什么在 图 5-13 中，来自先前操作的版本号在写入时被传回到数据库的原因。在 SSI 的冲突检测中会出现类似的想法，如 “可串行化快照隔离” 中所述：当事务要提交时，数据库将检查它所读取的数据版本是否仍然是最新的。为此，数据库跟踪哪些数据被哪些事务所读取。 序列号顺序虽然因果是一个重要的理论概念，但实际上跟踪所有的因果关系是不切实际的。在许多应用中，客户端在写入内容之前会先读取大量数据，我们无法弄清写入因果依赖于先前全部的读取内容，还是仅包括其中一部分。显式跟踪所有已读数据意味着巨大的额外开销。 但还有一个更好的方法：我们可以使用 序列号（sequence nunber） 或 时间戳（timestamp） 来排序事件。时间戳不一定来自日历时钟（或物理时钟，它们存在许多问题，如 “不可靠的时钟” 中所述）。它可以来自一个 逻辑时钟（logical clock），这是一个用来生成标识操作的数字序列的算法，典型实现是使用一个每次操作自增的计数器。 这样的序列号或时间戳是紧凑的（只有几个字节大小），它提供了一个全序关系：也就是说每个操作都有一个唯一的序列号，而且总是可以比较两个序列号，确定哪一个更大（即哪些操作后发生）。 特别是，我们可以使用 与因果一致（consistent with causality） 的全序来生成序列号 [^vii]：我们保证，如果操作 A 因果地发生在操作 B 前，那么在这个全序中 A 在 B 前（ A 具有比 B 更小的序列号）。并行操作之间可以任意排序。这样一个全序关系捕获了所有关于因果的信息，但也施加了一个比因果性要求更为严格的顺序。 [^vii]: 与因果关系不一致的全序很容易创建，但没啥用。例如你可以为每个操作生成随机的 UUID，并按照字典序比较 UUID，以定义操作的全序。这是一个有效的全序，但是随机的 UUID 并不能告诉你哪个操作先发生，或者操作是否为并发的。 在单主复制的数据库中（请参阅 “领导者与追随者”），复制日志定义了与因果一致的写操作。主库可以简单地为每个操作自增一个计数器，从而为复制日志中的每个操作分配一个单调递增的序列号。如果一个从库按照它们在复制日志中出现的顺序来应用写操作，那么从库的状态始终是因果一致的（即使它落后于领导者）。 非因果序列号生成器如果主库不存在（可能因为使用了多主数据库或无主数据库，或者因为使用了分区的数据库），如何为操作生成序列号就没有那么明显了。在实践中有各种各样的方法： 每个节点都可以生成自己独立的一组序列号。例如有两个节点，一个节点只能生成奇数，而另一个节点只能生成偶数。通常，可以在序列号的二进制表示中预留一些位，用于唯一的节点标识符，这样可以确保两个不同的节点永远不会生成相同的序列号。可以将日历时钟（物理时钟）的时间戳附加到每个操作上【55】。这种时间戳并不连续，但是如果它具有足够高的分辨率，那也许足以提供一个操作的全序关系。这一事实应用于 最后写入胜利 * 的冲突解决方法中（请参阅 “有序事件的时间戳”）。 可以预先分配序列号区块。例如，节点 A 可能要求从序列号 1 到 1,000 区块的所有权，而节点 B 可能要求序列号 1,001 到 2,000 区块的所有权。然后每个节点可以独立分配所属区块中的序列号，并在序列号告急时请求分配一个新的区块。 这三个选项都比单一主库的自增计数器表现要好，并且更具可伸缩性。它们为每个操作生成一个唯一的，近似自增的序列号。然而它们都有同一个问题：生成的序列号与因果不一致。 因为这些序列号生成器不能正确地捕获跨节点的操作顺序，所以会出现因果关系的问题： 每个节点每秒可以处理不同数量的操作。因此，如果一个节点产生偶数序列号而另一个产生奇数序列号，则偶数计数器可能落后于奇数计数器，反之亦然。如果你有一个奇数编号的操作和一个偶数编号的操作，你无法准确地说出哪一个操作在因果上先发生。 来自物理时钟的时间戳会受到时钟偏移的影响，这可能会使其与因果不一致。例如 图 8-3 展示了一个例子，其中因果上晚发生的操作，却被分配了一个更早的时间戳。[^vii] [^viii]: 可以使物理时钟时间戳与因果关系保持一致：在 “全局快照的同步时钟” 中，我们讨论了 Google 的 Spanner，它可以估计预期的时钟偏差，并在提交写入之前等待不确定性间隔。这种方法确保了实际上靠后的事务会有更大的时间戳。但是大多数时钟不能提供这种所需的不确定性度量。 在分配区块的情况下，某个操作可能会被赋予一个范围在 1,001 到 2,000 内的序列号，然而一个因果上更晚的操作可能被赋予一个范围在 1 到 1,000 之间的数字。这里序列号与因果关系也是不一致的。 兰伯特时间戳尽管刚才描述的三个序列号生成器与因果不一致，但实际上有一个简单的方法来产生与因果关系一致的序列号。它被称为兰伯特时间戳，莱斯利・兰伯特（Leslie Lamport）于 1978 年提出【56】，现在是分布式系统领域中被引用最多的论文之一。 图 9-8 说明了兰伯特时间戳的应用。每个节点都有一个唯一标识符，和一个保存自己执行操作数量的计数器。 兰伯特时间戳就是两者的简单组合：（计数器，节点 ID）$(counter, node ID)$。两个节点有时可能具有相同的计数器值，但通过在时间戳中包含节点 ID，每个时间戳都是唯一的。 图 9-8 Lamport 时间戳提供了与因果关系一致的全序。 兰伯特时间戳与物理的日历时钟没有任何关系，但是它提供了一个全序：如果你有两个时间戳，则 计数器 值大者是更大的时间戳。如果计数器值相同，则节点 ID 越大的，时间戳越大。 迄今，这个描述与上节所述的奇偶计数器基本类似。使兰伯特时间戳因果一致的关键思想如下所示：每个节点和每个客户端跟踪迄今为止所见到的最大 计数器 值，并在每个请求中包含这个最大计数器值。当一个节点收到最大计数器值大于自身计数器值的请求或响应时，它立即将自己的计数器设置为这个最大值。 这如 图 9-8 所示，其中客户端 A 从节点 2 接收计数器值 5 ，然后将最大值 5 发送到节点 1 。此时，节点 1 的计数器仅为 1 ，但是它立即前移至 5 ，所以下一个操作的计数器的值为 6 。 只要每一个操作都携带着最大计数器值，这个方案确保兰伯特时间戳的排序与因果一致，因为每个因果依赖都会导致时间戳增长。 兰伯特时间戳有时会与我们在 “检测并发写入” 中看到的版本向量相混淆。虽然两者有一些相似之处，但它们有着不同的目的：版本向量可以区分两个操作是并发的，还是一个因果依赖另一个；而兰伯特时间戳总是施行一个全序。从兰伯特时间戳的全序中，你无法分辨两个操作是并发的还是因果依赖的。 兰伯特时间戳优于版本向量的地方是，它更加紧凑。 光有时间戳排序还不够虽然兰伯特时间戳定义了一个与因果一致的全序，但它还不足以解决分布式系统中的许多常见问题。 例如，考虑一个需要确保用户名能唯一标识用户帐户的系统。如果两个用户同时尝试使用相同的用户名创建帐户，则其中一个应该成功，另一个应该失败（我们之前在 “领导者和锁” 中提到过这个问题）。 乍看之下，似乎操作的全序关系足以解决这一问题（例如使用兰伯特时间戳）：如果创建了两个具有相同用户名的帐户，选择时间戳较小的那个作为胜者（第一个抓到用户名的人），并让带有更大时间戳者失败。由于时间戳上有全序关系，所以这个比较总是可行的。 这种方法适用于事后确定胜利者：一旦你收集了系统中的所有用户名创建操作，就可以比较它们的时间戳。然而当某个节点需要实时处理用户创建用户名的请求时，这样的方法就无法满足了。节点需要 马上（right now） 决定这个请求是成功还是失败。在那个时刻，节点并不知道是否存在其他节点正在并发执行创建同样用户名的操作，罔论其它节点可能分配给那个操作的时间戳。 为了确保没有其他节点正在使用相同的用户名和较小的时间戳并发创建同名账户，你必须检查其它每个节点，看看它在做什么【56】。如果其中一个节点由于网络问题出现故障或不可达，则整个系统可能被拖至停机。这不是我们需要的那种容错系统。 这里的问题是，只有在所有的操作都被收集之后，操作的全序才会出现。如果另一个节点已经产生了一些操作，但你还不知道那些操作是什么，那就无法构造所有操作最终的全序关系：来自另一个节点的未知操作可能需要被插入到全序中的不同位置。 总之：为了实现诸如用户名上的唯一约束这种东西，仅有操作的全序是不够的，你还需要知道这个全序何时会尘埃落定。如果你有一个创建用户名的操作，并且确定在全序中没有任何其他节点可以在你的操作之前插入对同一用户名的声称，那么你就可以安全地宣告操作执行成功。 如何确定全序关系已经尘埃落定，这将在 全序广播 一节中详细说明。 全序广播如果你的程序只运行在单个 CPU 核上，那么定义一个操作全序是很容易的：可以简单认为就是 CPU 执行这些操作的顺序。但是在分布式系统中，让所有节点对同一个全局操作顺序达成一致可能相当棘手。在上一节中，我们讨论了按时间戳或序列号进行排序，但发现它还不如单主复制给力（如果你使用时间戳排序来实现唯一性约束，就不能容忍任何错误，因为你必须要从每个节点都获取到最新的序列号）。 如前所述，单主复制通过选择一个节点作为主库来确定操作的全序，并在主库的单个 CPU 核上对所有操作进行排序。接下来的挑战是，如果吞吐量超出单个主库的处理能力，这种情况下如何扩展系统；以及，如果主库失效（“处理节点宕机”），如何处理故障切换。在分布式系统文献中，这个问题被称为 全序广播（total order broadcast） 或 原子广播（atomic broadcast）[^ix]【25,57,58】。 [^ix]: “原子广播” 是一个传统的术语，非常混乱，而且与 “原子” 一词的其他用法不一致：它与 ACID 事务中的原子性没有任何关系，只是与原子操作（在多线程编程的意义上 ）或原子寄存器（线性一致存储）有间接的联系。全序组播（total order multicast）是另一个同义词。 顺序保证的范围 每个分区各有一个主库的分区数据库，通常只在每个分区内维持顺序，这意味着它们不能提供跨分区的一致性保证（例如，一致性快照，外键引用）。 跨所有分区的全序是可能的，但需要额外的协调【59】。 全序广播通常被描述为在节点间交换消息的协议。 非正式地讲，它要满足两个安全属性： 可靠交付（reliable delivery） 没有消息丢失：如果消息被传递到一个节点，它将被传递到所有节点。 全序交付（totally ordered delivery） 消息以相同的顺序传递给每个节点。 正确的全序广播算法必须始终保证可靠性和有序性，即使节点或网络出现故障。当然在网络中断的时候，消息是传不出去的，但是算法可以不断重试，以便在网络最终修复时，消息能及时通过并送达（当然它们必须仍然按照正确的顺序传递）。 使用全序广播像 ZooKeeper 和 etcd 这样的共识服务实际上实现了全序广播。这一事实暗示了全序广播与共识之间有着紧密联系，我们将在本章稍后进行探讨。 全序广播正是数据库复制所需的：如果每个消息都代表一次数据库的写入，且每个副本都按相同的顺序处理相同的写入，那么副本间将相互保持一致（除了临时的复制延迟）。这个原理被称为 状态机复制（state machine replication）【60】，我们将在 第十一章 中重新回到这个概念。 与之类似，可以使用全序广播来实现可串行化的事务：如 “真的串行执行” 中所述，如果每个消息都表示一个确定性事务，以存储过程的形式来执行，且每个节点都以相同的顺序处理这些消息，那么数据库的分区和副本就可以相互保持一致【61】。 全序广播的一个重要表现是，顺序在消息送达时被固化：如果后续的消息已经送达，节点就不允许追溯地将（先前）消息插入顺序中的较早位置。这个事实使得全序广播比时间戳排序更强。 考量全序广播的另一种方式是，这是一种创建日志的方式（如在复制日志、事务日志或预写式日志中）：传递消息就像追加写入日志。由于所有节点必须以相同的顺序传递相同的消息，因此所有节点都可以读取日志，并看到相同的消息序列。 全序广播对于实现提供防护令牌的锁服务也很有用（请参阅 “防护令牌”）。每个获取锁的请求都作为一条消息追加到日志末尾，并且所有的消息都按它们在日志中出现的顺序依次编号。序列号可以当成防护令牌用，因为它是单调递增的。在 ZooKeeper 中，这个序列号被称为 zxid 【15】。 使用全序广播实现线性一致的存储如 图 9-4 所示，在线性一致的系统中，存在操作的全序。这是否意味着线性一致与全序广播一样？不尽然，但两者之间有着密切的联系 [^x]。 [^x]: 从形式上讲，线性一致读写寄存器是一个 “更容易” 的问题。 全序广播等价于共识【67】，而共识问题在异步的崩溃 - 停止模型【68】中没有确定性的解决方案，而线性一致的读写寄存器 可以 在这种模型中实现【23,24,25】。 然而，支持诸如 比较并设置（CAS, compare-and-set），或 自增并返回（increment-and-get） 的原子操作使它等价于共识问题【28】。 因此，共识问题与线性一致寄存器问题密切相关。 全序广播是异步的：消息被保证以固定的顺序可靠地传送，但是不能保证消息 何时 被送达（所以一个接收者可能落后于其他接收者）。相比之下，线性一致性是新鲜性的保证：读取一定能看见最新的写入值。 但如果有了全序广播，你就可以在此基础上构建线性一致的存储。例如，你可以确保用户名能唯一标识用户帐户。 设想对于每一个可能的用户名，你都可以有一个带有 CAS 原子操作的线性一致寄存器。每个寄存器最初的值为空值（表示未使用该用户名）。当用户想要创建一个用户名时，对该用户名的寄存器执行 CAS 操作，在先前寄存器值为空的条件，将其值设置为用户的账号 ID。如果多个用户试图同时获取相同的用户名，则只有一个 CAS 操作会成功，因为其他用户会看到非空的值（由于线性一致性）。 你可以通过将全序广播当成仅追加日志【62,63】的方式来实现这种线性一致的 CAS 操作： 在日志中追加一条消息，试探性地指明你要声明的用户名。 读日志，并等待你刚才追加的消息被读回。[^xi] 检查是否有任何消息声称目标用户名的所有权。如果这些消息中的第一条就是你自己的消息，那么你就成功了：你可以提交声称的用户名（也许是通过向日志追加另一条消息）并向客户端确认。如果所需用户名的第一条消息来自其他用户，则中止操作。 [^xi]: 如果你不等待，而是在消息入队之后立即确认写入，则会得到类似于多核 x86 处理器内存的一致性模型【43】。 该模型既不是线性一致的也不是顺序一致的。 由于日志项是以相同顺序送达至所有节点，因此如果有多个并发写入，则所有节点会对最先到达者达成一致。选择冲突写入中的第一个作为胜利者，并中止后来者，以此确定所有节点对某个写入是提交还是中止达成一致。类似的方法可以在一个日志的基础上实现可串行化的多对象事务【62】。 尽管这一过程保证写入是线性一致的，但它并不保证读取也是线性一致的 —— 如果你从与日志异步更新的存储中读取数据，结果可能是陈旧的。 （精确地说，这里描述的过程提供了 顺序一致性（sequential consistency）【47,64】，有时也称为 时间线一致性（timeline consistency）【65,66】，比线性一致性稍微弱一些的保证）。为了使读取也线性一致，有几个选项： 你可以通过在日志中追加一条消息，然后读取日志，直到该消息被读回才执行实际的读取操作。消息在日志中的位置因此定义了读取发生的时间点（etcd 的法定人数读取有些类似这种情况【16】）。 如果日志允许以线性一致的方式获取最新日志消息的位置，则可以查询该位置，等待该位置前的所有消息都传达到你，然后执行读取。 （这是 Zookeeper sync() 操作背后的思想【15】）。 你可以从同步更新的副本中进行读取，因此可以确保结果是最新的（这种技术用于链式复制（chain replication）【63】；请参阅 “关于复制的研究”）。 使用线性一致性存储实现全序广播上一节介绍了如何从全序广播构建一个线性一致的 CAS 操作。我们也可以把它反过来，假设我们有线性一致的存储，接下来会展示如何在此基础上构建全序广播。 最简单的方法是假设你有一个线性一致的寄存器来存储一个整数，并且有一个原子 自增并返回 操作【28】。或者原子 CAS 操作也可以完成这项工作。 该算法很简单：每个要通过全序广播发送的消息首先对线性一致寄存器执行 自增并返回 操作。然后将从寄存器获得的值作为序列号附加到消息中。然后你可以将消息发送到所有节点（重新发送任何丢失的消息），而收件人将按序列号依序传递（deliver）消息。 请注意，与兰伯特时间戳不同，通过自增线性一致性寄存器获得的数字形式上是一个没有间隙的序列。因此，如果一个节点已经发送了消息 4 并且接收到序列号为 6 的传入消息，则它知道它在传递消息 6 之前必须等待消息 5 。兰伯特时间戳则与之不同 —— 事实上，这是全序广播和时间戳排序间的关键区别。 实现一个带有原子性 自增并返回 操作的线性一致寄存器有多困难？像往常一样，如果事情从来不出差错，那很容易：你可以简单地把它保存在单个节点内的变量中。问题在于处理当该节点的网络连接中断时的情况，并在该节点失效时能恢复这个值【59】。一般来说，如果你对线性一致性的序列号生成器进行过足够深入的思考，你不可避免地会得出一个共识算法。 这并非巧合：可以证明，线性一致的 CAS（或自增并返回）寄存器与全序广播都等价于 共识 问题【28,67】。也就是说，如果你能解决其中的一个问题，你可以把它转化成为其他问题的解决方案。这是相当深刻和令人惊讶的洞察！ 现在是时候正面处理共识问题了，我们将在本章的其余部分进行讨论。 分布式事务与共识共识 是分布式计算中最重要也是最基本的问题之一。从表面上看似乎很简单：非正式地讲，目标只是 让几个节点达成一致（get serveral nodes to agree on something）。你也许会认为这不会太难。不幸的是，许多出故障的系统都是因为错误地轻信这个问题很容易解决。 尽管共识非常重要，但关于它的内容出现在本书的后半部分，因为这个主题非常微妙，欣赏细微之处需要一些必要的知识。即使在学术界，对共识的理解也是在几十年的过程中逐渐沉淀而来，一路上也有着许多误解。现在我们已经讨论了复制（第五章），事务（第七章），系统模型（第八章），线性一致以及全序广播（本章），我们终于准备好解决共识问题了。 节点能达成一致，在很多场景下都非常重要，例如： 领导选举 在单主复制的数据库中，所有节点需要就哪个节点是领导者达成一致。如果一些节点由于网络故障而无法与其他节点通信，则可能会对领导权的归属引起争议。在这种情况下，共识对于避免错误的故障切换非常重要。错误的故障切换会导致两个节点都认为自己是领导者（脑裂，请参阅 “处理节点宕机”）。如果有两个领导者，它们都会接受写入，它们的数据会发生分歧，从而导致不一致和数据丢失。 原子提交 在支持跨多节点或跨多分区事务的数据库中，一个事务可能在某些节点上失败，但在其他节点上成功。如果我们想要维护事务的原子性（就 ACID 而言，请参阅 “原子性”），我们必须让所有节点对事务的结果达成一致：要么全部中止 &#x2F; 回滚（如果出现任何错误），要么它们全部提交（如果没有出错）。这个共识的例子被称为 原子提交（atomic commit） 问题 [^xii]。 [^xii]: 原子提交的形式化与共识稍有不同：原子事务只有在 所有 参与者投票提交的情况下才能提交，如果有任何参与者需要中止，则必须中止。 共识则允许就 任意一个 被参与者提出的候选值达成一致。 然而，原子提交和共识可以相互简化为对方【70,71】。 非阻塞 原子提交则要比共识更为困难 —— 请参阅 “三阶段提交”。 共识的不可能性 你可能已经听说过以作者 Fischer，Lynch 和 Paterson 命名的 FLP 结果【68】，它证明，如果存在节点可能崩溃的风险，则不存在 总是 能够达成共识的算法。在分布式系统中，我们必须假设节点可能会崩溃，所以可靠的共识是不可能的。然而这里我们正在讨论达成共识的算法，到底是怎么回事？ 答案是 FLP 结果是在 异步系统模型 中被证明的（请参阅 “系统模型与现实”），而这是一种限制性很强的模型，它假定确定性算法不能使用任何时钟或超时。如果允许算法使用 超时 或其他方法来识别可疑的崩溃节点（即使怀疑有时是错误的），则共识变为一个可解的问题【67】。即使仅仅允许算法使用随机数，也足以绕过这个不可能的结果【69】。 因此，虽然 FLP 是关于共识不可能性的重要理论结果，但现实中的分布式系统通常是可以达成共识的。 在本节中，我们将首先更详细地研究 原子提交 问题。具体来说，我们将讨论 两阶段提交（2PC, two-phase commit） 算法，这是解决原子提交问题最常见的办法，并在各种数据库、消息队列和应用服务器中被实现。事实证明 2PC 是一种共识算法，但不是一个非常好的共识算法【70,71】。 通过对 2PC 的学习，我们将继续努力实现更好的一致性算法，比如 ZooKeeper（Zab）和 etcd（Raft）中使用的算法。 原子提交与两阶段提交在 第七章 中我们了解到，事务原子性的目的是在多次写操作中途出错的情况下，提供一种简单的语义。事务的结果要么是成功提交，在这种情况下，事务的所有写入都是持久化的；要么是中止，在这种情况下，事务的所有写入都被回滚（即撤消或丢弃）。 原子性可以防止失败的事务搅乱数据库，避免数据库陷入半成品结果和半更新状态。这对于多对象事务（请参阅 “单对象和多对象操作”）和维护次级索引的数据库尤其重要。每个次级索引都是与主数据相分离的数据结构 —— 因此，如果你修改了一些数据，则还需要在次级索引中进行相应的更改。原子性确保次级索引与主数据保持一致（如果索引与主数据不一致，就没什么用了）。 从单节点到分布式原子提交对于在单个数据库节点执行的事务，原子性通常由存储引擎实现。当客户端请求数据库节点提交事务时，数据库将使事务的写入持久化（通常在预写式日志中，请参阅 “让 B 树更可靠”），然后将提交记录追加到磁盘中的日志里。如果数据库在这个过程中间崩溃，当节点重启时，事务会从日志中恢复：如果提交记录在崩溃之前成功地写入磁盘，则认为事务被提交；否则来自该事务的任何写入都被回滚。 因此，在单个节点上，事务的提交主要取决于数据持久化落盘的 顺序：首先是数据，然后是提交记录【72】。事务提交或终止的关键决定时刻是磁盘完成写入提交记录的时刻：在此之前，仍有可能中止（由于崩溃），但在此之后，事务已经提交（即使数据库崩溃）。因此，是单一的设备（连接到单个磁盘的控制器，且挂载在单台机器上）使得提交具有原子性。 但是，如果一个事务中涉及多个节点呢？例如，你也许在分区数据库中会有一个多对象事务，或者是一个按关键词分区的次级索引（其中索引条目可能位于与主数据不同的节点上；请参阅 “分区与次级索引”）。大多数 “NoSQL” 分布式数据存储不支持这种分布式事务，但是很多关系型数据库集群支持（请参阅 “实践中的分布式事务”）。 在这些情况下，仅向所有节点发送提交请求并独立提交每个节点的事务是不够的。这样很容易发生违反原子性的情况：提交在某些节点上成功，而在其他节点上失败： 某些节点可能会检测到违反约束或冲突，因此需要中止，而其他节点则可以成功进行提交。 某些提交请求可能在网络中丢失，最终由于超时而中止，而其他提交请求则通过。 在提交记录完全写入之前，某些节点可能会崩溃，并在恢复时回滚，而其他节点则成功提交。 如果某些节点提交了事务，但其他节点却放弃了这些事务，那么这些节点就会彼此不一致（如 图 7-3 所示）。而且一旦在某个节点上提交了一个事务，如果事后发现它在其它节点上被中止了，它是无法撤回的。出于这个原因，一旦确定事务中的所有其他节点也将提交，节点就必须进行提交。 事务提交必须是不可撤销的 —— 事务提交之后，你不能改变主意，并追溯性地中止事务。这个规则的原因是，一旦数据被提交，其结果就对其他事务可见，因此其他客户端可能会开始依赖这些数据。这个原则构成了 读已提交 隔离等级的基础，在 “读已提交” 一节中讨论了这个问题。如果一个事务在提交后被允许中止，所有那些读取了 已提交却又被追溯声明不存在数据 的事务也必须回滚。 （提交事务的结果有可能通过事后执行另一个补偿事务（compensating transaction）来取消【73,74】，但从数据库的角度来看，这是一个单独的事务，因此任何关于跨事务正确性的保证都是应用自己的问题。） 两阶段提交简介两阶段提交（two-phase commit） 是一种用于实现跨多个节点的原子事务提交的算法，即确保所有节点提交或所有节点中止。 它是分布式数据库中的经典算法【13,35,75】。 2PC 在某些数据库内部使用，也以 XA 事务 的形式对应用可用【76,77】（例如 Java Transaction API 支持）或以 SOAP Web 服务的 WS-AtomicTransaction 形式提供给应用【78,79】。 图 9-9 说明了 2PC 的基本流程。2PC 中的提交 &#x2F; 中止过程分为两个阶段（因此而得名），而不是单节点事务中的单个提交请求。 图 9-9 两阶段提交（2PC）的成功执行 不要把2PC和2PL搞混了 两阶段提交（2PC）和两阶段锁定（请参阅 “两阶段锁定”）是两个完全不同的东西。 2PC 在分布式数据库中提供原子提交，而 2PL 提供可串行化的隔离等级。为了避免混淆，最好把它们看作完全独立的概念，并忽略名称中不幸的相似性。 2PC 使用一个通常不会出现在单节点事务中的新组件：协调者（coordinator，也称为 事务管理器，即 transaction manager）。协调者通常在请求事务的相同应用进程中以库的形式实现（例如，嵌入在 Java EE 容器中），但也可以是单独的进程或服务。这种协调者的例子包括 Narayana、JOTM、BTM 或 MSDTC。 正常情况下，2PC 事务以应用在多个数据库节点上读写数据开始。我们称这些数据库节点为 参与者（participants）。当应用准备提交时，协调者开始阶段 1 ：它发送一个 准备（prepare） 请求到每个节点，询问它们是否能够提交。然后协调者会跟踪参与者的响应： 如果所有参与者都回答 “是”，表示它们已经准备好提交，那么协调者在阶段 2 发出 提交（commit） 请求，然后提交真正发生。 如果任意一个参与者回复了 “否”，则协调者在阶段 2 中向所有节点发送 中止（abort） 请求。 这个过程有点像西方传统婚姻仪式：司仪分别询问新娘和新郎是否要结婚，通常是从两方都收到 “我愿意” 的答复。收到两者的回复后，司仪宣布这对情侣成为夫妻：事务就提交了，这一幸福事实会广播至所有的参与者中。如果新娘与新郎之一没有回复 “我愿意”，婚礼就会中止【73】。 系统承诺这个简短的描述可能并没有说清楚为什么两阶段提交保证了原子性，而跨多个节点的一阶段提交却没有。在两阶段提交的情况下，准备请求和提交请求当然也可以轻易丢失。 2PC 又有什么不同呢？ 为了理解它的工作原理，我们必须更详细地分解这个过程： 当应用想要启动一个分布式事务时，它向协调者请求一个事务 ID。此事务 ID 是全局唯一的。 应用在每个参与者上启动单节点事务，并在单节点事务上捎带上这个全局事务 ID。所有的读写都是在这些单节点事务中各自完成的。如果在这个阶段出现任何问题（例如，节点崩溃或请求超时），则协调者或任何参与者都可以中止。 当应用准备提交时，协调者向所有参与者发送一个 准备 请求，并打上全局事务 ID 的标记。如果任意一个请求失败或超时，则协调者向所有参与者发送针对该事务 ID 的中止请求。 参与者收到准备请求时，需要确保在任意情况下都的确可以提交事务。这包括将所有事务数据写入磁盘（出现故障，电源故障，或硬盘空间不足都不能是稍后拒绝提交的理由）以及检查是否存在任何冲突或违反约束。通过向协调者回答 “是”，节点承诺，只要请求，这个事务一定可以不出差错地提交。换句话说，参与者放弃了中止事务的权利，但没有实际提交。 当协调者收到所有准备请求的答复时，会就提交或中止事务作出明确的决定（只有在所有参与者投赞成票的情况下才会提交）。协调者必须把这个决定写到磁盘上的事务日志中，如果它随后就崩溃，恢复后也能知道自己所做的决定。这被称为 提交点（commit point）。 一旦协调者的决定落盘，提交或放弃请求会发送给所有参与者。如果这个请求失败或超时，协调者必须永远保持重试，直到成功为止。没有回头路：如果已经做出决定，不管需要多少次重试它都必须被执行。如果参与者在此期间崩溃，事务将在其恢复后提交 —— 由于参与者投了赞成，因此恢复后它不能拒绝提交。 因此，该协议包含两个关键的 “不归路” 点：当参与者投票 “是” 时，它承诺它稍后肯定能够提交（尽管协调者可能仍然选择放弃）；以及一旦协调者做出决定，这一决定是不可撤销的。这些承诺保证了 2PC 的原子性（单节点原子提交将这两个事件合为了一体：将提交记录写入事务日志）。 回到婚姻的比喻，在说 “我愿意” 之前，你和你的新娘 &#x2F; 新郎有中止这个事务的自由，只要回复 “没门！” 就行（或者有类似效果的话）。然而在说了 “我愿意” 之后，你就不能撤回那个声明了。如果你说 “我愿意” 后晕倒了，没有听到司仪说 “你们现在是夫妻了”，那也并不会改变事务已经提交的现实。当你稍后恢复意识时，可以通过查询司仪的全局事务 ID 状态来确定你是否已经成婚，或者你可以等待司仪重试下一次提交请求（因为重试将在你无意识期间一直持续）。 协调者失效我们已经讨论了在 2PC 期间，如果参与者之一或网络发生故障时会发生什么情况：如果任何一个 准备 请求失败或者超时，协调者就会中止事务。如果任何提交或中止请求失败，协调者将无条件重试。但是如果协调者崩溃，会发生什么情况就不太清楚了。 如果协调者在发送 准备 请求之前失败，参与者可以安全地中止事务。但是，一旦参与者收到了准备请求并投了 “是”，就不能再单方面放弃 —— 必须等待协调者回答事务是否已经提交或中止。如果此时协调者崩溃或网络出现故障，参与者什么也做不了只能等待。参与者的这种事务状态称为 存疑（in doubt） 的或 不确定（uncertain） 的。 情况如 图 9-10 所示。在这个特定的例子中，协调者实际上决定提交，数据库 2 收到提交请求。但是，协调者在将提交请求发送到数据库 1 之前发生崩溃，因此数据库 1 不知道是否提交或中止。即使 超时 在这里也没有帮助：如果数据库 1 在超时后单方面中止，它将最终与执行提交的数据库 2 不一致。同样，单方面提交也是不安全的，因为另一个参与者可能已经中止了。 图 9-10 参与者投赞成票后，协调者崩溃。数据库 1 不知道是否提交或中止 没有协调者的消息，参与者无法知道是提交还是放弃。原则上参与者可以相互沟通，找出每个参与者是如何投票的，并达成一致，但这不是 2PC 协议的一部分。 可以完成 2PC 的唯一方法是等待协调者恢复。这就是为什么协调者必须在向参与者发送提交或中止请求之前，将其提交或中止决定写入磁盘上的事务日志：协调者恢复后，通过读取其事务日志来确定所有存疑事务的状态。任何在协调者日志中没有提交记录的事务都会中止。因此，2PC 的 提交点 归结为协调者上的常规单节点原子提交。 三阶段提交两阶段提交被称为 阻塞（blocking）- 原子提交协议，因为存在 2PC 可能卡住并等待协调者恢复的情况。理论上，可以使一个原子提交协议变为 非阻塞（nonblocking） 的，以便在节点失败时不会卡住。但是让这个协议能在实践中工作并没有那么简单。 作为 2PC 的替代方案，已经提出了一种称为 三阶段提交（3PC） 的算法【13,80】。然而，3PC 假定网络延迟有界，节点响应时间有限；在大多数具有无限网络延迟和进程暂停的实际系统中（见 第八章），它并不能保证原子性。 通常，非阻塞原子提交需要一个 完美的故障检测器（perfect failure detector）【67,71】—— 即一个可靠的机制来判断一个节点是否已经崩溃。在具有无限延迟的网络中，超时并不是一种可靠的故障检测机制，因为即使没有节点崩溃，请求也可能由于网络问题而超时。出于这个原因，2PC 仍然被使用，尽管大家都清楚可能存在协调者故障的问题。 实践中的分布式事务分布式事务的名声毁誉参半，尤其是那些通过两阶段提交实现的。一方面，它被视作提供了一个难以实现的重要的安全性保证；另一方面，它们因为导致运维问题，造成性能下降，做出超过能力范围的承诺而饱受批评【81,82,83,84】。许多云服务由于其导致的运维问题，而选择不实现分布式事务【85,86】。 分布式事务的某些实现会带来严重的性能损失 —— 例如据报告称，MySQL 中的分布式事务比单节点事务慢 10 倍以上【87】，所以当人们建议不要使用它们时就不足为奇了。两阶段提交所固有的性能成本，大部分是由于崩溃恢复所需的额外强制刷盘（fsync）【88】以及额外的网络往返。 但我们不应该直接忽视分布式事务，而应当更加仔细地审视这些事务，因为从中可以汲取重要的经验教训。首先，我们应该精确地说明 “分布式事务” 的含义。两种截然不同的分布式事务类型经常被混淆： 数据库内部的分布式事务 一些分布式数据库（即在其标准配置中使用复制和分区的数据库）支持数据库节点之间的内部事务。例如，VoltDB 和 MySQL Cluster 的 NDB 存储引擎就有这样的内部事务支持。在这种情况下，所有参与事务的节点都运行相同的数据库软件。 异构分布式事务 在 异构（heterogeneous） 事务中，参与者是由两种或两种以上的不同技术组成的：例如来自不同供应商的两个数据库，甚至是非数据库系统（如消息代理）。跨系统的分布式事务必须确保原子提交，尽管系统可能完全不同。 数据库内部事务不必与任何其他系统兼容，因此它们可以使用任何协议，并能针对特定技术进行特定的优化。因此数据库内部的分布式事务通常工作地很好。另一方面，跨异构技术的事务则更有挑战性。 恰好一次的消息处理异构的分布式事务处理能够以强大的方式集成不同的系统。例如：消息队列中的一条消息可以被确认为已处理，当且仅当用于处理消息的数据库事务成功提交。这是通过在同一个事务中原子提交 消息确认 和 数据库写入 两个操作来实现的。藉由分布式事务的支持，即使消息代理和数据库是在不同机器上运行的两种不相关的技术，这种操作也是可能的。 如果消息传递或数据库事务任意一者失败，两者都会中止，因此消息代理可能会在稍后安全地重传消息。因此，通过原子提交 消息处理及其副作用，即使在成功之前需要几次重试，也可以确保消息被 有效地（effectively） 恰好处理一次。中止会抛弃部分完成事务所导致的任何副作用。 然而，只有当所有受事务影响的系统都使用同样的 原子提交协议（atomic commit protocol） 时，这样的分布式事务才是可能的。例如，假设处理消息的副作用是发送一封邮件，而邮件服务器并不支持两阶段提交：如果消息处理失败并重试，则可能会发送两次或更多次的邮件。但如果处理消息的所有副作用都可以在事务中止时回滚，那么这样的处理流程就可以安全地重试，就好像什么都没有发生过一样。 在 第十一章 中将再次回到 “恰好一次” 消息处理的主题。让我们先来看看允许这种异构分布式事务的原子提交协议。 XA事务X&#x2F;Open XA（扩展架构（eXtended Architecture） 的缩写）是跨异构技术实现两阶段提交的标准【76,77】。它于 1991 年推出并得到了广泛的实现：许多传统关系数据库（包括 PostgreSQL、MySQL、DB2、SQL Server 和 Oracle）和消息代理（包括 ActiveMQ、HornetQ、MSMQ 和 IBM MQ） 都支持 XA。 XA 不是一个网络协议 —— 它只是一个用来与事务协调者连接的 C API。其他语言也有这种 API 的绑定；例如在 Java EE 应用的世界中，XA 事务是使用 Java 事务 API（JTA, Java Transaction API） 实现的，而许多使用 Java 数据库连接（JDBC, Java Database Connectivity） 的数据库驱动，以及许多使用 Java 消息服务（JMS） API 的消息代理都支持 Java 事务 API（JTA）。 XA 假定你的应用使用网络驱动或客户端库来与 参与者（数据库或消息服务）进行通信。如果驱动支持 XA，则意味着它会调用 XA API 以查明操作是否为分布式事务的一部分 —— 如果是，则将必要的信息发往数据库服务器。驱动还会向协调者暴露回调接口，协调者可以通过回调来要求参与者准备、提交或中止。 事务协调者需要实现 XA API。标准没有指明应该如何实现，但实际上协调者通常只是一个库，被加载到发起事务的应用的同一个进程中（而不是单独的服务）。它在事务中跟踪所有的参与者，并在要求它们 准备 之后收集参与者的响应（通过驱动回调），并使用本地磁盘上的日志记录每次事务的决定（提交 &#x2F; 中止）。 如果应用进程崩溃，或者运行应用的机器报销了，协调者也随之往生极乐。然后任何带有 准备了 但未提交事务的参与者都会在疑虑中卡死。由于协调程序的日志位于应用服务器的本地磁盘上，因此必须重启该服务器，且协调程序库必须读取日志以恢复每个事务的提交 &#x2F; 中止结果。只有这样，协调者才能使用数据库驱动的 XA 回调来要求参与者提交或中止。数据库服务器不能直接联系协调者，因为所有通信都必须通过客户端库。 怀疑时持有锁为什么我们这么关心存疑事务？系统的其他部分就不能继续正常工作，无视那些终将被清理的存疑事务吗？ 问题在于 锁（locking）。正如在 “读已提交” 中所讨论的那样，数据库事务通常获取待修改的行上的 行级排他锁，以防止脏写。此外，如果要使用可串行化的隔离等级，则使用两阶段锁定的数据库也必须为事务所读取的行加上共享锁（请参阅 “两阶段锁定”）。 在事务提交或中止之前，数据库不能释放这些锁（如 图 9-9 中的阴影区域所示）。因此，在使用两阶段提交时，事务必须在整个存疑期间持有这些锁。如果协调者已经崩溃，需要 20 分钟才能重启，那么这些锁将会被持有 20 分钟。如果协调者的日志由于某种原因彻底丢失，这些锁将被永久持有 —— 或至少在管理员手动解决该情况之前。 当这些锁被持有时，其他事务不能修改这些行。根据数据库的不同，其他事务甚至可能因为读取这些行而被阻塞。因此，其他事务没法儿简单地继续它们的业务了 —— 如果它们要访问同样的数据，就会被阻塞。这可能会导致应用大面积进入不可用状态，直到存疑事务被解决。 从协调者故障中恢复理论上，如果协调者崩溃并重新启动，它应该干净地从日志中恢复其状态，并解决任何存疑事务。然而在实践中，孤立（orphaned） 的存疑事务确实会出现【89,90】，即无论出于何种理由，协调者无法确定事务的结果（例如事务日志已经由于软件错误丢失或损坏）。这些事务无法自动解决，所以它们永远待在数据库中，持有锁并阻塞其他事务。 即使重启数据库服务器也无法解决这个问题，因为在 2PC 的正确实现中，即使重启也必须保留存疑事务的锁（否则就会冒违反原子性保证的风险）。这是一种棘手的情况。 唯一的出路是让管理员手动决定提交还是回滚事务。管理员必须检查每个存疑事务的参与者，确定是否有任何参与者已经提交或中止，然后将相同的结果应用于其他参与者。解决这个问题潜在地需要大量的人力，并且可能发生在严重的生产中断期间（不然为什么协调者处于这种糟糕的状态），并很可能要在巨大精神压力和时间压力下完成。 许多 XA 的实现都有一个叫做 启发式决策（heuristic decisions） 的紧急逃生舱口：允许参与者单方面决定放弃或提交一个存疑事务，而无需协调者做出最终决定【76,77,91】。要清楚的是，这里 启发式 是 可能破坏原子性（probably breaking atomicity） 的委婉说法，因为它违背了两阶段提交的系统承诺。因此，启发式决策只是为了逃出灾难性的情况而准备的，而不是为了日常使用的。 分布式事务的限制XA 事务解决了保持多个参与者（数据系统）相互一致的现实的和重要的问题，但正如我们所看到的那样，它也引入了严重的运维问题。特别来讲，这里的核心认识是：事务协调者本身就是一种数据库（存储了事务的结果），因此需要像其他重要数据库一样小心地打交道： 如果协调者没有复制，而是只在单台机器上运行，那么它是整个系统的失效单点（因为它的失效会导致其他应用服务器阻塞在存疑事务持有的锁上）。令人惊讶的是，许多协调者实现默认情况下并不是高可用的，或者只有基本的复制支持。 许多服务器端应用都是使用无状态模式开发的（受 HTTP 的青睐），所有持久状态都存储在数据库中，因此具有应用服务器可随意按需添加删除的优点。但是，当协调者成为应用服务器的一部分时，它会改变部署的性质。突然间，协调者的日志成为持久系统状态的关键部分 —— 与数据库本身一样重要，因为协调者日志是为了在崩溃后恢复存疑事务所必需的。这样的应用服务器不再是无状态的了。 由于 XA 需要兼容各种数据系统，因此它必须是所有系统的最小公分母。例如，它不能检测不同系统间的死锁（因为这将需要一个标准协议来让系统交换每个事务正在等待的锁的信息），而且它无法与 SSI（请参阅 可串行化快照隔离）协同工作，因为这需要一个跨系统定位冲突的协议。 对于数据库内部的分布式事务（不是 XA），限制没有这么大 —— 例如，分布式版本的 SSI 是可能的。然而仍然存在问题：2PC 成功提交一个事务需要所有参与者的响应。因此，如果系统的 任何 部分损坏，事务也会失败。因此，分布式事务又有 扩大失效（amplifying failures） 的趋势，这又与我们构建容错系统的目标背道而驰。 这些事实是否意味着我们应该放弃保持几个系统相互一致的所有希望？不完全是 —— 还有其他的办法，可以让我们在没有异构分布式事务的痛苦的情况下实现同样的事情。我们将在 第十一章 和 第十二章 回到这些话题。但首先，我们应该概括一下关于 共识 的话题。 容错共识非正式地，共识意味着让几个节点就某事达成一致。例如，如果有几个人 同时（concurrently） 尝试预订飞机上的最后一个座位，或剧院中的同一个座位，或者尝试使用相同的用户名注册一个帐户。共识算法可以用来确定这些 互不相容（mutually incompatible） 的操作中，哪一个才是赢家。 共识问题通常形式化如下：一个或多个节点可以 提议（propose） 某些值，而共识算法 决定（decides） 采用其中的某个值。在座位预订的例子中，当几个顾客同时试图订购最后一个座位时，处理顾客请求的每个节点可以 提议 将要服务的顾客的 ID，而 决定 指明了哪个顾客获得了座位。 在这种形式下，共识算法必须满足以下性质【25】：[^xiii] [^xiii]: 这种共识的特殊形式被称为 统一共识（uniform consensus），相当于在具有不可靠故障检测器的异步系统中的 常规共识（regular consensus）【71】。学术文献通常指的是 进程（process） 而不是节点，但我们在这里使用 节点（node） 来与本书的其余部分保持一致。 一致同意（Uniform agreement） 没有两个节点的决定不同。 完整性（Integrity） 没有节点决定两次。 有效性（Validity） 如果一个节点决定了值 v ，则 v 由某个节点所提议。 终止（Termination） 由所有未崩溃的节点来最终决定值。 一致同意 和 完整性 属性定义了共识的核心思想：所有人都决定了相同的结果，一旦决定了，你就不能改变主意。有效性 属性主要是为了排除平凡的解决方案：例如，无论提议了什么值，你都可以有一个始终决定值为 null 的算法。；该算法满足 一致同意 和 完整性 属性，但不满足 有效性 属性。 如果你不关心容错，那么满足前三个属性很容易：你可以将一个节点硬编码为 “独裁者”，并让该节点做出所有的决定。但如果该节点失效，那么系统就无法再做出任何决定。事实上，这就是我们在两阶段提交的情况中所看到的：如果协调者失效，那么存疑的参与者就无法决定提交还是中止。 终止 属性形式化了容错的思想。它实质上说的是，一个共识算法不能简单地永远闲坐着等死 —— 换句话说，它必须取得进展。即使部分节点出现故障，其他节点也必须达成一项决定（终止 是一种 活性属性，而另外三种是 安全属性 —— 请参阅 “安全性和活性”）。 共识的系统模型假设，当一个节点 “崩溃” 时，它会突然消失而且永远不会回来。（不像软件崩溃，想象一下地震，包含你的节点的数据中心被山体滑坡所摧毁，你必须假设节点被埋在 30 英尺以下的泥土中，并且永远不会重新上线）在这个系统模型中，任何需要等待节点恢复的算法都不能满足 终止 属性。特别是，2PC 不符合终止属性的要求。 当然如果 所有 的节点都崩溃了，没有一个在运行，那么所有算法都不可能决定任何事情。算法可以容忍的失效数量是有限的：事实上可以证明，任何共识算法都需要至少占总体 多数（majority） 的节点正确工作，以确保终止属性【67】。多数可以安全地组成法定人数（请参阅 “读写的法定人数”）。 因此 终止 属性取决于一个假设，不超过一半的节点崩溃或不可达。然而即使多数节点出现故障或存在严重的网络问题，绝大多数共识的实现都能始终确保安全属性得到满足 —— 一致同意，完整性和有效性【92】。因此，大规模的中断可能会阻止系统处理请求，但是它不能通过使系统做出无效的决定来破坏共识系统。 大多数共识算法假设不存在 拜占庭式错误，正如在 “拜占庭故障” 一节中所讨论的那样。也就是说，如果一个节点没有正确地遵循协议（例如，如果它向不同节点发送矛盾的消息），它就可能会破坏协议的安全属性。克服拜占庭故障，稳健地达成共识是可能的，只要少于三分之一的节点存在拜占庭故障【25,93】。但我们没有地方在本书中详细讨论这些算法了。 共识算法和全序广播最著名的容错共识算法是 视图戳复制（VSR, Viewstamped Replication）【94,95】，Paxos 【96,97,98,99】，Raft 【22,100,101】以及 Zab 【15,21,102】 。这些算法之间有不少相似之处，但它们并不相同【103】。在本书中我们不会介绍各种算法的详细细节：了解一些它们共通的高级思想通常已经足够了，除非你准备自己实现一个共识系统。（可能并不明智，相当难【98,104】） 大多数这些算法实际上并不直接使用这里描述的形式化模型（提议与决定单个值，并满足一致同意、完整性、有效性和终止属性）。取而代之的是，它们决定了值的 顺序（sequence），这使它们成为全序广播算法，正如本章前面所讨论的那样（请参阅 “全序广播”）。 请记住，全序广播要求将消息按照相同的顺序，恰好传递一次，准确传送到所有节点。如果仔细思考，这相当于进行了几轮共识：在每一轮中，节点提议下一条要发送的消息，然后决定在全序中下一条要发送的消息【67】。 所以，全序广播相当于重复进行多轮共识（每次共识决定与一次消息传递相对应）： 由于 一致同意 属性，所有节点决定以相同的顺序传递相同的消息。 由于 完整性 属性，消息不会重复。 由于 有效性 属性，消息不会被损坏，也不能凭空编造。 由于 终止 属性，消息不会丢失。 视图戳复制，Raft 和 Zab 直接实现了全序广播，因为这样做比重复 一次一值（one value a time） 的共识更高效。在 Paxos 的情况下，这种优化被称为 Multi-Paxos。 单主复制与共识在 第五章 中，我们讨论了单主复制（请参阅 “领导者与追随者”），它将所有的写入操作都交给主库，并以相同的顺序将它们应用到从库，从而使副本保持在最新状态。这实际上不就是一个全序广播吗？为什么我们在 第五章 里一点都没担心过共识问题呢？ 答案取决于如何选择领导者。如果主库是由运维人员手动选择和配置的，那么你实际上拥有一种 独裁类型 的 “共识算法”：只有一个节点被允许接受写入（即决定写入复制日志的顺序），如果该节点发生故障，则系统将无法写入，直到运维手动配置其他节点作为主库。这样的系统在实践中可以表现良好，但它无法满足共识的 终止 属性，因为它需要人为干预才能取得 进展。 一些数据库会自动执行领导者选举和故障切换，如果旧主库失效，会提拔一个从库为新主库（请参阅 “处理节点宕机”）。这使我们向容错的全序广播更进一步，从而达成共识。 但是还有一个问题。我们之前曾经讨论过脑裂的问题，并且说过所有的节点都需要同意是谁领导，否则两个不同的节点都会认为自己是领导者，从而导致数据库进入不一致的状态。因此，选出一位领导者需要共识。但如果这里描述的共识算法实际上是全序广播算法，并且全序广播就像单主复制，而单主复制需要一个领导者，那么… 这样看来，要选出一个领导者，我们首先需要一个领导者。要解决共识问题，我们首先需要解决共识问题。我们如何跳出这个先有鸡还是先有蛋的问题？ 纪元编号和法定人数迄今为止所讨论的所有共识协议，在内部都以某种形式使用一个领导者，但它们并不能保证领导者是独一无二的。相反，它们可以做出更弱的保证：协议定义了一个 纪元编号（epoch number，在 Paxos 中被称为 投票编号，即 ballot number，在视图戳复制中被称为 视图编号，即 view number，以及在 Raft 中被为 任期号码，即 term number），并确保在每个时代中，领导者都是唯一的。 每次当现任领导被认为挂掉的时候，节点间就会开始一场投票，以选出一个新领导。这次选举被赋予一个递增的纪元编号，因此纪元编号是全序且单调递增的。如果两个不同的时代的领导者之间出现冲突（也许是因为前任领导者实际上并未死亡），那么带有更高纪元编号的领导说了算。 在任何领导者被允许决定任何事情之前，必须先检查是否存在其他带有更高纪元编号的领导者，它们可能会做出相互冲突的决定。领导者如何知道自己没有被另一个节点赶下台？回想一下在 “真相由多数所定义” 中提到的：一个节点不一定能相信自己的判断 —— 因为只有节点自己认为自己是领导者，并不一定意味着其他节点接受它作为它们的领导者。 相反，它必须从 法定人数（quorum） 的节点中获取选票（请参阅 “读写的法定人数”）。对领导者想要做出的每一个决定，都必须将提议值发送给其他节点，并等待法定人数的节点响应并赞成提案。法定人数通常（但不总是）由多数节点组成【105】。只有在没有意识到任何带有更高纪元编号的领导者的情况下，一个节点才会投票赞成提议。 因此，我们有两轮投票：第一次是为了选出一位领导者，第二次是对领导者的提议进行表决。关键的洞察在于，这两次投票的 法定人群 必须相互 重叠（overlap）：如果一个提案的表决通过，则至少得有一个参与投票的节点也必须参加过最近的领导者选举【105】。因此，如果在一个提案的表决过程中没有出现更高的纪元编号。那么现任领导者就可以得出这样的结论：没有发生过更高时代的领导选举，因此可以确定自己仍然在领导。然后它就可以安全地对提议值做出决定。 这一投票过程表面上看起来很像两阶段提交。最大的区别在于，2PC 中协调者不是由选举产生的，而且 2PC 则要求 所有 参与者都投赞成票，而容错共识算法只需要多数节点的投票。而且，共识算法还定义了一个恢复过程，节点可以在选举出新的领导者之后进入一个一致的状态，确保始终能满足安全属性。这些区别正是共识算法正确性和容错性的关键。 共识的局限性共识算法对于分布式系统来说是一个巨大的突破：它为其他充满不确定性的系统带来了基础的安全属性（一致同意，完整性和有效性），然而它们还能保持容错（只要多数节点正常工作且可达，就能取得进展）。它们提供了全序广播，因此它们也可以以一种容错的方式实现线性一致的原子操作（请参阅 “使用全序广播实现线性一致的存储”）。 尽管如此，它们并不是在所有地方都用上了，因为好处总是有代价的。 节点在做出决定之前对提议进行投票的过程是一种同步复制。如 “同步复制与异步复制” 中所述，通常数据库会配置为异步复制模式。在这种配置中发生故障切换时，一些已经提交的数据可能会丢失 —— 但是为了获得更好的性能，许多人选择接受这种风险。 共识系统总是需要严格多数来运转。这意味着你至少需要三个节点才能容忍单节点故障（其余两个构成多数），或者至少有五个节点来容忍两个节点发生故障（其余三个构成多数）。如果网络故障切断了某些节点同其他节点的连接，则只有多数节点所在的网络可以继续工作，其余部分将被阻塞（请参阅 “线性一致性的代价”）。 大多数共识算法假定参与投票的节点是固定的集合，这意味着你不能简单的在集群中添加或删除节点。共识算法的 动态成员扩展（dynamic membership extension） 允许集群中的节点集随时间推移而变化，但是它们比静态成员算法要难理解得多。 共识系统通常依靠超时来检测失效的节点。在网络延迟高度变化的环境中，特别是在地理上散布的系统中，经常发生一个节点由于暂时的网络问题，错误地认为领导者已经失效。虽然这种错误不会损害安全属性，但频繁的领导者选举会导致糟糕的性能表现，因系统最后可能花在权力倾扎上的时间要比花在建设性工作的多得多。 有时共识算法对网络问题特别敏感。例如 Raft 已被证明存在让人不悦的极端情况【106】：如果整个网络工作正常，但只有一条特定的网络连接一直不可靠，Raft 可能会进入领导者在两个节点间频繁切换的局面，或者当前领导者不断被迫辞职以致系统实质上毫无进展。其他一致性算法也存在类似的问题，而设计能健壮应对不可靠网络的算法仍然是一个开放的研究问题。 成员与协调服务像 ZooKeeper 或 etcd 这样的项目通常被描述为 “分布式键值存储” 或 “协调与配置服务”。这种服务的 API 看起来非常像数据库：你可以读写给定键的值，并遍历键。所以如果它们基本上算是数据库的话，为什么它们要把工夫全花在实现一个共识算法上呢？是什么使它们区别于其他任意类型的数据库？ 为了理解这一点，简单了解如何使用 ZooKeeper 这类服务是很有帮助的。作为应用开发人员，你很少需要直接使用 ZooKeeper，因为它实际上不适合当成通用数据库来用。更有可能的是，你会通过其他项目间接依赖它，例如 HBase、Hadoop YARN、OpenStack Nova 和 Kafka 都依赖 ZooKeeper 在后台运行。这些项目从它那里得到了什么？ ZooKeeper 和 etcd 被设计为容纳少量完全可以放在内存中的数据（虽然它们仍然会写入磁盘以保证持久性），所以你不会想着把所有应用数据放到这里。这些少量数据会通过容错的全序广播算法复制到所有节点上。正如前面所讨论的那样，数据库复制需要的就是全序广播：如果每条消息代表对数据库的写入，则以相同的顺序应用相同的写入操作可以使副本之间保持一致。 ZooKeeper 模仿了 Google 的 Chubby 锁服务【14,98】，不仅实现了全序广播（因此也实现了共识），而且还构建了一组有趣的其他特性，这些特性在构建分布式系统时变得特别有用： 线性一致性的原子操作 使用原子 CAS 操作可以实现锁：如果多个节点同时尝试执行相同的操作，只有一个节点会成功。共识协议保证了操作的原子性和线性一致性，即使节点发生故障或网络在任意时刻中断。分布式锁通常以 租约（lease） 的形式实现，租约有一个到期时间，以便在客户端失效的情况下最终能被释放（请参阅 “进程暂停”）。 操作的全序排序 如 “领导者和锁” 中所述，当某个资源受到锁或租约的保护时，你需要一个防护令牌来防止客户端在进程暂停的情况下彼此冲突。防护令牌是每次锁被获取时单调增加的数字。ZooKeeper 通过全序化所有操作来提供这个功能，它为每个操作提供一个单调递增的事务 ID（zxid）和版本号（cversion）【15】。 失效检测 客户端在 ZooKeeper 服务器上维护一个长期会话，客户端和服务器周期性地交换心跳包来检查节点是否还活着。即使连接暂时中断，或者 ZooKeeper 节点失效，会话仍保持在活跃状态。但如果心跳停止的持续时间超出会话超时，ZooKeeper 会宣告该会话已死亡。当会话超时时（ZooKeeper 称这些节点为 临时节点，即 ephemeral nodes），会话持有的任何锁都可以配置为自动释放。 变更通知 客户端不仅可以读取其他客户端创建的锁和值，还可以监听它们的变更。因此，客户端可以知道另一个客户端何时加入集群（基于新客户端写入 ZooKeeper 的值），或发生故障（因其会话超时，而其临时节点消失）。通过订阅通知，客户端不用再通过频繁轮询的方式来找出变更。 在这些功能中，只有线性一致的原子操作才真的需要共识。但正是这些功能的组合，使得像 ZooKeeper 这样的系统在分布式协调中非常有用。 将工作分配给节点ZooKeeper&#x2F;Chubby 模型运行良好的一个例子是，如果你有几个进程实例或服务，需要选择其中一个实例作为主库或首选服务。如果领导者失败，其他节点之一应该接管。这对单主数据库当然非常实用，但对作业调度程序和类似的有状态系统也很好用。 另一个例子是，当你有一些分区资源（数据库、消息流、文件存储、分布式 Actor 系统等），并需要决定将哪个分区分配给哪个节点时。当新节点加入集群时，需要将某些分区从现有节点移动到新节点，以便重新平衡负载（请参阅 “分区再平衡”）。当节点被移除或失效时，其他节点需要接管失效节点的工作。 这类任务可以通过在 ZooKeeper 中明智地使用原子操作，临时节点与通知来实现。如果设计得当，这种方法允许应用自动从故障中恢复而无需人工干预。不过这并不容易，尽管已经有不少在 ZooKeeper 客户端 API 基础之上提供更高层工具的库，例如 Apache Curator 【17】。但它仍然要比尝试从头实现必要的共识算法要好得多，这样的尝试鲜有成功记录【107】。 应用最初只能在单个节点上运行，但最终可能会增长到数千个节点。试图在如此之多的节点上进行多数投票将是非常低效的。相反，ZooKeeper 在固定数量的节点（通常是三到五个）上运行，并在这些节点之间执行其多数票，同时支持潜在的大量客户端。因此，ZooKeeper 提供了一种将协调节点（共识，操作排序和故障检测）的一些工作 “外包” 到外部服务的方式。 通常，由 ZooKeeper 管理的数据类型的变化十分缓慢：代表 “分区 7 中的节点运行在 10.1.1.23 上” 的信息可能会在几分钟或几小时的时间内发生变化。它不是用来存储应用的运行时状态的，后者每秒可能会改变数千甚至数百万次。如果应用状态需要从一个节点复制到另一个节点，则可以使用其他工具（如 Apache BookKeeper 【108】）。 服务发现ZooKeeper、etcd 和 Consul 也经常用于服务发现 —— 也就是找出你需要连接到哪个 IP 地址才能到达特定的服务。在云数据中心环境中，虚拟机来来往往很常见，你通常不会事先知道服务的 IP 地址。相反，你可以配置你的服务，使其在启动时注册服务注册表中的网络端点，然后可以由其他服务找到它们。 但是，服务发现是否需要达成共识还不太清楚。 DNS 是查找服务名称的 IP 地址的传统方式，它使用多层缓存来实现良好的性能和可用性。从 DNS 读取是绝对不线性一致性的，如果 DNS 查询的结果有点陈旧，通常不会有问题【109】。 DNS 的可用性和对网络中断的鲁棒性更重要。 尽管服务发现并不需要共识，但领导者选举却是如此。因此，如果你的共识系统已经知道领导是谁，那么也可以使用这些信息来帮助其他服务发现领导是谁。为此，一些共识系统支持只读缓存副本。这些副本异步接收共识算法所有决策的日志，但不主动参与投票。因此，它们能够提供不需要线性一致性的读取请求。 成员资格服务ZooKeeper 和它的小伙伴们可以看作是成员资格服务（membership services）研究的悠久历史的一部分，这个历史可以追溯到 20 世纪 80 年代，并且对建立高度可靠的系统（例如空中交通管制）非常重要【110】。 成员资格服务确定哪些节点当前处于活动状态并且是集群的活动成员。正如我们在 第八章 中看到的那样，由于无限的网络延迟，无法可靠地检测到另一个节点是否发生故障。但是，如果你通过共识来进行故障检测，那么节点可以就哪些节点应该被认为是存在或不存在达成一致。 即使它确实存在，仍然可能发生一个节点被共识错误地宣告死亡。但是对于一个系统来说，知道哪些节点构成了当前的成员关系是非常有用的。例如，选择领导者可能意味着简单地选择当前成员中编号最小的成员，但如果不同的节点对现有的成员都有谁有不同意见，则这种方法将不起作用。 本章小结在本章中，我们从几个不同的角度审视了关于一致性与共识的话题。我们深入研究了线性一致性（一种流行的一致性模型）：其目标是使多副本数据看起来好像只有一个副本一样，并使其上所有操作都原子性地生效。虽然线性一致性因为简单易懂而很吸引人 —— 它使数据库表现的好像单线程程序中的一个变量一样，但它有着速度缓慢的缺点，特别是在网络延迟很大的环境中。 我们还探讨了因果性，因果性对系统中的事件施加了顺序（什么发生在什么之前，基于因与果）。与线性一致不同，线性一致性将所有操作放在单一的全序时间线中，因果一致性为我们提供了一个较弱的一致性模型：某些事件可以是 并发 的，所以版本历史就像是一条不断分叉与合并的时间线。因果一致性没有线性一致性的协调开销，而且对网络问题的敏感性要低得多。 但即使捕获到因果顺序（例如使用兰伯特时间戳），我们发现有些事情也不能通过这种方式实现：在 “光有时间戳排序还不够” 一节的例子中，我们需要确保用户名是唯一的，并拒绝同一用户名的其他并发注册。如果一个节点要通过注册，则需要知道其他的节点没有在并发抢注同一用户名的过程中。这个问题引领我们走向 共识。 我们看到，达成共识意味着以这样一种方式决定某件事：所有节点一致同意所做决定，且这一决定不可撤销。通过深入挖掘，结果我们发现很广泛的一系列问题实际上都可以归结为共识问题，并且彼此等价（从这个意义上来讲，如果你有其中之一的解决方案，就可以轻易将它转换为其他问题的解决方案）。这些等价的问题包括： 线性一致性的 CAS 寄存器 寄存器需要基于当前值是否等于操作给出的参数，原子地 决定 是否设置新值。 原子事务提交 数据库必须 决定 是否提交或中止分布式事务。 全序广播 消息系统必须 决定 传递消息的顺序。 锁和租约 当几个客户端争抢锁或租约时，由锁来 决定 哪个客户端成功获得锁。 成员 &#x2F; 协调服务 给定某种故障检测器（例如超时），系统必须 决定 哪些节点活着，哪些节点因为会话超时需要被宣告死亡。 唯一性约束 当多个事务同时尝试使用相同的键创建冲突记录时，约束必须 决定 哪一个被允许，哪些因为违反约束而失败。 如果你只有一个节点，或者你愿意将决策的权能分配给单个节点，所有这些事都很简单。这就是在单领导者数据库中发生的事情：所有决策权归属于领导者，这就是为什么这样的数据库能够提供线性一致的操作，唯一性约束，完全有序的复制日志，以及更多。 但如果该领导者失效，或者如果网络中断导致领导者不可达，这样的系统就无法取得任何进展。应对这种情况可以有三种方法： 等待领导者恢复，接受系统将在这段时间阻塞的事实。许多 XA&#x2F;JTA 事务协调者选择这个选项。这种方法并不能完全达成共识，因为它不能满足 终止 属性的要求：如果领导者续命失败，系统可能会永久阻塞。 人工故障切换，让人类选择一个新的领导者节点，并重新配置系统使之生效，许多关系型数据库都采用这种方方式。这是一种来自 “天意” 的共识 —— 由计算机系统之外的运维人员做出决定。故障切换的速度受到人类行动速度的限制，通常要比计算机慢（得多）。 使用算法自动选择一个新的领导者。这种方法需要一种共识算法，使用成熟的算法来正确处理恶劣的网络条件是明智之举【107】。 尽管单领导者数据库可以提供线性一致性，且无需对每个写操作都执行共识算法，但共识对于保持及变更领导权仍然是必须的。因此从某种意义上说，使用单个领导者不过是 “缓兵之计”：共识仍然是需要的，只是在另一个地方，而且没那么频繁。好消息是，容错的共识算法与容错的共识系统是存在的，我们在本章中简要地讨论了它们。 像 ZooKeeper 这样的工具为应用提供了 “外包” 的共识、故障检测和成员服务。它们扮演了重要的角色，虽说使用不易，但总比自己去开发一个能经受 第八章 中所有问题考验的算法要好得多。如果你发现自己想要解决的问题可以归结为共识，并且希望它能容错，使用一个类似 ZooKeeper 的东西是明智之举。 尽管如此，并不是所有系统都需要共识：例如，无领导者复制和多领导者复制系统通常不会使用全局的共识。这些系统中出现的冲突（请参阅 “处理写入冲突”）正是不同领导者之间没有达成共识的结果，但这也许并没有关系：也许我们只是需要接受没有线性一致性的事实，并学会更好地与具有分支与合并版本历史的数据打交道。 本章引用了大量关于分布式系统理论的研究。虽然理论论文和证明并不总是容易理解，有时也会做出不切实际的假设，但它们对于指导这一领域的实践有着极其重要的价值：它们帮助我们推理什么可以做，什么不可以做，帮助我们找到反直觉的分布式系统缺陷。如果你有时间，这些参考资料值得探索。 这里已经到了本书 第二部分 的末尾，第二部介绍了复制（第五章）、分区（第六章）、事务（第七章）、分布式系统的故障模型（第八章）以及最后的一致性与共识（第九章）。现在我们已经奠定了扎实的理论基础，我们将在 第三部分 再次转向更实际的系统，并讨论如何使用异构的组件积木块构建强大的应用。 参考文献 Peter Bailis and Ali Ghodsi: “Eventual Consistency Today: Limitations, Extensions, and Beyond,” ACM Queue, volume 11, number 3, pages 55-63, March 2013. doi:10.1145&#x2F;2460276.2462076 Prince Mahajan, Lorenzo Alvisi, and Mike Dahlin: “Consistency, Availability, and Convergence,” University of Texas at Austin, Department of Computer Science, Tech Report UTCS TR-11-22, May 2011. Alex Scotti: “Adventures in Building Your Own Database,” at All Your Base, November 2015. Peter Bailis, Aaron Davidson, Alan Fekete, et al.: “Highly Available Transactions: Virtues and Limitations,” at 40th International Conference on Very Large Data Bases (VLDB), September 2014. Extended version published as pre-print arXiv:1302.0309 &amp;#91;cs.DB&amp;#93;. Paolo Viotti and Marko Vukolić: “Consistency in Non-Transactional Distributed Storage Systems,” arXiv:1512.00168, 12 April 2016. Maurice P. Herlihy and Jeannette M. Wing: “Linearizability: A Correctness Condition for Concurrent Objects,” ACM Transactions on Programming Languages and Systems (TOPLAS), volume 12, number 3, pages 463–492, July 1990. doi:10.1145&#x2F;78969.78972 Leslie Lamport: “On interprocess communication,” Distributed Computing, volume 1, number 2, pages 77–101, June 1986. doi:10.1007&#x2F;BF01786228 David K. Gifford: “Information Storage in a Decentralized Computer System,” Xerox Palo Alto Research Centers, CSL-81-8, June 1981. Martin Kleppmann: “Please Stop Calling Databases CP or AP,” martin.kleppmann.com, May 11, 2015. Kyle Kingsbury: “Call Me Maybe: MongoDB Stale Reads,” aphyr.com, April 20, 2015. Kyle Kingsbury: “Computational Techniques in Knossos,” aphyr.com, May 17, 2014. Peter Bailis: “Linearizability Versus Serializability,” bailis.org, September 24, 2014. Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: Concurrency Control and Recovery in Database Systems. Addison-Wesley, 1987. ISBN: 978-0-201-10715-9, available online at research.microsoft.com. Mike Burrows: “The Chubby Lock Service for Loosely-Coupled Distributed Systems,” at 7th USENIX Symposium on Operating System Design and Implementation (OSDI), November 2006. Flavio P. Junqueira and Benjamin Reed: ZooKeeper: Distributed Process Coordination. O’Reilly Media, 2013. ISBN: 978-1-449-36130-3 “etcd 2.0.12 Documentation,” CoreOS, Inc., 2015. “Apache Curator,” Apache Software Foundation, curator.apache.org, 2015. Morali Vallath: Oracle 10g RAC Grid, Services &amp; Clustering. Elsevier Digital Press, 2006. ISBN: 978-1-555-58321-7 Peter Bailis, Alan Fekete, Michael J Franklin, et al.: “Coordination-Avoiding Database Systems,” Proceedings of the VLDB Endowment, volume 8, number 3, pages 185–196, November 2014. Kyle Kingsbury: “Call Me Maybe: etcd and Consul,” aphyr.com, June 9, 2014. Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini: “Zab: High-Performance Broadcast for Primary-Backup Systems,” at 41st IEEE International Conference on Dependable Systems and Networks (DSN), June 2011. doi:10.1109&#x2F;DSN.2011.5958223 Diego Ongaro and John K. Ousterhout: “In Search of an Understandable Consensus Algorithm (Extended Version),” at USENIX Annual Technical Conference (ATC), June 2014. Hagit Attiya, Amotz Bar-Noy, and Danny Dolev: “Sharing Memory Robustly in Message-Passing Systems,” Journal of the ACM, volume 42, number 1, pages 124–142, January 1995. doi:10.1145&#x2F;200836.200869 Nancy Lynch and Alex Shvartsman: “Robust Emulation of Shared Memory Using Dynamic Quorum-Acknowledged Broadcasts,” at 27th Annual International Symposium on Fault-Tolerant Computing (FTCS), June 1997. doi:10.1109&#x2F;FTCS.1997.614100 Christian Cachin, Rachid Guerraoui, and Luís Rodrigues: Introduction to Reliable and Secure Distributed Programming, 2nd edition. Springer, 2011. ISBN: 978-3-642-15259-7, doi:10.1007&#x2F;978-3-642-15260-3 Sam Elliott, Mark Allen, and Martin Kleppmann: personal communication, thread on twitter.com, October 15, 2015. Niklas Ekström, Mikhail Panchenko, and Jonathan Ellis: “Possible Issue with Read Repair?,” email thread on cassandra-dev mailing list, October 2012. Maurice P. Herlihy: “Wait-Free Synchronization,” ACM Transactions on Programming Languages and Systems (TOPLAS), volume 13, number 1, pages 124–149, January 1991. doi:10.1145&#x2F;114005.102808 Armando Fox and Eric A. Brewer: “Harvest, Yield, and Scalable Tolerant Systems,” at 7th Workshop on Hot Topics in Operating Systems (HotOS), March 1999. doi:10.1109&#x2F;HOTOS.1999.798396 Seth Gilbert and Nancy Lynch: “Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services,” ACM SIGACT News, volume 33, number 2, pages 51–59, June 2002. doi:10.1145&#x2F;564585.564601 Seth Gilbert and Nancy Lynch: “Perspectives on the CAP Theorem,” IEEE Computer Magazine, volume 45, number 2, pages 30–36, February 2012. doi:10.1109&#x2F;MC.2011.389 Eric A. Brewer: “CAP Twelve Years Later: How the ‘Rules’ Have Changed,” IEEE Computer Magazine, volume 45, number 2, pages 23–29, February 2012. doi:10.1109&#x2F;MC.2012.37 Susan B. Davidson, Hector Garcia-Molina, and Dale Skeen: “Consistency in Partitioned Networks,” ACM Computing Surveys, volume 17, number 3, pages 341–370, September 1985. doi:10.1145&#x2F;5505.5508 Paul R. Johnson and Robert H. Thomas: “RFC 677: The Maintenance of Duplicate Databases,” Network Working Group, January 27, 1975. Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.: “Notes on Distributed Databases,” IBM Research, Research Report RJ2571(33471), July 1979. Michael J. Fischer and Alan Michael: “Sacrificing Serializability to Attain High Availability of Data in an Unreliable Network,” at 1st ACM Symposium on Principles of Database Systems (PODS), March 1982. doi:10.1145&#x2F;588111.588124 Eric A. Brewer: “NoSQL: Past, Present, Future,” at QCon San Francisco, November 2012. Henry Robinson: “CAP Confusion: Problems with ‘Partition Tolerance,’” blog.cloudera.com, April 26, 2010. Adrian Cockcroft: “Migrating to Microservices,” at QCon London, March 2014. Martin Kleppmann: “A Critique of the CAP Theorem,” arXiv:1509.05393, September 17, 2015. Nancy A. Lynch: “A Hundred Impossibility Proofs for Distributed Computing,” at 8th ACM Symposium on Principles of Distributed Computing (PODC), August 1989. doi:10.1145&#x2F;72981.72982 Hagit Attiya, Faith Ellen, and Adam Morrison: “Limitations of Highly-Available Eventually-Consistent Data Stores,” at ACM Symposium on Principles of Distributed Computing (PODC), July 2015. doi:10.1145&#x2F;2767386.2767419](http://dx.doi.org/10.1145/2767386.2767419) Peter Sewell, Susmit Sarkar, Scott Owens, et al.: “x86-TSO: A Rigorous and Usable Programmer’s Model for x86 Multiprocessors,” Communications of the ACM, volume 53, number 7, pages 89–97, July 2010. doi:10.1145&#x2F;1785414.1785443 Martin Thompson: “Memory Barriers&#x2F;Fences,” mechanical-sympathy.blogspot.co.uk, July 24, 2011. Ulrich Drepper: “What Every Programmer Should Know About Memory,” akkadia.org, November 21, 2007. Daniel J. Abadi: “Consistency Tradeoffs in Modern Distributed Database System Design,” IEEE Computer Magazine, volume 45, number 2, pages 37–42, February 2012. doi:10.1109&#x2F;MC.2012.33 Hagit Attiya and Jennifer L. Welch: “Sequential Consistency Versus Linearizability,” ACM Transactions on Computer Systems (TOCS), volume 12, number 2, pages 91–122, May 1994. doi:10.1145&#x2F;176575.176576 Mustaque Ahamad, Gil Neiger, James E. Burns, et al.: “Causal Memory: Definitions, Implementation, and Programming,” Distributed Computing, volume 9, number 1, pages 37–49, March 1995. doi:10.1007&#x2F;BF01784241 Wyatt Lloyd, Michael J. Freedman, Michael Kaminsky, and David G. Andersen: “Stronger Semantics for Low-Latency Geo-Replicated Storage,” at 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI), April 2013. Marek Zawirski, Annette Bieniusa, Valter Balegas, et al.: “SwiftCloud: Fault-Tolerant Geo-Replication Integrated All the Way to the Client Machine,” INRIA Research Report 8347, August 2013. Peter Bailis, Ali Ghodsi, Joseph M Hellerstein, and Ion Stoica: “Bolt-on Causal Consistency,” at ACM International Conference on Management of Data (SIGMOD), June 2013. Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: “Challenges to Adopting Stronger Consistency at Scale,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS), May 2015. Peter Bailis: “Causality Is Expensive (and What to Do About It),” bailis.org, February 5, 2014. Ricardo Gonçalves, Paulo Sérgio Almeida, Carlos Baquero, and Victor Fonte: “Concise Server-Wide Causality Management for Eventually Consistent Data Stores,” at 15th IFIP International Conference on Distributed Applications and Interoperable Systems (DAIS), June 2015. doi:10.1007&#x2F;978-3-319-19129-4_6 Rob Conery: “A Better ID Generator for PostgreSQL,” rob.conery.io, May 29, 2014. Leslie Lamport: “Time, Clocks, and the Ordering of Events in a Distributed System,” Communications of the ACM, volume 21, number 7, pages 558–565, July 1978. doi:10.1145&#x2F;359545.359563 Xavier Défago, André Schiper, and Péter Urbán: “Total Order Broadcast and Multicast Algorithms: Taxonomy and Survey,” ACM Computing Surveys, volume 36, number 4, pages 372–421, December 2004. doi:10.1145&#x2F;1041680.1041682 Hagit Attiya and Jennifer Welch: Distributed Computing: Fundamentals, Simulations and Advanced Topics, 2nd edition. John Wiley &amp; Sons, 2004. ISBN: 978-0-471-45324-6, doi:10.1002&#x2F;0471478210 Mahesh Balakrishnan, Dahlia Malkhi, Vijayan Prabhakaran, et al.: “CORFU: A Shared Log Design for Flash Clusters,” at 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI), April 2012. Fred B. Schneider: “Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial,” ACM Computing Surveys, volume 22, number 4, pages 299–319, December 1990. Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, et al.: “Calvin: Fast Distributed Transactions for Partitioned Database Systems,” at ACM International Conference on Management of Data (SIGMOD), May 2012. Mahesh Balakrishnan, Dahlia Malkhi, Ted Wobber, et al.: “Tango: Distributed Data Structures over a Shared Log,” at 24th ACM Symposium on Operating Systems Principles (SOSP), November 2013. doi:10.1145&#x2F;2517349.2522732 Robbert van Renesse and Fred B. Schneider: “Chain Replication for Supporting High Throughput and Availability,” at 6th USENIX Symposium on Operating System Design and Implementation (OSDI), December 2004. Leslie Lamport: “How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs,” IEEE Transactions on Computers, volume 28, number 9, pages 690–691, September 1979. doi:10.1109&#x2F;TC.1979.1675439 Enis Söztutar, Devaraj Das, and Carter Shanklin: “Apache HBase High Availability at the Next Level,” hortonworks.com, January 22, 2015. Brian F Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, et al.: “PNUTS: Yahoo!’s Hosted Data Serving Platform,” at 34th International Conference on Very Large Data Bases (VLDB), August 2008. doi:10.14778&#x2F;1454159.1454167 Tushar Deepak Chandra and Sam Toueg: “Unreliable Failure Detectors for Reliable Distributed Systems,” Journal of the ACM, volume 43, number 2, pages 225–267, March 1996. doi:10.1145&#x2F;226643.226647 Michael J. Fischer, Nancy Lynch, and Michael S. Paterson: “Impossibility of Distributed Consensus with One Faulty Process,” Journal of the ACM, volume 32, number 2, pages 374–382, April 1985. doi:10.1145&#x2F;3149.214121 Michael Ben-Or: “Another Advantage of Free Choice: Completely Asynchronous Agreement Protocols,” at 2nd ACM Symposium on Principles of Distributed Computing (PODC), August 1983. doi:10.1145&#x2F;800221.806707 Jim N. Gray and Leslie Lamport: “Consensus on Transaction Commit,” ACM Transactions on Database Systems (TODS), volume 31, number 1, pages 133–160, March 2006. doi:10.1145&#x2F;1132863.1132867 Rachid Guerraoui: “Revisiting the Relationship Between Non-Blocking Atomic Commitment and Consensus,” at 9th International Workshop on Distributed Algorithms (WDAG), September 1995. doi:10.1007&#x2F;BFb0022140 Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram, Ramnatthan Alagappan, et al.: “All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications,” at 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2014. Jim Gray: “The Transaction Concept: Virtues and Limitations,” at 7th International Conference on Very Large Data Bases (VLDB), September 1981. Hector Garcia-Molina and Kenneth Salem: “Sagas,” at ACM International Conference on Management of Data (SIGMOD), May 1987. doi:10.1145&#x2F;38713.38742 C. Mohan, Bruce G. Lindsay, and Ron Obermarck: “Transaction Management in the R* Distributed Database Management System,” ACM Transactions on Database Systems, volume 11, number 4, pages 378–396, December 1986. doi:10.1145&#x2F;7239.7266 “Distributed Transaction Processing: The XA Specification,” X&#x2F;Open Company Ltd., Technical Standard XO&#x2F;CAE&#x2F;91&#x2F;300, December 1991. ISBN: 978-1-872-63024-3 Mike Spille: “XA Exposed, Part II,” jroller.com, April 3, 2004. Ivan Silva Neto and Francisco Reverbel: “Lessons Learned from Implementing WS-Coordination and WS-AtomicTransaction,” at 7th IEEE&#x2F;ACIS International Conference on Computer and Information Science (ICIS), May 2008. doi:10.1109&#x2F;ICIS.2008.75 James E. Johnson, David E. Langworthy, Leslie Lamport, and Friedrich H. Vogt: “Formal Specification of a Web Services Protocol,” at 1st International Workshop on Web Services and Formal Methods (WS-FM), February 2004. doi:10.1016&#x2F;j.entcs.2004.02.022 Dale Skeen: “Nonblocking Commit Protocols,” at ACM International Conference on Management of Data (SIGMOD), April 1981. doi:10.1145&#x2F;582318.582339 Gregor Hohpe: “Your Coffee Shop Doesn’t Use Two-Phase Commit,” IEEE Software, volume 22, number 2, pages 64–66, March 2005. doi:10.1109&#x2F;MS.2005.52 Pat Helland: “Life Beyond Distributed Transactions: An Apostate’s Opinion,” at 3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 2007. Jonathan Oliver: “My Beef with MSDTC and Two-Phase Commits,” blog.jonathanoliver.com, April 4, 2011. Oren Eini (Ahende Rahien): “The Fallacy of Distributed Transactions,” ayende.com, July 17, 2014. Clemens Vasters: “Transactions in Windows Azure (with Service Bus) – An Email Discussion,” vasters.com, July 30, 2012. “Understanding Transactionality in Azure,” NServiceBus Documentation, Particular Software, 2015. Randy Wigginton, Ryan Lowe, Marcos Albe, and Fernando Ipar: “Distributed Transactions in MySQL,” at MySQL Conference and Expo, April 2013. Mike Spille: “XA Exposed, Part I,” jroller.com, April 3, 2004. Ajmer Dhariwal: “Orphaned MSDTC Transactions (-2 spids),” eraofdata.com, December 12, 2008. Paul Randal: “Real World Story of DBCC PAGE Saving the Day,” sqlskills.com, June 19, 2013. “in-doubt xact resolution Server Configuration Option,” SQL Server 2016 documentation, Microsoft, Inc., 2016. Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: “Consensus in the Presence of Partial Synchrony,” Journal of the ACM, volume 35, number 2, pages 288–323, April 1988. doi:10.1145&#x2F;42282.42283 Miguel Castro and Barbara H. Liskov: “Practical Byzantine Fault Tolerance and Proactive Recovery,” ACM Transactions on Computer Systems, volume 20, number 4, pages 396–461, November 2002. doi:10.1145&#x2F;571637.571640 Brian M. Oki and Barbara H. Liskov: “Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems,” at 7th ACM Symposium on Principles of Distributed Computing (PODC), August 1988. doi:10.1145&#x2F;62546.62549 Barbara H. Liskov and James Cowling: “Viewstamped Replication Revisited,” Massachusetts Institute of Technology, Tech Report MIT-CSAIL-TR-2012-021, July 2012. Leslie Lamport: “The Part-Time Parliament,” ACM Transactions on Computer Systems, volume 16, number 2, pages 133–169, May 1998. doi:10.1145&#x2F;279227.279229 Leslie Lamport: “Paxos Made Simple,” ACM SIGACT News, volume 32, number 4, pages 51–58, December 2001. Tushar Deepak Chandra, Robert Griesemer, and Joshua Redstone: “Paxos Made Live – An Engineering Perspective,” at 26th ACM Symposium on Principles of Distributed Computing (PODC), June 2007. Robbert van Renesse: “Paxos Made Moderately Complex,” cs.cornell.edu, March 2011. Diego Ongaro: “Consensus: Bridging Theory and Practice,” PhD Thesis, Stanford University, August 2014. Heidi Howard, Malte Schwarzkopf, Anil Madhavapeddy, and Jon Crowcroft: “Raft Refloated: Do We Have Consensus?,” ACM SIGOPS Operating Systems Review, volume 49, number 1, pages 12–21, January 2015. doi:10.1145&#x2F;2723872.2723876 André Medeiros: “ZooKeeper’s Atomic Broadcast Protocol: Theory and Practice,” Aalto University School of Science, March 20, 2012. Robbert van Renesse, Nicolas Schiper, and Fred B. Schneider: “Vive La Différence: Paxos vs. Viewstamped Replication vs. Zab,” IEEE Transactions on Dependable and Secure Computing, volume 12, number 4, pages 472–484, September 2014. doi:10.1109&#x2F;TDSC.2014.2355848 Will Portnoy: “Lessons Learned from Implementing Paxos,” blog.willportnoy.com, June 14, 2012. Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman: “Flexible Paxos: Quorum Intersection Revisited,” arXiv:1608.06696, August 24, 2016. Heidi Howard and Jon Crowcroft: “Coracle: Evaluating Consensus at the Internet Edge,” at Annual Conference of the ACM Special Interest Group on Data Communication (SIGCOMM), August 2015. doi:10.1145&#x2F;2829988.2790010 Kyle Kingsbury: “Call Me Maybe: Elasticsearch 1.5.0,” aphyr.com, April 27, 2015. Ivan Kelly: “BookKeeper Tutorial,” github.com, October 2014. Camille Fournier: “Consensus Systems for the Skeptical Architect,” at Craft Conference, Budapest, Hungary, April 2015. Kenneth P. Birman: “A History of the Virtual Synchrony Replication Model,” in Replication: Theory and Practice, Springer LNCS volume 5959, chapter 6, pages 91–120, 2010. ISBN: 978-3-642-11293-5, doi:10.1007&#x2F;978-3-642-11294-2_6"},{"title":"后记","path":"/wiki/ddia/colophon.html","content":"关于作者Martin Kleppmann 是英国剑桥大学分布式系统的研究员。此前他曾在互联网公司担任过软件工程师和企业家，其中包括 LinkedIn 和 Rapportive，负责大规模数据基础架构。在这个过程中，他以艰难的方式学习了一些东西，他希望这本书能够让你避免重蹈覆辙。 Martin 是一位常规会议演讲者，博主和开源贡献者。他认为，每个人都应该有深刻的技术理念，深层次的理解能帮助我们开发出更好的软件。 关于译者冯若航 PostgreSQL DBA @ TanTan Alibaba+-Finplus 架构师&#x2F;全栈工程师 (2015 ~ 2017) 后记《设计数据密集型应用》封面上的动物是 印度野猪（Sus scrofa cristatus），它是在印度、缅甸、尼泊尔、斯里兰卡和泰国发现的一种野猪的亚种。与欧洲野猪不同，它们有更高的背部鬃毛，没有体表绒毛，以及更大更直的头骨。 印度野猪有一头灰色或黑色的头发，脊背上有短而硬的毛。雄性有突出的犬齿（称为 T），用来与对手战斗或抵御掠食者。雄性比雌性大，这些物种平均肩高 33-35 英寸，体重 200-300 磅。他们的天敌包括熊、老虎和各种大型猫科动物。 这些动物夜行且杂食 —— 它们吃各种各样的东西，包括根、昆虫、腐肉、坚果、浆果和小动物。野猪经常因为破坏农作物的根被人们所熟知，他们造成大量的破坏，并被农民所敌视。他们每天需要摄入 4,000 ~ 4,500 卡路里的能量。野猪有发达的嗅觉，这有助于寻找地下植物和挖掘动物。然而，它们的视力很差。 野猪在人类文化中一直具有重要意义。在印度教传说中，野猪是毗湿奴神的化身。在古希腊的丧葬纪念碑中，它是一个勇敢失败者的象征（与胜利的狮子相反）。由于它的侵略，它被描绘在斯堪的纳维亚、日耳曼和盎格鲁撒克逊战士的盔甲和武器上。在中国十二生肖中，它象征着决心和急躁。 O’Reilly 封面上的许多动物都受到威胁，这些动物对世界都很重要。要了解有关如何提供帮助的更多信息，请访问 animals.oreilly.com。 封面图片来自 Shaw’s Zoology。封面字体是 URW Typewriter 和 Guardian Sans。文字字体是 Adobe Minion Pro；图中的字体是 Adobe Myriad Pro；标题字体是 Adobe Myriad Condensed；代码字体是 Dalton Maag 的 Ubuntu Mono。"},{"title":"术语表","path":"/wiki/ddia/glossary.html","content":"请注意，本术语表中的定义简短而简单，旨在传达核心思想，而不是术语的完整细微之处。 有关更多详细信息，请参阅正文中的参考资料。 异步（asynchronous） 不等待某些事情完成（例如，将数据发送到网络中的另一个节点），并且不会假设要花多长时间。请参阅“同步复制与异步复制”、“同步网络与异步网络”以及“系统模型与现实”。 原子（atomic） 在并发操作的上下文中：描述一个在单个时间点看起来生效的操作，所以另一个并发进程永远不会遇到处于“半完成”状态的操作。另见隔离。 在事务的上下文中：将一些写入操作分为一组，这组写入要么全部提交成功，要么遇到错误时全部回滚。请参阅“原子性”和“原子提交与两阶段提交”。 背压（backpressure） 接收方接收数据速度较慢时，强制降低发送方的数据发送速度。也称为流量控制。请参阅“消息传递系统”。 批处理（batch process） 一种计算，它将一些固定的（通常是大的）数据集作为输入，并将其他一些数据作为输出，而不修改输入。见第十章。 边界（bounded） 有一些已知的上限或大小。例如，网络延迟情况（请参阅“超时与无穷的延迟”）和数据集（请参阅第十一章的介绍）。 拜占庭故障（Byzantine fault） 表现异常的节点，这种异常可能以任意方式出现，例如向其他节点发送矛盾或恶意消息。请参阅“拜占庭故障”。 缓存（cache） 一种组件，通过存储最近使用过的数据，加快未来对相同数据的读取速度。缓存中通常存放部分数据：因此，如果缓存中缺少某些数据，则必须从某些底层较慢的数据存储系统中，获取完整的数据副本。 CAP定理（CAP theorem） 一个被广泛误解的理论结果，在实践中是没有用的。请参阅“CAP定理”。 因果关系（causality） 事件之间的依赖关系，当一件事发生在另一件事情之前。例如，后面的事件是对早期事件的回应，或者依赖于更早的事件，或者应该根据先前的事件来理解。请参阅““此前发生”的关系和并发”和“顺序与因果关系”。 共识（consensus） 分布式计算的一个基本问题，就是让几个节点同意某些事情（例如，哪个节点应该是数据库集群的领导者）。问题比乍看起来要困难得多。请参阅“容错共识”。 数据仓库（data warehouse） 一个数据库，其中来自几个不同的OLTP系统的数据已经被合并和准备用于分析目的。请参阅“数据仓库”。 声明式（declarative） 描述某些东西应有的属性，但不知道如何实现它的确切步骤。在查询的上下文中，查询优化器采用声明性查询并决定如何最好地执行它。请参阅“数据查询语言”。 非规范化（denormalize） 为了加速读取，在标准数据集中引入一些冗余或重复数据，通常采用缓存或索引的形式。非规范化的值是一种预先计算的查询结果，像物化视图。请参阅“单对象和多对象操作”和“从同一事件日志中派生多个视图”。 衍生数据（derived data） 一种数据集，根据其他数据通过可重复运行的流程创建。必要时，你可以运行该流程再次创建衍生数据。衍生数据通常用于提高特定数据的读取速度。常见的衍生数据有索引、缓存和物化视图。请参阅第三部分的介绍。 确定性（deterministic） 描述一个函数，如果给它相同的输入，则总是产生相同的输出。这意味着它不能依赖于随机数字、时间、网络通信或其他不可预测的事情。 分布式（distributed） 在由网络连接的多个节点上运行。对于部分节点故障，具有容错性：系统的一部分发生故障时，其他部分仍可以正常工作，通常情况下，软件无需了解故障相关的确切情况。请参阅“故障与部分失效”。 持久（durable） 以某种方式存储数据，即使发生各种故障，也不会丢失数据。请参阅“持久性”。 ETL（Extract-Transform-Load） 提取-转换-加载（Extract-Transform-Load）。从源数据库中提取数据，将其转换为更适合分析查询的形式，并将其加载到数据仓库或批处理系统中的过程。请参阅“数据仓库”。 故障切换（failover） 在具有单一领导者的系统中，故障切换是将领导角色从一个节点转移到另一个节点的过程。请参阅“处理节点宕机”。 容错（fault-tolerant） 如果出现问题（例如，机器崩溃或网络连接失败），可以自动恢复。请参阅“可靠性”。 流量控制（flow control） 见背压（backpressure）。 追随者（follower） 一种数据副本，仅处理领导者或主库发出的数据变更，不直接接受来自客户端的任何写入。也称为备库、从库、只读副本或热备份。请参阅“领导者与追随者”。 全文检索（full-text search） 通过任意关键字来搜索文本，通常具有附加特征，例如匹配类似的拼写词或同义词。全文索引是一种支持这种查询的次级索引。请参阅“全文搜索和模糊索引”。 图（graph） 一种数据结构，由顶点（可以指向的东西，也称为节点或实体）和边（从一个顶点到另一个顶点的连接，也称为关系或弧）组成。请参阅“图数据模型”。 散列（hash） 将输入转换为看起来像随机数值的函数。相同的输入会转换为相同的数值，不同的输入一般会转换为不同的数值，也可能转换为相同数值（也被称为冲突）。请参阅“根据键的散列分区”。 幂等（idempotent） 用于描述一种操作可以安全地重试执行，即执行多次的效果和执行一次的效果相同。请参阅“幂等性”。 索引（index） 一种数据结构。通过索引，你可以根据特定字段的值，在所有数据记录中进行高效检索。请参阅“驱动数据库的数据结构”。 隔离性（isolation） 在事务上下文中，用于描述并发执行事务的互相干扰程度。串行运行具有最强的隔离性，不过其它程度的隔离也通常被使用。请参阅“隔离性”。 连接（join） 汇集有共同点的记录。在一个记录与另一个记录有关（外键，文档参考，图中的边）的情况下最常用，查询需要获取参考所指向的记录。请参阅“多对一和多对多的关系”和“Reduce侧连接与分组”。 领导者（leader） 当数据或服务被复制到多个节点时，领导者是被指定为可以接受数据变更的副本。领导者可以通过某些协议选举产生，也可以由管理员手动选择。领导者也被称为主库。请参阅“领导者与追随者”。 线性化（linearizable） 表现为系统中只有一份通过原子操作更新的数据副本。请参阅“线性一致性”。 局部性（locality） 一种性能优化方式，如果经常在相同的时间请求一些离散数据，把这些数据放到一个位置。请参阅“查询的数据局部性”。 锁（lock） 一种保证只有一个线程、节点或事务可以访问的机制，如果其它线程、节点或事务想访问相同元素，则必须等待锁被释放。请参阅“两阶段锁定”和“领导者和锁”。 日志（log） 日志是一个只能以追加方式写入的文件，用于存放数据。预写式日志用于在存储引擎崩溃时恢复数据（请参阅“让B树更可靠”）；结构化日志存储引擎使用日志作为它的主要存储格式（请参阅“SSTables和LSM树”）；复制型日志用于把写入从领导者复制到追随者（请参阅“领导者与追随者”）；事件性日志可以表现为数据流（请参阅“分区日志”）。 物化（materialize） 急切地计算并写出结果，而不是在请求时计算。请参阅“聚合：数据立方体和物化视图”和“物化中间状态”。 节点（node） 计算机上运行的一些软件的实例，通过网络与其他节点通信以完成某项任务。 规范化（normalized） 以没有冗余或重复的方式进行结构化。 在规范化数据库中，当某些数据发生变化时，你只需要在一个地方进行更改，而不是在许多不同的地方复制很多次。 请参阅“多对一和多对多的关系”。 OLAP（Online Analytic Processing） 在线分析处理。 通过对大量记录进行聚合（例如，计数，总和，平均）来表征的访问模式。 请参阅“事务处理还是分析？”。 OLTP（Online Transaction Processing） 在线事务处理。 访问模式的特点是快速查询，读取或写入少量记录，这些记录通常通过键索引。 请参阅“事务处理还是分析？”。 分区（partitioning） 将单机上的大型数据集或计算结果拆分为较小部分，并将其分布到多台机器上。 也称为分片。见第六章。 百分位点（percentile） 通过计算有多少值高于或低于某个阈值来衡量值分布的方法。 例如，某个时间段的第95个百分位响应时间是时间t，则该时间段中，95%的请求完成时间小于t，5%的请求完成时间要比t长。 请参阅“描述性能”。 主键（primary key） 唯一标识记录的值（通常是数字或字符串）。 在许多应用程序中，主键由系统在创建记录时生成（例如，按顺序或随机）; 它们通常不由用户设置。 另请参阅次级索引。 法定人数（quorum） 在操作完成之前，需要对操作进行投票的最少节点数量。 请参阅“读写的法定人数”。 再平衡（rebalance） 将数据或服务从一个节点移动到另一个节点以实现负载均衡。 请参阅“分区再平衡”。 复制（replication） 在几个节点（副本）上保留相同数据的副本，以便在某些节点无法访问时，数据仍可访问。请参阅第五章。 模式（schema） 一些数据结构的描述，包括其字段和数据类型。 可以在数据生命周期的不同点检查某些数据是否符合模式（请参阅“文档模型中的模式灵活性”），模式可以随时间变化（请参阅第四章）。 次级索引（secondary index） 与主要数据存储器一起维护的附加数据结构，使你可以高效地搜索与某种条件相匹配的记录。 请参阅“其他索引结构”和“分区与次级索引”。 可串行化（serializable） 保证多个并发事务同时执行时，它们的行为与按顺序逐个执行事务相同。 请参阅第七章的“可串行化”。 无共享（shared-nothing） 与共享内存或共享磁盘架构相比，独立节点（每个节点都有自己的CPU，内存和磁盘）通过传统网络连接。 见第二部分的介绍。 偏斜（skew） 各分区负载不平衡，例如某些分区有大量请求或数据，而其他分区则少得多。也被称为热点。请参阅“负载偏斜与热点消除”和“处理偏斜”。 时间线异常导致事件以不期望的顺序出现。 请参阅“快照隔离和可重复读”中的关于读取偏差的讨论，“写入偏差与幻读”中的写入偏差以及“有序事件的时间戳”中的时钟偏斜。 脑裂（split brain） 两个节点同时认为自己是领导者的情况，这种情况可能违反系统担保。 请参阅“处理节点宕机”和“真相由多数所定义”。 存储过程（stored procedure） 一种对事务逻辑进行编码的方式，它可以完全在数据库服务器上执行，事务执行期间无需与客户端通信。 请参阅“真的串行执行”。 流处理（stream process） 持续运行的计算。可以持续接收事件流作为输入，并得出一些输出。 见第十一章。 同步（synchronous） 异步的反义词。 记录系统（system of record） 一个保存主要权威版本数据的系统，也被称为真相的来源。首先在这里写入数据变更，其他数据集可以从记录系统衍生。 请参阅第三部分的介绍。 超时（timeout） 检测故障的最简单方法之一，即在一段时间内观察是否缺乏响应。 但是，不可能知道超时是由于远程节点的问题还是网络中的问题造成的。 请参阅“超时与无穷的延迟”。 全序（total order） 一种比较事物的方法（例如时间戳），可以让你总是说出两件事中哪一件更大，哪件更小。 总的来说，有些东西是无法比拟的（不能说哪个更大或更小）的顺序称为偏序。 请参阅“因果顺序不是全序的”。 事务（transaction） 为了简化错误处理和并发问题，将几个读写操作分组到一个逻辑单元中。 见第七章。 两阶段提交（2PC, two-phase commit） 一种确保多个数据库节点全部提交或全部中止事务的算法。 请参阅“原子提交与两阶段提交”。 两阶段锁定（2PL, two-phase locking） 一种用于实现可串行化隔离的算法，该算法通过事务获取对其读取或写入的所有数据的锁，直到事务结束。 请参阅“两阶段锁定”。 无边界（unbounded） 没有任何已知的上限或大小。 反义词是边界（bounded）。"},{"title":"数据密集型应用系统设计","path":"/wiki/ddia/index.html","content":"设计数据密集型应用 - 中文翻译Tips作者： Martin Kleppmann原名：《Designing Data-Intensive Applications》译者：冯若航 （@Vonng） 译序 不懂数据库的全栈工程师不是好架构师 —— Vonng 现今，尤其是在互联网领域，大多数应用都属于数据密集型应用。本书从底层数据结构到顶层架构设计，将数据系统设计中的精髓娓娓道来。其中的宝贵经验无论是对架构师、DBA、还是后端工程师、甚至产品经理都会有帮助。 这是一本理论结合实践的书，书中很多问题，译者在实际场景中都曾遇到过，读来让人击节扼腕。如果能早点读到这本书，该少走多少弯路啊！ 这也是一本深入浅出的书，讲述概念的来龙去脉而不是卖弄定义，介绍事物发展演化历程而不是事实堆砌，将复杂的概念讲述的浅显易懂，但又直击本质不失深度。每章最后的引用质量非常好，是深入学习各个主题的绝佳索引。 本书为数据系统的设计、实现、与评价提供了很好的概念框架。读完并理解本书内容后，读者可以轻松看破大多数的技术忽悠，与技术砖家撕起来虎虎生风🤣。 这是 2017 年译者读过最好的一本技术类书籍，这么好的书没有中文翻译，实在是遗憾。某不才，愿为先进技术文化的传播贡献一份力量。既可以深入学习有趣的技术主题，又可以锻炼中英文语言文字功底，何乐而不为？ 前言 在我们的社会中，技术是一种强大的力量。数据、软件、通信可以用于坏的方面：不公平的阶级固化，损害公民权利，保护既得利益集团。但也可以用于好的方面：让底层人民发出自己的声音，让每个人都拥有机会，避免灾难。本书献给所有将技术用于善途的人们。 计算是一种流行文化，流行文化鄙视历史。 流行文化关乎个体身份和参与感，但与合作无关。流行文化活在当下，也与过去和未来无关。 我认为大部分（为了钱）编写代码的人就是这样的， 他们不知道自己的文化来自哪里。 —— 阿兰・凯接受 Dobb 博士的杂志采访时（2012 年） 目录 序言 第一部分：数据系统基础 第一章：可靠性、可伸缩性和可维护性 关于数据系统的思考 可靠性 可伸缩性 可维护性 本章小结 第二章：数据模型与查询语言 关系模型与文档模型 数据查询语言 图数据模型 本章小结 第三章：存储与检索 驱动数据库的数据结构 事务处理还是分析？ 列式存储 本章小结 第四章：编码与演化 编码数据的格式 数据流的类型 本章小结 第二部分：分布式数据 第五章：复制 领导者与追随者 复制延迟问题 多主复制 无主复制 本章小结 第六章：分区 分区与复制 键值数据的分区 分区与次级索引 分区再平衡 请求路由 本章小结 第七章：事务 事务的棘手概念 弱隔离级别 可串行化 本章小结 第八章：分布式系统的麻烦 故障与部分失效 不可靠的网络 不可靠的时钟 知识、真相与谎言 本章小结 第九章：一致性与共识 一致性保证 线性一致性 顺序保证 分布式事务与共识 本章小结 第三部分：衍生数据 第十章：批处理 使用Unix工具的批处理 MapReduce和分布式文件系统 MapReduce之后 本章小结 第十一章：流处理 传递事件流 数据库与流 流处理 本章小结 第十二章：数据系统的未来 数据集成 分拆数据库 将事情做正确 做正确的事情 本章小结 术语表 后记 PDF 资源数据密集型应用系统设计 - 阿里云盘"},{"title":"第一部分：数据系统的基石","path":"/wiki/ddia/part-i.html","content":"本书前四章介绍了数据系统底层的基础概念，无论是在单台机器上运行的单点数据系统，还是分布在多台机器上的分布式数据系统都适用。 第一章 将介绍本书使用的术语和方法。可靠性，可伸缩性和可维护性 ，这些词汇到底意味着什么？如何实现这些目标？ 第二章 将对几种不同的 数据模型和查询语言 进行比较。从程序员的角度看，这是数据库之间最明显的区别。不同的数据模型适用于不同的应用场景。 第三章 将深入 存储引擎 内部，研究数据库如何在磁盘上摆放数据。不同的存储引擎针对不同的负载进行优化，选择合适的存储引擎对系统性能有巨大影响。 第四章 将对几种不同的 数据编码 进行比较。特别研究了这些格式在应用需求经常变化、模式需要随时间演变的环境中表现如何。 第二部分将专门讨论在 分布式数据系统 中特有的问题。"},{"title":"第二部分：分布式数据","path":"/wiki/ddia/part-ii.html","content":"一个成功的技术，现实的优先级必须高于公关，你可以糊弄别人，但糊弄不了自然规律。 —— 罗杰斯委员会报告（1986） 在本书的 第一部分 中，我们讨论了数据系统的各个方面，但仅限于数据存储在单台机器上的情况。现在我们到了 第二部分，进入更高的层次，并提出一个问题：如果 多台机器 参与数据的存储和检索，会发生什么？ 你可能会出于各种各样的原因，希望将数据库分布到多台机器上： 可伸缩性 如果你的数据量、读取负载、写入负载超出单台机器的处理能力，可以将负载分散到多台计算机上。 容错 &#x2F; 高可用性 如果你的应用需要在单台机器（或多台机器，网络或整个数据中心）出现故障的情况下仍然能继续工作，则可使用多台机器，以提供冗余。一台故障时，另一台可以接管。 延迟 如果在世界各地都有用户，你也许会考虑在全球范围部署多个服务器，从而每个用户可以从地理上最近的数据中心获取服务，避免了等待网络数据包穿越半个世界。 伸缩至更高的载荷如果你需要的只是伸缩至更高的 载荷（load），最简单的方法就是购买更强大的机器（有时称为 垂直伸缩，即 vertical scaling，或 向上伸缩，即 scale up）。许多处理器，内存和磁盘可以在同一个操作系统下相互连接，快速的相互连接允许任意处理器访问内存或磁盘的任意部分。在这种 共享内存架构（shared-memory architecture） 中，所有的组件都可以看作一台单独的机器 [^i]。 [^i]: 在大型机中，尽管任意处理器都可以访问内存的任意部分，但总有一些内存区域与一些处理器更接近（称为 非均匀内存访问（nonuniform memory access, NUMA）【1】）。 为了有效利用这种架构特性，需要对处理进行细分，以便每个处理器主要访问临近的内存，这意味着即使表面上看起来只有一台机器在运行，分区（partitioning） 仍然是必要的。 共享内存方法的问题在于，成本增长速度快于线性增长：一台有着双倍处理器数量，双倍内存大小，双倍磁盘容量的机器，通常成本会远远超过原来的两倍。而且可能因为存在瓶颈，并不足以处理双倍的载荷。 共享内存架构可以提供有限的容错能力，高端机器可以使用热插拔的组件（不关机更换磁盘，内存模块，甚至处理器）—— 但它必然囿于单个地理位置的桎梏。 另一种方法是 共享磁盘架构（shared-disk architecture），它使用多台具有独立处理器和内存的机器，但将数据存储在机器之间共享的磁盘阵列上，这些磁盘通过快速网络连接 [^ii]。这种架构用于某些数据仓库，但竞争和锁定的开销限制了共享磁盘方法的可伸缩性【2】。 [^ii]: 网络附属存储（Network Attached Storage, NAS），或 存储区网络（Storage Area Network, SAN） 无共享架构相比之下，无共享架构【3】（shared-nothing architecture，有时被称为 水平伸缩，即 horizontal scaling，或 向外伸缩，即 scaling out）已经相当普及。在这种架构中，运行数据库软件的每台机器 &#x2F; 虚拟机都称为 节点（node）。每个节点只使用各自的处理器，内存和磁盘。节点之间的任何协调，都是在软件层面使用传统网络实现的。 无共享系统不需要使用特殊的硬件，所以你可以用任意机器 —— 比如性价比最好的机器。你也许可以跨多个地理区域分布数据从而减少用户延迟，或者在损失一整个数据中心的情况下幸免于难。随着云端虚拟机部署的出现，即使是小公司，现在无需 Google 级别的运维，也可以实现异地分布式架构。 在这一部分里，我们将重点放在无共享架构上。它不见得是所有场景的最佳选择，但它是最需要你谨慎从事的架构。如果你的数据分布在多个节点上，你需要意识到这样一个分布式系统中约束和权衡 —— 数据库并不能魔术般地把这些东西隐藏起来。 虽然分布式无共享架构有许多优点，但它通常也会给应用带来额外的复杂度，有时也会限制你可用数据模型的表达力。在某些情况下，一个简单的单线程程序可以比一个拥有超过 100 个 CPU 核的集群表现得更好【4】。另一方面，无共享系统可以非常强大。接下来的几章，将详细讨论分布式数据会带来的问题。 复制 vs 分区数据分布在多个节点上有两种常见的方式： 复制（Replication） 在几个不同的节点上保存数据的相同副本，可能放在不同的位置。 复制提供了冗余：如果一些节点不可用，剩余的节点仍然可以提供数据服务。 复制也有助于改善性能。 第五章 将讨论复制。 分区 (Partitioning) 将一个大型数据库拆分成较小的子集（称为 分区，即 partitions），从而不同的分区可以指派给不同的 节点（nodes，亦称 分片，即 sharding）。 第六章 将讨论分区。 复制和分区是不同的机制，但它们经常同时使用。如 图 II-1 所示。 图 II-1 一个数据库切分为两个分区，每个分区都有两个副本 理解了这些概念，就可以开始讨论在分布式系统中需要做出的困难抉择。第七章 将讨论 事务（Transaction），这对于了解数据系统中可能出现的各种问题，以及我们可以做些什么很有帮助。第八章 和 第九章 将讨论分布式系统的根本局限性。 在本书的 第三部分 中，将讨论如何将多个（可能是分布式的）数据存储集成为一个更大的系统，以满足复杂的应用需求。 但首先，我们来聊聊分布式的数据。 索引 复制 分区 事务 分布式系统的麻烦 一致性与共识 参考文献 Ulrich Drepper: “What Every Programmer Should Know About Memory,” akka‐dia.org, November 21, 2007. Ben Stopford: “Shared Nothing vs. Shared Disk Architectures: An Independent View,” benstopford.com, November 24, 2009. Michael Stonebraker: “The Case for Shared Nothing,” IEEE Database EngineeringBulletin, volume 9, number 1, pages 4–9, March 1986. Frank McSherry, Michael Isard, and Derek G. Murray: “Scalability! But at What COST?,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS),May 2015."},{"title":"第三部分：衍生数据","path":"/wiki/ddia/part-iii.html","content":"在本书的 第一部分 和 第二部分 中，我们自底向上地把所有关于分布式数据库的主要考量都过了一遍。从数据在磁盘上的布局，一直到出现故障时分布式系统一致性的局限。但所有的讨论都假定了应用中只用了一种数据库。 现实世界中的数据系统往往更为复杂。大型应用程序经常需要以多种方式访问和处理数据，没有一个数据库可以同时满足所有这些不同的需求。因此应用程序通常组合使用多种组件：数据存储、索引、缓存、分析系统等等，并实现在这些组件中移动数据的机制。 本书的最后一部分，会研究将多个不同数据系统（可能有着不同数据模型，并针对不同的访问模式进行优化）集成为一个协调一致的应用架构时，会遇到的问题。软件供应商经常会忽略这一方面的生态建设，并声称他们的产品能够满足你的所有需求。在现实世界中，集成不同的系统是实际应用中最重要的事情之一。 记录系统和衍生数据系统从高层次上看，存储和处理数据的系统可以分为两大类： 记录系统（System of record） 记录系统，也被称为 真相源（source of truth），持有数据的权威版本。当新的数据进入时（例如，用户输入）首先会记录在这里。每个事实正正好好表示一次（表示通常是 正规化的，即 normalized）。如果其他系统和 记录系统 之间存在任何差异，那么记录系统中的值是正确的（根据定义）。 衍生数据系统（Derived data systems） 衍生系统 中的数据，通常是另一个系统中的现有数据以某种方式进行转换或处理的结果。如果丢失衍生数据，可以从原始来源重新创建。典型的例子是 缓存（cache）：如果数据在缓存中，就可以由缓存提供服务；如果缓存不包含所需数据，则降级由底层数据库提供。非规范化的值，索引和物化视图亦属此类。在推荐系统中，预测汇总数据通常衍生自用户日志。 从技术上讲，衍生数据是 冗余的（redundant），因为它重复了已有的信息。但是衍生数据对于获得良好的只读查询性能通常是至关重要的。它通常是非规范化的。可以从单个源头衍生出多个不同的数据集，使你能从不同的 “视角” 洞察数据。 并不是所有的系统都在其架构中明确区分 记录系统 和 衍生数据系统，但是这是一种有用的区分方式，因为它明确了系统中的数据流：系统的哪一部分具有哪些输入和哪些输出，以及它们如何相互依赖。 大多数数据库，存储引擎和查询语言，本质上既不是记录系统也不是衍生系统。数据库只是一个工具：如何使用它取决于你自己。记录系统和衍生数据系统之间的区别不在于工具，而在于应用程序中的使用方式。 通过梳理数据的衍生关系，可以清楚地理解一个令人困惑的系统架构。这将贯穿本书的这一部分。 章节概述我们将从 第十章 开始，研究例如 MapReduce 这样 面向批处理（batch-oriented） 的数据流系统。对于建设大规模数据系统，我们将看到，它们提供了优秀的工具和思想。第十一章 将把这些思想应用到 流式数据（data streams） 中，使我们能用更低的延迟完成同样的任务。第十二章 将对本书进行总结，探讨如何使用这些工具来构建可靠，可伸缩和可维护的应用。 索引 批处理 流处理 数据系统的未来"},{"title":"序言","path":"/wiki/ddia/preface.html","content":"如果近几年从业于软件工程，特别是服务器端和后端系统开发，那么你很有可能已经被大量关于数据存储和处理的时髦词汇轰炸过了： NoSQL！大数据！Web-Scale！分片！最终一致性！ACID！ CAP 定理！云服务！MapReduce！实时！ 在最近十年中，我们看到了很多有趣的进展，关于数据库，分布式系统，以及在此基础上构建应用程序的方式。这些进展有着各种各样的驱动力： 谷歌、雅虎、亚马逊、脸书、领英、微软和推特等互联网公司正在和巨大的流量 &#x2F; 数据打交道，这迫使他们去创造能有效应对如此规模的新工具。 企业需要变得敏捷，需要低成本地检验假设，需要通过缩短开发周期和保持数据模型的灵活性，快速地响应新的市场洞察。 免费和开源软件变得非常成功，在许多环境中比商业软件和定制软件更受欢迎。 处理器主频几乎没有增长，但是多核处理器已经成为标配，网络也越来越快。这意味着并行化程度只增不减。 即使你在一个小团队中工作，现在也可以构建分布在多台计算机甚至多个地理区域的系统，这要归功于譬如亚马逊网络服务（AWS）等基础设施即服务（IaaS）概念的践行者。 许多服务都要求高可用，因停电或维护导致的服务不可用，变得越来越难以接受。 数据密集型应用（data-intensive applications） 正在通过使用这些技术进步来推动可能性的边界。一个应用被称为 数据密集型 的，如果 数据是其主要挑战（数据量，数据复杂度或数据变化速度）—— 与之相对的是 计算密集型，即处理器速度是其瓶颈。 帮助数据密集型应用存储和处理数据的工具与技术，正迅速地适应这些变化。新型数据库系统（“NoSQL”）已经备受关注，而消息队列，缓存，搜索索引，批处理和流处理框架以及相关技术也非常重要。很多应用组合使用这些工具与技术。 这些生意盎然的时髦词汇体现出人们对新的可能性的热情，这是一件好事。但是作为软件工程师和架构师，如果要开发优秀的应用，我们还需要对各种层出不穷的技术及其利弊权衡有精准的技术理解。为了获得这种洞察，我们需要深挖时髦词汇背后的内容。 幸运的是，在技术迅速变化的背后总是存在一些持续成立的原则，无论你使用了特定工具的哪个版本。如果你理解了这些原则，就可以领会这些工具的适用场景，如何充分利用它们，以及如何避免其中的陷阱。这正是本书的初衷。 本书的目标是帮助你在飞速变化的数据处理和数据存储技术大观园中找到方向。本书并不是某个特定工具的教程，也不是一本充满枯燥理论的教科书。相反，我们将看到一些成功数据系统的样例：许多流行应用每天都要在生产中满足可伸缩性、性能、以及可靠性的要求，而这些技术构成了这些应用的基础。 我们将深入这些系统的内部，理清它们的关键算法，讨论背后的原则和它们必须做出的权衡。在这个过程中，我们将尝试寻找 思考 数据系统的有效方式 —— 不仅关于它们 如何 工作，还包括它们 为什么 以这种方式工作，以及哪些问题是我们需要问的。 阅读本书后，你能很好地决定哪种技术适合哪种用途，并了解如何将工具组合起来，为一个良好应用架构奠定基础。本书并不足以使你从头开始构建自己的数据库存储引擎，不过幸运的是这基本上很少有必要。你将获得对系统底层发生事情的敏锐直觉，这样你就有能力推理它们的行为，做出优秀的设计决策，并追踪任何可能出现的问题。 本书的目标读者如果你开发的应用具有用于存储或处理数据的某种服务器 &#x2F; 后端系统，而且使用网络（例如，Web 应用、移动应用或连接到互联网的传感器），那么本书就是为你准备的。 本书是为软件工程师，软件架构师，以及喜欢写代码的技术经理准备的。如果你需要对所从事系统的架构做出决策 —— 例如你需要选择解决某个特定问题的工具，并找出如何最好地使用这些工具，那么这本书对你尤有价值。但即使你无法选择你的工具，本书仍将帮助你更好地了解所使用工具的长处和短处。 你应当具有一些开发 Web 应用或网络服务的经验，且应当熟悉关系型数据库和 SQL。任何你了解的非关系型数据库和其他与数据相关工具都会有所帮助，但不是必需的。对常见网络协议如 TCP 和 HTTP 的大概理解是有帮助的。编程语言或框架的选择对阅读本书没有任何不同影响。 如果以下任意一条对你为真，你会发现这本书很有价值： 你想了解如何使数据系统可伸缩，例如，支持拥有数百万用户的 Web 或移动应用。 你需要提高应用程序的可用性（最大限度地减少停机时间），保持稳定运行。 你正在寻找使系统在长期运行过程易于维护的方法，即使系统规模增长，需求与技术也发生变化。 你对事物的运作方式有着天然的好奇心，并且希望知道一些主流网站和在线服务背后发生的事情。这本书打破了各种数据库和数据处理系统的内幕，探索这些系统设计中的智慧是非常有趣的。 有时在讨论可伸缩的数据系统时，人们会说：“你又不在谷歌或亚马逊，别操心可伸缩性了，直接上关系型数据库”。这个陈述有一定的道理：为了不必要的伸缩性而设计程序，不仅会浪费不必要的精力，并且可能会把你锁死在一个不灵活的设计中。实际上这是一种 “过早优化” 的形式。不过，选择合适的工具确实很重要，而不同的技术各有优缺点。我们将看到，关系数据库虽然很重要，但绝不是数据处理的终章。 本书涉及的领域本书并不会尝试告诉读者如何安装或使用特定的软件包或 API，因为已经有大量文档给出了详细的使用说明。相反，我们会讨论数据系统的基础 —— 各种原则与利弊权衡，并探讨了不同产品所做出的不同设计决策。 在电子书中包含了在线资源全文的链接。所有链接在出版时都进行了验证，但不幸的是，由于网络的自然规律，链接往往会频繁地破损。如果你遇到链接断开的情况，或者正在阅读本书的打印副本，可以使用搜索引擎查找参考文献。对于学术论文，你可以在 Google 学术中搜索标题，查找可以公开获取的 PDF 文件。或者，你也可以在 https://github.com/ept/ddia-references 中找到所有的参考资料，我们在那儿维护最新的链接。 我们主要关注的是数据系统的 架构（architecture），以及它们被集成到数据密集型应用中的方式。本书没有足够的空间覆盖部署、运维、安全、管理等领域 —— 这些都是复杂而重要的主题，仅仅在本书中用粗略的注解讨论这些对它们很不公平。每个领域都值得用单独的书去讲。 本书中描述的许多技术都被涵盖在 大数据（Big Data） 这个时髦词的范畴中。然而 “大数据” 这个术语被滥用，缺乏明确定义，以至于在严肃的工程讨论中没有用处。这本书使用歧义更小的术语，如 “单节点” 之于 “分布式系统”，或 “在线 &#x2F; 交互式系统” 之于 “离线 &#x2F; 批处理系统”。 本书对 自由和开源软件（FOSS） 有一定偏好，因为阅读、修改和执行源码是了解某事物详细工作原理的好方法。开放的平台也可以降低供应商垄断的风险。然而在适当的情况下，我们也会讨论专利软件（闭源软件，软件即服务 SaaS，或一些在文献中描述过但未公开发行的公司内部软件）。 本书纲要本书分为三部分： 在 第一部分 中，我们会讨论设计数据密集型应用所赖的基本思想。我们从 第一章 开始，讨论我们实际要达到的目标：可靠性、可伸缩性和可维护性；我们该如何思考这些概念；以及如何实现它们。在 第二章 中，我们比较了几种不同的数据模型和查询语言，看看它们如何适用于不同的场景。在 第三章 中将讨论存储引擎：数据库如何在磁盘上摆放数据，以便能高效地再次找到它。第四章 转向数据编码（序列化），以及随时间演化的模式。 在 第二部分 中，我们从讨论存储在一台机器上的数据转向讨论分布在多台机器上的数据。这对于可伸缩性通常是必需的，但带来了各种独特的挑战。我们首先讨论复制（第五章）、分区 &#x2F; 分片（第六章）和事务（第七章）。然后我们将探索关于分布式系统问题的更多细节（第八章），以及在分布式系统中实现一致性与共识意味着什么（第九章）。 在 第三部分 中，我们讨论那些从其他数据集衍生出一些数据集的系统。衍生数据经常出现在异构系统中：当没有单个数据库可以把所有事情都做的很好时，应用需要集成几种不同的数据库、缓存、索引等。在 第十章 中我们将从一种衍生数据的批处理方法开始，然后在此基础上建立在 第十一章 中讨论的流处理。最后，在 第十二章 中，我们将所有内容汇总，讨论在将来构建可靠、可伸缩和可维护的应用程序的方法。 参考文献与延伸阅读本书中讨论的大部分内容已经在其它地方以某种形式出现过了 —— 会议演示文稿、研究论文、博客文章、代码、BUG 跟踪器、邮件列表以及工程习惯中。本书总结了不同来源资料中最重要的想法，并在文本中包含了指向原始文献的链接。 如果你想更深入地探索一个领域，那么每章末尾的参考文献都是很好的资源，其中大部分可以免费在线获取。 O‘Reilly SafariSafari (formerly Safari Books Online) is a membership-based training and reference platform for enterprise, government, educators, and individuals. Members have access to thousands of books, training videos, Learning Paths, interac‐ tive tutorials, and curated playlists from over 250 publishers, including O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Pro‐ fessional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley &amp; Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones &amp; Bartlett, and Course Technology, among others. For more information, please visit http://oreilly.com/safari. 致谢本书融合了学术研究和工业实践的经验，融合并系统化了大量其他人的想法与知识。在计算领域，我们往往会被各种新鲜花样所吸引，但我认为前人完成的工作中，有太多值得我们学习的地方了。本书有 800 多处引用：文章、博客、讲座、文档等，对我来说这些都是宝贵的学习资源。我非常感谢这些材料的作者分享他们的知识。 我也从与人交流中学到了很多东西，很多人花费了宝贵的时间与我讨论想法并耐心解释。特别感谢 Joe Adler, Ross Anderson, Peter Bailis, Márton Balassi, Alastair Beresford, Mark Callaghan, Mat Clayton, Patrick Collison, Sean Cribbs, Shirshanka Das, Niklas Ekström, Stephan Ewen, Alan Fekete, Gyula Fóra, Camille Fournier, Andres Freund, John Garbutt, Seth Gilbert, Tom Haggett, Pat Hel‐ land, Joe Hellerstein, Jakob Homan, Heidi Howard, John Hugg, Julian Hyde, Conrad Irwin, Evan Jones, Flavio Junqueira, Jessica Kerr, Kyle Kingsbury, Jay Kreps, Carl Lerche, Nicolas Liochon, Steve Loughran, Lee Mallabone, Nathan Marz, Caitie McCaffrey, Josie McLellan, Christopher Meiklejohn, Ian Meyers, Neha Narkhede, Neha Narula, Cathy O’Neil, Onora O’Neill, Ludovic Orban, Zoran Perkov, Julia Powles, Chris Riccomini, Henry Robinson, David Rosenthal, Jennifer Rullmann, Matthew Sackman, Martin Scholl, Amit Sela, Gwen Shapira, Greg Spurrier, Sam Stokes, Ben Stopford, Tom Stuart, Diana Vasile, Rahul Vohra, Pete Warden, 以及 Brett Wooldridge. 更多人通过审阅草稿并提供反馈意见在本书的创作过程中做出了无价的贡献。我要特别感谢 Raul Agepati, Tyler Akidau, Mattias Andersson, Sasha Baranov, Veena Basavaraj, David Beyer, Jim Brikman, Paul Carey, Raul Castro Fernandez, Joseph Chow, Derek Elkins, Sam Elliott, Alexander Gallego, Mark Grover, Stu Halloway, Heidi Howard, Nicola Kleppmann, Stefan Kruppa, Bjorn Madsen, Sander Mak, Stefan Podkowinski, Phil Potter, Hamid Ramazani, Sam Stokes, 以及 Ben Summers。当然对于本书中的任何遗留错误或难以接受的见解，我都承担全部责任。 为了帮助这本书落地，并且耐心地处理我缓慢的写作和不寻常的要求，我要对编辑 Marie Beaugureau，Mike Loukides，Ann Spencer 和 O’Reilly 的所有团队表示感谢。我要感谢 Rachel Head 帮我找到了合适的术语。我要感谢 Alastair Beresford，Susan Goodhue，Neha Narkhede 和 Kevin Scott，在其他工作事务之外给了我充分地创作时间和自由。 特别感谢 Shabbir Diwan 和 Edie Freedman，他们非常用心地为各章配了地图。他们提出了不落俗套的灵感，创作了这些地图，美丽而引人入胜，真是太棒了。 最后我要表达对家人和朋友们的爱，没有他们，我将无法走完这个将近四年的写作历程。你们是最棒的。"},{"title":"设计模式","path":"/wiki/design/index.html","content":"❓ 设计模式是什么设计模式是软件设计中常见问题的典型解决方案。 每个模式就像一张蓝图， 你可以通过对其进行定制来解决代码中的特定设计问题。 设计模式与方法或库的使用方式不同， 你很难直接在自己的程序中套用某个设计模式。 模式并不是一段特定的代码， 而是解决特定问题的一般性概念。 你可以根据模式来实现符合自己程序实际所需的解决方案。 💖 设计模式优势设计模式是软件设计中对一些常见问题的解决思路。使用设计模式可以写出可扩展、可读、可维护的高质量代码。 设计模式是针对软件设计中常见问题的工具箱， 其中的工具就是各种经过实践验证的解决方案。 即使你从未遇到过这些问题， 了解模式仍然非常有用， 因为它能指导你如何使用面向对象的设计原则来解决各种问题。 为什么要学习设计模式？ 比较功利的一个目的是：应对面试 告别被人吐槽的烂代码 提高复杂代码的设计和开发能力 更容易看懂源码 📃 设计模式分类不同设计模式的复杂程度、 细节层次以及在整个系统中的应用范围等方面各不相同。 我喜欢将其类比于道路的建造： 如果你希望让十字路口更加安全， 那么可以安装一些交通信号灯， 或者修建包含行人地下通道在内的多层互通式立交桥。 最基础的、 底层的模式通常被称为惯用技巧。 这类模式一般只能在一种编程语言中使用。 最通用的、 高层的模式是构架模式。 开发者可以在任何编程语言中使用这类模式。 与其他模式不同， 它们可用于整个应用程序的架构设计。 此外， 所有模式可以根据其意图或目的来分类。 本书覆盖了三种主要的模式类别： 创建型模式： 提供创建对象的机制， 增加已有代码的灵活性和可复用性。 结构型模式： 介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效。 行为型模式： 负责对象间的高效沟通和职责委派。"},{"title":"工厂模式","path":"/wiki/design/create/factory.html","content":"简介一般情况下，工厂模式分为三种更加细分的类型：简单工厂、工厂方法和抽象工厂。 在这三种细分的工厂模式中，简单工厂、工厂方法原理比较简单，在实际的项目中也比较常 用。而抽象工厂的原理稍微复杂点，在实际的项目中相对也不常用。 工厂模式的原理和实现较为简单，重点还是要搞清楚应 用场景：什么时候该用工厂模式？相对于直接 new 来创建对象，用工厂模式来创建究竟有 什么好处呢？ 简单工厂下面通过一个例子来看一下什么是简单工厂 12345678910111213141516171819202122232425262728public class RuleConfigSource &#123; public RuleConfig load(String ruleConfigFilePath) &#123; String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParser parser = null; if (&quot;json&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new JsonRuleConfigParser(); &#125; else if (&quot;xml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new XmlRuleConfigParser(); &#125; else if (&quot;yaml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new YamlRuleConfigParser(); &#125; else if (&quot;properties&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new PropertiesRuleConfigParser(); &#125; else &#123; throw new InvalidRuleConfigException(&quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); &#125; String configText = &quot;&quot;; // 从 ruleConfigFilePath 文件中读取配置文本到 configText 中 RuleConfig ruleConfig = parser.parse(configText); return configConfig; &#125; private String getFileExtension(String filePath) &#123; // 解析文件名获取扩展名，例如 rule.json 返回 json return &quot;json&quot;; &#125;&#125; 为了让代码逻辑更加清晰，可读性更好，我们可以将代码中涉及 parser 创建的部分逻辑剥离出来，抽象成 createParser() 函数。重构之后的代码如下所示： 123456789101112131415161718192021222324252627282930313233343536public class RuleConfigSource &#123; public RuleConfig load(String ruleConfigFilePath) &#123; String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParser parser = createParser(ruleConfigFileExtension); if (parser == null) &#123; throw new InvalidRuleConfigException(&quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); &#125; String configText = &quot;&quot;; // 从 ruleConfigFilePath 文件中读取配置文本到 configText 中 RuleConfig ruleConfig = parser.parse(configText); return configConfig; &#125; private String getFileExtension(String filePath) &#123; // 解析文件名获取扩展名，例如 rule.json 返回 json return &quot;json&quot;; &#125; private IRuleConfigParser createParser(String configFormat) &#123; IRuleConfigParser parser = null; if (&quot;json&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new JsonRuleConfigParser(); &#125; else if (&quot;xml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new XmlRuleConfigParser(); &#125; else if (&quot;yaml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new YamlRuleConfigParser(); &#125; else if (&quot;properties&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new PropertiesRuleConfigParser(); &#125; return parser; &#125;&#125; 为了让类的职责更加单一、代码更加清晰，我们还可以进一步将 createParser(） 函数剥离 到一个独立的类中，让这个类只负责对象的创建。而这个类就是我们现在要讲的简单工厂模式类。具体的代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738public class RuleConfigSource &#123; public RuleConfig load(String ruleConfigFilePath) &#123; String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParser parser = RuleConfigParserFactory.createParser(ruleConfigFileExtension); if (parser == null) &#123; throw new InvalidRuleConfigException(&quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); &#125; String configText = &quot;&quot;; // 从 ruleConfigFilePath 文件中读取配置文本到 configText 中 RuleConfig ruleConfig = parser.parse(configText); return configConfig; &#125; private String getFileExtension(String filePath) &#123; // 解析文件名获取扩展名，例如 rule.json 返回 json return &quot;json&quot;; &#125;&#125;public class RuleConfigParserFactory&#123; public static IRuleConfigParser createParser(String configFormat) &#123; IRuleConfigParser parser = null; if (&quot;json&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new JsonRuleConfigParser(); &#125; else if (&quot;xml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new XmlRuleConfigParser(); &#125; else if (&quot;yaml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new YamlRuleConfigParser(); &#125; else if (&quot;properties&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parser = new PropertiesRuleConfigParser(); &#125; return parser; &#125;&#125; 在上面的代码实现中，我们每次调用 RuleConfigParserFactory 的 createParser() 的时候，都要创建一个新的 parser。实际上，如果 parser 可以复用，为了节省内存和对象创建的时间，我们可以将 parser 事先创建好缓存起来。当调用 createParser() 函数的时候，我们从缓存中取出 parser 对象直接使用。 123456789101112131415161718public class RuleConfigParserFactory&#123; private static final Map&lt;String, RuleConfigParser&gt; cachedParsers = new HashMap&lt;&gt;(); static &#123; cachedParsers.put(&quot;json&quot;, new JsonRuleConfigParser()); cachedParsers.put(&quot;xml&quot;, new XmlRuleConfigParser()); cachedParsers.put(&quot;yaml&quot;, new YamlRuleConfigParser()); cachedParsers.put(&quot;properties&quot;, new PropertiesRuleConfigParser()); &#125; public static IRuleConfigParser createParser(String configFormat) &#123; if (configFormat == null || configFormat.isEmpty()) &#123; return null; // or IllegalArgumentException &#125; IRuleConfigParser parser = cachedParsers.get(configFormat.toLowerCase()); return parser; &#125;&#125; 尽管简单工厂模式的代码实现中，有多处计分支判断逻辑，违背开闭原则，但权衡扩展性和可读性，这样的代码实现在大多数情况下（比如，不需要频繁地添加 parser，也没有太多的 parser）是没有问题的。 工厂方法如果我们非得要将 f分支逻辑去掉，那该怎么办呢？比较经典处理方法就是利用多态。按照多态的实现思路，对上面的代码进行重构。重构之后的代码如下所示： 12345678910111213141516171819202122232425262728293031public interface IRuleConfigParserFactory &#123; IRuleConfigParser createParser();&#125;public class JsonRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new JsonRuleConfigParser(); &#125;&#125;public class XmlRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new XmlRuleConfigParser(); &#125;&#125;public class YamlRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new YamlRuleConfigParser(); &#125;&#125;public class PropertiesRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new PropertiesRuleConfigParser(); &#125;&#125; 实际上，这就是工厂方法模式的典型代码实现。这样当我们新增一种 parser 的时候，只需要新增一个实现了 IRuleConfigParserFactory 接口的 Factory 类即可。 所以，工厂方法模式比起简单工厂模式更加符合开闭原则。 从上面的工厂方法的实现来看，一切都很完美，但是实际上存在挺大的问题。问题存在于这些工厂类的使用上。 接下来，我们看一下，如何用这些工厂类来实现 RuleConfigSource 的 load) 函数。具体的代码如下所示： 1234567891011121314151617181920212223242526272829public class RuleConfigSource &#123; public RuleConfig load(String ruleConfigFilePath) &#123; String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParserFactory parserFactory = null; if (&quot;json&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new JsonRuleConfigParserFactory(); &#125; else if (&quot;xml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new XmlRuleConfigParserFactory(); &#125; else if (&quot;yaml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new YamlRuleConfigParserFactory(); &#125; else if (&quot;properties&quot;.equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new PropertiesRuleConfigParserFactory(); &#125; else &#123; throw new InvalidRuleConfigException(&quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); &#125; IRuleConfigParser parser = parserFactory.createParser(); String configText = &quot;&quot;; // 从 ruleConfigFilePath 文件中读取配置文本到 configText 中 RuleConfig ruleConfig = parser.parse(configText); return configConfig; &#125; private String getFileExtension(String filePath) &#123; // 解析文件名获取扩展名，例如 rule.json 返回 json return &quot;json&quot;; &#125;&#125; 从上面的代码实现来看，工厂类对象的创建逻辑又耦合进了 load 函数中，跟我们最初的 代码版本非常相似。 要解决这个问题，我们可以为工厂类再创建一个简单工厂，也就是工厂的工厂，用来创建工厂类对象。这段话听起来有点绕，我把代码实现出来了，你一看就能明白了。其中， RuleConfigParserFactoryMap 类是创建工厂对象的工厂类，getParserFactory() 返回的是缓存好的单例工厂对象。 1234567891011121314151617181920212223242526272829303132333435363738394041public class RuleConfigSource &#123; public RuleConfig load(String ruleConfigFilePath) &#123; String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParserFactory parserFactory = RuleConfigParserFactoryMap.getParserFactory(ruleConfigFileExtension); if (parserFactory == null) &#123; throw new InvalidRuleConfigException(&quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); &#125; IRuleConfigParser parser = parserFactory.createParser(); String configText = &quot;&quot;; // 从 ruleConfigFilePath 文件中读取配置文本到 configText 中 RuleConfig ruleConfig = parser.parse(configText); return configConfig; &#125; private String getFileExtension(String filePath) &#123; // 解析文件名获取扩展名，例如 rule.json 返回 json return &quot;json&quot;; &#125;&#125;// 因为工厂类只包含方法，不包含成员变量，完全可以复用，不需要每次都创建新的工厂对象public class RuleConfigParserFactoryMap&#123; private static final Map&lt;String, RuleConfigParserFactory&gt; cachedFactories = new HashMap&lt;&gt;(); static &#123; cachedFactories.put(&quot;json&quot;, new JsonRuleConfigParserFactory()); cachedFactories.put(&quot;xml&quot;, new XmlRuleConfigParserFactory()); cachedFactories.put(&quot;yaml&quot;, new YamlRuleConfigParserFactory()); cachedFactories.put(&quot;properties&quot;, new PropertiesRuleConfigParserFactory()); &#125; public static IRuleConfigParserFactory getParserFactory(String type) &#123; if (type == null || type.isEmpty()) &#123; return null; // or IllegalArgumentException &#125; IRuleConfigParserFactory parserFactory = cachedFactories.get(type.toLowerCase()); return parserFactory; &#125;&#125; 当我们需要添加新的规则配置解析器的时候，我们只需要创建新的 parser 类和 parser factory 类，并且在 RuleConfigParserFactoryMap 类中，将新的 parser factory 对象添加到 cachedFactories 中即可。代码的改动非常少，基本上符合开闭原则。 实际上，对于规则配置文件解析这个应用场景来说，工厂模式需要额外创建诸多 Factory 类，也会增加代码的复杂性，而且，每个 Factory 类只是做简单的 new 操作，功能非常单薄（只有一行代码），也没必要设计成独立的类，所以，在这个应用场景下，简单工厂模式简单好用，比工方法厂模式更加合适。 那什么时候该用工厂方法模式，而非简单工厂模式呢？ 应用场景之所以将某个代码块剥离出来，独立为函数或者类，原因是这个代码块的逻辑过于复杂，剥离之后能让代码更加清晰，更加可读、可维护。但是，如果代码块本身并不复杂，就几行代码而己，我们完全没必要将它拆分成单独的函数或者类。 基于这个设计思想，当对象的创建逻辑比较复杂，不只是简单的 new 一下就可以，而是要组合其他类对象，做各种初始化操作的时候，我们推荐使用工厂方法模式，将复杂的创建逻辑拆分到多个工厂类中，让每个工厂类都不至于过于复杂。而使用简单工厂模式，将所有的创建逻辑都放到一个工厂类中，会导致这个工厂类变得很复杂。 除此之外，在某些场景下，如果对象不可复用，那工厂类每次都要返回不同的对象。如果我们使用简单工厂模式来实现，就只能选择第一种包含 if 分支逻辑的实现方式。如果我们还想避免烦人的 if-else 分支逻辑，这个时候，我们就推荐使用工厂方法模式。 抽象工厂在简单工厂和工厂方法中，类只有一种分类方式。比如，在规则配置解析那个例子中，解析器类只会根据配置文件格式(Json、Xml、Yaml…..）来分类。但是，如果类有两种分类方式，比如，我们既可以按照配置文件格式来分类，也可以按照解析的对象（Rule 规则配置还是 System 系统配置）来分类，那就会对应下面这8个 parser 类。 1234567891011针对规则配置的解析器：基于接口IRuleConfigParserJsonRulLeConfigParserXmLRuleConfigParserYamlRuLeConfigParserPropertiesRuleConfigParser针对系统配置的解析器：基于接口ISystemConfigParserJsonSystemConfigParserXmLSystemConfigParserYamlSystemConfigParserPropertiesSystemConfigParser 针对这种特殊的场景，如果还是继续用工厂方法来实现的话，我们要针对每个 parser 都编写一个工厂类，也就是要编写8个工厂类。如果我们未来还需要增加针对业务配置的解析器（比如 IBizConfigParser)，那就要再对应地增加 4 个工厂类。而我们知道，过多的类也会让系统难维护。这个问题该怎么解决呢？ 抽象工厂就是针对这种非常特殊的场景而诞生的。我们可以让一个工厂负责创建多个不同类型的对象 (RuleConfigParser、ISystemConfigParser 等），而不是只创建一种 parser 对象。这样就可以有效地减少工厂类的个数。具体的代码实现如下所示： 123456789101112131415161718192021222324252627282930public interface IRuleConfigParserFactory &#123; IRuleConfigParser createParser(); ISystemConfigParser createSystemParser();&#125;public class JsonRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new JsonRuleConfigParser(); &#125; @Override public ISystemConfigParser createSystemParser() &#123; return new JsonSystemConfigParser(); &#125;&#125;public class XmlRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new XmlRuleConfigParser(); &#125; @Override public ISystemConfigParser createSystemParser() &#123; return new XmlSystemConfigParser(); &#125;&#125;// ...略 总结三种工厂模式中，简单工厂和工厂方法比较常用，抽象工厂的应用场景比较特殊，所以很少用到，不是我们学习的重点。所以，下面我重点对前两种工厂模式的应用场景进行总结。 当创建逻辑比较复杂，是一个“大工程”的时候，我们就考虑使用工厂模式，封装对象的创建过程，将对象的创建和使用相分离。何为创建逻辑比较复杂呢？可以参考下面两种情况。 第一种情况：类似规则配置解析的例子，代码中存在 if-else 分支判断，动态地根据不同的类型创建不同的对象。针对这种情况，我们就考虑使用工厂模式，将这一大坨 if-else 创建对象的代码抽离出来，放到工厂类中。 第二种情况：尽管我们不需要根据不同的类型创建不同的对象，但是，单个对象本身的创建过程比较复杂，比如前面提到的要组合其他类对象，做各种初始化操作。在这种情况下，我们也可以考虑使用工厂模式，将对象的创建过程封装到工厂类中。 对于第一种情况，当每个对象的创建逻辑都比较简单的时候，我推荐使用简单工厂模式，将多个对象的创建逻辑放到一个工厂类中。 当每个对象的创建逻辑都比较复杂的时候，为了避免设计一个过于庞大的简单工厂类，我推荐使用工厂方法模式，将创建逻辑拆分得更细，每个对象的创建逻辑独立到各自的工厂类中。 同理，对于第二种情况，因为单个对象本身的创建逻辑就比较复杂，所以，我建议使用工厂方法模式。 除了刚刚提到的这几种情况之外，如果创建对象的逻辑并不复杂，那我们就直接通过 new 来创建对象就可以了，不需要使用工厂模式。 现在，我们上升一个思维层面来看工厂模式，它的作用无外乎下面这四个。这也是判断要不要使用工厂模式的最本质的参考标准。 封装变化： 创建逻辑有可能变化，封装成工厂类之后，创建逻辑的变更对调用者透明。 代码复用： 创建代码抽离到独立的工厂类之后可以复用。 隔离复杂性： 封装复杂的创建逻辑，调用者无需了解如何创建对象。 控制复杂度： 将创建代码抽离出来，让原本的函数或类职责更单一，代码更简洁。"},{"title":"单例模式","path":"/wiki/design/create/single.html","content":"什么是单例模式单例设计模式( Singleton Design Pattern)理解起来非常简单。一个类只允许创建个对象(或者实例),那这个类就是一个单例类,这种设计模式就叫作单例设计模式,简称单例模式。 单例模式共分为两大类： 懒汉模式：实例在第一次使用时创建 饿汉模式：实例在类装载时创建 为什么要使用单例?站在业务概念的角度,有些数据在系统中只应该保存一份,就比较适合设计为单例类。比如,系统的配置信息类、连接池类、ID生成器类。 除此之外,我们还可以使用单例解决资源访问冲突的问题。 单例存在哪些问题?大部分情况下,我们在项目中使用单例,都是用它来表示一些全局唯一类,比如配置信息类、连接池类、ID生成器类。 单例模式书写简洁、使用方便,在代码中,我们不需要创建对象,直接通过类似 IdGenerator.getInstance() 这样的方法来调用就可以了。 但是,这种使用方法有点类似硬编码(hard code),会带来诸多问题。 1. 单例类对OOP（面向对象）特性的支持不友好 面向对象的四大特性是封装、抽象、继承、多态。单例这种设计模式对于其中的抽象、继承、多态都支持得不好。 以 IdGenerator 为例，IdGenerator 的使用方式违背了基于接口而非实现的设计原则,也就违背了广义上理解的OOP的抽象特性。 如果未来某一天,我们希望针对不同的业务采用不同的ID生成算法。比如,订单ID和用户ID采用不同的ID生成器来生成。为了应对这个需求变化,我们需要修改所有用到 GeNerator类的地方,这样代码的改动就会比较大。 除此之外,单例对继承、多态特性的支持也不友好。这里我之所以会用“不友好”这个词,而非“完全不支持”,是因为从理论上来讲,单例类也可以被继承、也可以实现多态,只是实现起来会非常奇怪,会导致代码的可读性变差。不明白设计意图的人,看到这样的设计,会觉得莫名其妙。 所以,一旦你选择将某个类设计成到单例类,也就意味着放弃了继承和多态这两个强有力的面向对象特性,也就相当于损失了可以应对未来需求变化的扩展性。 2. 单例类会隐藏类之间的依赖关系 我们知道,代码的可读性非常重要。在阅读代码的时候,我们希望一眼就能看出类与类之间的依赖关系,搞清楚这个类依赖了哪些外部类。 通过构造函数、参数传递等方式声明的类之间的依赖关系,我们通过查看函数的定义,就能很容易识别出来。 但是,单例类不需要显示创建、不需要依赖参数传递,在函数中直接调用就可以了。如果代码比较复杂,这种调用关系就会非常隐蔽。在阅读代码的时候,我们就需要仔细查看每个函数的代码实现,才能知道这个类到底依赖了哪些单例类。 3. 单例类对代码的扩展性不友好 单例类只有一个对象实例，如果未来某一天，我们需要在代码中创建两个实例或多个实例，那就要对代码有比较大的改动。 以数据库连接池为例，在系统设计初期,我们觉得系统中只应该有一个数据库连接池,这样能方便我们控制对数据库连接资源的消耗。所以,我们把数据库连接池类设计成了单例类。 但之后我们发现,系统中有些SQL语句运行得非常慢。这些SQL语句在执行的时候,长时间占用数据库连接资源,导致其他SQL请求无法响应。 为了解决这个问题,我们希望将慢SQL与其他SQL隔离开来执行。为了实现这样的目的,我们可以在系统中创建两个数据库连接池,慢SQL独享—个数据库连接池,其他SQL独享另外—个数据库连接池,这样就能避免慢SQL影响到其他SQL的执行。 如果我们将数据库连接池设计成单例类,显然就无法适应这样的需求变更,也就是说,单例类在某些情况下会影响代码的扩展性、灵活性。所以,数据库连接池、线程池这类的资源池,最好还是不要设计成单例类。实际上,一些开源的数据库连接池、线程池也确实没有设计成单例类。 4. 单例对代码的可测试性不友好 单例模式的使用会影响到代码的可测试性。如果单例类依赖比较重的外部资源,比如DB,我们在写单元测试的时候,希望能通过mock的方式将它替换掉。而单例类这种硬编码式的使用方式,导致无法实现mock替换 除此之外,如果单例类持有成员变量(比如 GeNerator中的id成员变量),那它实际上相当于一种全局变量,被所有的代码共享。如果这个全局变量是一个可变全局变量,也就是说,它的成员变量是可以被修改的,那我们在编写单元测试的时候,还需要注意不同测试用例之间,修改了单例类中的同一个成员变量的值,从而导致测试结果互相影响的问题。 5. 单例不支持有参数的构造函数 单例不支持有参数的构造函数,比如我们创建一个连接池的单例对象,我们没法通过参数来指定连接池的大小。针对这个问题,我们来看下都有哪些解决方案。 创建完实例之后，在调用init函数传递参数。 将参数放到 getInstance()方法中。 将参数放到另外一个全局变量中。 有何替代的解决方案?为了保证全局唯一,除了使用单例,我们还可以用静态方法来实现。 这也是项目开发中经常用到的一种实现思路。不过,静态方法这种实现思路,并不能解决我们之前提到的问题。 如果要完全解决这些问题,我们可能要从根上,寻找其他方式来实现全局唯一类了。比如,通过工厂模式、IOC容器(比如 Spring IOC容器)来保证,由程序员自己来保证(自己在编写代码的时候自己保证不要创建两个类对象)。 有人把单例当作反模式,主张杜绝在项目中使用。我个人觉得这有点极端。模式没有对错,关键看你怎么用。如果单例类并没有后续扩展的需求,并且不依赖外部系统,那设计成单例类就没有太大问题。对于一些全局的类,我们在其他地方new的话,还要在类之间传来传去,不如直接做成单例类,使用起来简洁方便。 单例与静态类的区别? 静态类比单例模式的效率更高，因为静态方法在编译期就完成了静态绑定。 单例对象可以被延迟初始化。而静态类总是在类被加载的时候就初始化。 在做单元测试的时候，静态类比单例类更难被 mock，因此也更难被测试。而单例类很容易被 mock 来执行单元测试。 Java 中的静态方法是不能被覆写的，这就导致某些情况不够灵活。而你随时可以继承一个非 final 的单例类来覆写其中的方法。 具体实现饿汉模式按照定义我们可以写出一个基本代码： 12345678910111213141516171819public class Singleton &#123; // 使用private将构造方法私有化，以防外界通过该构造方法创建多个实例 private Singleton() &#123; &#125; // 由于不能使用构造方法创建实例，所以需要在类的内部创建该类的唯一实例 // 使用static修饰singleton 在外界可以通过类名调用该实例 类名.成员名 final static Singleton singleton = new Singleton(); // 1 // 如果使用private封装该实例，则需要添加get方法实现对外界的开放 private static final Singleton instance = new Singleton(); // 2 // 添加static，将该方法变成类所有通过类名访问 public static Singleton getInstance()&#123; return instance; &#125; //1和2选一种即可，推荐2&#125; 对于饿汉模式来说，这种写法已经很完美了，唯一的缺点就是，由于instance的初始化是在类加载时进行的，类加载是由ClassLoader来实现的，如果初始化太早，就会造成资源浪费。 当然，如果所需的单例占用的资源很少，并且也不依赖于其他数据，那么这种实现方式也是很好的。 类装载的时机： new一个对象时 使用反射创建它的实例时 子类被加载时，如果父类还没有加载，就先加载父类 JVM启动时执行主类 会先被加载 懒汉模式懒汉模式的代码如下 123456789101112// 代码一public class Singleton &#123; private static Singleton instance = null; private Singleton()&#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 每次获取instance之前先进行判断，如果instance为空就new一个出来，否则就直接返回已存在的instance。 这种写法在单线程的时候是没问题的。但是，当有多个线程一起工作的时候，如果有两个线程同时运行到 if (instance &#x3D;&#x3D; null)，都判断为null（第一个线程判断为空之后，并没有继续向下执行，当第二个线程判断的时候instance依然为空），最终两个线程就各自会创建一个实例出来。这样就破环了单例模式 实例的唯一性 要想保证实例的唯一性就需要使用 synchronized 加上一个同步锁 12345678910111213// 代码二public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; synchronized(Singleton.class)&#123; if (instance == null) instance = new Singleton(); &#125; return instance; &#125;&#125; 加上synchronized关键字之后，getInstance方法就会锁上了。如果有两个线程（T1、T2）同时执行到这个方法时，会有其中一个线程T1获得同步锁，得以继续执行，而另一个线程T2则需要等待，当第T1执行完毕getInstance之后（完成了null判断、对象创建、获得返回值之后），T2线程才会执行执行。 所以这段代码也就避免了代码一中，可能出现因为多线程导致多个实例的情况。但是，这种写法也有一个问题：给getInstance方法加锁，虽然避免了可能会出现的多个实例问题，但是会强制除T1之外的所有线程等待，实际上会对程序的执行效率造成负面影响。 双重检查（Double-Check）代码二相对于代码一的效率问题，其实是为了解决1%几率的问题，而使用了一个100%出现的防护盾。那有一个优化的思路，就是把100%出现的防护盾，也改为1%的几率出现，使之只出现在可能会导致多个实例出现的地方。 代码如下： 123456789101112131415// 代码三public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null)&#123; synchronized(Singleton.class)&#123; if (instance == null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 这段代码看起来有点复杂，注意其中有两次if(instance&#x3D;&#x3D;null)的判断，这个叫做 双重检查 Double-Check。 第一个 if(instance&#x3D;&#x3D;null)，其实是为了解决代码二中的效率问题，只有instance为null的时候，才进入synchronized的代码段大大减少了几率。 第二个if(instance&#x3D;&#x3D;null)，则是跟代码二一样，是为了防止可能出现多个实例的情况。 这段代码看起来已经完美无瑕了。当然，只是『看起来』，还是有小概率出现问题的。想要充分理解需要先弄清楚以下几个概念：原子操作、指令重排。 原子操作 简单来说，原子操作（atomic）就是不可分割的操作，在计算机中，就是指不会因为线程调度被打断的操作。比如，简单的赋值是一个原子操作： 1m = 6; // 这是个原子操作 假如m原先的值为0，那么对于这个操作，要么执行成功m变成了6，要么是没执行 m还是0，而不会出现诸如m&#x3D;3这种中间态——即使是在并发的线程中。 但是，声明并赋值就不是一个原子操作： 1int n=6;//这不是一个原子操作 对于这个语句，至少有两个操作：①声明一个变量n ②给n赋值为6——这样就会有一个中间状态：变量n已经被声明了但是还没有被赋值的状态。这样，在多线程中，由于线程执行顺序的不确定性，如果两个线程都使用m，就可能会导致不稳定的结果出现。 指令重排 简单来说，就是计算机为了提高执行效率，会做的一些优化，在不影响最终结果的情况下，可能会对一些语句的执行顺序进行调整。比如，这一段代码： 1234int a ; // 语句1 a = 8 ; // 语句2int b = 9 ; // 语句3int c = a + b ; // 语句4 正常来说，对于顺序结构，执行的顺序是自上到下，也即1234。但是，由于指令重排的原因，因为不影响最终的结果，所以，实际执行的顺序可能会变成3124或者1324。 由于语句3和4没有原子性的问题，语句3和语句4也可能会拆分成原子操作，再重排。——也就是说，对于非原子性的操作，在不影响最终结果的情况下，其拆分成的原子操作可能会被重新排列执行顺序。 OK，了解了原子操作和指令重排的概念之后，我们再继续看代码三的问题。 主要在于singleton &#x3D; new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。 给 singleton 分配内存 调用 Singleton 的构造函数来初始化成员变量，形成实例 将singleton对象指向分配的内存空间（执行完这步 singleton才是非 null了） 在JVM的即时编译器中存在指令重排序的优化。 也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错。 再稍微解释一下，就是说，由于有一个『instance已经不为null但是仍没有完成初始化』的中间状态，而这个时候，如果有其他线程刚好运行到第一层if (instance &#x3D;&#x3D;null)这里，这里读取到的instance已经不为null了，所以就直接把这个中间状态的instance拿去用了，就会产生问题。这里的关键在于线程T1对instance的写操作没有完成，线程T2就执行了读操作。 对于代码三出现的问题，解决方案为：给instance的声明加上volatile关键字代码如下： 1234567891011121314public class Singleton &#123; private static volatile Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null)&#123; synchronized(Singleton.class)&#123; if (instance == null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; volatile关键字的一个作用是禁止指令重排，把instance声明为volatile之后，对它的写操作就会有一个内存屏障，这样，在它的赋值完成之前，就不用会调用读操作。 注意：volatile阻止的不是singleton &#x3D; new Singleton()这句话内部[1-2-3]的指令重排，而是保证了在一个写操作（[1-2-3]）完成之前，不会调用读操作（if (instance &#x3D;&#x3D; null)）。 静态内部类123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 这种写法的巧妙之处在于：对于内部类SingletonHolder，它是一个饿汉式的单例实现，在SingletonHolder初始化的时候会由ClassLoader来保证同步，使INSTANCE是一个真单例。 同时，由于SingletonHolder是一个内部类，只在外部类的Singleton的getInstance()中被使用，所以它被加载的时机也就是在getInstance()方法第一次被调用的时候。 它利用了ClassLoader来保证了同步，同时又能让开发者控制类加载的时机。从内部看是一个饿汉式的单例，但是从外部看来，又的确是懒汉式的实现 枚举1234567891011121314151617181920212223public class Singleton &#123; // 枚举类型是线程安全的，并且只会装载一次 private enum SingletonEnum &#123; INSTANCE; // 声明单例对象 private final Singleton instance; // 实例化 SingletonEnum() &#123; instance = new Singleton(); &#125; private Singleton getInstance() &#123; return instance; &#125; &#125; // 获取实例（单例对象） public static Singleton getInstance() &#123; return SingletonEnum.INSTANCE.getInstance(); &#125; private Singleton() &#123; &#125;&#125; 是不是很简单？而且因为自动序列化机制，保证了线程的绝对安全。三个词概括该方式：简单、高效、安全 这种写法在功能上与共有域方法相近，但是它更简洁，无偿地提供了序列化机制，绝对防止对此实例化，即使是在面对复杂的序列化或者反射攻击的时候。虽然这中方法还没有广泛采用，但是单元素的枚举类型已经成为实现Singleton的最佳方法。"},{"title":"并发设计模式：Immutability 模式","path":"/wiki/design/other/immutability.html","content":"在并发模式下, 多个线程同时读写同一共享变量存在并发问题。 其中导致出现并发问题的必要条件之一就是 读写 ，如果没有写，只存在读，是不会存在并发问题的。 如果让一个共享变量只有读操作，没有写操作，如此则可以解决并发问题。该理论的具体实现就是 不变性（Immutability）模式 。所谓不变性，简单来讲，就是对象一旦被创建之后，状态就不再发生变化。换句话说，就是变量一旦被赋值，就不允许修改了（没有写操作）；没有修改操作，也就是保持了不变性。 实现具备不可变性的类将一个类所有的属性都设置成 final ，并且只允许存在读方法，那么这个类基本上就具备不可变性了。更严格的做法是这个类本身也是 final 的，也就是不允许继承。因为子类可以覆盖父类的方法，有可能改变不可变性。 Java SDK 里很多类都具备不可变性，只是由于它们的使用太简单，最后反而被忽略了。例如经常用到的 String 和 Long、Integer、Double 等基础类型的包装类都具备不可变性，这些对象的线程安全性都是靠不可变性来保证的。如果你仔细翻看这些类的声明、属性和方法，你会发现它们都严格遵守不可变类的三点要求：类和属性都是 final 的，所有方法均是只读的。 看到这里你可能会疑惑，Java 的 String 方法也有类似字符替换操作，怎么能说所有方法都是只读的呢？下面通过 String 的源代码来看一哈。 下面的示例代码源自 Java 1.8 SDK。String 这个类以及它的属性 value[] 都是 final 的；而 replace() 方法的实现，没有修改 value[]，而是将替换后的字符串作为返回值返回了。 1234567891011121314151617181920212223242526272829303132333435363738394041public final class String &#123; private final char value[]; // 字符替换 String replace(char oldChar, char newChar) &#123; // 无需替换，直接返回 this if (oldChar == newChar)&#123; return this; &#125; int len = value.length; int i = -1; /* avoid getfield opcode */ char[] val = value; // 定位到需要替换的字符位置 while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; // 未找到 oldChar，无需替换 if (i &gt;= len) &#123; return this; &#125; // 创建一个 buf[]，这是关键 // 用来保存替换后的字符串 char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; // 创建一个新的字符串返回 // 原字符串不会发生任何变化 return new String(buf, true); &#125;&#125; 由上面的代码可以发现，String 是通过创建一个新的不可变对象 来实现 修改 的功能。如果 所有的修改操作都创建一个新的不可变对象，你可能会有这种担心：是不是创建的对象太多了，有点太浪费内存呢？是的，这样做的确有些浪费，那如何解决呢？ 利用享元模式避免创建重复对象利用享元模式可以减少创建对象的数量，从而减少内存占用。Java 语言里面 Long、Integer、Short、Byte 等这些基本数据类型的包装类都用到了享元模式。 下面以 Long 这个类作为例子，看看它是如何利用享元模式来优化对象的创建的。 享元模式本质上其实就是一个 对象池，利用享元模式创建对象的逻辑也很简单：创建之前，首先去对象池里看看是不是存在；如果已经存在，就利用对象池里的对象；如果不存在，就会新创建一个对象，并且把这个新创建出来的对象放进对象池里。 Long 这个类并没有照搬享元模式，Long 内部维护了一个静态的对象池，仅缓存了 [-128,127] 之间的数字，这个对象池在 JVM 启动的时候就创建好了，而且这个对象池一直都不会变化，也就是说它是静态的。之所以采用这样的设计，是因为 Long 这个对象的状态共有 2 的 64 次方 种，实在太多，并不适合全部缓存，而 [-128,127] 之间的数字利用率最高。下面的示例代码出自 Java 1.8，valueOf() 方法就用到了 LongCache 这个缓存。 1234567891011121314151617181920Long valueOf(long l) &#123; final int offset = 128; // [-128,127] 直接的数字做了缓存 if (l &gt;= -128 &amp;&amp; l &lt;= 127) &#123; return LongCache .cache[(int)l + offset]; &#125; return new Long(l);&#125;// 缓存，等价于对象池// 仅缓存 [-128,127] 直接的数字static class LongCache &#123; static final Long cache[] = new Long[-(-128) + 127 + 1]; static &#123; for(int i=0; i&lt;cache.length; i++) cache[i] = new Long(i-128); &#125;&#125; 注意： “Integer 和 String 类型的对象不适合做锁”，其实基本上所有的基础类型的包装类都不适合做锁，因为它们内部用到了享元模式，这会导致看上去私有的锁，其实是共有的。例如在下面代码中，本意是 A 用锁 al，B 用锁 bl，各自管理各自的，互不影响。但实际上 al 和 bl 是一个对象，结果 A 和 B 共用的是一把锁。 12345678910111213141516class A &#123; Long al=Long.valueOf(1); public void setAX()&#123; synchronized (al) &#123; // 省略代码无数 &#125; &#125;&#125;class B &#123; Long bl=Long.valueOf(1); public void setBY()&#123; synchronized (bl) &#123; // 省略代码无数 &#125; &#125;&#125; 使用 Immutability 模式的注意事项在使用 Immutability 模式的时候，需要注意以下两点： 对象的所有属性都是 final 的，并不能保证不可变性； 不可变对象也需要正确发布。 在 Java 语言中，final 修饰的属性一旦被赋值，就不可以再修改，但是如果属性的类型是普通对象，那么这个普通对象的属性是可以被修改的。例如下面的代码中，Bar 的属性 foo 虽然是 final 的，依然可以通过 setAge() 方法来设置 foo 的属性 age。所以，在使用 Immutability 模式的时候一定要确认保持不变性的边界在哪里，是否要求属性对象也具备不可变性。 123456789class Foo&#123; int age=0;&#125;final class Bar &#123; final Foo foo; void setAge(int a)&#123; foo.age=a; &#125;&#125; 下面我们再看看如何正确地发布不可变对象。不可变对象虽然是线程安全的，但是并不意味着引用这些不可变对象的对象就是线程安全的。例如在下面的代码中，Foo 具备不可变性，线程安全，但是类 Bar 并不是线程安全的，类 Bar 中持有对 Foo 的引用 foo，对 foo 这个引用的修改在多线程中并不能保证可见性和原子性。 1234567891011//Foo 线程安全final class Foo&#123; final int age=0;&#125;//Bar 线程不安全class Bar &#123; Foo foo; void setFoo(Foo f)&#123; this.foo=f; &#125;&#125; 如果你的程序仅仅需要 foo 保持可见性，无需保证原子性，那么可以将 foo 声明为 volatile 变量，这样就能保证可见性。如果你的程序需要保证原子性，那么可以通过原子类来实现。下面的示例代码是合理库存的原子化实现，你应该很熟悉了，其中就是用原子类解决了不可变对象引用的原子性问题。 12345678910111213141516171819202122232425262728public class SafeWM &#123; class WMRange&#123; final int upper; final int lower; WMRange(int upper,int lower)&#123; // 省略构造函数实现 &#125; &#125; final AtomicReference&lt;WMRange&gt; rf = new AtomicReference&lt;&gt;( new WMRange(0,0) ); // 设置库存上限 void setUpper(int v)&#123; while(true)&#123; WMRange or = rf.get(); // 检查参数合法性 if(v &lt; or.lower)&#123; throw new IllegalArgumentException(); &#125; WMRange nr = new WMRange(v, or.lower); if(rf.compareAndSet(or, nr))&#123; return; &#125; &#125; &#125;&#125; 总结具备不变性的对象，只有一种状态，这个状态由对象内部所有的不变属性共同决定。其实还有一种更简单的不变性对象，那就是 无状态。无状态对象内部没有属性，只有方法。除了无状态的对象，你可能还听说过无状态的服务、无状态的协议等等。无状态有很多好处，最核心的一点就是性能。在多线程领域，无状态对象没有线程安全问题，无需同步处理，自然性能很好；在分布式领域，无状态意味着可以无限地水平扩展，所以分布式领域里面性能的瓶颈一定不是出在无状态的服务节点上。"},{"title":"并发设计模式：生产者-消费者模式","path":"/wiki/design/other/producer-consumer.html","content":"在讨论基于阻塞队列的生产者消费者模式之前我们先搞清楚到底什么是生产者 - 消费者模式（producer-consumer 模式）？ 什么是生产者 - 消费者模式比如有两个进程 A 和 B，它们共享一个 固定大小的缓冲区，A 进程产生数据放入缓冲区，B 进程从缓冲区中取出数据进行计算，那么这里其实就是一个生产者和消费者的模式，A 相当于生产者，B 相当于消费者 生产者 - 消费者模式的特点 保证生产者不会在缓冲区满的时候继续向缓冲区放入数据，而消费者也不会在缓冲区空的时候，消耗数据 当缓冲区满的时候，生产者会进入休眠状态，当下次消费者开始消耗缓冲区的数据时，生产者才会被唤醒，开始往缓冲区中添加数据；当缓冲区空的时候，消费者也会进入休眠状态，直到生产者往缓冲区中添加数据时才会被唤醒 为什么要使用生产者消费者模式在多线程开发中，如果生产者生产数据的速度很快，而消费者消费数据的速度很慢，那么生产者就必须等待消费者消费完了数据才能够继续生产数据，因为生产那么多也没有地方放啊；同理如果消费者的速度大于生产者那么消费者就会经常处理等待状态，所以为了达到生产者和消费者生产数据和消费数据之间的平衡，那么就需要一个缓冲区用来存储生产者生产的数据，所以就引入了生产者 - 消费者模式 简单来说这里的缓冲区的作用就是为了平衡生产者和消费者的处理能力，起到一个数据缓存的作用，同时也达到了一个解耦的作用 生产者 - 消费者模式的应用场景生产者 - 消费者模式一般用于将生产数据的一方和消费数据的一方分割开来，将生产数据与消费数据的过程解耦开来 Excutor 任务执行框架： 通过将任务的提交和任务的执行解耦开来，提交任务的操作相当于生产者，执行任务的操作相当于消费者 例如使用 Excutor 构建 web 服务器，用于处理线程的请求：生产者将任务提交给线程池，线程池创建线程处理任务，如果需要运行的任务数大于线程池的基本线程数，那么就把任务扔到阻塞队列（通过线程池 + 阻塞队列的方式比只使用一个阻塞队列的效率高很多，因为消费者能够处理就直接处理掉了，不用每个消费者都要先从阻塞队列中取出任务再执行） 消息中间件 双十一的时候，会产生大量的订单，那么不可能同时处理那么多的订单，需要将订单放入一个队列里面，然后由专门的线程处理订单。这里用户下单就是生产者，处理订单的线程就是消费者；再比如 12306 的抢票功能，先由一个容器存储用户提交的订单，然后再由专门处理订单的线程慢慢处理，这样可以在短时间内支持高并发服务 任务的处理时间比较长的情况下 比如上传附近并处理，那么这个时候可以将用户上传和处理附件分成两个过程，用一个队列暂时存储用户上传的附近，然后立刻返回用户上传成功，然后有专门的线程处理队列中的附近 生产者 - 消费者模式的优点 解耦：将生产者类和消费者类进行解耦，消除代码之间的依赖性，简化工作负载的管理 复用：通过将生产者类和消费者类独立开来，那么可以对生产者类和消费者类进行独立的复用与扩展 调整并发数：由于生产者和消费者的处理速度是不一样的，可以调整并发数，给予慢的一方多的并发数，来提高任务的处理速度 异步：对于生产者和消费者来说能够各司其职，生产者只需要关心缓冲区是否还有数据，不需要等待消费者处理完；同样的对于消费者来说，也只需要关注缓冲区的内容，不需要关注生产者，通过异步的方式支持高并发，将一个耗时的流程拆成生产和消费两个阶段，这样生产者因为执行 put() 的时间比较短，而支持高并发 支持分布式：生产者和消费者通过队列进行通讯，所以不需要运行在同一台机器上，在分布式环境中可以通过 redis 的 list 作为队列，而消费者只需要轮询队列中是否有数据。同时还能支持集群的伸缩性，当某台机器宕掉的时候，不会导致整个集群宕掉 生产者 - 消费者模式的实现首先我们从最简单的开始，假设只有一个生产者线程执行 put 操作，向缓冲区中添加数据，同时也只有一个消费者线程从缓冲区中取出数据 UML 实体关系图, 从 UML 类图中可以看出，我们的 producer 和 consumer 类都持有一个对 container 对象的引用，这样的设计模式实际上在很多设计模式都有用到，比如我们的装饰者模式等等，它们共同的目的都是为了达到解耦和复用的效果 在实现生产者 - 消费者模式之前我们需要搞清两个问题： 如何保证容器中数据状态的一致性 如何保证消费者和生产者之间的同步和协作关系 1）容器中数据状态的一致性：当一个 consumer 执行了 take() 方法之后，此时容器为空，但是还没来得及更新容器的 size, 那么另外一个 consumer 来了之后以为 size 不等于 0，那么继续执行 take(), 从而造成了了状态的不一致性 2）为了保证当容器里面没有数据的时候，消费者不会继续 take，此时消费者释放锁，处于阻塞状态；并且一旦生产者添加了一条数据之后，此时重新唤醒消费者，消费者重新获取到容器的锁，继续执行 take(); 当容器里面满的时候，生产者也不会继续 put, 此时生产者释放锁，处于阻塞状态；一旦消费者 take 了一条数据，此时应该唤醒生产者重新获取到容器的锁，继续 put 所以对于该容器的任何访问都需要进行同步，也就是说在获取容器的数据之前，需要先获取到容器的锁。 而这里对于容器状态的同步可以参考如下几种方法： Object 的 wait() &#x2F; notify() 方法 Semaphore 的 acquire()&#x2F;release() 方法 BlockingQueue 阻塞队列方法 Lock 和 Condition 的 await() &#x2F; signal() 方法 PipedInputStream&#x2F; PipedOutputStream 要构建一个生产者消费者模式，那么首先就需要构建一个固定大小的缓冲区，并且该缓冲区具有可阻塞的 put 方法和 take 方法 利用内部线程之间的通信：Object 的 wait() &#x2F; notify() 方法接下来我们采用第一种方法来实现该模型：使用 Object 的 wait() &#x2F; notify() 方法实现生产者 - 消费者模型 ps: 采用 wait()&#x2F;notify() 方法的缺点是不能实现单生产者单消费者模式，因为要是用 notify() 就必须使用同步代码块 ###$ 创建 Container 容器类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package test;import java.util.LinkedList;public class Container &#123; LinkedList&lt;Integer&gt; list = new LinkedList&lt;Integer&gt;(); int capacity = 10; // 向容器中添加数据 public void put(int value)&#123; while (true)&#123; try &#123; //sleep不能放在同步代码块里面，因为sleep不会释放锁， // 当前线程会一直占有produce线程，直到达到容量，调用wait()方法主动释放锁 Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (this)&#123; //当容器满的时候，producer处于等待状态 while (list.size() == capacity)&#123; System.out.println(&quot;container is full,waiting ....&quot;); try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //没有满，则继续produce System.out.println(&quot;producer--&quot;+ Thread.currentThread().getName()+&quot;--put:&quot; + value); list.add(value++); //唤醒其他所有处于wait()的线程，包括消费者和生产者 notifyAll(); &#125; &#125; &#125; // 从容器中获取数据 public Integer take()&#123; Integer val = 0; while (true)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (this)&#123; //如果容器中没有数据，consumer处于等待状态 while (list.size() == 0)&#123; System.out.println(&quot;container is empty,waiting ...&quot;); try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //如果有数据，继续consume val = list.removeFirst(); System.out.println(&quot;consumer--&quot;+ Thread.currentThread().getName()+&quot;--take:&quot; + val); //唤醒其他所有处于wait()的线程，包括消费者和生产者 //notify必须放在同步代码块里面 notifyAll(); &#125; &#125; &#125;&#125; 这里需要注意的是 sleep() 不能放在 synchronized 代码块里面，因为我们知道 sleep() 执行之后是不会释放锁的，也就是说当前线程仍然持有对 container 对象的互斥锁，这个时候当前线程继续判断 list.size 是否等于 capacity，不等于就继续 put, 然后又 sleep 一会，然后又继续，直到当 list.size &#x3D;&#x3D; capacity, 这个时候终于进入 wait() 方法，我们知道 wait() 方法会释放锁，这个时候其他线程才有机会获取到 container 的互斥锁， notifyAll() 不能单独放在 producer 类里面，因为 notifyAll（）必须放在同步代码块里面 弊端： 这里由于不能区分哪些是 not empty 或者 not full 或者 is full&#x2F;empty 线程，所以需要唤醒所有其他等待的线程，但实际上我们需要的是唤醒那些 not empty 或者 not full 的线程就够了 创建生产者类1234567891011121314package test;import java.util.Random;public class Producer implements Runnable&#123; private Container container; public Producer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; container.put(new Random().nextInt(100)); &#125;&#125; 创建消费者类123456789101112131415package test;import java.util.Random;public class Consumer implements Runnable&#123; private Container container; public Consumer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; Integer val = container.take(); &#125;&#125; 测试类12345678910111213141516171819202122232425262728293031323334package test;import test1.Consumer;import test1.Container;import test1.Producer;public class Main &#123; public static void main(String[] args)&#123; Container container = new Container(); Thread producer1 = new Thread(new Producer(container)); Thread producer2 = new Thread(new Producer(container)); Thread producer3 = new Thread(new Producer(container)); Thread producer4 = new Thread(new Producer(container)); producer1.start(); producer2.start(); producer3.start(); producer4.start(); Thread consumer1 = new Thread(new Consumer(container)); Thread consumer2 = new Thread(new Consumer(container)); Thread consumer3 = new Thread(new Consumer(container)); Thread consumer4 = new Thread(new Consumer(container)); Thread consumer5 = new Thread(new Consumer(container)); Thread consumer6 = new Thread(new Consumer(container)); consumer1.start(); consumer2.start(); consumer3.start(); consumer4.start(); consumer5.start(); consumer6.start(); &#125;&#125; 运行结果 12345678910111213141516171819202122producer--Thread-1--put:80producer--Thread-2--put:19producer--Thread-3--put:8producer--Thread-0--put:74consumer--Thread-8--take:80consumer--Thread-4--take:19consumer--Thread-6--take:8consumer--Thread-9--take:74container is empty,waiting ...container is empty,waiting ...producer--Thread-2--put:20consumer--Thread-7--take:20container is empty,waiting ...producer--Thread-3--put:9producer--Thread-1--put:81producer--Thread-0--put:75consumer--Thread-5--take:9consumer--Thread-6--take:81consumer--Thread-8--take:75container is empty,waiting ...container is empty,waiting ...container is empty,waiting ... 利用信号量实现生产者 - 消费者模型思路 生产者消费者模型中的共享资源是一个固定大小的缓冲区，该模式需要当缓冲区满的时候，生产者不再生产数据，直到消费者消费了一个数据之后，才继续生产；同理当缓冲区空的时候，消费者不再消费数据，直到生产者生产了一个数据之后，才继续消费 如果要通过信号量来解决这个问题：关键在于找到能够跟踪缓冲区的 size 大小变化，并根据缓冲区的数量变化来控制消费者和生产者线程之间的协作和运行 那么很容易很够想到用两个信号量：empytyCount 和 fullCount 分别来表示缓冲区满或者空的状态，进而能够更加容易控制消费者和生产者到底什么时候处于阻塞状态，什么时候处于运行状态 emptyCount &#x3D; N fullCount &#x3D; 0 useQueue &#x3D; 1 同时为了使得程序更加具有健壮性，我们还添加一个二进制信号量 useQueue, 确保队列的状态的完整性不受损害。例如当两个生产者同时向空队列添加数据时，从而破坏了队列内部的状态，使得其他计数信号量或者返回的缓冲区的 size 大小不具有一致性。（当然这里也可以使用 mutex 来代替二进制信号量） 123456789101112produce: P(emptyCount)//信号量emptyCount减一 P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态） putItemIntoQueue(item)//执行put操作 V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区） V(fullCount)//信号量fullCount加一consume: P(fullCount)//fullCount -= 1 P(useQueue)//useQueue -= 1(useQueue = 0) item ← getItemFromQueue() V(useQueue)//useQueue += 1 (useQueue = 1) V(emptyCount)//emptyCount += 1 ps: 这里的两个 PV 操作是否可以颠倒 P 操作不可以首先生产者获取到信号量 emptyCount，执行 P(emptyCount)，确保 emptyCount 不等于 0，也就是还有空间添加数据，从而才能够进入临界区 container然后执行 put 操作，执行 put 操作之前需要为缓冲区加把锁，防止在 put 的过程中，其他线程对缓冲区进行修改，所以这个时候需要获取另外一个信号量 useQueue相反，如果先执行了 P(useQueue)，并且此时的 emptyCount &#x3D; 0，那么生产者就会一直阻塞，直到消费者消费了一个数据；但是此时消费者又无法获取到互斥信号量 useQueue，也会一直阻塞，所以就形成了一个死锁所以这两个 p 操作是不能交换顺序的，信号量 emptyCount 是 useQueue 的基础和前提条件 V 操作可以此时如果生产者已经执行完 put 操作，那么可以先释放互斥信号量，再执行 V(fullCount)；或者先执行 V(fullCount) 再释放互斥信号量都没有关系。不会对其他的生产者消费者的状态产生影响；但是最好的还是先释放互斥锁，再执行 V(fullCount)，这样可以保证当容器满的时候，消费者能够及时的获取到互斥锁 代码实现 Container 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package test;import java.util.LinkedList;import java.util.List;import java.util.concurrent.Semaphore;public class Container &#123; Semaphore fullCount = new Semaphore(0); Semaphore emptyCount = new Semaphore(10); Semaphore isUse = new Semaphore(1); List list = new LinkedList&lt;Integer&gt;(); public void put(Integer val)&#123; try &#123; emptyCount.acquire(); isUse.acquire(); list.add(val); System.out.println(&quot;producer--&quot;+ Thread.currentThread().getName()+&quot;--put:&quot; + val+&quot;===size:&quot;+list.size()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; isUse.release(); fullCount.release(); &#125; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public Integer get()&#123; Integer val1 = 0; try &#123; fullCount.acquire(); isUse.acquire(); val1 = (Integer) list.remove(0); System.out.println(&quot;consumer--&quot;+ Thread.currentThread().getName()+&quot;--take:&quot; + val1+&quot;===size:&quot;+list.size()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; isUse.release(); emptyCount.release(); &#125; return val1; &#125;&#125; 生产者 123456789101112131415161718package test;import java.util.Random;public class Producer implements Runnable&#123; private Container container; public Producer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; while (true)&#123; container.put(new Random().nextInt(100)); &#125; &#125;&#125; 消费者 12345678910111213141516package test;public class Consumer implements Runnable&#123; private Container container; public Consumer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; while (true)&#123; Integer val = container.get(); &#125; &#125;&#125; 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package test;public class Test &#123; public static void main(String[] args)&#123; Container container = new Container(); Thread producer1 = new Thread(new Producer(container)); Thread producer2 = new Thread(new Producer(container)); Thread producer3 = new Thread(new Producer(container)); Thread consumer1 = new Thread(new Consumer(container)); Thread consumer2 = new Thread(new Consumer(container)); Thread consumer3 = new Thread(new Consumer(container)); Thread consumer4 = new Thread(new Consumer(container)); producer1.start(); producer2.start(); producer3.start(); consumer1.start(); consumer2.start(); consumer3.start(); consumer4.start(); &#125;&#125;producer--Thread-0--put:74===size:1producer--Thread-4--put:16===size:2producer--Thread-2--put:51===size:3producer--Thread-1--put:77===size:4producer--Thread-3--put:93===size:5consumer--Thread-6--take:74===size:4consumer--Thread-6--take:16===size:3consumer--Thread-6--take:51===size:2consumer--Thread-6--take:77===size:1consumer--Thread-5--take:93===size:0producer--Thread-4--put:19===size:1producer--Thread-3--put:68===size:2producer--Thread-0--put:72===size:3consumer--Thread-6--take:19===size:2consumer--Thread-6--take:68===size:1consumer--Thread-5--take:72===size:0producer--Thread-1--put:82===size:1producer--Thread-2--put:32===size:2consumer--Thread-5--take:82===size:1 基于阻塞队列的生产者消费者模型由于这里的缓冲区由 BlockingQueue 容器代替，那么这里我们就不需要重新创建一个容器类了，直接创建生产者类和消费者类，并且同样的都需要拥有一个容器类 BlockingQueue 的实例应用 创建生产者类 12345678910111213141516171819202122232425262728package test;import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;public class Producer implements Runnable&#123; private ArrayBlockingQueue&lt;Integer&gt; queue ; public Producer(ArrayBlockingQueue&lt;Integer&gt; queue) &#123; this.queue = queue; &#125; @Override public void run() &#123; Random random = new Random(); while (true)&#123; try &#123; Thread.sleep(100); if(queue.size() == 10) System.out.println(&quot;================the queue is full,the producer thread is waiting..................&quot;); int item = random.nextInt(100); queue.put(item); System.out.println(&quot;producer:&quot; + Thread.currentThread().getName() + &quot; produce:&quot; + item+&quot;;the size of the queue:&quot; + queue.size()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建消费者类 12345678910111213141516171819202122232425package test;import java.util.concurrent.ArrayBlockingQueue;public class Consumer implements Runnable &#123; private ArrayBlockingQueue&lt;Integer&gt; queue; public Consumer(ArrayBlockingQueue&lt;Integer&gt; queue) &#123; this.queue = queue; &#125; @Override public void run() &#123; while (true)&#123; try &#123; Thread.sleep(100); if(queue.size() == 0) System.out.println(&quot;=============the queue is empty,the consumer thread is waiting................&quot;); Integer item = queue.take(); System.out.println(&quot;consumer:&quot; + Thread.currentThread().getName() + &quot; consume:&quot; + item+&quot;;the size of the queue:&quot; + queue.size()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 测试类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package test;import java.util.concurrent.ArrayBlockingQueue;public class Test &#123; public static void main(String[] args)&#123; ArrayBlockingQueue&lt;Integer&gt; queue = new ArrayBlockingQueue&lt;Integer&gt;(10); Thread producer1 = new Thread(new Producer(queue)); Thread producer2 = new Thread(new Producer(queue)); Thread producer3 = new Thread(new Producer(queue)); Thread producer4 = new Thread(new Producer(queue)); Thread producer5 = new Thread(new Producer(queue)); producer1.start(); producer2.start(); producer3.start(); producer4.start(); producer5.start(); Thread consumer1 = new Thread(new Consumer(queue)); Thread consumer2 = new Thread(new Consumer(queue)); consumer1.start(); consumer2.start(); try &#123; producer1.join(); producer2.join(); producer3.join(); producer4.join(); producer5.join(); consumer1.join(); consumer2.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;=============the queue is empty,the consumer thread is waiting................consumer:Thread-5 consume:64;the size of the queue:0producer:Thread-3 produce:64;the size of the queue:1consumer:Thread-6 consume:87;the size of the queue:0producer:Thread-1 produce:1;the size of the queue:3producer:Thread-4 produce:87;the size of the queue:2producer:Thread-2 produce:71;the size of the queue:2producer:Thread-0 produce:76;the size of the queue:1consumer:Thread-6 consume:71;the size of the queue:2producer:Thread-1 produce:26;the size of the queue:6producer:Thread-3 produce:6;the size of the queue:6producer:Thread-0 produce:76;the size of the queue:5producer:Thread-2 produce:37;the size of the queue:6 Lock 和 Condition 的 await() &#x2F; signal() 方法容器类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package test;import java.util.LinkedList;import java.util.List;import java.util.Vector;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class Container&#123; private final Lock lock = new ReentrantLock(); //表示生产者线程 private final Condition notFull = lock.newCondition(); //表示消费者线程 private final Condition notEmpty = lock.newCondition(); private int capacity; private List&lt;Integer&gt; list = new LinkedList&lt;&gt;(); public Container(int capacity) &#123; this.capacity = capacity; &#125; public Integer take()&#123; lock.lock(); try &#123; while (list.size() == 0) try &#123; System.out.println(&quot;the list is empty........&quot;); notEmpty.await();//阻塞消费者线程 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Integer val = list.remove(0); System.out.println(&quot;consumer--&quot;+ Thread.currentThread().getName()+&quot;--take:&quot; + val+&quot;===size:&quot;+list.size()); notFull.signalAll();//唤醒所有生产者线程 return val; &#125;finally &#123; lock.unlock(); &#125; &#125; public void put(Integer val)&#123; lock.lock(); try &#123; while (list.size() == capacity)&#123; try &#123; System.out.println(&quot;the list is full........&quot;); notFull.await();//阻塞生产者线程 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; list.add(val); System.out.println(&quot;producer--&quot;+ Thread.currentThread().getName()+&quot;--put:&quot; + val+&quot;===size:&quot;+ list.size()); notEmpty.signalAll();//唤醒所有消费者线程 &#125;finally &#123; lock.unlock(); &#125; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 生产者 123456789101112131415161718192021package test;import java.util.Random;import java.util.TreeMap;import java.util.concurrent.locks.Condition;public class Producer implements Runnable &#123; private Container container; public Producer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; while (true)&#123; container.put(new Random().nextInt(100)); &#125; &#125;&#125; 消费者 1234567891011121314151617package test;public class Consumer implements Runnable &#123; private Container container; public Consumer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; while (true)&#123; Integer val = container.take(); &#125; &#125;&#125; 测试类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package test;public class Test &#123; public static void main(String[] args)&#123; Container container = new Container(5); Thread producer1 = new Thread(new Producer(container)); Thread producer2 = new Thread(new Producer(container)); Thread producer3 = new Thread(new Producer(container)); Thread producer4 = new Thread(new Producer(container)); Thread producer5 = new Thread(new Producer(container)); Thread consumer1 = new Thread(new Consumer(container)); Thread consumer2 = new Thread(new Consumer(container)); producer1.start(); producer2.start(); producer3.start(); producer4.start(); producer5.start(); consumer1.start(); consumer2.start(); &#125;&#125;the list is empty........producer--Thread-3--put:77===size:1consumer--Thread-6--take:77===size:0the list is empty........producer--Thread-4--put:55===size:1producer--Thread-0--put:62===size:2producer--Thread-1--put:90===size:3producer--Thread-2--put:57===size:4consumer--Thread-5--take:55===size:3consumer--Thread-5--take:62===size:2consumer--Thread-5--take:90===size:1consumer--Thread-5--take:57===size:0the list is empty........the list is empty........producer--Thread-0--put:10===size:1producer--Thread-1--put:21===size:2producer--Thread-3--put:3===size:3producer--Thread-4--put:75===size:4producer--Thread-2--put:94===size:5consumer--Thread-5--take:10===size:4 使用信号量实现对于单生产者单消费者，只用保证缓冲区满的时候，生产者不会继续向缓冲区放数据，缓冲区空的时候，消费者不会继续从缓冲区取数据，而不存在同时有两个生产者使用缓冲区资源，造成数据不一致的状态。 所以对于单生产者单消费者，如果采用信号量模型来实现的话，那么只需要两个信号量：empytyCount 和 fullCount 分别来表示缓冲区满或者空的状态，进而能够更加容易控制消费者和生产者到底什么时候处于阻塞状态，什么时候处于运行状态; 而不需要使用互斥信号量了 emptyCount &#x3D; N fullCount &#x3D; 0 ; 12345678produce: P(emptyCount)//信号量emptyCount减一 putItemIntoQueue(item)//执行put操作 V(fullCount)//信号量fullCount加一consume: P(fullCount)//fullCount -= 1 item ← getItemFromQueue() V(emptyCount)//emptyCount += 1 1234567891011121314151617181920212223242526272829303132333435363738394041424344package test;import java.time.temporal.ValueRange;import java.util.LinkedList;import java.util.List;import java.util.concurrent.Semaphore;public class Container &#123; Semaphore emptyCount = new Semaphore(10); Semaphore fullCount = new Semaphore(0); List&lt;Integer&gt; list = new LinkedList&lt;Integer&gt;(); public void put(int val)&#123; try &#123; emptyCount.acquire(); list.add(val); System.out.println(&quot;producer--&quot;+ Thread.currentThread().getName()+&quot;--put:&quot; + val+&quot;===size:&quot;+list.size()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; fullCount.release(); &#125; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public Integer take()&#123; Integer val = 0; try &#123; fullCount.acquire(); val = list.remove(0); System.out.println(&quot;consumer--&quot;+ Thread.currentThread().getName()+&quot;--take:&quot; + val+&quot;===size:&quot;+list.size()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; emptyCount.release(); &#125; return val; &#125;&#125; 生产者 12345678910111213141516171819package test;import java.util.Random;public class Producer implements Runnable &#123; private Container container; public Producer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; while (true)&#123; container.put(new Random().nextInt(100)); &#125; &#125;&#125; 消费者类 12345678910111213141516171819package test;import test8.Container;public class Consumer implements Runnable &#123; private Container container; public Consumer(Container container) &#123; this.container = container; &#125; @Override public void run() &#123; while (true)&#123; Integer take = container.take(); &#125; &#125;&#125; 测试 12345678910111213141516171819202122232425package test;public class Test &#123; public static void main(String[] args)&#123; Container container = new Container(); Thread producer = new Thread(new Producer(container)); Thread consumer = new Thread(new Consumer(container)); producer.start(); consumer.start(); &#125;&#125;producer--Thread-0--put:62===size:1consumer--Thread-1--take:62===size:0producer--Thread-0--put:40===size:1consumer--Thread-1--take:40===size:0producer--Thread-0--put:86===size:1consumer--Thread-1--take:86===size:0producer--Thread-0--put:15===size:1consumer--Thread-1--take:15===size:0producer--Thread-0--put:83===size:1consumer--Thread-1--take:83===size:0producer--Thread-0--put:13===size:1consumer--Thread-1--take:13===size:0 多生产者单消费者（MPSC）对于多生产者单消费者来说，多生产者之间具有互斥关系，所以这里需要一个互斥锁来实现缓冲区的互斥访问，那么具体的实现方式就是在单生产者单消费者的基础之上，加一个互斥信号量 useQueue 如果采用信号量来实现的话可以如下： emptyCount &#x3D; N fullCount &#x3D; 0 useQueue &#x3D; 1 12345678910produce: P(emptyCount)//信号量emptyCount减一 P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态） putItemIntoQueue(item)//执行put操作 V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区） V(fullCount)//信号量fullCount加一consume: P(fullCount)//fullCount -= 1 item ← getItemFromQueue() V(emptyCount)//emptyCount += 1 具体的实现和单生产者单消费者差不多，只不过在生产者类里面多加了一个互斥信号量 useQueue 单生产者多消费者（SPMC）对于单生产者多消费者同多生产者多消费者 emptyCount &#x3D; N fullCount &#x3D; 0 useQueue &#x3D; 1 12345678910produce: P(emptyCount)//信号量emptyCount减一 putItemIntoQueue(item)//执行put操作 V(fullCount)//信号量fullCount加一consume: P(fullCount)//fullCount -= 1 P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态） item ← getItemFromQueue() V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区） V(emptyCount)//emptyCount += 1 具体的实现和单生产者单消费者差不多，只不过在消费者类里面多加了一个互斥信号量 useQueue 多生产者多消费者（MPMC）- 单缓冲区 (SB)对于多生产者多消费者问题，是一个同步 + 互斥问题，不仅需要生产者和消费者之间的同步协作，还需要实现对缓冲区资源的互斥访问；这个可以参考前面对生产者消费者 4 种实现方式 采用信号量 emptyCount &#x3D; N fullCount &#x3D; 0 useQueue &#x3D; 1 123456789101112produce: P(emptyCount)//信号量emptyCount减一 P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态） putItemIntoQueue(item)//执行put操作 V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区） V(fullCount)//信号量fullCount加一consume: P(fullCount)//fullCount -= 1 P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态） item ← getItemFromQueue() V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区） V(emptyCount)//emptyCount += 1 多生产者多消费者（MPMC）- 双缓冲区 (MB)为什么要用双缓冲区：读写分离减少释放锁和获取锁的开销 用一个缓冲区，生产者和消费者需要先获取到缓冲区的锁才能进行 put 和 take 操作，每一次 put 和 take 都需要获取一次锁，这需要大量的同步与互斥操作，十分损耗性能。 所以如果采用双缓冲区的话，一个缓冲区 bufferA 用于生产者执行 put 操作，一个缓冲区 bufferB 用于消费者执行 take 操作；生产者线程和消费者线程在使用各自的缓冲区之前都需要先获取到缓冲区对应的锁，才能进行操作； 生产者和消费者各自使用自己独立的缓冲区，那么就不存在同一个缓冲区被 put 的同时进行 take 操作 所以一旦生产者和消费者一旦获取到了对应缓冲区的锁，那么每一次执行 put&#x2F;take 操作时就不用再次重新获取锁了，从而减少了很多获取锁、释放锁的性能开销 缓冲区的切换 如果 bufferA 被 put 满了，那么生产者释放 bufferA 的锁，并等待消费者释放 bufferB 的锁；当 bufferB 被 take 空了，消费者释放 bufferB 的锁，此时生产者获取到 bufferB 的锁，对 bufferB 进行 put; 消费者获取到 bufferA 的锁，对 bufferA 进行 take, 那么就完成了一次缓冲区的切换 双缓冲区的状态 并发读写bufferA 和 bufferB 都处于工作状态，一个读一个写 单个缓冲区空闲假设 bufferA 已经满了，那么生产者就会释放 bufferA 的锁，尝试获取 bufferB，而此时 bufferB 还在执行 take 操作，消费者还没释放 bufferB 的锁，那么生产者进入等待状态 缓冲区的切换当 bufferB 为空，那么此时消费者释放 bufferB 的锁，尝试获取 bufferA 的锁，此时消费者被唤醒，重新尝试获取 bufferB 的锁 双缓冲区的死锁问题如果操作完当前的缓冲区之后，先获取另外一个缓冲区的锁，再释放当前缓冲区的锁，就会发生死锁问题。如果 bufferA 和 bufferB 的线程同时尝试获取对方的锁，那么就会一直循环等待下去 需要注意的问题由于双缓冲区是为了避免每次读写的时候不用进行同步与互斥操作，所以对于一些本来就是线程安全的类例如 arrayblockingqueue 就不适合作为双缓冲区，因为他们内部已经实现了每次读写操作的时候进行加锁和释放 应用场景： 共享内存和共享文件 逻辑处理线程和 IO 处理线程分离。 I&#x2F;0 处理线程负责网络数据的发送和接收，连接的建立和维护。 逻辑处理线程处理从 IO 线程接收到的包。 多生产者多消费者（MPMC）- 多缓冲区 (MB)多个缓冲区构成一个缓冲池，同样需要两个同步信号量 emtpyCount 和 fullCount，还有一个互斥信号量 useQueue, 同时还需要两个变量指示哪些是空缓冲区哪些是有数据的缓冲区，多缓冲区和双缓冲区一样，同样是以空间换时间，减少单个读写操作的同步与互斥操作，对于同一个缓冲区而言，不可能同时会 put 和 take 多生产者多消费者 (MPMC)- 环形缓冲区（Ring buffer）为什么要引入环形缓冲区 讨论为什么要引入环形缓冲区，其实也就是在讨论队列缓冲区有什么弊端，而环形缓冲区是如何解决这种弊端的 那么我们先认识一下什么是环形缓冲区 循环缓冲区的有用特性是，当使用一个循环缓冲区时，它不需要将其元素打乱。 FIFO 所有的 push&#x2F;pop 操作都是在一个固定的存储空间内进行，少掉了对于缓冲区元素所用存储空间的分配、释放 队列缓冲区 如果使用非循环缓冲区，那么在使用一个缓冲区时，需要移动所有元素 LIFO 在执行 push 和 pop 操作时，涉及到内存的分配与释放开销大"},{"title":"dsfasdfasdf","path":"/wiki/design/structural/structural-pattern.html","content":"sdfhi"}]